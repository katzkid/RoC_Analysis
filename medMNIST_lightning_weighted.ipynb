{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4da4f6",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0556d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from predict_ensemble_and_evaluate import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate import plot_roc_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d840881",
   "metadata": {},
   "source": [
    "## Display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c24af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for displaying ROC curves of multiple classifiers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def plot_roc_connected_points(results_list, results_original_roc, name=\"All Classifiers\"):\n",
    "    \"\"\"\n",
    "    Creates a plot showing the performance of all classifiers as points,\n",
    "    and connects them with a line, sorted by FPR.\n",
    "\n",
    "    Args:\n",
    "        results_list: A list of dictionaries, where each dict contains 'fpr', 'tpr'.\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # Convert results to a pandas DataFrame for easy sorting\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Sort the models by their False Positive Rate to create a left-to-right line\n",
    "    df_sorted = df.sort_values(by='fpr').reset_index(drop=True)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Plot all the individual model points as a scatter plot\n",
    "    # This helps see the actual performance of each model\n",
    "    plt.scatter(df_sorted['fpr'], df_sorted['tpr'], c='black', marker='o', \n",
    "                alpha=0.6, s=100, zorder=3) # zorder=3 puts dots on top\n",
    "\n",
    "    # Plot the line connecting the points\n",
    "    plt.plot(df_sorted['fpr'], df_sorted['tpr'], color='red', lw=1.5,\n",
    "             alpha=0.8, label=f'{name} (AUC = {auc(df_sorted[\"fpr\"], df_sorted[\"tpr\"]):.2f})')\n",
    "    \n",
    "    #plot the original ROC curve\n",
    "    plt.plot(results_original_roc[\"fpr\"], results_original_roc[\"tpr\"], color='blue', \n",
    "             label=f'{results_original_roc[\"name\"]} (AUC = {results_original_roc[\"auc\"]:.2f})')\n",
    "\n",
    "    # Plot the \"no-skill\" line for reference\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Performance Path of All Trained Classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23b1ea",
   "metadata": {},
   "source": [
    "## Best ROC envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e350c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def get_best_roc_envelope(results_list, results_original_roc=None):\n",
    "    \"\"\"\n",
    "    Constructs the upper envelope of multiple ROC curves to find the best\n",
    "    possible operating points from a collection of classifiers.\n",
    "\n",
    "    This function is guaranteed to produce a list of points that starts at\n",
    "    (FPR=0, TPR=0) and ends at (FPR=1, TPR=1), making it suitable for direct\n",
    "    plotting and AUC calculation.\n",
    "\n",
    "    Args:\n",
    "        results_list (list): A list of dictionaries. Each dictionary must\n",
    "                             contain a 'model' name and the full ROC results\n",
    "                             under the 'full_roc' key.\n",
    "        results_original_roc (dict, optional): A dictionary containing the\n",
    "                                               ROC results of a baseline model,\n",
    "                                               which will also be included in\n",
    "                                               the comparison. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of dictionaries representing the optimal ROC\n",
    "              envelope. Each dictionary includes 'model', 'fpr', 'tpr',\n",
    "              and 'threshold'.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "\n",
    "    # Step 1: Gather all ROC points from all provided models\n",
    "    all_results = results_list[:]\n",
    "    if results_original_roc:\n",
    "        all_results.append({\n",
    "            'model': 'original_baseline',\n",
    "            'full_roc': results_original_roc\n",
    "        })\n",
    "\n",
    "    for result in all_results:\n",
    "        model_name = result.get('model', 'Unknown Model')\n",
    "        roc = result.get('full_roc')\n",
    "        if not roc: continue\n",
    "\n",
    "        fprs = roc.get('fpr', [])\n",
    "        tprs = roc.get('tpr', [])\n",
    "        thresholds = roc.get('thresholds', [])\n",
    "\n",
    "        for i in range(len(fprs)):\n",
    "            threshold = thresholds[i] if i < len(thresholds) else thresholds[-1]\n",
    "            all_points.append({\n",
    "                'model': model_name, 'fpr': fprs[i],\n",
    "                'tpr': tprs[i], 'threshold': threshold\n",
    "            })\n",
    "\n",
    "    # Handle case with no data by returning a default diagonal line\n",
    "    if not all_points:\n",
    "        return [\n",
    "            {'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf},\n",
    "            {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}\n",
    "        ]\n",
    "\n",
    "    # Step 2: Sort all points by FPR (asc), then by TPR (desc)\n",
    "    all_points.sort(key=lambda p: (p['fpr'], -p['tpr']))\n",
    "\n",
    "    # Step 3: Build the core ROC envelope\n",
    "    best_roc_envelope = []\n",
    "    max_tpr_so_far = -1.0\n",
    "    for point in all_points:\n",
    "        # Add a point only if it has a strictly higher TPR\n",
    "        if point['tpr'] > max_tpr_so_far:\n",
    "            # To prevent adding multiple points at the same FPR, check if the last point\n",
    "            # has the same FPR and replace it if so (since this one has a higher TPR)\n",
    "            if best_roc_envelope and best_roc_envelope[-1]['fpr'] == point['fpr']:\n",
    "                best_roc_envelope[-1] = point\n",
    "            else:\n",
    "                best_roc_envelope.append(point)\n",
    "            max_tpr_so_far = point['tpr']\n",
    "    \n",
    "    # Step 4: Manually add extremities if they are missing\n",
    "    \n",
    "    # Ensure the curve starts at (0, 0)\n",
    "    if not best_roc_envelope or best_roc_envelope[0]['fpr'] > 0.0:\n",
    "        best_roc_envelope.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if best_roc_envelope[-1]['fpr'] < 1.0 or best_roc_envelope[-1]['tpr'] < 1.0:\n",
    "        best_roc_envelope.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "    return best_roc_envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b781e9",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c3deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(dataset):\n",
    "    if dataset == \"data1\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [0, 2], [2, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "    elif dataset == \"data2\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [1, 2], [1, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    elif dataset == \"pneumoniaMNIST\":\n",
    "        import medmnist\n",
    "        print(medmnist.__version__)\n",
    "        # read in the PneumoniaMNIST dataset\n",
    "        from medmnist import PneumoniaMNIST\n",
    "        train_data = PneumoniaMNIST(split='train', download=True, size=224)\n",
    "        test_data = PneumoniaMNIST(split='test', download=True, size=224)\n",
    "        val_data = PneumoniaMNIST(split='val', download=True, size=224)\n",
    "        # print the dataset info\n",
    "        print(f\"Number of training samples: {len(train_data)}\")\n",
    "        print(f\"Number of test samples: {len(test_data)}\")\n",
    "        print(f\"Number of validation samples: {len(val_data)}\")\n",
    "\n",
    "    return train_data, test_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfe355",
   "metadata": {},
   "source": [
    "## Calculate Statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af73be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_fpr_tpr(clf_model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the False Positive Rate (FPR) and True Positive Rate (TPR) at a given threshold.\n",
    "\n",
    "    Args:\n",
    "        X_test: The test features.\n",
    "        y_test: The true test labels (0 or 1).\n",
    "        threshold: The probability threshold.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the FPR and TPR. Returns None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_prob = clf_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  #Avoid division by zero\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0 #Avoid division by zero\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        misclassification_rate = 1 - accuracy\n",
    "\n",
    "        return {\"fpr\": fpr, \"tpr\": tpr, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy, \"misclassification_rate\": misclassification_rate}\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error calculating FPR and TPR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59034e",
   "metadata": {},
   "source": [
    "## PneumoniaMNIST: NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74133e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a060ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 1\n",
    "NUM_CHANNELS = 1\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 4\n",
    "K_FOLDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d23d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, image_height, image_width):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        #Convoluional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 224x224 -> 112x112\n",
    "        \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 112x112 -> 56x56\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 56x56 -> 28x28\n",
    "        \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 28x28 -> 14x14\n",
    "        )\n",
    "\n",
    "        # --- DYNAMIC FLATTENED SIZE CALCULATION ---\n",
    "        # Create a dummy tensor with the specified input dimensions\n",
    "        dummy_input = torch.randn(1, in_channels, image_height, image_width)\n",
    "        # Pass it through the feature extractor to see the output shape\n",
    "        dummy_output = self.features(dummy_input)\n",
    "        # The number of elements in the output tensor is our flattened size\n",
    "        self.flattened_size = dummy_output.numel()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        # Output layer: num_classes=1 for binary classification (outputting logits)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a702675",
   "metadata": {},
   "source": [
    "### Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac92eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitSimpleCNN(pl.LightningModule):\n",
    "    def __init__(self, in_channels, num_classes, learning_rate, image_height, image_width, training_mode='full_network'):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define the model\n",
    "        self.model = SimpleCNN(\n",
    "            in_channels=self.hparams.in_channels, \n",
    "            num_classes=self.hparams.num_classes,\n",
    "            image_height=self.hparams.image_height,\n",
    "            image_width=self.hparams.image_width\n",
    "        )\n",
    "        \n",
    "        # Define loss function\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Define metrics\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.val_auc = torchmetrics.AUROC(task=\"binary\")\n",
    "\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.test_auc = torchmetrics.AUROC(task=\"binary\")\n",
    "        self.test_precision = torchmetrics.Precision(task=\"binary\")\n",
    "        self.test_recall = torchmetrics.Recall(task=\"binary\")\n",
    "        self.test_f1 = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "        # This list will store outputs from each test step\n",
    "        self.last_test_results = {}\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def freeze_feature_extractor(self):\n",
    "        print(\"Freezing feature extractor layers...\")\n",
    "        for param in self.model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.fc1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        # For BCEWithLogitsLoss, labels must be float\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = self(inputs) # Forward pass\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # We need to handle which part of the network we are training\n",
    "        if self.hparams.training_mode == 'full_network':\n",
    "            self.log('train_loss_full', loss)\n",
    "        elif self.hparams.training_mode == 'classifier_only':\n",
    "            self.log('train_loss_classifier', loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.train_accuracy(outputs, labels.int())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.val_accuracy(outputs, labels.int())\n",
    "        self.val_auc(outputs, labels.int())\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('val_acc', self.val_accuracy, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('val_auc', self.val_auc, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels.float())\n",
    "        \n",
    "        # Append predictions and labels to our list\n",
    "        self.test_step_outputs.append({\"preds\": outputs.detach(), \"labels\": labels.detach()})\n",
    "        \n",
    "        # Log the loss for this batch\n",
    "        self.log('test_loss', loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Concatenate all predictions and labels from the list we built\n",
    "        all_preds = torch.cat([x['preds'] for x in self.test_step_outputs])\n",
    "        all_labels = torch.cat([x['labels'] for x in self.test_step_outputs])\n",
    "\n",
    "        # Calculate final metrics over the entire test set\n",
    "        test_acc = self.test_accuracy(all_preds, all_labels.int())\n",
    "        test_auc_val = self.test_auc(all_preds, all_labels.int())\n",
    "        test_prec = self.test_precision(all_preds, all_labels.int())\n",
    "        test_rec = self.test_recall(all_preds, all_labels.int())\n",
    "        test_f1_val = self.test_f1(all_preds, all_labels.int())\n",
    "        test_cm_val = torchmetrics.functional.confusion_matrix(all_preds, all_labels.int(), task=\"binary\")\n",
    "\n",
    "        # Log the final metrics\n",
    "        self.log(\"test_acc_epoch\", test_acc)\n",
    "        self.log(\"test_auc_epoch\", test_auc_val)\n",
    "\n",
    "        print(f\"\\n--- Final Test Metrics ---\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"AUC: {test_auc_val:.4f}\")\n",
    "        print(f\"Precision: {test_prec:.4f}\")\n",
    "        print(f\"Recall: {test_rec:.4f}\")\n",
    "        print(f\"F1-Score: {test_f1_val:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{test_cm_val}\")\n",
    "        print(\"--------------------------\\n\")\n",
    "\n",
    "        # Calculate data for the ROC Curve\n",
    "        fpr, tpr, thresholds = torchmetrics.functional.roc(\n",
    "            torch.sigmoid(all_preds),\n",
    "            all_labels.int(),\n",
    "            task=\"binary\"\n",
    "        )\n",
    "\n",
    "        # Store the results to be retrieved later in the main script\n",
    "        self.last_test_results = {\n",
    "            \"fpr\": fpr.cpu(),\n",
    "            \"tpr\": tpr.cpu(),\n",
    "            \"auc\": test_auc_val,\n",
    "            \"f1\": test_f1_val,\n",
    "            \"precision\": test_prec,\n",
    "            \"recall\": test_rec,\n",
    "            \"cm\": test_cm_val,\n",
    "            \"thresholds\": thresholds.cpu(),\n",
    "        }\n",
    "        # Free up memory\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ea652",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d785e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n",
      "Number of training samples: 4708\n",
      "Number of test samples: 624\n",
      "Number of validation samples: 524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "train_dataset, test_dataset, val_dataset = generate_data(\"pneumoniaMNIST\")\n",
    "\n",
    "# Ensure you have your train_loader and val_loader defined here\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset.transform = data_transforms\n",
    "test_dataset.transform = data_transforms\n",
    "val_dataset.transform = data_transforms\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#new_train_loader = data.DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) to be done inside the sample_ratio loop\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# initialize fold\n",
    "kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "971945fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training for 1 epochs on train_loader ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e30561a429844d6acfcfa661aa8caa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec53786aabe8488aba5ee390a152ab6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf464fd3c9f4914a26ed92b61966458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model from Phase 1 saved to: /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.92-v2.ckpt\n",
      "\n",
      "--- Testing model from Phase 1 checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.92-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.92-v2.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.92-v2.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13d3aa2552e46f2a7c6b2595d1cddf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Metrics ---\n",
      "Accuracy: 0.8590\n",
      "AUC: 0.9215\n",
      "Precision: 0.8447\n",
      "Recall: 0.9487\n",
      "F1-Score: 0.8937\n",
      "Confusion Matrix:\n",
      "tensor([[166,  68],\n",
      "        [ 20, 370]], device='cuda:0')\n",
      "--------------------------\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.8589743375778198\n",
      "     test_auc_epoch         0.9215044975280762\n",
      "        test_loss           0.36427053809165955\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import numpy as np\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "# 1. Instantiate the LightningModule\n",
    "model = LitSimpleCNN(\n",
    "    in_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    training_mode='full_network',  # Change to 'classifier_only' if you want to train only the classifier\n",
    "    image_height=IMAGE_SIZE,\n",
    "    image_width=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# Optional: Print model summary\n",
    "# You need to move the model to a device first for torchsummary to work\n",
    "# summary(model.to('cuda'), (NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE))\n",
    "# model.to('cpu') # Move it back if needed\n",
    "\n",
    "checkpoint_callback_full_model = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='simple-cnn-full-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "train_trainer_1 = pl.Trainer(\n",
    "    accelerator='gpu',                 # Use GPUs\n",
    "    devices=1,                         # Number of GPUs to use\n",
    "    #strategy='ddp_notebook',                    # DistributedDataParallel strategy (best for multi-GPU)\n",
    "    max_epochs=NUM_EPOCHS,              # Total number of epochs\n",
    "    callbacks=[checkpoint_callback_full_model, progress_bar],\n",
    "    logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"simple_cnn-full\")\n",
    ")\n",
    "\n",
    "test_trainer_1 = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,  # Use only one GPU\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"simple_cnn_test\"),\n",
    "    # Add the callback here so the Trainer can find the 'best' path\n",
    "    callbacks=[checkpoint_callback_full_model],\n",
    "    precision='16-mixed'  # Use mixed precision for testing\n",
    ")\n",
    "\n",
    "\n",
    "# --- Phase 1 Training ---\n",
    "print(f\"--- Starting Training for {NUM_EPOCHS} epochs on train_loader ---\")\n",
    "train_trainer_1.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# CRITICAL: Save the path to the best model from this phase\n",
    "path_after_phase1 = checkpoint_callback_full_model.best_model_path\n",
    "print(f\"Best model from Phase 1 saved to: {path_after_phase1}\")\n",
    "\n",
    "# --- Test 1: Evaluate the model from after Phase 1 ---\n",
    "print(f\"\\n--- Testing model from Phase 1 checkpoint: {path_after_phase1} ---\")\n",
    "model.current_test_title = \"ROC Curve after training on DataLoader 1\"\n",
    "model = LitSimpleCNN.load_from_checkpoint(path_after_phase1)  # Load the model from Phase 1\n",
    "model.eval().to('cuda:0')  # Ensure the model is in evaluation mode and on GPU\n",
    "test_trainer_1.test(model, dataloaders=test_loader, ckpt_path=path_after_phase1)\n",
    "results_phase1 = model.last_test_results.copy()\n",
    "results_original_roc = {\"fpr\": results_phase1[\"fpr\"], \"tpr\": results_phase1[\"tpr\"], \"thresholds\": results_phase1[\"thresholds\"], \"name\": \"Original NN PneumoniaMNIST\", \"auc\": results_phase1[\"auc\"], \"model\": model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c2d0a",
   "metadata": {},
   "source": [
    "### Weighted ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226e70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSIFIERS = 20\n",
    "w = np.linspace(0.001, 0.999, NUM_CLASSIFIERS, endpoint=True)\n",
    "pos_weights = w/(1-w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d1bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fold 1/4 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51270447c744ded89c06659e16438e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49ea64a4d094ba48c091fb13ec0b524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1f85f1fcbb4dbb804ff50ddd10649b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.34-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.34-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e84e97066f4f1382bb179c0ff78b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb97b859b2d24700b8f2d9e5c8a99bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd93dd42c5864ce789212f90a2be0759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 2 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.82-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.82-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027503c3f4e94b6e8517fd8ef0547ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4da289973224c34a095a695faa7ab2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3215f28ab0e34d8dba01b839c125076e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 3 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.94-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.94-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8c354ad23a4fab88e3701a4c0e6539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e263fadaf3a34c2f9afa854084784594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3279118f17844e6ab855ee85e96883e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 4 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.97-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.97-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785ba14ce8744458bcb50c51c2736f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd9c9a796ed4fa0bfb5d3ca46608bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56178a4b6844fcc92b08379cb951607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 5 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.92-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.92-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead68076a8f34207acbb4033abc214cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ce731c522849f7a464c8bab560d933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74f3828ff8f48db92ac5bfc103d4b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 6 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b3166a4c7f489f95c8cead398d0eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e1ddfdcb68422e9edd979f20b7dad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6659019df2ca497d858a502950f13bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 7 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.98-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9b70da6e424625be34edfb0597b85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc8c6993e134ff1a7895e22dde31375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b862b7a331f4c2aad5d7a62ffc6aa3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 8 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d39200ce9f47d998f380f8b2b4fd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e612fff38484052afc574986a6b4bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2e1a8d18164633ac6c55ea050b754a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 9 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.97-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.97-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f10e707001456985a8a248d5184c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ed6f45691c444a9c778922c11b99f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33866d180c674ee2892682c15c502d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 10 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.98-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86661b20241e451280549b11d0229c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd15134585d415dbf06f6cc811e68b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91e0a8053514e95b9cf88413a4bab9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 11 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.98-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5404c195c4bc495c90f0b33e13d3b56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f43c464c9774fd493da2efdd29fa782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7ebbe5ff1a473f8d20ea7d98a25db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 12 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v5.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447d293e5c49485aab293a0d465a253a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445e9bf15fc94828938ab002e16ab58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fed5daa0fb7411ea1e04a06ee5cc026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 13 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.97-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636c26bc268f4ac0bf28dbea0ca8a4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842ae462befc49f3a74f5ace851e12d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133f2f20a00e487faab93069639719b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 14 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.97-v5.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9e507004e44a69b8ee9151cdea8aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749d6b1517944421a269f44c260ef7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23d7d8ed0354beba829e0bbad4cb47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 15 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6795bb24cd24a0d8e4bc93a27e2689c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf58124dea7b4676877995669eef0a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ca798070b94431963d35910d742616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 16 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.97-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87ffe1aeca84e78a98b7870699c7973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc2200079e9467b95adb308d0c7b4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4358a236b0e4b178f3140a8062829a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 17 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v4.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a74d565ecd44ed38b1aa7af6e1401fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1995c3bff41482db203c91b8ca2f548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9f2d2a65a94ae0af38911deff5dbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 18 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.97-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.97-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e058859b3b564163a5feac37d407796c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c84137232e44ed9820318e56fe6cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140d4a23089d4af3a1ab926b3728c6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 19 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v4.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v4.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc92331cd309460ba795f7cdf33c462b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab696753baa4df79fb188fa07576315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6074bff3974e485396fe814242a760be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 20 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.91-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.91-v1.ckpt to continue...\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.34-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with model 1: 100%|██████████| 19/19 [00:01<00:00, 18.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.1029, FPR=0.0000, F1-Score=0.1866\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.82-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 2: 100%|██████████| 19/19 [00:00<00:00, 22.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.8105, FPR=0.0155, F1-Score=0.8925\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.94-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 3: 100%|██████████| 19/19 [00:00<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9556, FPR=0.0404, F1-Score=0.9697\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.97-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 4: 100%|██████████| 19/19 [00:00<00:00, 22.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9731, FPR=0.0683, F1-Score=0.9737\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.92-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 5: 100%|██████████| 19/19 [00:00<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9064, FPR=0.0155, F1-Score=0.9480\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 6: 100%|██████████| 19/19 [00:00<00:00, 23.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9860, FPR=0.0963, F1-Score=0.9751\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 7: 100%|██████████| 19/19 [00:00<00:00, 27.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9895, FPR=0.0994, F1-Score=0.9763\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 8: 100%|██████████| 19/19 [00:00<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9813, FPR=0.0559, F1-Score=0.9801\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.97-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 9: 100%|██████████| 19/19 [00:00<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9918, FPR=0.0839, F1-Score=0.9803\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 10: 100%|██████████| 19/19 [00:00<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9836, FPR=0.0901, F1-Score=0.9751\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 11: 100%|██████████| 19/19 [00:00<00:00, 22.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9848, FPR=0.0528, F1-Score=0.9825\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 12: 100%|██████████| 19/19 [00:00<00:00, 23.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9965, FPR=0.1304, F1-Score=0.9743\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 13: 100%|██████████| 19/19 [00:00<00:00, 22.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9813, FPR=0.0497, F1-Score=0.9813\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 14: 100%|██████████| 19/19 [00:00<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9626, FPR=0.0248, F1-Score=0.9763\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 15: 100%|██████████| 19/19 [00:00<00:00, 22.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9696, FPR=0.0373, F1-Score=0.9776\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 16: 100%|██████████| 19/19 [00:00<00:00, 22.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9906, FPR=0.0901, F1-Score=0.9786\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 17: 100%|██████████| 19/19 [00:00<00:00, 23.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9801, FPR=0.0621, F1-Score=0.9784\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.97-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 18: 100%|██████████| 19/19 [00:01<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9848, FPR=0.0932, F1-Score=0.9751\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v4.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 19: 100%|██████████| 19/19 [00:00<00:00, 22.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9813, FPR=0.0839, F1-Score=0.9750\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.91-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 20: 100%|██████████| 19/19 [00:00<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9977, FPR=0.3261, F1-Score=0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fold 2/4 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9011518529f48c6ad09c18bbdcafac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8490ac370a6e460da22734b84c2bc801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691e694e2b454c43b6ef14e444ab88f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.26-v6.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.26-v6.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e500a3793d044b1d997622b5d9fce4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c34916bcbf24ae3a8e575399f777549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3c544141dd4ed1907447363e7c8015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 2 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.85-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.85-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447f817001d34805afd1dd551ce0c664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb181b65b3384770a956a9490306a20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9123ea7d211a420e9fc060428ce1caaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 3 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.90-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.90-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a02008880f3425f9b08febff1fe86af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684347b38f7049008173287909838beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a286b8d2e884a80b2e7e058f5a5f14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 4 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b70f44bd8945c4a112f36376709f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a9fe5f27f24a4dbb150dfe3f51ef19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369cc075cd3f4ca6b0cd88cbf8d5107c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 5 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.96-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.96-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3293eafc9fda41fc96af916dbd85ead4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dd21279e184601b9721bbcf2091cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96ea8951cb24ef29875a05c3bf51b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 6 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v4.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0788117fe74ea0a74d4ffc5a26e660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7f5e957d234072aad7246c290bc650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b565b7c450f14e5ca8c1c5dff5de604c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 7 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.94-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.94-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d18ef12f8af4cd0a8b4ca4591bf9f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45b3584753f491c909ad7bc50dcc64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9081eebf1c141758e946c340a8e7526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 8 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e876e8873124ee28d33c769ed7d9163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a66b59c4db4db68fbb039029951050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38968c16a9ea41a3a2ff147646b9c144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 9 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.97-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b36506a7bf24fc8bc8049f026b25be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f86df2cdfe4669a8b5b8451499cd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ab0ed64556413fbeacf49fb6591580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 10 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70005a62b894360b1d17dd95bdd6025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f6f99c3410421fbb2f1356fa4d4e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5fa7bd689c4d6296555118dd1339e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 11 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v5.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc1075e64bd427a9ed2e00885ccabac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f96657975549ddaf204a60546918c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0068a48c39b4451ab425e4864af8f37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 12 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v6.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v6.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b781793b2b694afeb51f7bf65666f9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dc3837b5b049a996c59e5502b70760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b957bb6134774b9699e36dd179e98d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 13 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.98-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc16d8610fc94d2d91e0a4818f25a64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d3449fbd344b9f9c934e734cd136f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9686c4dcf934970aac1e5a075e483a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 14 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.97-v6.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.97-v6.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3eed674a6d34bcd86318551087342d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7bd2290e234f749ed96331fa4c0619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9208fff4e12f42b3bf36e48ad3b9f668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 15 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v4.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec757300ddf54b1ba1423a44eb523332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d305c2698dfa4216bd4cec948c38167d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efe2ce21fe443faae620bbd99c3b045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 16 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.95-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.95-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfc7eff707d466bbdd4818a8c1b36ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824e0417d469400ba87866f680e10166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db84ec4f2af04e008b59709ace3f520b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 17 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v5.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a089e0e64b8c4735b0d5f8588f2612c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f639809e234f0190c5639bca39394b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d103890eacfe4796b32f5d6556fa6355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 18 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.97-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469d7f18c8d8432a92f908d8d5e803f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa789121baaa4aff8ad650e3af5a8184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acde2a816c8042b089ed7f3a3d805661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 19 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v5.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v5.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef509404e0c44b8cb7b4e3d410b48e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddc0cdc3d78457bae60a2f5d2ea3bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c8ccfd3aca4d52a5151a4c70fbd0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 20 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.90-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.90-v2.ckpt to continue...\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.26-v6.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with model 1: 100%|██████████| 19/19 [00:01<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.0057, FPR=0.0000, F1-Score=0.0114\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.85-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 2: 100%|██████████| 19/19 [00:00<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.8529, FPR=0.0065, F1-Score=0.9195\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.90-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 3: 100%|██████████| 19/19 [00:00<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9092, FPR=0.0228, F1-Score=0.9484\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 4: 100%|██████████| 19/19 [00:00<00:00, 19.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9517, FPR=0.0423, F1-Score=0.9679\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.96-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 5: 100%|██████████| 19/19 [00:00<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9621, FPR=0.0326, F1-Score=0.9750\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 6: 100%|██████████| 19/19 [00:00<00:00, 25.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9713, FPR=0.0391, F1-Score=0.9786\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.94-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 7: 100%|██████████| 19/19 [00:00<00:00, 26.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9333, FPR=0.0163, F1-Score=0.9627\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 8: 100%|██████████| 19/19 [00:00<00:00, 28.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9770, FPR=0.0619, F1-Score=0.9776\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 9: 100%|██████████| 19/19 [00:00<00:00, 26.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9862, FPR=0.0717, F1-Score=0.9806\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 10: 100%|██████████| 19/19 [00:00<00:00, 26.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9816, FPR=0.0586, F1-Score=0.9805\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 11: 100%|██████████| 19/19 [00:00<00:00, 24.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9862, FPR=0.0879, F1-Score=0.9778\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v6.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 12: 100%|██████████| 19/19 [00:00<00:00, 24.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9931, FPR=0.1140, F1-Score=0.9768\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 13: 100%|██████████| 19/19 [00:00<00:00, 24.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9839, FPR=0.0423, F1-Score=0.9845\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.97-v6.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 14: 100%|██████████| 19/19 [00:00<00:00, 23.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9874, FPR=0.0977, F1-Score=0.9767\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 15: 100%|██████████| 19/19 [00:00<00:00, 23.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9920, FPR=0.1010, F1-Score=0.9785\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.95-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 16: 100%|██████████| 19/19 [00:01<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9943, FPR=0.2085, F1-Score=0.9616\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 17: 100%|██████████| 19/19 [00:00<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9851, FPR=0.0945, F1-Score=0.9761\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.97-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 18: 100%|██████████| 19/19 [00:00<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9828, FPR=0.0684, F1-Score=0.9794\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v5.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 19: 100%|██████████| 19/19 [00:00<00:00, 21.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9931, FPR=0.1596, F1-Score=0.9692\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.90-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 20: 100%|██████████| 19/19 [00:00<00:00, 22.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9943, FPR=0.4072, F1-Score=0.9301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fold 3/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6746a4b9931c4e0787a90f619317bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4f5ac23ee84bc5bc608137d2667d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fee7f099ef74865a3b0db38d952b6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.26-v7.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.26-v7.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ad490df55e423899091e59cf20539f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e4ba8be5cb461e8b02a58728b0bda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34577920d91a4d909640e9852fc90112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 2 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.76-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.76-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bfed514a29447890acb885d7b42465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae55ed89eea4b7c9e8c4bd497a3b2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43dea6518444641a94005f47dccc58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 3 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.94-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.94-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ece37a611ad45d583b1774bf7ef363c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f2dd0be3434f8581a6546aefe29af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c8d21b1c2f496a8ff1adc8f2a5f344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 4 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.91-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.91-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0b69cca49b480ab050fc1e3295190f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04aff26625446a094f547b057d5c44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804facf12aa04ffdb6011c6db170dc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 5 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.95-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.95-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ea7b05aa8b4341bbae44ca365591e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4de82136eb543a3ba5cd0ba1218b95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24db1d9be5104abaaf73a9e53ac9d987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 6 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.94-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.94-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb94617c2464aad89b3aefb6cde180f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a591e0318b41df9e847444d38d872e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af84f50e3bf54da2aea260187ae184fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 7 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf8171667ac4df78f39941fa58e529b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbabcf77bb374334b33fe620eb929b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a5d3f2fb1a4bf7b1a11e46525f9015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 8 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.96-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.96-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5010f9d7a3fb41bf89ec197d6c36fdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8650889eee6f48f495af00c25d3fa9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbf03a980854016bae89c831912262f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 9 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.96-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cc2c35d64d4a5cb3b2690315e50328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd85c08afd324dca9602dbaa9d2bb5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c581d93fde2140ddbb3f260f3b73094a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 10 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v4.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e422c53875434be98d7a093f0adf6a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c8483b66e94afd80e3af83fa0d1eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e83b1673954949a4c8361a45f1113d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 11 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v6.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v6.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474ba2fc908640769e81d8e65419ae61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c3a8615ad74c6c9c8d5b35b4c54eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbb32a80fb04caabba4865b223bcab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 12 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v7.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v7.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78395ee49f1d48ef8b710ed777e21417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0eb5c0b7c140c694cc97785e0ecbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5e8b694ff34dc7a27a5ccd054ff714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 13 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.96-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e5c0b3bfe24d91b99021344dd6244b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6757a257854bb19935a091e14bc6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18b847b458a4b789270f7768c2ca411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 14 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.98-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb0508acbc0485fb08f491f8ce495b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d34f807a3c145a58238f0d54260d558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9696f68cee6483ebfb45d9912eb3453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 15 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.93-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.93-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50106d37a1ca415e8ecfea9ca9af3f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaf8704007d457bbdfd39ff0d636629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0ef4ddff124918b9b3f94b72a92efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 16 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.97-v4.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f3b38f6fba45d88610132397f6d32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59694942b57341ee921088868879c48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12b96521b664dc8b157432dbf6ef3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 17 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v6.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v6.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b563c4336fe461aa014929537444094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550c5ae4eabf43e28144647db1c1e29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4df364831144d48223e9252b3c35b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 18 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.96-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287b37fcfbf1434c952c6fbe2faf1b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d7a320c5f148e486da9f36af0260e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2323f9c73d0f416f9c1ca68fcace6e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 19 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v6.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v6.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d083d6899a9a4825ab7bb54c97a4003a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972047c98ca242ebbae22d7bba051164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0affb3aa184262a4f28e457720c33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 20 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.90-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.90-v3.ckpt to continue...\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.26-v7.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with model 1: 100%|██████████| 19/19 [00:01<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.0000, FPR=0.0000, F1-Score=0.0000\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.76-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 2: 100%|██████████| 19/19 [00:00<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.7188, FPR=0.0036, F1-Score=0.8358\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.94-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 3: 100%|██████████| 19/19 [00:00<00:00, 20.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9453, FPR=0.0463, F1-Score=0.9647\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.91-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 4: 100%|██████████| 19/19 [00:00<00:00, 22.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9129, FPR=0.0178, F1-Score=0.9517\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.95-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 5: 100%|██████████| 19/19 [00:00<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9632, FPR=0.0285, F1-Score=0.9768\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.94-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 6: 100%|██████████| 19/19 [00:00<00:00, 24.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9375, FPR=0.0249, F1-Score=0.9639\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 7: 100%|██████████| 19/19 [00:00<00:00, 27.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9665, FPR=0.0285, F1-Score=0.9785\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.96-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 8: 100%|██████████| 19/19 [00:00<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9710, FPR=0.0356, F1-Score=0.9797\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 9: 100%|██████████| 19/19 [00:00<00:00, 25.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9821, FPR=0.0356, F1-Score=0.9854\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 10: 100%|██████████| 19/19 [00:00<00:00, 26.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9609, FPR=0.0214, F1-Score=0.9767\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v6.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 11: 100%|██████████| 19/19 [00:00<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9777, FPR=0.0498, F1-Score=0.9810\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.97-v7.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 12: 100%|██████████| 19/19 [00:00<00:00, 23.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9900, FPR=0.0569, F1-Score=0.9861\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 13: 100%|██████████| 19/19 [00:00<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9721, FPR=0.0356, F1-Score=0.9803\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 14: 100%|██████████| 19/19 [00:00<00:00, 23.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9799, FPR=0.0569, F1-Score=0.9810\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.93-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 15: 100%|██████████| 19/19 [00:00<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9967, FPR=0.2313, F1-Score=0.9633\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 16: 100%|██████████| 19/19 [00:00<00:00, 24.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9788, FPR=0.0427, F1-Score=0.9826\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v6.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 17: 100%|██████████| 19/19 [00:00<00:00, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9833, FPR=0.0783, F1-Score=0.9794\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 18: 100%|██████████| 19/19 [00:00<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9933, FPR=0.0996, F1-Score=0.9813\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v6.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 19: 100%|██████████| 19/19 [00:00<00:00, 23.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9922, FPR=0.0676, F1-Score=0.9856\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.90-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 20: 100%|██████████| 19/19 [00:00<00:00, 23.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9989, FPR=0.3488, F1-Score=0.9476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fold 4/4 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5fe84fcf2e4539a1376a3d9453cffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f088f3928a344995a942069d6c98e3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c8e9b71aa94dd1ae67ea543804446a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.64-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.64-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1948bd6ddbd434d949a6ea65381762a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d91a910cab40069718024f0a643f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ceb4146cbdd400d9f1e5bb682fb531e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 2 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.84-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.84-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2029d0263b94014be9cba9eec957e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a04df9a44042219e5358915a9480bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9d2da5272b4f1a8504c48b28ee6c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 3 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.95-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.95-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988d5059e2624f828a93e01ca3643887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434a363cc277454c8ffabdbb44f3c5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75663399653b45e5a33087b6ddbd93de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 4 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f8949d741749e19b5c41b7f0ada6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08855a0864e247669af8f3f6ee474c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28621c5566e740e5a2888620031b7833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 5 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.90-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.90-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7392136efaf34064a97a34ce6cf73e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efdd5455a2b4b1e91f744c8f8043b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab1b6befabb424f99c9f86ab78137e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 6 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v5.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a47b3cf0094dae8817c272bd23a0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d814c82e3a84c3ebd2e363079f7ead5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb86eb6448f4b088c0064f46ffb6fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 7 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27e29432fc44112bc8b589207c6bd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975b55d3d4ce45a1b0be04c1bdf7759b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0a8d7a53634e5ea4ac752b091532cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 8 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.98-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c7e8f6cc204e398c98f6640afd49d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6db2b69bef4f61901376becffd6d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcb1c71cb6140c89c26227ba8e61913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 9 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.96-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9cbf0d73df419dacd8deb42f4fd856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e7044408884ab98a67011a9bd86021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8788139908544fbb650d847d1a15d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 10 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v5.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3b4512550e4ecb8c3ee5805d82fe3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a098c645ad2f4230b0a928f91d99a931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cece02fbee428fa47b03d7b1ea4296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 11 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v7.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v7.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2decf9f98b80497b99d8f472bc5cfd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6268ef4fb44a88935b6cfcb49cddd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07d24b82ad9490cac656e11428c6689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 12 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.98-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9498b356eefb4372a642a1e53426ac96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50bffc03dbb4c18b4e6c06e26dcdbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05aa984e87c4aafb2f967bb0581cfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 13 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.97-v4.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f97c53cd70349a19aecdde6d145aac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cca39156e842e8a4387a733b0066fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f27332fbc24fe58d11479711136dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 14 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.96-v2.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ddee0359bf483d82327f6c0f1be652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f48fafd664948e08bbe4372a9c352fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2ff35db9734774a8b15901a32cbc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 15 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v5.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233fe803d79c4396b8a4b86d97a2ed25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c39a89ae6e947d999460b1577a9b909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4824661961ff46079880873144ec22ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 16 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.96-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.96-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004ecbe2c5424f21879f64726b555425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8327b494566427292f4e18c7985117b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd725c711e21470988f0d7b6ade9400f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 17 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v7.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v7.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd126d408964808a86aa5bf84be2037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608b675c6da84a269288d37cff846cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea93a30a22f4a00a87974b173cf5588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 18 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.96-v3.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbc4ce32f0a4e9f8eac305710e8cb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60ad3fc536c4ce0bf754133bfd15b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21192d805d647d1b9819b120052b700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 19 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v7.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v7.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313132c8585c42b2859afad7b3178dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710b11dc37714172a6d58eccb2b7f432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67be61bc789c40cf98a60dd8ed741de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 20 complete. Best model path: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.96-v1.ckpt ---\n",
      "Loading best weights from /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.96-v1.ckpt to continue...\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.64-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with model 1: 100%|██████████| 19/19 [00:01<00:00, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.5166, FPR=0.0000, F1-Score=0.6813\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.84-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 2: 100%|██████████| 19/19 [00:00<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.8076, FPR=0.0066, F1-Score=0.8924\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.95-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 3: 100%|██████████| 19/19 [00:00<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9416, FPR=0.0362, F1-Score=0.9637\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 4: 100%|██████████| 19/19 [00:00<00:00, 25.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9599, FPR=0.0395, F1-Score=0.9727\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.90-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 5: 100%|██████████| 19/19 [00:00<00:00, 26.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.8660, FPR=0.0000, F1-Score=0.9282\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 6: 100%|██████████| 19/19 [00:00<00:00, 28.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9771, FPR=0.0428, F1-Score=0.9810\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 7: 100%|██████████| 19/19 [00:00<00:00, 26.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9393, FPR=0.0033, F1-Score=0.9681\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 8: 100%|██████████| 19/19 [00:00<00:00, 24.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9874, FPR=0.0658, F1-Score=0.9823\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 9: 100%|██████████| 19/19 [00:00<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9966, FPR=0.1217, F1-Score=0.9775\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 10: 100%|██████████| 19/19 [00:00<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9656, FPR=0.0066, F1-Score=0.9814\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_11/best-model-epoch=00-val_acc=0.97-v7.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 11: 100%|██████████| 19/19 [00:00<00:00, 22.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9771, FPR=0.0461, F1-Score=0.9805\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_12/best-model-epoch=00-val_acc=0.98-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 12: 100%|██████████| 19/19 [00:00<00:00, 22.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9885, FPR=0.0691, F1-Score=0.9824\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_13/best-model-epoch=00-val_acc=0.97-v4.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 13: 100%|██████████| 19/19 [00:00<00:00, 23.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9885, FPR=0.0757, F1-Score=0.9812\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_14/best-model-epoch=00-val_acc=0.96-v2.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 14: 100%|██████████| 19/19 [00:00<00:00, 23.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9920, FPR=0.1053, F1-Score=0.9780\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_15/best-model-epoch=00-val_acc=0.97-v5.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 15: 100%|██████████| 19/19 [00:00<00:00, 23.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9817, FPR=0.0493, F1-Score=0.9822\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_16/best-model-epoch=00-val_acc=0.96-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 16: 100%|██████████| 19/19 [00:00<00:00, 21.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9565, FPR=0.0362, F1-Score=0.9715\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_17/best-model-epoch=00-val_acc=0.97-v7.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 17: 100%|██████████| 19/19 [00:00<00:00, 23.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9840, FPR=0.0592, F1-Score=0.9817\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_18/best-model-epoch=00-val_acc=0.96-v3.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 18: 100%|██████████| 19/19 [00:00<00:00, 24.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9954, FPR=0.1776, F1-Score=0.9677\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_19/best-model-epoch=00-val_acc=0.96-v7.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 19: 100%|██████████| 19/19 [00:01<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9931, FPR=0.1316, F1-Score=0.9742\n",
      "\n",
      "--- Testing model from checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/stage_20/best-model-epoch=00-val_acc=0.96-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 20: 100%|██████████| 19/19 [00:00<00:00, 24.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9782, FPR=0.0559, F1-Score=0.9794\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import numpy as np\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "\n",
    "#store the weighted classifiers of all the folds\n",
    "list_folds_weighted_clfs = []  # List to store classifiers from all folds\n",
    "#store the best classfiers of all the folds\n",
    "list_folds_best_models = []  # List to store best models from all folds\n",
    "\n",
    "# Dictionary to store the history of each fold\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# 3. K-fold cross-validation loop\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "    list_weighted_clfs = [] # List to store the models trained at each stage\n",
    "    best_clfs = [] # List to store the best models from each stage\n",
    "    best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "    # 1. Instantiate the LightningModule\n",
    "    model = LitSimpleCNN(\n",
    "        in_channels=NUM_CHANNELS,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        training_mode='full_network',  # Change to 'classifier_only' if you want to train only the classifier\n",
    "        image_height=IMAGE_SIZE,\n",
    "        image_width=IMAGE_SIZE\n",
    "    )\n",
    "    print(f\"--- Starting Fold {fold + 1}/{K_FOLDS} ---\")\n",
    "\n",
    "    # 4. Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "                      train_dataset,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      sampler=train_subsampler,\n",
    "                      num_workers=NUM_WORKERS)\n",
    "    fold_loader = data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sampler=val_subsampler,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "\n",
    "    for i, pos_weight in enumerate(pos_weights):\n",
    "        model.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_acc',\n",
    "            dirpath=f'checkpoints/stage_{i+1}/',\n",
    "            filename=f'best-model-{{epoch:02d}}-{{val_acc:.2f}}',\n",
    "            save_top_k=1,\n",
    "            mode='max',\n",
    "        )\n",
    "        \n",
    "        # 3. Instantiate a NEW Trainer for this specific stage\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator='gpu',\n",
    "            devices=1,\n",
    "            #strategy='ddp_notebook',\n",
    "            max_epochs=NUM_EPOCHS, # Number of epochs for THIS stage\n",
    "            callbacks=[checkpoint_callback, progress_bar],\n",
    "            logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=f\"stage_{i+1}_ratio_{pos_weight}\"),\n",
    "            precision='16-mixed'\n",
    "            )\n",
    "        \n",
    "        # 4. Train the model. It will start with weights from the previous stage.\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "        \n",
    "        # 5. Get the path to the best model from THIS stage and store it\n",
    "        if checkpoint_callback.best_model_path:\n",
    "            best_path_this_stage = checkpoint_callback.best_model_path\n",
    "            print(f\"--- Stage {i+1} complete. Best model path: {best_path_this_stage} ---\")\n",
    "            best_model_paths.append(best_path_this_stage)\n",
    "            \n",
    "            # 6. CRITICAL: Load the best weights back into the model object\n",
    "            # This ensures we carry forward the BEST model to the next stage, not the last.\n",
    "            print(f\"Loading best weights from {best_path_this_stage} to continue...\")\n",
    "            model = LitSimpleCNN.load_from_checkpoint(best_path_this_stage, strict=False)  # Use strict=False to ignore missing keys if any\n",
    "        else:\n",
    "            print(f\"--- Stage {i+1} complete. No checkpoint was saved. ---\")\n",
    "            # If no checkpoint was saved, the model object will just have the weights from the last epoch.\n",
    "\n",
    "    # 7. Test the model after each stage\n",
    "    # Loop through each saved model checkpoint\n",
    "    for i, checkpoint_path in enumerate(best_model_paths):\n",
    "        print(f\"\\n--- Testing model from checkpoint: {checkpoint_path} ---\")\n",
    "\n",
    "        # 1. Load the PyTorch model from the checkpoint\n",
    "        pytorch_model = LitSimpleCNN.load_from_checkpoint(checkpoint_path, strict=False)\n",
    "        pytorch_model.eval()  # Set model to evaluation mode\n",
    "        pytorch_model.to('cuda:0') # Move model to GPU\n",
    "\n",
    "        # --- Generate Predictions for the ENTIRE test set ---\n",
    "        # We will collect the raw model outputs (logits) and true labels\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Wrap the loop in torch.no_grad() for efficiency\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(fold_loader, desc=f\"Predicting with model {i+1}\"):\n",
    "                inputs, labels = batch\n",
    "                \n",
    "                # Move data to the GPU\n",
    "                inputs = inputs.to('cuda:0')\n",
    "                \n",
    "                # Get model output (raw logits) for the batch\n",
    "                logits = pytorch_model(inputs)\n",
    "                \n",
    "                # Append batch results to lists (move back to CPU to prevent GPU memory buildup)\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        # Concatenate all batch results into single tensors\n",
    "        # These now contain the predictions and labels for the full test set\n",
    "        full_dataset_logits = torch.cat(all_logits)\n",
    "        full_dataset_labels = torch.cat(all_labels).int() # Ensure labels are integers\n",
    "\n",
    "        # --- Now, Calculate ALL Metrics using the generated predictions ---\n",
    "\n",
    "        # 2. Calculate the full ROC curve data\n",
    "        # Use the raw logits, torchmetrics will handle applying all thresholds\n",
    "        array_of_all_fprs, array_of_all_tprs, threshold_vals = torchmetrics.functional.roc(\n",
    "            preds=full_dataset_logits,\n",
    "            target=full_dataset_labels,\n",
    "            task=\"binary\"\n",
    "        )\n",
    "\n",
    "        # 3. Calculate confusion matrix based on a hard threshold (e.g., 0.5)\n",
    "        # Note: The model outputs logits, so the threshold 0.5 for probabilities corresponds to 0.0 for logits.\n",
    "        hard_preds = (full_dataset_logits > 0.0).int().numpy()\n",
    "        tn, fp, fn, tp = confusion_matrix(full_dataset_labels.numpy(), hard_preds).ravel()\n",
    "\n",
    "        # 4. Calculate metrics from the confusion matrix\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # This is also Recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "        \n",
    "        print(f\"Results at threshold 0.5: TPR={tpr:.4f}, FPR={fpr:.4f}, F1-Score={f1:.4f}\")\n",
    "        \n",
    "        # 5. Store the comprehensive results for this model\n",
    "        list_weighted_clfs.append({\n",
    "            \"fpr\": fpr,\n",
    "            \"tpr\": tpr,\n",
    "            \"model\": pytorch_model, # Optional: store the model object itself\n",
    "            \"threshold\": 0.5,\n",
    "            \"full_roc\": {\n",
    "                \"fpr\": array_of_all_fprs,\n",
    "                \"tpr\": array_of_all_tprs,\n",
    "                \"thresholds\": threshold_vals\n",
    "            }\n",
    "        })\n",
    "        best_clfs = get_best_roc_envelope(list_weighted_clfs, results_original_roc)\n",
    "    list_folds_weighted_clfs.append(list_weighted_clfs)\n",
    "    list_folds_best_models.append(best_clfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57f49a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to pickle/medMNIST_weighted.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_models': [[{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.8959),\n",
       "    'threshold': tensor(0.9962)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0031),\n",
       "    'tpr': tensor(0.9181),\n",
       "    'threshold': tensor(0.9893)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0062),\n",
       "    'tpr': tensor(0.9263),\n",
       "    'threshold': tensor(0.9968)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0093),\n",
       "    'tpr': tensor(0.9474),\n",
       "    'threshold': tensor(0.9653)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0124),\n",
       "    'tpr': tensor(0.9579),\n",
       "    'threshold': tensor(0.9351)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0186),\n",
       "    'tpr': tensor(0.9626),\n",
       "    'threshold': tensor(0.9893)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0248),\n",
       "    'tpr': tensor(0.9684),\n",
       "    'threshold': tensor(0.8447)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0280),\n",
       "    'tpr': tensor(0.9731),\n",
       "    'threshold': tensor(0.8055)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0311),\n",
       "    'tpr': tensor(0.9743),\n",
       "    'threshold': tensor(0.7291)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0373),\n",
       "    'tpr': tensor(0.9766),\n",
       "    'threshold': tensor(0.6680)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0435),\n",
       "    'tpr': tensor(0.9801),\n",
       "    'threshold': tensor(0.6223)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0466),\n",
       "    'tpr': tensor(0.9825),\n",
       "    'threshold': tensor(0.5767)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0497),\n",
       "    'tpr': tensor(0.9836),\n",
       "    'threshold': tensor(0.5509)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0528),\n",
       "    'tpr': tensor(0.9848),\n",
       "    'threshold': tensor(0.5242)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0590),\n",
       "    'tpr': tensor(0.9860),\n",
       "    'threshold': tensor(0.4446)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0621),\n",
       "    'tpr': tensor(0.9871),\n",
       "    'threshold': tensor(0.8282)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0652),\n",
       "    'tpr': tensor(0.9883),\n",
       "    'threshold': tensor(0.7987)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0714),\n",
       "    'tpr': tensor(0.9895),\n",
       "    'threshold': tensor(0.7156)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0776),\n",
       "    'tpr': tensor(0.9906),\n",
       "    'threshold': tensor(0.5714)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0807),\n",
       "    'tpr': tensor(0.9918),\n",
       "    'threshold': tensor(0.5252)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0839),\n",
       "    'tpr': tensor(0.9930),\n",
       "    'threshold': tensor(0.4872)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0932),\n",
       "    'tpr': tensor(0.9942),\n",
       "    'threshold': tensor(0.3953)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1025),\n",
       "    'tpr': tensor(0.9953),\n",
       "    'threshold': tensor(0.3216)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1118),\n",
       "    'tpr': tensor(0.9965),\n",
       "    'threshold': tensor(0.7186)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1522),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(3.4976e-05)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1739),\n",
       "    'tpr': tensor(0.9988),\n",
       "    'threshold': tensor(0.0308)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.3168),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0008)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.8954),\n",
       "    'threshold': tensor(0.7445)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0033),\n",
       "    'tpr': tensor(0.9264),\n",
       "    'threshold': tensor(0.9999)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0065),\n",
       "    'tpr': tensor(0.9322),\n",
       "    'threshold': tensor(0.7920)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0098),\n",
       "    'tpr': tensor(0.9540),\n",
       "    'threshold': tensor(0.9999)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0163),\n",
       "    'tpr': tensor(0.9632),\n",
       "    'threshold': tensor(0.9713)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0195),\n",
       "    'tpr': tensor(0.9667),\n",
       "    'threshold': tensor(0.6785)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0228),\n",
       "    'tpr': tensor(0.9713),\n",
       "    'threshold': tensor(0.1952)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0293),\n",
       "    'tpr': tensor(0.9759),\n",
       "    'threshold': tensor(0.9817)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0326),\n",
       "    'tpr': tensor(0.9816),\n",
       "    'threshold': tensor(0.5871)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0358),\n",
       "    'tpr': tensor(0.9839),\n",
       "    'threshold': tensor(0.5320)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0456),\n",
       "    'tpr': tensor(0.9851),\n",
       "    'threshold': tensor(0.9519)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0489),\n",
       "    'tpr': tensor(0.9874),\n",
       "    'threshold': tensor(0.9445)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0619),\n",
       "    'tpr': tensor(0.9885),\n",
       "    'threshold': tensor(0.9117)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0684),\n",
       "    'tpr': tensor(0.9908),\n",
       "    'threshold': tensor(0.2237)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0717),\n",
       "    'tpr': tensor(0.9920),\n",
       "    'threshold': tensor(0.1898)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0879),\n",
       "    'tpr': tensor(0.9931),\n",
       "    'threshold': tensor(0.1112)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1368),\n",
       "    'tpr': tensor(0.9943),\n",
       "    'threshold': tensor(0.0233)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1564),\n",
       "    'tpr': tensor(0.9954),\n",
       "    'threshold': tensor(0.0002)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1629),\n",
       "    'tpr': tensor(0.9966),\n",
       "    'threshold': tensor(0.0435)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1792),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(0.0211)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1922),\n",
       "    'tpr': tensor(0.9989),\n",
       "    'threshold': tensor(9.3917e-06)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1954),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0104)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.7109),\n",
       "    'threshold': tensor(5.0512e-05)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0036),\n",
       "    'tpr': tensor(0.8973),\n",
       "    'threshold': tensor(0.9677)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0071),\n",
       "    'tpr': tensor(0.9453),\n",
       "    'threshold': tensor(0.8899)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0107),\n",
       "    'tpr': tensor(0.9464),\n",
       "    'threshold': tensor(0.8624)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0142),\n",
       "    'tpr': tensor(0.9509),\n",
       "    'threshold': tensor(0.8457)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0178),\n",
       "    'tpr': tensor(0.9520),\n",
       "    'threshold': tensor(0.8377)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0214),\n",
       "    'tpr': tensor(0.9676),\n",
       "    'threshold': tensor(0.4480)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0249),\n",
       "    'tpr': tensor(0.9721),\n",
       "    'threshold': tensor(0.2419)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0285),\n",
       "    'tpr': tensor(0.9866),\n",
       "    'threshold': tensor(0.1562)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0391),\n",
       "    'tpr': tensor(0.9888),\n",
       "    'threshold': tensor(0.0956)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0463),\n",
       "    'tpr': tensor(0.9900),\n",
       "    'threshold': tensor(0.0772)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0534),\n",
       "    'tpr': tensor(0.9911),\n",
       "    'threshold': tensor(0.7811)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0569),\n",
       "    'tpr': tensor(0.9922),\n",
       "    'threshold': tensor(0.3512)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0676),\n",
       "    'tpr': tensor(0.9933),\n",
       "    'threshold': tensor(0.4385)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0712),\n",
       "    'tpr': tensor(0.9944),\n",
       "    'threshold': tensor(0.4095)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0783),\n",
       "    'tpr': tensor(0.9955),\n",
       "    'threshold': tensor(0.2223)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0890),\n",
       "    'tpr': tensor(0.9967),\n",
       "    'threshold': tensor(0.1113)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1530),\n",
       "    'tpr': tensor(0.9978),\n",
       "    'threshold': tensor(0.0054)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1957),\n",
       "    'tpr': tensor(0.9989),\n",
       "    'threshold': tensor(0.0017)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2420),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0079)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.9084),\n",
       "    'threshold': tensor(0.8700)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0033),\n",
       "    'tpr': tensor(0.9473),\n",
       "    'threshold': tensor(0.4369)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0066),\n",
       "    'tpr': tensor(0.9679),\n",
       "    'threshold': tensor(0.4760)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0099),\n",
       "    'tpr': tensor(0.9702),\n",
       "    'threshold': tensor(0.4565)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0132),\n",
       "    'tpr': tensor(0.9771),\n",
       "    'threshold': tensor(0.3745)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0164),\n",
       "    'tpr': tensor(0.9794),\n",
       "    'threshold': tensor(0.3416)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0230),\n",
       "    'tpr': tensor(0.9817),\n",
       "    'threshold': tensor(0.2385)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0329),\n",
       "    'tpr': tensor(0.9828),\n",
       "    'threshold': tensor(0.2042)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0428),\n",
       "    'tpr': tensor(0.9851),\n",
       "    'threshold': tensor(0.1586)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0493),\n",
       "    'tpr': tensor(0.9885),\n",
       "    'threshold': tensor(0.1194)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0526),\n",
       "    'tpr': tensor(0.9897),\n",
       "    'threshold': tensor(0.1101)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0625),\n",
       "    'tpr': tensor(0.9908),\n",
       "    'threshold': tensor(0.0726)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0658),\n",
       "    'tpr': tensor(0.9920),\n",
       "    'threshold': tensor(0.0719)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0724),\n",
       "    'tpr': tensor(0.9931),\n",
       "    'threshold': tensor(0.0536)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0789),\n",
       "    'tpr': tensor(0.9943),\n",
       "    'threshold': tensor(0.0426)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0855),\n",
       "    'tpr': tensor(0.9954),\n",
       "    'threshold': tensor(0.0332)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1086),\n",
       "    'tpr': tensor(0.9966),\n",
       "    'threshold': tensor(0.5559)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1184),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(0.1695)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1447),\n",
       "    'tpr': tensor(0.9989),\n",
       "    'threshold': tensor(0.1260)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2336),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0423)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}]],\n",
       " 'weighted_clfs': [[{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.10292397660818714),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.6811e-01, 9.5809e-01,  ..., 1.6914e-08, 2.4539e-09,\n",
       "             2.1526e-09])}},\n",
       "   {'fpr': np.float64(0.015527950310559006),\n",
       "    'tpr': np.float64(0.8105263157894737),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9755e-01, 9.9675e-01,  ..., 2.8364e-06, 2.0510e-06,\n",
       "             8.6191e-07])}},\n",
       "   {'fpr': np.float64(0.040372670807453416),\n",
       "    'tpr': np.float64(0.9555555555555556),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9968e-01, 9.9966e-01,  ..., 6.0520e-06, 3.9669e-06,\n",
       "             7.6337e-07])}},\n",
       "   {'fpr': np.float64(0.06832298136645963),\n",
       "    'tpr': np.float64(0.9730994152046784),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0047,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.7860e-07, 1.0749e-07,\n",
       "             2.1376e-08])}},\n",
       "   {'fpr': np.float64(0.015527950310559006),\n",
       "    'tpr': np.float64(0.9064327485380117),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9998e-01,  ..., 5.8526e-09, 5.7954e-10,\n",
       "             3.9462e-10])}},\n",
       "   {'fpr': np.float64(0.09627329192546584),\n",
       "    'tpr': np.float64(0.9859649122807017),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9998e-01,  ..., 8.5523e-06, 6.4789e-06,\n",
       "             5.1900e-06])}},\n",
       "   {'fpr': np.float64(0.09937888198757763),\n",
       "    'tpr': np.float64(0.9894736842105263),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 9.9999e-01,  ..., 6.7978e-06, 4.9337e-06,\n",
       "             4.0759e-06])}},\n",
       "   {'fpr': np.float64(0.055900621118012424),\n",
       "    'tpr': np.float64(0.9812865497076023),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9999e-01,  ..., 1.0555e-08, 4.6581e-09,\n",
       "             1.0192e-09])}},\n",
       "   {'fpr': np.float64(0.08385093167701864),\n",
       "    'tpr': np.float64(0.991812865497076),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0124, 0.0124, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0217, 0.0248, 0.0280, 0.0280, 0.0280, 0.0280,\n",
       "             0.0311, 0.0311, 0.0311, 0.0311, 0.0342, 0.0342, 0.0373, 0.0373, 0.0373,\n",
       "             0.0404, 0.0435, 0.0466, 0.0466, 0.0497, 0.0528, 0.0528, 0.0528, 0.0528,\n",
       "             0.0528, 0.0559, 0.0559, 0.0559, 0.0559, 0.0590, 0.0590, 0.0590, 0.0621,\n",
       "             0.0621, 0.0621, 0.0652, 0.0652, 0.0683, 0.0714, 0.0714, 0.0745, 0.0776,\n",
       "             0.0776, 0.0807, 0.0807, 0.0839, 0.0839, 0.0870, 0.0932, 0.0932, 0.0963,\n",
       "             0.0994, 0.1025, 0.1025, 0.1056, 0.1087, 0.1118, 0.1149, 0.1180, 0.1211,\n",
       "             0.1242, 0.1242, 0.1273, 0.1304, 0.1335, 0.1366, 0.1398, 0.1429, 0.1460,\n",
       "             0.1491, 0.1522, 0.1553, 0.1584, 0.1584, 0.1615, 0.1646, 0.1677, 0.1708,\n",
       "             0.1739, 0.1739, 0.1770, 0.1801, 0.1832, 0.1863, 0.1894, 0.1925, 0.1957,\n",
       "             0.1988, 0.2019, 0.2050, 0.2081, 0.2112, 0.2143, 0.2174, 0.2205, 0.2236,\n",
       "             0.2267, 0.2298, 0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516,\n",
       "             0.2547, 0.2578, 0.2609, 0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795,\n",
       "             0.2826, 0.2857, 0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075,\n",
       "             0.3106, 0.3137, 0.3168, 0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354,\n",
       "             0.3385, 0.3416, 0.3447, 0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634,\n",
       "             0.3665, 0.3696, 0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913,\n",
       "             0.3944, 0.3975, 0.4006, 0.4037, 0.4068, 0.4099, 0.4130, 0.4161, 0.4193,\n",
       "             0.4224, 0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4410, 0.4441, 0.4472,\n",
       "             0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752,\n",
       "             0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031,\n",
       "             0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311,\n",
       "             0.5342, 0.5373, 0.5404, 0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590,\n",
       "             0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870,\n",
       "             0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6118,\n",
       "             0.6149, 0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398,\n",
       "             0.6429, 0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677,\n",
       "             0.6708, 0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957,\n",
       "             0.6988, 0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236,\n",
       "             0.7267, 0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453, 0.7484, 0.7516,\n",
       "             0.7547, 0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795,\n",
       "             0.7826, 0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075,\n",
       "             0.8106, 0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354,\n",
       "             0.8385, 0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634,\n",
       "             0.8665, 0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913,\n",
       "             0.8944, 0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193,\n",
       "             0.9224, 0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472,\n",
       "             0.9503, 0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752,\n",
       "             0.9783, 0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0058, 0.0094, 0.0129, 0.0164, 0.0211, 0.0234, 0.0316,\n",
       "             0.0398, 0.0421, 0.0480, 0.0573, 0.0608, 0.0702, 0.0760, 0.0784, 0.0807,\n",
       "             0.0830, 0.0865, 0.0889, 0.0971, 0.0994, 0.1018, 0.1029, 0.1053, 0.1076,\n",
       "             0.1123, 0.1205, 0.1287, 0.1333, 0.1357, 0.1380, 0.1415, 0.1497, 0.1556,\n",
       "             0.1602, 0.1614, 0.1673, 0.1719, 0.1754, 0.1778, 0.1789, 0.1848, 0.1883,\n",
       "             0.1895, 0.1918, 0.1930, 0.1953, 0.2035, 0.2094, 0.2117, 0.2140, 0.2187,\n",
       "             0.2269, 0.2304, 0.2327, 0.2363, 0.2386, 0.2398, 0.2433, 0.2444, 0.2456,\n",
       "             0.2480, 0.2503, 0.2538, 0.2573, 0.2608, 0.2620, 0.2655, 0.2667, 0.2690,\n",
       "             0.2725, 0.2737, 0.2749, 0.2772, 0.2807, 0.2830, 0.2854, 0.2889, 0.2912,\n",
       "             0.2947, 0.2959, 0.2982, 0.3006, 0.3053, 0.3064, 0.3111, 0.3146, 0.3181,\n",
       "             0.3205, 0.3240, 0.3275, 0.3310, 0.3333, 0.3404, 0.3415, 0.3427, 0.3439,\n",
       "             0.3462, 0.3474, 0.3497, 0.3520, 0.3532, 0.3544, 0.3567, 0.3626, 0.3637,\n",
       "             0.3661, 0.3673, 0.3684, 0.3708, 0.3719, 0.3731, 0.3743, 0.3766, 0.3789,\n",
       "             0.3825, 0.3836, 0.3848, 0.3860, 0.3871, 0.3883, 0.3895, 0.3918, 0.3930,\n",
       "             0.3953, 0.3965, 0.3988, 0.4000, 0.4047, 0.4058, 0.4082, 0.4094, 0.4105,\n",
       "             0.4117, 0.4129, 0.4140, 0.4152, 0.4175, 0.4211, 0.4222, 0.4234, 0.4246,\n",
       "             0.4257, 0.4281, 0.4292, 0.4304, 0.4327, 0.4351, 0.4363, 0.4374, 0.4398,\n",
       "             0.4409, 0.4421, 0.4456, 0.4468, 0.4480, 0.4491, 0.4503, 0.4526, 0.4550,\n",
       "             0.4561, 0.4573, 0.4585, 0.4608, 0.4620, 0.4632, 0.4667, 0.4678, 0.4702,\n",
       "             0.4713, 0.4737, 0.4749, 0.4760, 0.4784, 0.4795, 0.4807, 0.4819, 0.4830,\n",
       "             0.4854, 0.4877, 0.4889, 0.4912, 0.4924, 0.4936, 0.4947, 0.4959, 0.4982,\n",
       "             0.5006, 0.5018, 0.5029, 0.5053, 0.5064, 0.5076, 0.5099, 0.5111, 0.5135,\n",
       "             0.5146, 0.5170, 0.5193, 0.5205, 0.5216, 0.5240, 0.5251, 0.5263, 0.5275,\n",
       "             0.5287, 0.5298, 0.5310, 0.5322, 0.5333, 0.5345, 0.5357, 0.5380, 0.5392,\n",
       "             0.5404, 0.5415, 0.5427, 0.5439, 0.5450, 0.5474, 0.5485, 0.5497, 0.5509,\n",
       "             0.5520, 0.5532, 0.5556, 0.5567, 0.5579, 0.5591, 0.5602, 0.5614, 0.5649,\n",
       "             0.5661, 0.5684, 0.5696, 0.5708, 0.5731, 0.5754, 0.5766, 0.5778, 0.5789,\n",
       "             0.5801, 0.5813, 0.5825, 0.5836, 0.5848, 0.5871, 0.5883, 0.5895, 0.5906,\n",
       "             0.5918, 0.5930, 0.5942, 0.5965, 0.5977, 0.5988, 0.6023, 0.6035, 0.6047,\n",
       "             0.6058, 0.6070, 0.6094, 0.6105, 0.6117, 0.6129, 0.6152, 0.6164, 0.6175,\n",
       "             0.6187, 0.6199, 0.6211, 0.6222, 0.6234, 0.6246, 0.6257, 0.6269, 0.6281,\n",
       "             0.6292, 0.6304, 0.6316, 0.6327, 0.6339, 0.6351, 0.6363, 0.6374, 0.6386,\n",
       "             0.6398, 0.6409, 0.6421, 0.6433, 0.6444, 0.6456, 0.6468, 0.6491, 0.6503,\n",
       "             0.6515, 0.6526, 0.6538, 0.6550, 0.6561, 0.6573, 0.6585, 0.6596, 0.6608,\n",
       "             0.6620, 0.6632, 0.6643, 0.6655, 0.6667, 0.6678, 0.6690, 0.6702, 0.6713,\n",
       "             0.6725, 0.6737, 0.6772, 0.6784, 0.6795, 0.6807, 0.6819, 0.6830, 0.6842,\n",
       "             0.6854, 0.6889, 0.6901, 0.6912, 0.6924, 0.6936, 0.6947, 0.6971, 0.6982,\n",
       "             0.6994, 0.7006, 0.7018, 0.7029, 0.7041, 0.7053, 0.7064, 0.7076, 0.7088,\n",
       "             0.7088, 0.7099, 0.7111, 0.7123, 0.7135, 0.7146, 0.7158, 0.7170, 0.7181,\n",
       "             0.7193, 0.7205, 0.7216, 0.7228, 0.7240, 0.7251, 0.7263, 0.7275, 0.7287,\n",
       "             0.7298, 0.7310, 0.7333, 0.7345, 0.7357, 0.7368, 0.7380, 0.7392, 0.7404,\n",
       "             0.7415, 0.7427, 0.7439, 0.7450, 0.7474, 0.7485, 0.7497, 0.7509, 0.7520,\n",
       "             0.7532, 0.7544, 0.7556, 0.7567, 0.7579, 0.7591, 0.7602, 0.7614, 0.7626,\n",
       "             0.7637, 0.7649, 0.7661, 0.7673, 0.7684, 0.7696, 0.7708, 0.7719, 0.7731,\n",
       "             0.7754, 0.7766, 0.7778, 0.7789, 0.7801, 0.7813, 0.7825, 0.7836, 0.7848,\n",
       "             0.7860, 0.7871, 0.7883, 0.7895, 0.7906, 0.7918, 0.7930, 0.7942, 0.7953,\n",
       "             0.7965, 0.7977, 0.7988, 0.8000, 0.8012, 0.8023, 0.8035, 0.8047, 0.8058,\n",
       "             0.8070, 0.8082, 0.8094, 0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164,\n",
       "             0.8175, 0.8187, 0.8199, 0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269,\n",
       "             0.8281, 0.8292, 0.8304, 0.8316, 0.8327, 0.8339, 0.8351, 0.8363, 0.8374,\n",
       "             0.8386, 0.8398, 0.8409, 0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480,\n",
       "             0.8480, 0.8491, 0.8503, 0.8515, 0.8526, 0.8538, 0.8550, 0.8561, 0.8573,\n",
       "             0.8585, 0.8596, 0.8608, 0.8620, 0.8632, 0.8643, 0.8655, 0.8667, 0.8678,\n",
       "             0.8690, 0.8702, 0.8713, 0.8725, 0.8737, 0.8749, 0.8760, 0.8772, 0.8784,\n",
       "             0.8795, 0.8807, 0.8819, 0.8830, 0.8842, 0.8854, 0.8865, 0.8877, 0.8889,\n",
       "             0.8901, 0.8912, 0.8924, 0.8936, 0.8947, 0.8959, 0.8971, 0.8982, 0.8994,\n",
       "             0.9006, 0.9018, 0.9029, 0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099,\n",
       "             0.9111, 0.9123, 0.9135, 0.9146, 0.9158, 0.9170, 0.9181, 0.9193, 0.9205,\n",
       "             0.9216, 0.9228, 0.9240, 0.9251, 0.9263, 0.9275, 0.9287, 0.9298, 0.9310,\n",
       "             0.9322, 0.9333, 0.9333, 0.9345, 0.9345, 0.9357, 0.9368, 0.9380, 0.9392,\n",
       "             0.9404, 0.9415, 0.9427, 0.9439, 0.9450, 0.9462, 0.9474, 0.9485, 0.9497,\n",
       "             0.9509, 0.9520, 0.9520, 0.9532, 0.9544, 0.9556, 0.9567, 0.9579, 0.9591,\n",
       "             0.9602, 0.9614, 0.9626, 0.9626, 0.9626, 0.9626, 0.9637, 0.9649, 0.9661,\n",
       "             0.9661, 0.9673, 0.9684, 0.9696, 0.9696, 0.9708, 0.9708, 0.9719, 0.9731,\n",
       "             0.9731, 0.9731, 0.9731, 0.9743, 0.9743, 0.9743, 0.9754, 0.9766, 0.9778,\n",
       "             0.9789, 0.9789, 0.9801, 0.9813, 0.9825, 0.9825, 0.9836, 0.9848, 0.9848,\n",
       "             0.9860, 0.9871, 0.9871, 0.9883, 0.9883, 0.9883, 0.9895, 0.9895, 0.9895,\n",
       "             0.9906, 0.9906, 0.9918, 0.9918, 0.9930, 0.9930, 0.9930, 0.9942, 0.9942,\n",
       "             0.9942, 0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953,\n",
       "             0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01, 9.9967e-01,\n",
       "             9.9966e-01, 9.9965e-01, 9.9963e-01, 9.9961e-01, 9.9961e-01, 9.9961e-01,\n",
       "             9.9960e-01, 9.9960e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9957e-01, 9.9956e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9954e-01,\n",
       "             9.9954e-01, 9.9953e-01, 9.9952e-01, 9.9952e-01, 9.9952e-01, 9.9951e-01,\n",
       "             9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01, 9.9948e-01,\n",
       "             9.9948e-01, 9.9946e-01, 9.9946e-01, 9.9945e-01, 9.9943e-01, 9.9943e-01,\n",
       "             9.9943e-01, 9.9943e-01, 9.9942e-01, 9.9939e-01, 9.9938e-01, 9.9937e-01,\n",
       "             9.9936e-01, 9.9936e-01, 9.9934e-01, 9.9932e-01, 9.9932e-01, 9.9931e-01,\n",
       "             9.9931e-01, 9.9929e-01, 9.9928e-01, 9.9928e-01, 9.9925e-01, 9.9917e-01,\n",
       "             9.9914e-01, 9.9914e-01, 9.9913e-01, 9.9911e-01, 9.9906e-01, 9.9901e-01,\n",
       "             9.9900e-01, 9.9897e-01, 9.9894e-01, 9.9893e-01, 9.9891e-01, 9.9889e-01,\n",
       "             9.9888e-01, 9.9883e-01, 9.9882e-01, 9.9872e-01, 9.9872e-01, 9.9870e-01,\n",
       "             9.9860e-01, 9.9824e-01, 9.9822e-01, 9.9820e-01, 9.9805e-01, 9.9802e-01,\n",
       "             9.9792e-01, 9.9790e-01, 9.9780e-01, 9.9766e-01, 9.9758e-01, 9.9746e-01,\n",
       "             9.9729e-01, 9.9701e-01, 9.9700e-01, 9.9672e-01, 9.9670e-01, 9.9613e-01,\n",
       "             9.9603e-01, 9.9540e-01, 9.9492e-01, 9.9467e-01, 9.9439e-01, 9.9397e-01,\n",
       "             9.9350e-01, 9.9248e-01, 9.9241e-01, 9.9206e-01, 9.9174e-01, 9.9108e-01,\n",
       "             9.9106e-01, 9.9051e-01, 9.8933e-01, 9.8887e-01, 9.8814e-01, 9.8645e-01,\n",
       "             9.8637e-01, 9.8569e-01, 9.8377e-01, 9.8373e-01, 9.8175e-01, 9.8056e-01,\n",
       "             9.7799e-01, 9.7518e-01, 9.7357e-01, 9.7264e-01, 9.7199e-01, 9.7151e-01,\n",
       "             9.7097e-01, 9.7040e-01, 9.5185e-01, 9.5025e-01, 9.4346e-01, 9.3734e-01,\n",
       "             9.2660e-01, 9.1663e-01, 9.1640e-01, 9.1344e-01, 9.1174e-01, 9.0249e-01,\n",
       "             8.8831e-01, 8.8627e-01, 8.6988e-01, 8.4859e-01, 8.3528e-01, 8.3296e-01,\n",
       "             8.2852e-01, 8.2818e-01, 8.0287e-01, 7.9875e-01, 7.5960e-01, 7.3094e-01,\n",
       "             7.1562e-01, 6.3391e-01, 5.8839e-01, 5.7138e-01, 5.3788e-01, 5.2519e-01,\n",
       "             5.2425e-01, 4.8719e-01, 4.3461e-01, 4.0020e-01, 3.9526e-01, 3.9109e-01,\n",
       "             3.8767e-01, 3.2592e-01, 3.2159e-01, 3.2154e-01, 2.6625e-01, 2.5528e-01,\n",
       "             2.2931e-01, 2.0071e-01, 1.9903e-01, 1.7496e-01, 1.5961e-01, 1.5591e-01,\n",
       "             1.4948e-01, 1.3845e-01, 1.3595e-01, 1.2526e-01, 1.0610e-01, 9.8423e-02,\n",
       "             8.1208e-02, 8.1030e-02, 6.5904e-02, 6.0172e-02, 5.8927e-02, 5.5145e-02,\n",
       "             4.5247e-02, 3.7269e-02, 3.5846e-02, 3.1247e-02, 3.0751e-02, 2.7358e-02,\n",
       "             2.5782e-02, 2.4694e-02, 2.4614e-02, 2.3028e-02, 2.2704e-02, 2.1418e-02,\n",
       "             2.0628e-02, 1.8869e-02, 1.8263e-02, 1.8223e-02, 1.6607e-02, 1.6053e-02,\n",
       "             1.5109e-02, 1.4262e-02, 1.3804e-02, 1.2374e-02, 1.1946e-02, 1.1693e-02,\n",
       "             1.0677e-02, 9.5601e-03, 9.2858e-03, 8.7968e-03, 8.4620e-03, 8.1275e-03,\n",
       "             7.7085e-03, 7.3499e-03, 6.9200e-03, 6.5637e-03, 6.3062e-03, 6.2846e-03,\n",
       "             6.2455e-03, 6.0762e-03, 5.6932e-03, 5.4689e-03, 5.2184e-03, 5.2165e-03,\n",
       "             5.1562e-03, 4.9957e-03, 4.9788e-03, 4.9192e-03, 4.6402e-03, 4.5143e-03,\n",
       "             4.4777e-03, 4.3175e-03, 4.1082e-03, 3.6142e-03, 3.1572e-03, 2.9143e-03,\n",
       "             2.7688e-03, 2.7195e-03, 2.6845e-03, 2.4225e-03, 2.3721e-03, 2.3678e-03,\n",
       "             2.3524e-03, 2.3011e-03, 2.2247e-03, 2.1328e-03, 2.0225e-03, 2.0197e-03,\n",
       "             1.9784e-03, 1.9359e-03, 1.4415e-03, 1.4337e-03, 1.4089e-03, 1.3272e-03,\n",
       "             1.3084e-03, 1.2895e-03, 1.2043e-03, 1.1921e-03, 1.1802e-03, 1.1441e-03,\n",
       "             1.0800e-03, 1.0533e-03, 9.8002e-04, 9.6132e-04, 9.5280e-04, 9.3237e-04,\n",
       "             8.3664e-04, 8.1528e-04, 7.9224e-04, 7.8799e-04, 7.5532e-04, 7.2397e-04,\n",
       "             7.1543e-04, 6.1752e-04, 6.0764e-04, 6.0502e-04, 5.8073e-04, 5.3572e-04,\n",
       "             5.3510e-04, 4.7376e-04, 4.6297e-04, 4.2371e-04, 4.2185e-04, 4.1816e-04,\n",
       "             4.0948e-04, 4.0509e-04, 3.8681e-04, 3.4242e-04, 3.3970e-04, 3.3284e-04,\n",
       "             3.1564e-04, 3.0515e-04, 3.0351e-04, 2.6740e-04, 2.6377e-04, 2.5922e-04,\n",
       "             1.9763e-04, 1.8684e-04, 1.8198e-04, 1.5828e-04, 1.5731e-04, 1.5682e-04,\n",
       "             1.5596e-04, 1.5421e-04, 1.5165e-04, 1.4247e-04, 1.3757e-04, 1.3208e-04,\n",
       "             1.2993e-04, 1.2945e-04, 1.2617e-04, 1.2533e-04, 1.1646e-04, 1.1359e-04,\n",
       "             1.1098e-04, 9.9745e-05, 9.3604e-05, 8.8096e-05, 8.4372e-05, 8.3564e-05,\n",
       "             8.1304e-05, 7.8911e-05, 6.9437e-05, 6.9167e-05, 6.5499e-05, 6.2762e-05,\n",
       "             6.1573e-05, 5.8674e-05, 5.5373e-05, 5.2780e-05, 5.2306e-05, 5.2086e-05,\n",
       "             5.1457e-05, 5.1276e-05, 4.8034e-05, 4.7527e-05, 4.4964e-05, 4.4163e-05,\n",
       "             4.2980e-05, 4.0618e-05, 4.0235e-05, 4.0136e-05, 3.7933e-05, 3.6996e-05,\n",
       "             3.6444e-05, 3.5089e-05, 3.5037e-05, 3.3193e-05, 3.2788e-05, 3.2289e-05,\n",
       "             2.9950e-05, 2.8664e-05, 2.7299e-05, 2.7256e-05, 2.4558e-05, 2.4210e-05,\n",
       "             2.0512e-05, 1.9745e-05, 1.9074e-05, 1.6846e-05, 1.6716e-05, 1.5659e-05,\n",
       "             1.5062e-05, 1.4790e-05, 1.4210e-05, 1.3411e-05, 1.3198e-05, 1.3136e-05,\n",
       "             1.1997e-05, 1.1664e-05, 1.1516e-05, 1.1515e-05, 1.0395e-05, 9.9239e-06,\n",
       "             9.5760e-06, 9.0351e-06, 8.9948e-06, 8.8463e-06, 8.7636e-06, 8.6112e-06,\n",
       "             8.5290e-06, 8.3978e-06, 7.9923e-06, 7.4216e-06, 7.4080e-06, 6.0595e-06,\n",
       "             5.6752e-06, 4.9410e-06, 4.7968e-06, 4.6467e-06, 4.6433e-06, 4.5249e-06,\n",
       "             4.4731e-06, 4.4555e-06, 4.4032e-06, 3.3367e-06, 3.3261e-06, 2.9426e-06,\n",
       "             2.8519e-06, 2.7308e-06, 2.6432e-06, 2.1274e-06, 2.0084e-06, 1.6573e-06,\n",
       "             1.6287e-06, 1.4047e-06, 1.3194e-06, 1.3105e-06, 1.2928e-06, 1.2821e-06,\n",
       "             1.2154e-06, 1.1541e-06, 1.1438e-06, 9.7147e-07, 9.5821e-07, 9.3931e-07,\n",
       "             8.8988e-07, 8.3880e-07, 7.9923e-07, 7.8753e-07, 7.3331e-07, 7.1067e-07,\n",
       "             7.0272e-07, 6.7246e-07, 5.0719e-07, 4.5623e-07, 4.3629e-07, 3.4729e-07,\n",
       "             1.9427e-07, 1.7614e-07, 1.7591e-07, 1.6876e-07, 1.6370e-07, 1.3587e-07,\n",
       "             1.1187e-07, 1.0920e-07, 9.9363e-08, 9.8519e-08, 9.5520e-08, 8.7987e-08,\n",
       "             6.5062e-08, 5.7512e-08, 4.0855e-08, 3.9901e-08, 3.8460e-08, 3.1499e-08,\n",
       "             2.1621e-08, 9.5133e-09, 6.1396e-09, 5.1411e-09, 1.6146e-09, 1.4212e-09,\n",
       "             1.3560e-09, 6.4432e-10])}},\n",
       "   {'fpr': np.float64(0.09006211180124224),\n",
       "    'tpr': np.float64(0.9836257309941521),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.2759e-07, 1.0988e-07,\n",
       "             8.1581e-08])}},\n",
       "   {'fpr': np.float64(0.052795031055900624),\n",
       "    'tpr': np.float64(0.9847953216374269),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9999e-01,  ..., 5.0046e-08, 4.9964e-08,\n",
       "             4.0131e-08])}},\n",
       "   {'fpr': np.float64(0.13043478260869565),\n",
       "    'tpr': np.float64(0.9964912280701754),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0217,\n",
       "             0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0280,\n",
       "             0.0280, 0.0280, 0.0280, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342,\n",
       "             0.0373, 0.0404, 0.0435, 0.0466, 0.0497, 0.0497, 0.0528, 0.0559, 0.0590,\n",
       "             0.0590, 0.0621, 0.0621, 0.0621, 0.0621, 0.0652, 0.0652, 0.0652, 0.0683,\n",
       "             0.0683, 0.0714, 0.0745, 0.0745, 0.0745, 0.0776, 0.0776, 0.0776, 0.0807,\n",
       "             0.0839, 0.0839, 0.0839, 0.0870, 0.0901, 0.0932, 0.0963, 0.0963, 0.0994,\n",
       "             0.0994, 0.1025, 0.1025, 0.1025, 0.1025, 0.1056, 0.1087, 0.1118, 0.1118,\n",
       "             0.1149, 0.1180, 0.1211, 0.1242, 0.1273, 0.1304, 0.1335, 0.1366, 0.1398,\n",
       "             0.1429, 0.1460, 0.1491, 0.1522, 0.1553, 0.1584, 0.1615, 0.1646, 0.1677,\n",
       "             0.1708, 0.1708, 0.1739, 0.1770, 0.1801, 0.1832, 0.1863, 0.1894, 0.1925,\n",
       "             0.1957, 0.1988, 0.2019, 0.2050, 0.2081, 0.2112, 0.2143, 0.2174, 0.2205,\n",
       "             0.2236, 0.2267, 0.2298, 0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484,\n",
       "             0.2516, 0.2547, 0.2578, 0.2609, 0.2640, 0.2671, 0.2671, 0.2702, 0.2733,\n",
       "             0.2764, 0.2795, 0.2826, 0.2857, 0.2888, 0.2919, 0.2950, 0.2981, 0.3012,\n",
       "             0.3043, 0.3075, 0.3106, 0.3137, 0.3168, 0.3199, 0.3230, 0.3261, 0.3292,\n",
       "             0.3323, 0.3354, 0.3385, 0.3416, 0.3447, 0.3478, 0.3509, 0.3540, 0.3571,\n",
       "             0.3602, 0.3634, 0.3665, 0.3696, 0.3727, 0.3758, 0.3789, 0.3820, 0.3851,\n",
       "             0.3882, 0.3913, 0.3944, 0.3975, 0.4006, 0.4037, 0.4068, 0.4099, 0.4130,\n",
       "             0.4161, 0.4193, 0.4224, 0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4410,\n",
       "             0.4441, 0.4472, 0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689,\n",
       "             0.4720, 0.4720, 0.4752, 0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938,\n",
       "             0.4969, 0.5000, 0.5031, 0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217,\n",
       "             0.5248, 0.5280, 0.5311, 0.5342, 0.5373, 0.5404, 0.5435, 0.5466, 0.5497,\n",
       "             0.5528, 0.5559, 0.5590, 0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776,\n",
       "             0.5807, 0.5839, 0.5870, 0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056,\n",
       "             0.6087, 0.6118, 0.6149, 0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335,\n",
       "             0.6366, 0.6398, 0.6429, 0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615,\n",
       "             0.6646, 0.6677, 0.6708, 0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894,\n",
       "             0.6925, 0.6957, 0.6988, 0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174,\n",
       "             0.7205, 0.7236, 0.7267, 0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453,\n",
       "             0.7484, 0.7516, 0.7547, 0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733,\n",
       "             0.7764, 0.7795, 0.7826, 0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012,\n",
       "             0.8043, 0.8075, 0.8106, 0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292,\n",
       "             0.8323, 0.8354, 0.8385, 0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571,\n",
       "             0.8602, 0.8634, 0.8665, 0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851,\n",
       "             0.8882, 0.8913, 0.8944, 0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130,\n",
       "             0.9161, 0.9193, 0.9224, 0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410,\n",
       "             0.9441, 0.9472, 0.9503, 0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689,\n",
       "             0.9720, 0.9752, 0.9783, 0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0702, 0.1146, 0.1520, 0.1813, 0.2035, 0.2281, 0.2456, 0.2620,\n",
       "             0.2784, 0.2865, 0.2936, 0.3053, 0.3170, 0.3275, 0.3357, 0.3439, 0.3497,\n",
       "             0.3556, 0.3602, 0.3649, 0.3684, 0.3754, 0.3848, 0.3871, 0.3953, 0.4012,\n",
       "             0.4070, 0.4117, 0.4140, 0.4164, 0.4187, 0.4222, 0.4246, 0.4292, 0.4316,\n",
       "             0.4351, 0.4398, 0.4433, 0.4491, 0.4526, 0.4561, 0.4596, 0.4632, 0.4655,\n",
       "             0.4667, 0.4713, 0.4737, 0.4772, 0.4807, 0.4854, 0.4901, 0.4936, 0.4947,\n",
       "             0.4971, 0.4982, 0.5006, 0.5018, 0.5053, 0.5076, 0.5088, 0.5111, 0.5146,\n",
       "             0.5158, 0.5181, 0.5216, 0.5228, 0.5240, 0.5251, 0.5275, 0.5287, 0.5322,\n",
       "             0.5333, 0.5357, 0.5368, 0.5380, 0.5404, 0.5415, 0.5439, 0.5450, 0.5462,\n",
       "             0.5485, 0.5544, 0.5567, 0.5591, 0.5614, 0.5637, 0.5673, 0.5696, 0.5719,\n",
       "             0.5731, 0.5754, 0.5766, 0.5778, 0.5801, 0.5813, 0.5848, 0.5871, 0.5883,\n",
       "             0.5906, 0.5930, 0.5942, 0.5965, 0.5977, 0.5988, 0.6000, 0.6035, 0.6058,\n",
       "             0.6070, 0.6094, 0.6105, 0.6117, 0.6140, 0.6164, 0.6175, 0.6187, 0.6211,\n",
       "             0.6234, 0.6246, 0.6257, 0.6281, 0.6292, 0.6316, 0.6327, 0.6351, 0.6363,\n",
       "             0.6374, 0.6409, 0.6421, 0.6444, 0.6480, 0.6491, 0.6503, 0.6515, 0.6538,\n",
       "             0.6550, 0.6561, 0.6573, 0.6596, 0.6608, 0.6620, 0.6632, 0.6643, 0.6655,\n",
       "             0.6667, 0.6690, 0.6702, 0.6713, 0.6725, 0.6737, 0.6749, 0.6772, 0.6784,\n",
       "             0.6795, 0.6807, 0.6819, 0.6842, 0.6854, 0.6865, 0.6877, 0.6901, 0.6924,\n",
       "             0.6936, 0.6947, 0.6971, 0.6982, 0.7006, 0.7018, 0.7029, 0.7041, 0.7076,\n",
       "             0.7088, 0.7111, 0.7123, 0.7135, 0.7146, 0.7158, 0.7170, 0.7181, 0.7193,\n",
       "             0.7205, 0.7216, 0.7228, 0.7251, 0.7263, 0.7287, 0.7298, 0.7310, 0.7322,\n",
       "             0.7333, 0.7345, 0.7357, 0.7368, 0.7380, 0.7392, 0.7415, 0.7427, 0.7439,\n",
       "             0.7450, 0.7462, 0.7474, 0.7485, 0.7497, 0.7509, 0.7520, 0.7532, 0.7544,\n",
       "             0.7556, 0.7567, 0.7579, 0.7591, 0.7602, 0.7614, 0.7637, 0.7637, 0.7649,\n",
       "             0.7661, 0.7673, 0.7673, 0.7684, 0.7696, 0.7708, 0.7719, 0.7731, 0.7743,\n",
       "             0.7754, 0.7766, 0.7778, 0.7789, 0.7801, 0.7813, 0.7825, 0.7836, 0.7860,\n",
       "             0.7871, 0.7883, 0.7895, 0.7906, 0.7918, 0.7930, 0.7942, 0.7953, 0.7965,\n",
       "             0.7977, 0.7988, 0.8000, 0.8000, 0.8012, 0.8023, 0.8035, 0.8047, 0.8058,\n",
       "             0.8082, 0.8094, 0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164, 0.8175,\n",
       "             0.8187, 0.8199, 0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269, 0.8281,\n",
       "             0.8292, 0.8304, 0.8316, 0.8327, 0.8339, 0.8363, 0.8374, 0.8386, 0.8409,\n",
       "             0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480, 0.8491, 0.8515, 0.8538,\n",
       "             0.8550, 0.8561, 0.8573, 0.8585, 0.8596, 0.8608, 0.8620, 0.8620, 0.8632,\n",
       "             0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702, 0.8713, 0.8725, 0.8737,\n",
       "             0.8749, 0.8760, 0.8772, 0.8772, 0.8784, 0.8795, 0.8807, 0.8819, 0.8830,\n",
       "             0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912, 0.8924, 0.8936,\n",
       "             0.8947, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029, 0.9041,\n",
       "             0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135, 0.9146,\n",
       "             0.9158, 0.9170, 0.9181, 0.9193, 0.9205, 0.9216, 0.9228, 0.9240, 0.9251,\n",
       "             0.9263, 0.9275, 0.9287, 0.9298, 0.9310, 0.9322, 0.9333, 0.9345, 0.9357,\n",
       "             0.9368, 0.9380, 0.9392, 0.9404, 0.9415, 0.9427, 0.9439, 0.9450, 0.9462,\n",
       "             0.9474, 0.9485, 0.9497, 0.9509, 0.9520, 0.9532, 0.9544, 0.9556, 0.9556,\n",
       "             0.9556, 0.9567, 0.9579, 0.9591, 0.9602, 0.9614, 0.9626, 0.9637, 0.9637,\n",
       "             0.9649, 0.9661, 0.9673, 0.9673, 0.9684, 0.9696, 0.9708, 0.9719, 0.9731,\n",
       "             0.9731, 0.9731, 0.9731, 0.9731, 0.9731, 0.9743, 0.9743, 0.9743, 0.9743,\n",
       "             0.9754, 0.9754, 0.9766, 0.9778, 0.9789, 0.9789, 0.9801, 0.9813, 0.9813,\n",
       "             0.9825, 0.9825, 0.9825, 0.9836, 0.9848, 0.9848, 0.9860, 0.9871, 0.9871,\n",
       "             0.9871, 0.9883, 0.9895, 0.9895, 0.9895, 0.9895, 0.9895, 0.9906, 0.9906,\n",
       "             0.9918, 0.9918, 0.9930, 0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9974e-01,\n",
       "             9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9967e-01, 9.9967e-01, 9.9966e-01,\n",
       "             9.9966e-01, 9.9964e-01, 9.9962e-01, 9.9962e-01, 9.9961e-01, 9.9960e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9956e-01, 9.9953e-01, 9.9953e-01, 9.9953e-01,\n",
       "             9.9953e-01, 9.9952e-01, 9.9950e-01, 9.9948e-01, 9.9948e-01, 9.9946e-01,\n",
       "             9.9944e-01, 9.9941e-01, 9.9940e-01, 9.9939e-01, 9.9937e-01, 9.9933e-01,\n",
       "             9.9933e-01, 9.9931e-01, 9.9930e-01, 9.9928e-01, 9.9925e-01, 9.9912e-01,\n",
       "             9.9911e-01, 9.9909e-01, 9.9905e-01, 9.9902e-01, 9.9901e-01, 9.9898e-01,\n",
       "             9.9897e-01, 9.9894e-01, 9.9889e-01, 9.9863e-01, 9.9851e-01, 9.9840e-01,\n",
       "             9.9808e-01, 9.9792e-01, 9.9771e-01, 9.9742e-01, 9.9734e-01, 9.9733e-01,\n",
       "             9.9722e-01, 9.9675e-01, 9.9667e-01, 9.9648e-01, 9.9648e-01, 9.9619e-01,\n",
       "             9.9521e-01, 9.9485e-01, 9.9398e-01, 9.9348e-01, 9.9331e-01, 9.9176e-01,\n",
       "             9.9127e-01, 9.9086e-01, 9.9064e-01, 9.9055e-01, 9.9045e-01, 9.9033e-01,\n",
       "             9.8991e-01, 9.8990e-01, 9.8970e-01, 9.8969e-01, 9.8872e-01, 9.8745e-01,\n",
       "             9.8418e-01, 9.8400e-01, 9.8076e-01, 9.7691e-01, 9.7610e-01, 9.7309e-01,\n",
       "             9.7008e-01, 9.6915e-01, 9.6643e-01, 9.6264e-01, 9.6111e-01, 9.6104e-01,\n",
       "             9.6030e-01, 9.5655e-01, 9.4843e-01, 9.4010e-01, 9.3323e-01, 9.2804e-01,\n",
       "             9.2103e-01, 9.1534e-01, 9.0595e-01, 9.0586e-01, 8.9822e-01, 8.2039e-01,\n",
       "             8.0339e-01, 7.9290e-01, 7.7936e-01, 7.7585e-01, 7.4083e-01, 7.2917e-01,\n",
       "             7.2477e-01, 7.1995e-01, 7.1863e-01, 7.0122e-01, 6.7885e-01, 6.4721e-01,\n",
       "             6.4587e-01, 6.0774e-01, 5.2762e-01, 4.9415e-01, 4.7062e-01, 4.6448e-01,\n",
       "             3.7526e-01, 3.7366e-01, 3.2093e-01, 2.4752e-01, 2.3327e-01, 1.9566e-01,\n",
       "             1.9548e-01, 1.9357e-01, 1.8723e-01, 1.8512e-01, 1.6990e-01, 1.6064e-01,\n",
       "             1.5320e-01, 1.4431e-01, 1.3224e-01, 1.2423e-01, 1.1806e-01, 1.0216e-01,\n",
       "             9.9304e-02, 9.4088e-02, 9.2573e-02, 9.0637e-02, 8.7438e-02, 8.0440e-02,\n",
       "             7.3338e-02, 7.1258e-02, 6.9927e-02, 6.8470e-02, 6.4102e-02, 6.3445e-02,\n",
       "             6.2413e-02, 6.1455e-02, 6.1008e-02, 5.2117e-02, 4.8495e-02, 4.8455e-02,\n",
       "             4.6697e-02, 4.0245e-02, 3.9506e-02, 3.7011e-02, 3.5734e-02, 3.2748e-02,\n",
       "             3.2424e-02, 3.2129e-02, 3.1507e-02, 3.0637e-02, 2.8133e-02, 2.5197e-02,\n",
       "             2.4834e-02, 2.3590e-02, 2.3214e-02, 2.1111e-02, 2.0012e-02, 1.8084e-02,\n",
       "             1.7801e-02, 1.5759e-02, 1.5528e-02, 1.5513e-02, 1.3313e-02, 1.3307e-02,\n",
       "             1.2056e-02, 1.1787e-02, 1.1679e-02, 1.1628e-02, 1.1269e-02, 1.1041e-02,\n",
       "             1.0838e-02, 1.0516e-02, 9.2187e-03, 8.3959e-03, 8.0289e-03, 7.9882e-03,\n",
       "             7.8948e-03, 7.4960e-03, 6.9939e-03, 6.7101e-03, 6.2653e-03, 5.9041e-03,\n",
       "             5.5621e-03, 5.2729e-03, 5.1650e-03, 4.6244e-03, 4.5796e-03, 4.4222e-03,\n",
       "             4.2776e-03, 4.2535e-03, 4.2098e-03, 4.1089e-03, 3.8131e-03, 3.4957e-03,\n",
       "             3.3570e-03, 3.0788e-03, 3.0523e-03, 2.7258e-03, 2.5992e-03, 2.5346e-03,\n",
       "             2.5240e-03, 2.4655e-03, 2.3805e-03, 2.3296e-03, 2.1767e-03, 2.0197e-03,\n",
       "             1.9827e-03, 1.8749e-03, 1.8494e-03, 1.5683e-03, 1.5622e-03, 1.4809e-03,\n",
       "             1.4247e-03, 1.2551e-03, 1.2005e-03, 1.1621e-03, 1.0572e-03, 1.0502e-03,\n",
       "             1.0436e-03, 9.7898e-04, 9.6862e-04, 9.4402e-04, 9.2912e-04, 9.0861e-04,\n",
       "             7.8332e-04, 7.5445e-04, 7.3559e-04, 6.9711e-04, 6.7643e-04, 6.6384e-04,\n",
       "             6.5225e-04, 6.3411e-04, 6.2184e-04, 5.8800e-04, 5.6021e-04, 5.0001e-04,\n",
       "             4.8860e-04, 4.5942e-04, 4.5760e-04, 4.1313e-04, 4.0806e-04, 3.9620e-04,\n",
       "             3.9514e-04, 3.9281e-04, 3.6979e-04, 3.4874e-04, 3.2872e-04, 3.2726e-04,\n",
       "             3.1227e-04, 3.0896e-04, 3.0404e-04, 2.8220e-04, 2.7405e-04, 2.4840e-04,\n",
       "             2.4734e-04, 2.3485e-04, 2.3255e-04, 2.1873e-04, 2.1552e-04, 2.0717e-04,\n",
       "             1.9784e-04, 1.8448e-04, 1.7236e-04, 1.6037e-04, 1.5910e-04, 1.5679e-04,\n",
       "             1.5334e-04, 1.4623e-04, 1.4359e-04, 1.4306e-04, 1.3501e-04, 1.2408e-04,\n",
       "             1.2212e-04, 1.2060e-04, 1.1393e-04, 1.1258e-04, 8.8704e-05, 8.5756e-05,\n",
       "             8.3212e-05, 7.6273e-05, 7.6158e-05, 7.6158e-05, 7.2116e-05, 6.9937e-05,\n",
       "             6.8471e-05, 6.8124e-05, 6.7853e-05, 6.3841e-05, 6.1188e-05, 5.0184e-05,\n",
       "             4.9545e-05, 4.8682e-05, 4.4902e-05, 4.3442e-05, 4.1302e-05, 4.0886e-05,\n",
       "             4.0363e-05, 4.0359e-05, 3.9248e-05, 3.9228e-05, 3.8600e-05, 3.6987e-05,\n",
       "             3.6651e-05, 3.1628e-05, 3.1582e-05, 2.9110e-05, 2.6033e-05, 2.5320e-05,\n",
       "             2.5135e-05, 2.4627e-05, 2.2220e-05, 2.1501e-05, 1.8835e-05, 1.8551e-05,\n",
       "             1.8473e-05, 1.8434e-05, 1.6160e-05, 1.2801e-05, 1.2449e-05, 1.2412e-05,\n",
       "             1.2310e-05, 1.2226e-05, 1.1033e-05, 1.0836e-05, 1.0087e-05, 9.7774e-06,\n",
       "             8.9031e-06, 8.8440e-06, 8.6992e-06, 8.6008e-06, 8.0304e-06, 6.8588e-06,\n",
       "             6.8440e-06, 6.2252e-06, 5.3562e-06, 4.7625e-06, 4.4356e-06, 3.9464e-06,\n",
       "             3.9299e-06, 3.8445e-06, 3.7558e-06, 3.4928e-06, 3.2070e-06, 3.1365e-06,\n",
       "             3.1219e-06, 3.0256e-06, 2.7174e-06, 2.7046e-06, 2.6634e-06, 2.5008e-06,\n",
       "             2.4971e-06, 2.3053e-06, 2.2337e-06, 2.1830e-06, 2.1561e-06, 1.5681e-06,\n",
       "             9.9449e-07, 9.6866e-07, 9.0567e-07, 8.7263e-07, 7.0394e-07, 6.2281e-07,\n",
       "             4.7540e-07, 4.5872e-07, 4.3515e-07, 3.7700e-07, 3.7243e-07, 3.0021e-07,\n",
       "             2.7268e-07, 2.0363e-07, 1.5338e-07, 1.5234e-07, 1.0633e-07, 1.0049e-07,\n",
       "             6.7709e-08, 5.6624e-08, 5.5146e-08, 4.1702e-08, 3.3875e-08, 2.9401e-08,\n",
       "             1.2102e-08, 1.1202e-08, 5.4592e-09, 4.0980e-09])}},\n",
       "   {'fpr': np.float64(0.049689440993788817),\n",
       "    'tpr': np.float64(0.9812865497076023),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0047,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.6908e-10, 1.1577e-10,\n",
       "             1.1370e-10])}},\n",
       "   {'fpr': np.float64(0.024844720496894408),\n",
       "    'tpr': np.float64(0.9625730994152046),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0186, 0.0186, 0.0217, 0.0217,\n",
       "             0.0217, 0.0248, 0.0248, 0.0248, 0.0248, 0.0280, 0.0280, 0.0311, 0.0311,\n",
       "             0.0342, 0.0373, 0.0404, 0.0404, 0.0404, 0.0404, 0.0404, 0.0435, 0.0435,\n",
       "             0.0435, 0.0435, 0.0435, 0.0435, 0.0466, 0.0466, 0.0466, 0.0497, 0.0497,\n",
       "             0.0528, 0.0559, 0.0559, 0.0559, 0.0621, 0.0652, 0.0652, 0.0683, 0.0714,\n",
       "             0.0745, 0.0776, 0.0807, 0.0839, 0.0870, 0.0901, 0.0901, 0.0901, 0.0932,\n",
       "             0.0932, 0.0963, 0.0994, 0.1025, 0.1025, 0.1056, 0.1056, 0.1087, 0.1118,\n",
       "             0.1118, 0.1149, 0.1180, 0.1211, 0.1242, 0.1242, 0.1242, 0.1273, 0.1304,\n",
       "             0.1335, 0.1335, 0.1366, 0.1398, 0.1429, 0.1460, 0.1491, 0.1522, 0.1553,\n",
       "             0.1584, 0.1615, 0.1646, 0.1677, 0.1708, 0.1739, 0.1739, 0.1739, 0.1770,\n",
       "             0.1801, 0.1832, 0.1863, 0.1894, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050,\n",
       "             0.2081, 0.2112, 0.2143, 0.2174, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329,\n",
       "             0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578,\n",
       "             0.2609, 0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857,\n",
       "             0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137,\n",
       "             0.3168, 0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416,\n",
       "             0.3447, 0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696,\n",
       "             0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975,\n",
       "             0.4006, 0.4037, 0.4068, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224, 0.4255,\n",
       "             0.4286, 0.4317, 0.4348, 0.4379, 0.4410, 0.4441, 0.4472, 0.4503, 0.4534,\n",
       "             0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752, 0.4783, 0.4814,\n",
       "             0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031, 0.5062, 0.5093,\n",
       "             0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311, 0.5342, 0.5373,\n",
       "             0.5404, 0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590, 0.5621, 0.5652,\n",
       "             0.5683, 0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870, 0.5901, 0.5932,\n",
       "             0.5963, 0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149, 0.6180, 0.6211,\n",
       "             0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429, 0.6460, 0.6491,\n",
       "             0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708, 0.6739, 0.6770,\n",
       "             0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988, 0.7019, 0.7050,\n",
       "             0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267, 0.7298, 0.7329,\n",
       "             0.7360, 0.7391, 0.7422, 0.7453, 0.7484, 0.7484, 0.7516, 0.7547, 0.7578,\n",
       "             0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826, 0.7857,\n",
       "             0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106, 0.8137,\n",
       "             0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385, 0.8416,\n",
       "             0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665, 0.8696,\n",
       "             0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944, 0.8975,\n",
       "             0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224, 0.9255,\n",
       "             0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503, 0.9534,\n",
       "             0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783, 0.9814,\n",
       "             0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.2199, 0.2947, 0.3322, 0.3673, 0.3813, 0.3953, 0.4070, 0.4152,\n",
       "             0.4246, 0.4327, 0.4398, 0.4515, 0.4573, 0.4632, 0.4702, 0.4795, 0.4854,\n",
       "             0.4901, 0.4959, 0.5029, 0.5123, 0.5170, 0.5193, 0.5251, 0.5287, 0.5310,\n",
       "             0.5345, 0.5392, 0.5415, 0.5427, 0.5439, 0.5474, 0.5485, 0.5497, 0.5520,\n",
       "             0.5567, 0.5579, 0.5591, 0.5614, 0.5626, 0.5684, 0.5719, 0.5731, 0.5743,\n",
       "             0.5766, 0.5801, 0.5813, 0.5825, 0.5836, 0.5871, 0.5883, 0.5895, 0.5906,\n",
       "             0.5942, 0.5953, 0.5965, 0.5977, 0.5988, 0.6000, 0.6012, 0.6023, 0.6047,\n",
       "             0.6070, 0.6082, 0.6105, 0.6117, 0.6129, 0.6140, 0.6175, 0.6187, 0.6199,\n",
       "             0.6234, 0.6246, 0.6257, 0.6281, 0.6292, 0.6304, 0.6327, 0.6339, 0.6351,\n",
       "             0.6363, 0.6386, 0.6398, 0.6409, 0.6433, 0.6444, 0.6468, 0.6480, 0.6491,\n",
       "             0.6503, 0.6515, 0.6538, 0.6550, 0.6561, 0.6585, 0.6608, 0.6632, 0.6643,\n",
       "             0.6655, 0.6678, 0.6690, 0.6702, 0.6713, 0.6737, 0.6749, 0.6760, 0.6772,\n",
       "             0.6795, 0.6807, 0.6819, 0.6830, 0.6842, 0.6854, 0.6865, 0.6889, 0.6901,\n",
       "             0.6912, 0.6924, 0.6936, 0.6947, 0.6959, 0.6971, 0.6982, 0.6994, 0.7006,\n",
       "             0.7018, 0.7041, 0.7053, 0.7064, 0.7088, 0.7099, 0.7111, 0.7123, 0.7135,\n",
       "             0.7146, 0.7158, 0.7170, 0.7181, 0.7193, 0.7216, 0.7228, 0.7240, 0.7251,\n",
       "             0.7263, 0.7275, 0.7287, 0.7298, 0.7310, 0.7322, 0.7333, 0.7345, 0.7357,\n",
       "             0.7368, 0.7380, 0.7392, 0.7404, 0.7415, 0.7427, 0.7439, 0.7450, 0.7462,\n",
       "             0.7474, 0.7485, 0.7497, 0.7509, 0.7520, 0.7532, 0.7544, 0.7556, 0.7567,\n",
       "             0.7579, 0.7591, 0.7602, 0.7614, 0.7626, 0.7637, 0.7649, 0.7661, 0.7673,\n",
       "             0.7684, 0.7696, 0.7708, 0.7719, 0.7731, 0.7743, 0.7754, 0.7766, 0.7778,\n",
       "             0.7801, 0.7813, 0.7825, 0.7836, 0.7848, 0.7860, 0.7871, 0.7883, 0.7895,\n",
       "             0.7906, 0.7918, 0.7930, 0.7942, 0.7953, 0.7965, 0.7977, 0.7988, 0.8000,\n",
       "             0.8012, 0.8023, 0.8035, 0.8047, 0.8058, 0.8070, 0.8082, 0.8082, 0.8094,\n",
       "             0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164, 0.8175, 0.8187, 0.8199,\n",
       "             0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269, 0.8281, 0.8292, 0.8304,\n",
       "             0.8316, 0.8327, 0.8339, 0.8351, 0.8363, 0.8374, 0.8386, 0.8398, 0.8409,\n",
       "             0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480, 0.8491, 0.8503, 0.8515,\n",
       "             0.8526, 0.8538, 0.8550, 0.8561, 0.8573, 0.8585, 0.8596, 0.8608, 0.8620,\n",
       "             0.8632, 0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702, 0.8713, 0.8725,\n",
       "             0.8737, 0.8749, 0.8760, 0.8772, 0.8784, 0.8795, 0.8807, 0.8819, 0.8830,\n",
       "             0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912, 0.8924, 0.8936,\n",
       "             0.8947, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029, 0.9041,\n",
       "             0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135, 0.9146,\n",
       "             0.9158, 0.9170, 0.9170, 0.9181, 0.9193, 0.9205, 0.9216, 0.9216, 0.9228,\n",
       "             0.9240, 0.9251, 0.9263, 0.9275, 0.9287, 0.9298, 0.9310, 0.9322, 0.9333,\n",
       "             0.9345, 0.9357, 0.9368, 0.9380, 0.9392, 0.9404, 0.9415, 0.9415, 0.9427,\n",
       "             0.9439, 0.9450, 0.9462, 0.9474, 0.9485, 0.9497, 0.9509, 0.9520, 0.9532,\n",
       "             0.9532, 0.9544, 0.9556, 0.9567, 0.9579, 0.9579, 0.9591, 0.9591, 0.9602,\n",
       "             0.9614, 0.9614, 0.9626, 0.9637, 0.9649, 0.9649, 0.9661, 0.9661, 0.9673,\n",
       "             0.9673, 0.9673, 0.9673, 0.9684, 0.9696, 0.9708, 0.9719, 0.9719, 0.9731,\n",
       "             0.9743, 0.9754, 0.9766, 0.9778, 0.9778, 0.9789, 0.9801, 0.9801, 0.9813,\n",
       "             0.9813, 0.9813, 0.9825, 0.9836, 0.9836, 0.9836, 0.9848, 0.9848, 0.9848,\n",
       "             0.9848, 0.9848, 0.9848, 0.9848, 0.9848, 0.9848, 0.9860, 0.9871, 0.9871,\n",
       "             0.9883, 0.9883, 0.9883, 0.9883, 0.9895, 0.9895, 0.9906, 0.9906, 0.9906,\n",
       "             0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9930, 0.9942, 0.9942, 0.9942,\n",
       "             0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953,\n",
       "             0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9965, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01, 9.9967e-01,\n",
       "             9.9966e-01, 9.9966e-01, 9.9964e-01, 9.9964e-01, 9.9964e-01, 9.9964e-01,\n",
       "             9.9963e-01, 9.9960e-01, 9.9960e-01, 9.9960e-01, 9.9958e-01, 9.9958e-01,\n",
       "             9.9956e-01, 9.9955e-01, 9.9952e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01,\n",
       "             9.9949e-01, 9.9947e-01, 9.9946e-01, 9.9944e-01, 9.9944e-01, 9.9943e-01,\n",
       "             9.9943e-01, 9.9939e-01, 9.9939e-01, 9.9938e-01, 9.9937e-01, 9.9937e-01,\n",
       "             9.9937e-01, 9.9936e-01, 9.9935e-01, 9.9932e-01, 9.9928e-01, 9.9924e-01,\n",
       "             9.9924e-01, 9.9922e-01, 9.9920e-01, 9.9917e-01, 9.9917e-01, 9.9916e-01,\n",
       "             9.9912e-01, 9.9901e-01, 9.9899e-01, 9.9899e-01, 9.9898e-01, 9.9891e-01,\n",
       "             9.9880e-01, 9.9879e-01, 9.9878e-01, 9.9874e-01, 9.9871e-01, 9.9859e-01,\n",
       "             9.9858e-01, 9.9857e-01, 9.9856e-01, 9.9855e-01, 9.9850e-01, 9.9850e-01,\n",
       "             9.9846e-01, 9.9824e-01, 9.9813e-01, 9.9804e-01, 9.9804e-01, 9.9800e-01,\n",
       "             9.9769e-01, 9.9763e-01, 9.9749e-01, 9.9695e-01, 9.9679e-01, 9.9663e-01,\n",
       "             9.9656e-01, 9.9636e-01, 9.9622e-01, 9.9602e-01, 9.9588e-01, 9.9581e-01,\n",
       "             9.9561e-01, 9.9506e-01, 9.9489e-01, 9.9485e-01, 9.9438e-01, 9.9390e-01,\n",
       "             9.9309e-01, 9.9115e-01, 9.8982e-01, 9.8945e-01, 9.8927e-01, 9.8886e-01,\n",
       "             9.8807e-01, 9.8765e-01, 9.8619e-01, 9.8519e-01, 9.8488e-01, 9.8459e-01,\n",
       "             9.8125e-01, 9.7978e-01, 9.7854e-01, 9.7459e-01, 9.7214e-01, 9.7100e-01,\n",
       "             9.5879e-01, 9.5249e-01, 9.4223e-01, 9.3883e-01, 9.2238e-01, 8.9377e-01,\n",
       "             8.8712e-01, 8.7001e-01, 8.6169e-01, 8.5482e-01, 8.5128e-01, 8.4625e-01,\n",
       "             8.3321e-01, 7.4329e-01, 7.1342e-01, 6.9087e-01, 6.8968e-01, 6.8869e-01,\n",
       "             5.9620e-01, 5.9043e-01, 5.8882e-01, 5.6096e-01, 5.4645e-01, 5.2808e-01,\n",
       "             4.9399e-01, 4.8397e-01, 4.4516e-01, 4.3210e-01, 3.8514e-01, 3.7327e-01,\n",
       "             3.6647e-01, 3.1820e-01, 2.8244e-01, 2.7387e-01, 2.5822e-01, 2.3351e-01,\n",
       "             2.1121e-01, 1.8512e-01, 1.7332e-01, 1.4859e-01, 1.4741e-01, 1.3658e-01,\n",
       "             1.3160e-01, 8.7399e-02, 8.5643e-02, 8.4207e-02, 8.3852e-02, 7.3874e-02,\n",
       "             7.1828e-02, 7.1108e-02, 6.4114e-02, 2.0555e-02, 1.7006e-02, 1.1338e-02,\n",
       "             1.0861e-02, 1.0218e-02, 1.0093e-02, 9.4810e-03, 9.2038e-03, 6.9609e-03,\n",
       "             6.8632e-03, 5.4629e-03, 4.0514e-03, 3.6236e-03, 2.7599e-03, 2.7198e-03,\n",
       "             2.4963e-03, 2.3744e-03, 2.3660e-03, 1.8366e-03, 1.5576e-03, 1.5351e-03,\n",
       "             1.0059e-03, 8.4018e-04, 5.4894e-04, 5.3065e-04, 3.4762e-04, 2.8891e-04,\n",
       "             2.0172e-04, 1.9667e-04, 1.9541e-04, 1.9154e-04, 1.4127e-04, 1.1453e-04,\n",
       "             1.1125e-04, 8.7960e-05, 6.6651e-05, 6.5621e-05, 6.3718e-05, 6.2754e-05,\n",
       "             5.6882e-05, 4.5279e-05, 4.3804e-05, 3.9136e-05, 3.8264e-05, 3.6171e-05,\n",
       "             2.9058e-05, 2.8253e-05, 2.3588e-05, 2.3295e-05, 1.7672e-05, 1.6979e-05,\n",
       "             1.6827e-05, 1.1412e-05, 1.0411e-05, 8.3529e-06, 7.9781e-06, 6.8888e-06,\n",
       "             6.8548e-06, 6.2552e-06, 5.9188e-06, 5.7607e-06, 4.5956e-06, 3.9626e-06,\n",
       "             3.9451e-06, 3.7434e-06, 3.5788e-06, 2.9749e-06, 2.6925e-06, 2.5224e-06,\n",
       "             2.5069e-06, 2.1091e-06, 1.9132e-06, 1.8314e-06, 1.6307e-06, 1.4544e-06,\n",
       "             1.3544e-06, 1.1646e-06, 1.1163e-06, 1.0402e-06, 1.0321e-06, 9.2471e-07,\n",
       "             9.0520e-07, 8.5011e-07, 7.0312e-07, 6.9550e-07, 6.8458e-07, 6.6047e-07,\n",
       "             6.5476e-07, 6.4768e-07, 5.4162e-07, 5.3646e-07, 5.0525e-07, 4.6344e-07,\n",
       "             4.5294e-07, 4.1586e-07, 4.0503e-07, 4.0189e-07, 3.7007e-07, 3.3643e-07,\n",
       "             3.0727e-07, 2.6859e-07, 2.4986e-07, 2.1514e-07, 2.1232e-07, 1.8010e-07,\n",
       "             1.7582e-07, 1.7115e-07, 1.5130e-07, 1.2453e-07, 1.2379e-07, 1.0541e-07,\n",
       "             1.0494e-07, 8.0588e-08, 8.0543e-08, 7.5449e-08, 7.1513e-08, 6.7391e-08,\n",
       "             5.9522e-08, 5.7161e-08, 4.9180e-08, 4.5189e-08, 3.8383e-08, 3.4527e-08,\n",
       "             3.1759e-08, 3.0051e-08, 2.7791e-08, 2.3861e-08, 2.2406e-08, 1.9448e-08,\n",
       "             1.8990e-08, 1.7096e-08, 1.6984e-08, 1.5201e-08, 1.5105e-08, 1.4879e-08,\n",
       "             1.3586e-08, 1.3506e-08, 1.2699e-08, 1.2144e-08, 1.1245e-08, 1.1128e-08,\n",
       "             1.1048e-08, 7.7469e-09, 7.5692e-09, 7.2357e-09, 6.8173e-09, 6.6823e-09,\n",
       "             6.2322e-09, 6.2306e-09, 6.0154e-09, 5.4115e-09, 5.3944e-09, 5.0181e-09,\n",
       "             4.5178e-09, 4.2152e-09, 4.0449e-09, 3.7003e-09, 3.4506e-09, 3.0531e-09,\n",
       "             3.0296e-09, 2.9738e-09, 2.8191e-09, 2.6937e-09, 2.6585e-09, 2.4881e-09,\n",
       "             2.2591e-09, 2.2123e-09, 2.1504e-09, 2.0526e-09, 1.9688e-09, 1.9463e-09,\n",
       "             1.9208e-09, 1.5275e-09, 1.4027e-09, 1.1968e-09, 1.1857e-09, 1.1686e-09,\n",
       "             1.0184e-09, 9.3239e-10, 9.2102e-10, 8.8411e-10, 8.3967e-10, 7.2878e-10,\n",
       "             6.8458e-10, 6.3693e-10, 5.9220e-10, 5.8782e-10, 5.8725e-10, 5.7672e-10,\n",
       "             5.3212e-10, 4.7535e-10, 4.6048e-10, 4.4851e-10, 4.4605e-10, 4.3769e-10,\n",
       "             3.9835e-10, 3.9275e-10, 3.6789e-10, 3.3907e-10, 3.1478e-10, 3.1226e-10,\n",
       "             3.0663e-10, 2.9064e-10, 2.7265e-10, 2.7194e-10, 2.6177e-10, 2.4158e-10,\n",
       "             2.3722e-10, 2.2857e-10, 2.2607e-10, 2.2434e-10, 1.9313e-10, 1.8799e-10,\n",
       "             1.8795e-10, 1.6272e-10, 1.4340e-10, 1.3400e-10, 1.3046e-10, 1.1769e-10,\n",
       "             1.1226e-10, 1.0291e-10, 9.7018e-11, 9.5419e-11, 7.4549e-11, 7.1441e-11,\n",
       "             6.2949e-11, 5.6079e-11, 5.3517e-11, 5.1983e-11, 5.1222e-11, 4.8505e-11,\n",
       "             4.6338e-11, 4.2978e-11, 4.1657e-11, 3.9208e-11, 3.7615e-11, 3.7573e-11,\n",
       "             3.6683e-11, 3.4821e-11, 3.4361e-11, 3.2329e-11, 3.1474e-11, 3.0893e-11,\n",
       "             2.9129e-11, 2.8532e-11, 2.7166e-11, 2.5653e-11, 2.1989e-11, 1.9322e-11,\n",
       "             1.8670e-11, 1.8464e-11, 1.7880e-11, 1.7356e-11, 1.7063e-11, 1.7011e-11,\n",
       "             1.4461e-11, 1.4104e-11, 1.4041e-11, 1.2905e-11, 1.2447e-11, 1.2200e-11,\n",
       "             1.0124e-11, 9.8870e-12, 9.4026e-12, 7.8529e-12, 7.0735e-12, 7.0571e-12,\n",
       "             6.5709e-12, 5.9698e-12, 5.8775e-12, 5.4859e-12, 4.7793e-12, 4.7061e-12,\n",
       "             3.9108e-12, 3.1147e-12, 2.7307e-12, 2.6464e-12, 2.5216e-12, 2.2176e-12,\n",
       "             2.1963e-12, 1.8792e-12, 1.7441e-12, 1.7247e-12, 1.4578e-12, 1.4025e-12,\n",
       "             1.0591e-12, 9.7671e-13, 8.4407e-13, 5.1422e-13, 4.5371e-13, 3.8772e-13,\n",
       "             3.8517e-13, 3.4303e-13, 3.3784e-13, 3.2623e-13, 2.4517e-13, 2.0895e-13,\n",
       "             1.9274e-13, 1.0803e-13, 7.8687e-14, 6.1254e-14, 5.7251e-14, 5.3346e-14,\n",
       "             3.7853e-14, 2.8975e-14, 1.9319e-14, 1.9047e-14, 1.3243e-14, 8.9379e-15,\n",
       "             6.0585e-15, 2.9088e-15, 1.3383e-15, 1.3226e-15, 1.3158e-15, 1.0706e-15,\n",
       "             9.1079e-16, 6.4738e-16, 2.7909e-16])}},\n",
       "   {'fpr': np.float64(0.037267080745341616),\n",
       "    'tpr': np.float64(0.9695906432748538),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0186, 0.0186, 0.0217, 0.0217, 0.0217, 0.0217,\n",
       "             0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0280,\n",
       "             0.0280, 0.0280, 0.0280, 0.0280, 0.0311, 0.0342, 0.0342, 0.0342, 0.0342,\n",
       "             0.0342, 0.0373, 0.0373, 0.0373, 0.0373, 0.0404, 0.0435, 0.0435, 0.0466,\n",
       "             0.0466, 0.0497, 0.0528, 0.0528, 0.0528, 0.0528, 0.0528, 0.0528, 0.0528,\n",
       "             0.0559, 0.0559, 0.0559, 0.0590, 0.0621, 0.0652, 0.0652, 0.0652, 0.0683,\n",
       "             0.0745, 0.0776, 0.0776, 0.0776, 0.0776, 0.0807, 0.0839, 0.0839, 0.0870,\n",
       "             0.0870, 0.0870, 0.0901, 0.0932, 0.0932, 0.0932, 0.0932, 0.0963, 0.0994,\n",
       "             0.1025, 0.1056, 0.1087, 0.1087, 0.1118, 0.1149, 0.1180, 0.1211, 0.1242,\n",
       "             0.1242, 0.1273, 0.1304, 0.1335, 0.1366, 0.1398, 0.1429, 0.1460, 0.1491,\n",
       "             0.1522, 0.1522, 0.1553, 0.1584, 0.1615, 0.1646, 0.1677, 0.1708, 0.1739,\n",
       "             0.1770, 0.1801, 0.1832, 0.1863, 0.1894, 0.1925, 0.1957, 0.1988, 0.2019,\n",
       "             0.2050, 0.2081, 0.2112, 0.2143, 0.2174, 0.2205, 0.2236, 0.2267, 0.2298,\n",
       "             0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578,\n",
       "             0.2609, 0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857,\n",
       "             0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137,\n",
       "             0.3168, 0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416,\n",
       "             0.3447, 0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696,\n",
       "             0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975,\n",
       "             0.4006, 0.4037, 0.4068, 0.4099, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224,\n",
       "             0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4379, 0.4410, 0.4441, 0.4472,\n",
       "             0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752,\n",
       "             0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031,\n",
       "             0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311,\n",
       "             0.5342, 0.5373, 0.5404, 0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590,\n",
       "             0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870,\n",
       "             0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149,\n",
       "             0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429,\n",
       "             0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708,\n",
       "             0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988,\n",
       "             0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267,\n",
       "             0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453, 0.7484, 0.7516, 0.7547,\n",
       "             0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826,\n",
       "             0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106,\n",
       "             0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385,\n",
       "             0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665,\n",
       "             0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944,\n",
       "             0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224,\n",
       "             0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503,\n",
       "             0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783,\n",
       "             0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.1287, 0.1731, 0.2058, 0.2327, 0.2491, 0.2538, 0.2678, 0.2795,\n",
       "             0.2924, 0.3029, 0.3111, 0.3146, 0.3193, 0.3275, 0.3322, 0.3415, 0.3497,\n",
       "             0.3567, 0.3591, 0.3637, 0.3661, 0.3719, 0.3766, 0.3801, 0.3813, 0.3860,\n",
       "             0.3918, 0.3930, 0.3942, 0.3977, 0.4012, 0.4023, 0.4047, 0.4094, 0.4140,\n",
       "             0.4164, 0.4175, 0.4187, 0.4211, 0.4234, 0.4257, 0.4269, 0.4316, 0.4327,\n",
       "             0.4339, 0.4374, 0.4386, 0.4409, 0.4444, 0.4468, 0.4491, 0.4538, 0.4550,\n",
       "             0.4585, 0.4596, 0.4667, 0.4702, 0.4713, 0.4737, 0.4749, 0.4760, 0.4784,\n",
       "             0.4795, 0.4807, 0.4819, 0.4842, 0.4854, 0.4865, 0.4877, 0.4889, 0.4912,\n",
       "             0.4924, 0.4982, 0.4994, 0.5006, 0.5018, 0.5041, 0.5053, 0.5064, 0.5076,\n",
       "             0.5088, 0.5099, 0.5111, 0.5135, 0.5158, 0.5170, 0.5181, 0.5193, 0.5205,\n",
       "             0.5216, 0.5228, 0.5240, 0.5251, 0.5287, 0.5298, 0.5310, 0.5322, 0.5333,\n",
       "             0.5380, 0.5404, 0.5415, 0.5450, 0.5462, 0.5474, 0.5485, 0.5497, 0.5509,\n",
       "             0.5520, 0.5532, 0.5544, 0.5567, 0.5579, 0.5591, 0.5602, 0.5614, 0.5626,\n",
       "             0.5637, 0.5649, 0.5661, 0.5673, 0.5684, 0.5696, 0.5708, 0.5731, 0.5754,\n",
       "             0.5766, 0.5789, 0.5801, 0.5813, 0.5825, 0.5836, 0.5848, 0.5871, 0.5883,\n",
       "             0.5895, 0.5906, 0.5918, 0.5930, 0.5942, 0.5953, 0.5965, 0.5988, 0.6000,\n",
       "             0.6012, 0.6023, 0.6035, 0.6058, 0.6070, 0.6082, 0.6094, 0.6105, 0.6117,\n",
       "             0.6129, 0.6140, 0.6152, 0.6164, 0.6175, 0.6187, 0.6199, 0.6211, 0.6222,\n",
       "             0.6234, 0.6246, 0.6269, 0.6281, 0.6292, 0.6304, 0.6316, 0.6327, 0.6339,\n",
       "             0.6351, 0.6363, 0.6374, 0.6386, 0.6398, 0.6409, 0.6421, 0.6444, 0.6456,\n",
       "             0.6468, 0.6480, 0.6503, 0.6515, 0.6526, 0.6538, 0.6561, 0.6573, 0.6585,\n",
       "             0.6608, 0.6620, 0.6632, 0.6643, 0.6655, 0.6667, 0.6678, 0.6690, 0.6702,\n",
       "             0.6713, 0.6725, 0.6737, 0.6749, 0.6760, 0.6772, 0.6784, 0.6795, 0.6807,\n",
       "             0.6819, 0.6830, 0.6842, 0.6854, 0.6865, 0.6877, 0.6901, 0.6912, 0.6924,\n",
       "             0.6936, 0.6947, 0.6959, 0.6971, 0.6982, 0.6994, 0.7018, 0.7029, 0.7041,\n",
       "             0.7053, 0.7064, 0.7076, 0.7088, 0.7099, 0.7111, 0.7123, 0.7135, 0.7146,\n",
       "             0.7158, 0.7170, 0.7181, 0.7193, 0.7205, 0.7216, 0.7228, 0.7240, 0.7251,\n",
       "             0.7263, 0.7275, 0.7287, 0.7298, 0.7310, 0.7322, 0.7333, 0.7345, 0.7357,\n",
       "             0.7368, 0.7380, 0.7392, 0.7404, 0.7415, 0.7427, 0.7439, 0.7450, 0.7462,\n",
       "             0.7474, 0.7485, 0.7497, 0.7509, 0.7520, 0.7532, 0.7544, 0.7556, 0.7567,\n",
       "             0.7579, 0.7591, 0.7602, 0.7614, 0.7626, 0.7637, 0.7649, 0.7661, 0.7673,\n",
       "             0.7684, 0.7696, 0.7708, 0.7719, 0.7731, 0.7743, 0.7754, 0.7766, 0.7778,\n",
       "             0.7789, 0.7801, 0.7813, 0.7825, 0.7836, 0.7848, 0.7860, 0.7871, 0.7883,\n",
       "             0.7895, 0.7906, 0.7918, 0.7930, 0.7942, 0.7953, 0.7965, 0.7977, 0.7988,\n",
       "             0.8000, 0.8012, 0.8023, 0.8035, 0.8047, 0.8058, 0.8070, 0.8082, 0.8094,\n",
       "             0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164, 0.8175, 0.8187, 0.8199,\n",
       "             0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269, 0.8281, 0.8292, 0.8304,\n",
       "             0.8316, 0.8327, 0.8339, 0.8351, 0.8363, 0.8374, 0.8386, 0.8398, 0.8409,\n",
       "             0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480, 0.8491, 0.8503, 0.8515,\n",
       "             0.8526, 0.8538, 0.8550, 0.8561, 0.8573, 0.8585, 0.8596, 0.8608, 0.8620,\n",
       "             0.8632, 0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702, 0.8713, 0.8725,\n",
       "             0.8737, 0.8749, 0.8760, 0.8772, 0.8784, 0.8795, 0.8807, 0.8819, 0.8830,\n",
       "             0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912, 0.8924, 0.8936,\n",
       "             0.8947, 0.8959, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029,\n",
       "             0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135,\n",
       "             0.9146, 0.9158, 0.9170, 0.9181, 0.9181, 0.9193, 0.9205, 0.9216, 0.9228,\n",
       "             0.9240, 0.9251, 0.9251, 0.9263, 0.9275, 0.9287, 0.9298, 0.9298, 0.9310,\n",
       "             0.9322, 0.9333, 0.9345, 0.9357, 0.9368, 0.9380, 0.9392, 0.9404, 0.9404,\n",
       "             0.9415, 0.9427, 0.9439, 0.9439, 0.9450, 0.9450, 0.9462, 0.9474, 0.9485,\n",
       "             0.9485, 0.9497, 0.9509, 0.9520, 0.9532, 0.9544, 0.9556, 0.9567, 0.9567,\n",
       "             0.9579, 0.9591, 0.9602, 0.9614, 0.9614, 0.9614, 0.9626, 0.9637, 0.9649,\n",
       "             0.9661, 0.9661, 0.9673, 0.9684, 0.9696, 0.9696, 0.9696, 0.9708, 0.9708,\n",
       "             0.9719, 0.9719, 0.9719, 0.9731, 0.9743, 0.9754, 0.9766, 0.9778, 0.9789,\n",
       "             0.9789, 0.9801, 0.9813, 0.9813, 0.9813, 0.9813, 0.9825, 0.9836, 0.9836,\n",
       "             0.9836, 0.9836, 0.9848, 0.9860, 0.9871, 0.9871, 0.9871, 0.9883, 0.9883,\n",
       "             0.9895, 0.9906, 0.9906, 0.9906, 0.9918, 0.9930, 0.9942, 0.9942, 0.9942,\n",
       "             0.9942, 0.9942, 0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01,\n",
       "             9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9967e-01, 9.9967e-01, 9.9967e-01, 9.9967e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9964e-01, 9.9964e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9952e-01,\n",
       "             9.9952e-01, 9.9949e-01, 9.9946e-01, 9.9946e-01, 9.9943e-01, 9.9937e-01,\n",
       "             9.9937e-01, 9.9937e-01, 9.9936e-01, 9.9935e-01, 9.9934e-01, 9.9933e-01,\n",
       "             9.9933e-01, 9.9933e-01, 9.9932e-01, 9.9931e-01, 9.9929e-01, 9.9929e-01,\n",
       "             9.9929e-01, 9.9928e-01, 9.9925e-01, 9.9925e-01, 9.9924e-01, 9.9924e-01,\n",
       "             9.9924e-01, 9.9914e-01, 9.9912e-01, 9.9911e-01, 9.9911e-01, 9.9911e-01,\n",
       "             9.9910e-01, 9.9909e-01, 9.9907e-01, 9.9904e-01, 9.9904e-01, 9.9903e-01,\n",
       "             9.9901e-01, 9.9901e-01, 9.9900e-01, 9.9899e-01, 9.9899e-01, 9.9894e-01,\n",
       "             9.9892e-01, 9.9891e-01, 9.9890e-01, 9.9881e-01, 9.9880e-01, 9.9876e-01,\n",
       "             9.9871e-01, 9.9869e-01, 9.9868e-01, 9.9862e-01, 9.9859e-01, 9.9856e-01,\n",
       "             9.9851e-01, 9.9836e-01, 9.9834e-01, 9.9833e-01, 9.9813e-01, 9.9812e-01,\n",
       "             9.9780e-01, 9.9767e-01, 9.9749e-01, 9.9739e-01, 9.9736e-01, 9.9730e-01,\n",
       "             9.9703e-01, 9.9684e-01, 9.9644e-01, 9.9637e-01, 9.9621e-01, 9.9565e-01,\n",
       "             9.9563e-01, 9.9562e-01, 9.9523e-01, 9.9495e-01, 9.9488e-01, 9.9476e-01,\n",
       "             9.9455e-01, 9.9445e-01, 9.9358e-01, 9.9334e-01, 9.9305e-01, 9.9272e-01,\n",
       "             9.9205e-01, 9.9194e-01, 9.9193e-01, 9.9129e-01, 9.9055e-01, 9.9043e-01,\n",
       "             9.8934e-01, 9.8932e-01, 9.8752e-01, 9.8735e-01, 9.8663e-01, 9.8521e-01,\n",
       "             9.8365e-01, 9.8323e-01, 9.8297e-01, 9.8224e-01, 9.8165e-01, 9.7888e-01,\n",
       "             9.7733e-01, 9.7472e-01, 9.7057e-01, 9.6692e-01, 9.6571e-01, 9.6566e-01,\n",
       "             9.5949e-01, 9.5941e-01, 9.5627e-01, 9.5432e-01, 9.5408e-01, 9.5180e-01,\n",
       "             9.5133e-01, 9.5075e-01, 9.5029e-01, 9.4750e-01, 9.4481e-01, 9.4332e-01,\n",
       "             9.3577e-01, 9.0903e-01, 8.7078e-01, 8.6963e-01, 8.6572e-01, 8.5339e-01,\n",
       "             8.4431e-01, 8.2653e-01, 8.1046e-01, 8.0885e-01, 7.9484e-01, 7.3398e-01,\n",
       "             7.3351e-01, 7.1080e-01, 6.9687e-01, 6.9647e-01, 6.7831e-01, 6.6799e-01,\n",
       "             6.6536e-01, 6.6258e-01, 6.4771e-01, 6.2360e-01, 5.7386e-01, 5.7329e-01,\n",
       "             5.4786e-01, 5.1237e-01, 3.8948e-01, 3.2945e-01, 2.8302e-01, 2.6444e-01,\n",
       "             2.5163e-01, 1.9449e-01, 1.6960e-01, 1.2610e-01, 1.2307e-01, 1.2240e-01,\n",
       "             8.2377e-02, 7.9617e-02, 6.7875e-02, 5.8376e-02, 5.1055e-02, 4.5639e-02,\n",
       "             4.1239e-02, 3.9828e-02, 1.9723e-02, 1.3784e-02, 1.0338e-02, 9.9810e-03,\n",
       "             9.2076e-03, 6.5902e-03, 6.1816e-03, 4.7033e-03, 4.6474e-03, 3.2582e-03,\n",
       "             2.4727e-03, 2.0522e-03, 2.0069e-03, 1.8530e-03, 1.1253e-03, 1.0133e-03,\n",
       "             9.5740e-04, 6.5166e-04, 5.1912e-04, 4.8577e-04, 4.0967e-04, 3.5222e-04,\n",
       "             3.1056e-04, 2.8880e-04, 2.6389e-04, 2.3162e-04, 2.1592e-04, 2.0790e-04,\n",
       "             2.0458e-04, 1.5475e-04, 1.4757e-04, 1.3237e-04, 1.2220e-04, 9.3050e-05,\n",
       "             9.1224e-05, 7.7155e-05, 7.0172e-05, 4.5904e-05, 4.3770e-05, 3.8270e-05,\n",
       "             3.6115e-05, 3.4976e-05, 3.3970e-05, 2.7419e-05, 2.2394e-05, 1.6358e-05,\n",
       "             1.4966e-05, 1.4054e-05, 1.3395e-05, 1.3171e-05, 1.0469e-05, 9.5442e-06,\n",
       "             9.4850e-06, 9.0082e-06, 8.3090e-06, 8.0027e-06, 7.4962e-06, 5.6397e-06,\n",
       "             5.5363e-06, 4.3459e-06, 3.7830e-06, 2.3001e-06, 1.5313e-06, 1.4935e-06,\n",
       "             1.4604e-06, 1.4375e-06, 1.2601e-06, 8.5394e-07, 7.3993e-07, 6.8337e-07,\n",
       "             6.6283e-07, 6.0589e-07, 5.8810e-07, 4.9761e-07, 4.9267e-07, 4.8517e-07,\n",
       "             3.9040e-07, 3.7533e-07, 3.5296e-07, 3.4059e-07, 3.2411e-07, 3.2369e-07,\n",
       "             3.0366e-07, 2.9381e-07, 2.6069e-07, 2.4354e-07, 2.3653e-07, 2.2307e-07,\n",
       "             2.1999e-07, 1.6717e-07, 1.5718e-07, 1.4810e-07, 1.3560e-07, 1.3421e-07,\n",
       "             1.1395e-07, 1.1381e-07, 1.0138e-07, 9.2308e-08, 8.8958e-08, 8.0152e-08,\n",
       "             7.3872e-08, 6.8589e-08, 6.0627e-08, 5.7493e-08, 5.1736e-08, 5.1356e-08,\n",
       "             5.0864e-08, 4.9848e-08, 4.1111e-08, 3.2431e-08, 2.2115e-08, 1.9664e-08,\n",
       "             1.6653e-08, 1.3618e-08, 1.1900e-08, 1.0596e-08, 1.0197e-08, 9.8132e-09,\n",
       "             9.6105e-09, 9.2149e-09, 8.0859e-09, 7.1598e-09, 6.8433e-09, 5.9391e-09,\n",
       "             5.3274e-09, 5.2962e-09, 5.1820e-09, 5.1539e-09, 4.6638e-09, 4.5897e-09,\n",
       "             4.5209e-09, 4.4792e-09, 4.2905e-09, 3.5812e-09, 3.2694e-09, 2.5325e-09,\n",
       "             2.3983e-09, 2.3811e-09, 2.1005e-09, 1.9757e-09, 1.8304e-09, 1.7900e-09,\n",
       "             1.7558e-09, 1.7115e-09, 1.2674e-09, 1.1803e-09, 1.1766e-09, 1.0806e-09,\n",
       "             1.0448e-09, 1.0038e-09, 9.8270e-10, 9.6163e-10, 9.2558e-10, 8.6843e-10,\n",
       "             8.4920e-10, 8.2498e-10, 7.7608e-10, 7.6404e-10, 7.5202e-10, 7.3177e-10,\n",
       "             6.6063e-10, 6.2674e-10, 4.8089e-10, 4.8007e-10, 3.8527e-10, 3.2701e-10,\n",
       "             3.1737e-10, 2.9158e-10, 2.7978e-10, 2.6864e-10, 2.4453e-10, 2.4122e-10,\n",
       "             2.3882e-10, 2.1868e-10, 2.0962e-10, 2.0672e-10, 2.0015e-10, 1.9351e-10,\n",
       "             1.9244e-10, 1.6379e-10, 1.6071e-10, 1.4852e-10, 1.3720e-10, 1.3323e-10,\n",
       "             1.1854e-10, 1.0142e-10, 9.5962e-11, 9.3767e-11, 9.2219e-11, 9.1830e-11,\n",
       "             8.2490e-11, 7.2973e-11, 6.4405e-11, 5.9965e-11, 5.8740e-11, 5.7661e-11,\n",
       "             5.4622e-11, 5.1939e-11, 4.7126e-11, 4.6983e-11, 4.6002e-11, 4.5326e-11,\n",
       "             4.1469e-11, 3.2582e-11, 2.7774e-11, 2.5735e-11, 2.1363e-11, 2.0753e-11,\n",
       "             2.0711e-11, 1.9563e-11, 1.8510e-11, 1.7377e-11, 1.7109e-11, 1.5576e-11,\n",
       "             1.4144e-11, 1.2709e-11, 1.1771e-11, 9.9888e-12, 9.8758e-12, 9.6564e-12,\n",
       "             9.3433e-12, 8.7429e-12, 8.5015e-12, 8.2870e-12, 7.7341e-12, 7.6414e-12,\n",
       "             7.5857e-12, 7.0144e-12, 6.8053e-12, 6.2786e-12, 6.2763e-12, 5.9503e-12,\n",
       "             5.5646e-12, 4.5211e-12, 4.5112e-12, 3.9688e-12, 3.6616e-12, 3.3388e-12,\n",
       "             3.2903e-12, 2.7866e-12, 2.5032e-12, 2.4417e-12, 2.4114e-12, 2.1940e-12,\n",
       "             1.8920e-12, 1.7893e-12, 1.7364e-12, 1.5391e-12, 1.4701e-12, 1.2509e-12,\n",
       "             1.2185e-12, 1.1767e-12, 1.1299e-12, 1.1120e-12, 9.9484e-13, 9.3755e-13,\n",
       "             8.7498e-13, 8.4463e-13, 7.0548e-13, 6.9472e-13, 6.5608e-13, 4.0157e-13,\n",
       "             3.3892e-13, 3.2158e-13, 2.3159e-13, 2.0639e-13, 1.6023e-13, 9.8482e-14,\n",
       "             9.4109e-14, 8.6920e-14, 7.7514e-14, 5.7854e-14, 5.6908e-14, 5.1140e-14,\n",
       "             4.3229e-14, 3.9362e-14, 3.7480e-14, 3.6012e-14, 3.5393e-14, 3.4375e-14,\n",
       "             3.1893e-14, 3.1764e-14, 3.0686e-14, 2.8250e-14, 2.4819e-14, 2.2841e-14,\n",
       "             2.1064e-14, 1.5246e-14, 1.1748e-14, 1.1396e-14, 1.0010e-14, 6.5199e-15,\n",
       "             6.4978e-15, 6.0702e-15, 6.0684e-15, 4.4946e-15, 4.3267e-15, 3.5316e-15,\n",
       "             2.8273e-15, 2.1040e-15, 1.9740e-15, 1.6929e-15, 1.4184e-15, 1.1182e-15,\n",
       "             8.3385e-16, 6.4369e-16, 4.6935e-16, 3.5618e-16, 1.9582e-16, 1.9325e-16,\n",
       "             1.3851e-16, 8.2571e-17, 6.9200e-17, 2.7894e-17, 1.3862e-17, 1.1048e-17,\n",
       "             2.5355e-18])}},\n",
       "   {'fpr': np.float64(0.09006211180124224),\n",
       "    'tpr': np.float64(0.9906432748538012),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155, 0.0155, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0217, 0.0248, 0.0248, 0.0248,\n",
       "             0.0248, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280,\n",
       "             0.0280, 0.0280, 0.0280, 0.0311, 0.0311, 0.0311, 0.0311, 0.0342, 0.0342,\n",
       "             0.0342, 0.0342, 0.0373, 0.0373, 0.0404, 0.0435, 0.0435, 0.0466, 0.0466,\n",
       "             0.0466, 0.0497, 0.0528, 0.0528, 0.0559, 0.0559, 0.0559, 0.0559, 0.0559,\n",
       "             0.0590, 0.0590, 0.0590, 0.0621, 0.0621, 0.0652, 0.0683, 0.0683, 0.0714,\n",
       "             0.0714, 0.0714, 0.0745, 0.0745, 0.0776, 0.0776, 0.0807, 0.0839, 0.0870,\n",
       "             0.0901, 0.0932, 0.0963, 0.0963, 0.0994, 0.1025, 0.1056, 0.1087, 0.1118,\n",
       "             0.1149, 0.1180, 0.1180, 0.1211, 0.1242, 0.1273, 0.1273, 0.1304, 0.1335,\n",
       "             0.1398, 0.1429, 0.1460, 0.1491, 0.1491, 0.1522, 0.1553, 0.1584, 0.1615,\n",
       "             0.1646, 0.1646, 0.1677, 0.1708, 0.1739, 0.1770, 0.1801, 0.1832, 0.1863,\n",
       "             0.1894, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050, 0.2081, 0.2112, 0.2143,\n",
       "             0.2174, 0.2174, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329, 0.2360, 0.2391,\n",
       "             0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578, 0.2609, 0.2640, 0.2671,\n",
       "             0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857, 0.2888, 0.2919, 0.2919,\n",
       "             0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137, 0.3168, 0.3199,\n",
       "             0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416, 0.3447, 0.3478,\n",
       "             0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696, 0.3727, 0.3758,\n",
       "             0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975, 0.4006, 0.4037,\n",
       "             0.4068, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224, 0.4255, 0.4286, 0.4317,\n",
       "             0.4348, 0.4379, 0.4410, 0.4441, 0.4472, 0.4503, 0.4534, 0.4565, 0.4596,\n",
       "             0.4627, 0.4658, 0.4689, 0.4720, 0.4752, 0.4783, 0.4814, 0.4845, 0.4876,\n",
       "             0.4907, 0.4938, 0.4969, 0.5000, 0.5031, 0.5062, 0.5093, 0.5124, 0.5155,\n",
       "             0.5186, 0.5217, 0.5248, 0.5280, 0.5311, 0.5342, 0.5373, 0.5404, 0.5435,\n",
       "             0.5466, 0.5497, 0.5528, 0.5559, 0.5590, 0.5621, 0.5652, 0.5683, 0.5714,\n",
       "             0.5745, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870, 0.5901, 0.5932, 0.5963,\n",
       "             0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149, 0.6180, 0.6211, 0.6242,\n",
       "             0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429, 0.6460, 0.6491, 0.6522,\n",
       "             0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708, 0.6739, 0.6770, 0.6801,\n",
       "             0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988, 0.7019, 0.7050, 0.7081,\n",
       "             0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267, 0.7298, 0.7329, 0.7360,\n",
       "             0.7391, 0.7422, 0.7453, 0.7484, 0.7516, 0.7547, 0.7578, 0.7609, 0.7640,\n",
       "             0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826, 0.7857, 0.7888, 0.7919,\n",
       "             0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106, 0.8137, 0.8168, 0.8199,\n",
       "             0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385, 0.8416, 0.8447, 0.8478,\n",
       "             0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665, 0.8696, 0.8727, 0.8758,\n",
       "             0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944, 0.8975, 0.9006, 0.9037,\n",
       "             0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224, 0.9255, 0.9286, 0.9317,\n",
       "             0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503, 0.9534, 0.9565, 0.9596,\n",
       "             0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783, 0.9814, 0.9845, 0.9876,\n",
       "             0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.5532, 0.6257, 0.6596, 0.6901, 0.7064, 0.7181, 0.7275, 0.7380,\n",
       "             0.7404, 0.7450, 0.7497, 0.7556, 0.7567, 0.7626, 0.7661, 0.7743, 0.7789,\n",
       "             0.7836, 0.7895, 0.7965, 0.7988, 0.8035, 0.8047, 0.8047, 0.8082, 0.8094,\n",
       "             0.8105, 0.8140, 0.8152, 0.8175, 0.8187, 0.8222, 0.8234, 0.8246, 0.8269,\n",
       "             0.8292, 0.8304, 0.8327, 0.8351, 0.8363, 0.8374, 0.8386, 0.8409, 0.8421,\n",
       "             0.8433, 0.8444, 0.8468, 0.8503, 0.8515, 0.8526, 0.8550, 0.8561, 0.8573,\n",
       "             0.8596, 0.8608, 0.8620, 0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702,\n",
       "             0.8713, 0.8725, 0.8737, 0.8749, 0.8760, 0.8772, 0.8784, 0.8795, 0.8807,\n",
       "             0.8819, 0.8830, 0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912,\n",
       "             0.8924, 0.8936, 0.8947, 0.8959, 0.8971, 0.8994, 0.9006, 0.9018, 0.9029,\n",
       "             0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135,\n",
       "             0.9146, 0.9158, 0.9170, 0.9181, 0.9193, 0.9205, 0.9216, 0.9228, 0.9240,\n",
       "             0.9251, 0.9263, 0.9275, 0.9298, 0.9310, 0.9322, 0.9333, 0.9345, 0.9357,\n",
       "             0.9368, 0.9368, 0.9380, 0.9392, 0.9404, 0.9415, 0.9415, 0.9427, 0.9427,\n",
       "             0.9439, 0.9450, 0.9462, 0.9474, 0.9485, 0.9485, 0.9485, 0.9497, 0.9509,\n",
       "             0.9520, 0.9520, 0.9532, 0.9544, 0.9556, 0.9567, 0.9579, 0.9591, 0.9602,\n",
       "             0.9614, 0.9626, 0.9637, 0.9637, 0.9649, 0.9661, 0.9673, 0.9673, 0.9684,\n",
       "             0.9696, 0.9708, 0.9708, 0.9719, 0.9719, 0.9719, 0.9731, 0.9731, 0.9743,\n",
       "             0.9754, 0.9754, 0.9754, 0.9766, 0.9766, 0.9778, 0.9789, 0.9801, 0.9813,\n",
       "             0.9813, 0.9825, 0.9836, 0.9836, 0.9848, 0.9848, 0.9848, 0.9860, 0.9860,\n",
       "             0.9871, 0.9883, 0.9883, 0.9895, 0.9895, 0.9906, 0.9906, 0.9906, 0.9906,\n",
       "             0.9906, 0.9906, 0.9906, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9930, 0.9930, 0.9930, 0.9930, 0.9942, 0.9942, 0.9942,\n",
       "             0.9942, 0.9942, 0.9942, 0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953,\n",
       "             0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9981e-01, 9.9979e-01,\n",
       "             9.9976e-01, 9.9974e-01, 9.9973e-01, 9.9970e-01, 9.9969e-01, 9.9929e-01,\n",
       "             9.9927e-01, 9.9925e-01, 9.9921e-01, 9.9920e-01, 9.9908e-01, 9.9896e-01,\n",
       "             9.9893e-01, 9.9886e-01, 9.9875e-01, 9.9851e-01, 9.9813e-01, 9.9808e-01,\n",
       "             9.9789e-01, 9.9657e-01, 9.9629e-01, 9.9380e-01, 9.9316e-01, 9.9167e-01,\n",
       "             9.9163e-01, 9.9136e-01, 9.9049e-01, 9.9004e-01, 9.8975e-01, 9.8927e-01,\n",
       "             9.8879e-01, 9.8851e-01, 9.8815e-01, 9.8040e-01, 9.8014e-01, 9.7928e-01,\n",
       "             9.7909e-01, 9.7524e-01, 9.7475e-01, 9.7357e-01, 9.6962e-01, 9.6770e-01,\n",
       "             9.6570e-01, 9.6373e-01, 9.5682e-01, 9.3728e-01, 9.2733e-01, 9.2274e-01,\n",
       "             9.2192e-01, 9.2058e-01, 8.9217e-01, 8.8001e-01, 8.2719e-01, 8.2641e-01,\n",
       "             8.2349e-01, 8.0750e-01, 8.0006e-01, 7.9848e-01, 7.6287e-01, 7.2380e-01,\n",
       "             7.1645e-01, 7.0158e-01, 6.6471e-01, 5.7198e-01, 4.2366e-01, 3.8505e-01,\n",
       "             3.7568e-01, 3.1181e-01, 3.0851e-01, 2.7914e-01, 2.0760e-01, 1.8551e-01,\n",
       "             1.8378e-01, 1.0344e-01, 1.0122e-01, 9.6229e-02, 7.6813e-02, 7.5443e-02,\n",
       "             7.0978e-02, 6.6701e-02, 5.1244e-02, 5.0178e-02, 4.2760e-02, 3.8457e-02,\n",
       "             3.8068e-02, 3.3891e-02, 3.2148e-02, 3.1429e-02, 2.4254e-02, 1.9640e-02,\n",
       "             9.5321e-03, 8.6675e-03, 8.0861e-03, 7.0225e-03, 6.8615e-03, 6.2967e-03,\n",
       "             5.6492e-03, 4.8242e-03, 2.9979e-03, 2.7864e-03, 2.6160e-03, 2.3569e-03,\n",
       "             2.2324e-03, 2.0693e-03, 1.9756e-03, 1.4392e-03, 1.3612e-03, 1.1801e-03,\n",
       "             1.0323e-03, 9.8521e-04, 9.6125e-04, 7.6637e-04, 7.2549e-04, 7.1389e-04,\n",
       "             4.4926e-04, 3.6241e-04, 3.5738e-04, 3.2735e-04, 2.6902e-04, 2.3885e-04,\n",
       "             2.3332e-04, 2.3321e-04, 1.8972e-04, 1.6383e-04, 1.5052e-04, 1.4053e-04,\n",
       "             1.2720e-04, 1.2542e-04, 1.0886e-04, 1.0580e-04, 1.0293e-04, 1.0227e-04,\n",
       "             9.4871e-05, 8.8618e-05, 8.3294e-05, 7.5842e-05, 7.3299e-05, 6.9430e-05,\n",
       "             6.7963e-05, 6.5154e-05, 6.4734e-05, 6.1497e-05, 5.7116e-05, 5.6674e-05,\n",
       "             5.2548e-05, 4.0558e-05, 3.7893e-05, 3.7865e-05, 3.6541e-05, 3.5985e-05,\n",
       "             3.4754e-05, 3.1816e-05, 3.0476e-05, 2.8148e-05, 2.7137e-05, 2.4859e-05,\n",
       "             2.2549e-05, 2.1503e-05, 2.0350e-05, 1.8450e-05, 1.7532e-05, 1.5830e-05,\n",
       "             1.1816e-05, 1.1620e-05, 1.1327e-05, 1.0030e-05, 9.3735e-06, 8.4569e-06,\n",
       "             7.8722e-06, 6.7338e-06, 6.5977e-06, 6.3683e-06, 3.5486e-06, 3.4315e-06,\n",
       "             3.3007e-06, 3.2881e-06, 2.8759e-06, 2.8421e-06, 2.7807e-06, 2.7073e-06,\n",
       "             2.6965e-06, 2.5796e-06, 2.5151e-06, 2.2375e-06, 2.1484e-06, 2.1463e-06,\n",
       "             2.0599e-06, 1.9367e-06, 1.6327e-06, 1.6249e-06, 1.4644e-06, 1.4613e-06,\n",
       "             1.2550e-06, 1.2107e-06, 1.1353e-06, 1.1297e-06, 1.0889e-06, 1.0738e-06,\n",
       "             1.0098e-06, 1.0020e-06, 9.7890e-07, 9.3705e-07, 7.8094e-07, 7.6406e-07,\n",
       "             7.1105e-07, 6.6427e-07, 6.4926e-07, 6.2019e-07, 5.5546e-07, 5.5151e-07,\n",
       "             4.8874e-07, 4.7762e-07, 4.6936e-07, 4.4859e-07, 4.4743e-07, 4.3212e-07,\n",
       "             3.2674e-07, 3.1205e-07, 2.8726e-07, 2.5418e-07, 2.4886e-07, 2.1760e-07,\n",
       "             2.0127e-07, 1.9121e-07, 1.7606e-07, 1.6297e-07, 1.5925e-07, 1.4238e-07,\n",
       "             1.3712e-07, 1.3617e-07, 1.3110e-07, 1.2347e-07, 1.2123e-07, 1.1232e-07,\n",
       "             1.0851e-07, 1.0793e-07, 1.0227e-07, 9.7076e-08, 9.6169e-08, 9.3649e-08,\n",
       "             7.9102e-08, 7.4486e-08, 6.9820e-08, 6.7831e-08, 5.7823e-08, 5.3810e-08,\n",
       "             5.3772e-08, 5.1485e-08, 4.6102e-08, 4.5806e-08, 4.5220e-08, 4.2884e-08,\n",
       "             3.9634e-08, 3.9370e-08, 3.7421e-08, 3.6869e-08, 3.4242e-08, 3.3720e-08,\n",
       "             3.0766e-08, 2.8069e-08, 2.2664e-08, 2.1095e-08, 2.0524e-08, 1.9951e-08,\n",
       "             1.9509e-08, 1.8773e-08, 1.8481e-08, 1.8404e-08, 1.4890e-08, 1.2406e-08,\n",
       "             1.1866e-08, 1.1318e-08, 1.0400e-08, 9.6622e-09, 9.4428e-09, 8.8255e-09,\n",
       "             8.2776e-09, 7.8900e-09, 7.6243e-09, 7.3107e-09, 7.1522e-09, 6.6423e-09,\n",
       "             6.6157e-09, 6.0270e-09, 5.9445e-09, 5.8779e-09, 3.9993e-09, 3.8831e-09,\n",
       "             3.8403e-09, 3.8103e-09, 3.3099e-09, 2.9585e-09, 2.8175e-09, 2.7972e-09,\n",
       "             2.1258e-09, 2.0586e-09, 2.0108e-09, 1.6930e-09, 1.5586e-09, 1.4203e-09,\n",
       "             1.4077e-09, 1.3145e-09, 1.1066e-09, 1.0551e-09, 9.6316e-10, 9.2644e-10,\n",
       "             9.1525e-10, 9.1000e-10, 7.9007e-10, 6.4545e-10, 6.4460e-10, 3.7273e-10,\n",
       "             3.2577e-10, 2.7113e-10, 2.4478e-10, 2.1416e-10, 1.9619e-10, 1.8936e-10,\n",
       "             1.5442e-10, 1.5097e-10, 1.4089e-10, 1.3681e-10, 1.3454e-10, 1.1683e-10,\n",
       "             1.0565e-10, 7.6365e-11, 3.9805e-11, 1.9182e-11, 1.6170e-11, 1.4803e-11,\n",
       "             1.4470e-11, 1.0974e-11, 7.6006e-12, 7.5502e-12, 4.9702e-12, 4.1519e-12,\n",
       "             3.8748e-12, 3.8690e-12, 3.8316e-12, 3.7488e-12, 3.1854e-12, 2.5145e-12,\n",
       "             1.7968e-12, 1.2822e-12, 1.0936e-12, 9.0786e-13, 8.2926e-13, 6.0966e-13,\n",
       "             5.4975e-13, 5.1912e-13, 4.9868e-13, 2.5181e-13, 2.1941e-13, 8.2916e-14,\n",
       "             5.3200e-14, 5.0077e-14, 4.9232e-14, 4.8896e-14, 5.2021e-15, 4.9387e-15,\n",
       "             3.7060e-15, 1.4136e-15, 1.0967e-15, 7.6160e-17])}},\n",
       "   {'fpr': np.float64(0.062111801242236024),\n",
       "    'tpr': np.float64(0.9801169590643275),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0186, 0.0186, 0.0186, 0.0186, 0.0217, 0.0217,\n",
       "             0.0248, 0.0248, 0.0248, 0.0280, 0.0311, 0.0311, 0.0311, 0.0311, 0.0342,\n",
       "             0.0342, 0.0342, 0.0373, 0.0373, 0.0404, 0.0404, 0.0404, 0.0404, 0.0404,\n",
       "             0.0435, 0.0435, 0.0466, 0.0497, 0.0497, 0.0497, 0.0497, 0.0528, 0.0559,\n",
       "             0.0559, 0.0559, 0.0559, 0.0590, 0.0621, 0.0652, 0.0652, 0.0683, 0.0683,\n",
       "             0.0683, 0.0714, 0.0745, 0.0776, 0.0807, 0.0807, 0.0839, 0.0870, 0.0870,\n",
       "             0.0870, 0.0870, 0.0870, 0.0901, 0.0963, 0.0963, 0.0994, 0.1025, 0.1056,\n",
       "             0.1087, 0.1118, 0.1149, 0.1149, 0.1180, 0.1211, 0.1242, 0.1273, 0.1304,\n",
       "             0.1335, 0.1366, 0.1366, 0.1398, 0.1429, 0.1429, 0.1460, 0.1491, 0.1522,\n",
       "             0.1522, 0.1553, 0.1553, 0.1584, 0.1615, 0.1646, 0.1677, 0.1708, 0.1739,\n",
       "             0.1770, 0.1801, 0.1801, 0.1832, 0.1863, 0.1894, 0.1925, 0.1957, 0.1988,\n",
       "             0.1988, 0.2019, 0.2050, 0.2081, 0.2112, 0.2143, 0.2174, 0.2205, 0.2236,\n",
       "             0.2267, 0.2298, 0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516,\n",
       "             0.2547, 0.2578, 0.2609, 0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795,\n",
       "             0.2826, 0.2857, 0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075,\n",
       "             0.3106, 0.3137, 0.3168, 0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354,\n",
       "             0.3385, 0.3416, 0.3447, 0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634,\n",
       "             0.3665, 0.3696, 0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913,\n",
       "             0.3944, 0.3975, 0.4006, 0.4037, 0.4068, 0.4099, 0.4130, 0.4161, 0.4193,\n",
       "             0.4224, 0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4410, 0.4441, 0.4472,\n",
       "             0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752,\n",
       "             0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031,\n",
       "             0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311,\n",
       "             0.5342, 0.5373, 0.5404, 0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590,\n",
       "             0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870,\n",
       "             0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149,\n",
       "             0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429,\n",
       "             0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708,\n",
       "             0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988,\n",
       "             0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267,\n",
       "             0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453, 0.7453, 0.7484, 0.7516,\n",
       "             0.7547, 0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795,\n",
       "             0.7826, 0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075,\n",
       "             0.8106, 0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354,\n",
       "             0.8385, 0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634,\n",
       "             0.8665, 0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913,\n",
       "             0.8944, 0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193,\n",
       "             0.9224, 0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472,\n",
       "             0.9503, 0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752,\n",
       "             0.9783, 0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0912, 0.1368, 0.1719, 0.2023, 0.2152, 0.2339, 0.2503, 0.2596,\n",
       "             0.2713, 0.2819, 0.2901, 0.2971, 0.3064, 0.3123, 0.3216, 0.3263, 0.3322,\n",
       "             0.3333, 0.3392, 0.3427, 0.3474, 0.3567, 0.3579, 0.3626, 0.3637, 0.3661,\n",
       "             0.3684, 0.3731, 0.3789, 0.3836, 0.3883, 0.3965, 0.4000, 0.4035, 0.4082,\n",
       "             0.4117, 0.4140, 0.4164, 0.4187, 0.4269, 0.4304, 0.4327, 0.4339, 0.4363,\n",
       "             0.4386, 0.4421, 0.4433, 0.4444, 0.4491, 0.4526, 0.4561, 0.4573, 0.4585,\n",
       "             0.4608, 0.4632, 0.4678, 0.4713, 0.4749, 0.4760, 0.4772, 0.4807, 0.4819,\n",
       "             0.4854, 0.4877, 0.4924, 0.4936, 0.4971, 0.4982, 0.5018, 0.5053, 0.5064,\n",
       "             0.5076, 0.5123, 0.5135, 0.5158, 0.5170, 0.5205, 0.5228, 0.5251, 0.5263,\n",
       "             0.5287, 0.5298, 0.5310, 0.5322, 0.5345, 0.5357, 0.5368, 0.5380, 0.5415,\n",
       "             0.5427, 0.5439, 0.5450, 0.5462, 0.5474, 0.5485, 0.5532, 0.5544, 0.5556,\n",
       "             0.5567, 0.5591, 0.5602, 0.5637, 0.5649, 0.5673, 0.5684, 0.5719, 0.5731,\n",
       "             0.5743, 0.5766, 0.5778, 0.5789, 0.5825, 0.5848, 0.5860, 0.5871, 0.5883,\n",
       "             0.5906, 0.5918, 0.5930, 0.5942, 0.5953, 0.5977, 0.5988, 0.6000, 0.6023,\n",
       "             0.6035, 0.6047, 0.6070, 0.6082, 0.6094, 0.6105, 0.6117, 0.6129, 0.6152,\n",
       "             0.6175, 0.6187, 0.6199, 0.6222, 0.6234, 0.6246, 0.6257, 0.6269, 0.6281,\n",
       "             0.6304, 0.6351, 0.6363, 0.6374, 0.6398, 0.6421, 0.6433, 0.6456, 0.6468,\n",
       "             0.6480, 0.6491, 0.6503, 0.6526, 0.6538, 0.6550, 0.6573, 0.6585, 0.6596,\n",
       "             0.6608, 0.6632, 0.6643, 0.6655, 0.6667, 0.6678, 0.6690, 0.6702, 0.6713,\n",
       "             0.6725, 0.6737, 0.6749, 0.6760, 0.6772, 0.6784, 0.6795, 0.6807, 0.6819,\n",
       "             0.6830, 0.6842, 0.6854, 0.6865, 0.6877, 0.6889, 0.6912, 0.6924, 0.6936,\n",
       "             0.6947, 0.6959, 0.6971, 0.6982, 0.7006, 0.7018, 0.7029, 0.7041, 0.7053,\n",
       "             0.7064, 0.7076, 0.7088, 0.7099, 0.7111, 0.7123, 0.7146, 0.7170, 0.7181,\n",
       "             0.7193, 0.7205, 0.7216, 0.7228, 0.7240, 0.7251, 0.7263, 0.7287, 0.7298,\n",
       "             0.7310, 0.7322, 0.7333, 0.7345, 0.7357, 0.7368, 0.7380, 0.7392, 0.7404,\n",
       "             0.7427, 0.7439, 0.7450, 0.7462, 0.7474, 0.7485, 0.7497, 0.7509, 0.7520,\n",
       "             0.7532, 0.7544, 0.7556, 0.7567, 0.7579, 0.7591, 0.7602, 0.7614, 0.7626,\n",
       "             0.7637, 0.7649, 0.7673, 0.7684, 0.7696, 0.7708, 0.7719, 0.7743, 0.7754,\n",
       "             0.7766, 0.7778, 0.7789, 0.7801, 0.7813, 0.7825, 0.7836, 0.7848, 0.7860,\n",
       "             0.7871, 0.7883, 0.7895, 0.7906, 0.7918, 0.7930, 0.7942, 0.7953, 0.7965,\n",
       "             0.7977, 0.7988, 0.8000, 0.8012, 0.8023, 0.8035, 0.8047, 0.8058, 0.8082,\n",
       "             0.8094, 0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164, 0.8175, 0.8187,\n",
       "             0.8199, 0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269, 0.8281, 0.8292,\n",
       "             0.8304, 0.8316, 0.8316, 0.8327, 0.8339, 0.8351, 0.8363, 0.8374, 0.8386,\n",
       "             0.8398, 0.8409, 0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480, 0.8491,\n",
       "             0.8503, 0.8515, 0.8526, 0.8538, 0.8550, 0.8561, 0.8573, 0.8585, 0.8596,\n",
       "             0.8608, 0.8620, 0.8632, 0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702,\n",
       "             0.8713, 0.8725, 0.8737, 0.8749, 0.8760, 0.8772, 0.8784, 0.8795, 0.8807,\n",
       "             0.8819, 0.8830, 0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912,\n",
       "             0.8924, 0.8936, 0.8947, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018,\n",
       "             0.9029, 0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123,\n",
       "             0.9135, 0.9146, 0.9158, 0.9170, 0.9170, 0.9181, 0.9193, 0.9205, 0.9216,\n",
       "             0.9228, 0.9240, 0.9251, 0.9263, 0.9263, 0.9275, 0.9287, 0.9298, 0.9298,\n",
       "             0.9310, 0.9322, 0.9333, 0.9345, 0.9357, 0.9368, 0.9380, 0.9392, 0.9392,\n",
       "             0.9404, 0.9415, 0.9427, 0.9439, 0.9450, 0.9462, 0.9474, 0.9485, 0.9497,\n",
       "             0.9509, 0.9520, 0.9532, 0.9532, 0.9544, 0.9556, 0.9567, 0.9567, 0.9579,\n",
       "             0.9579, 0.9591, 0.9602, 0.9602, 0.9602, 0.9614, 0.9626, 0.9637, 0.9637,\n",
       "             0.9649, 0.9661, 0.9661, 0.9673, 0.9673, 0.9684, 0.9696, 0.9708, 0.9719,\n",
       "             0.9719, 0.9731, 0.9731, 0.9731, 0.9743, 0.9754, 0.9766, 0.9766, 0.9766,\n",
       "             0.9778, 0.9789, 0.9801, 0.9801, 0.9801, 0.9801, 0.9813, 0.9813, 0.9825,\n",
       "             0.9836, 0.9836, 0.9836, 0.9836, 0.9836, 0.9848, 0.9848, 0.9848, 0.9860,\n",
       "             0.9871, 0.9883, 0.9895, 0.9895, 0.9895, 0.9906, 0.9906, 0.9906, 0.9906,\n",
       "             0.9906, 0.9906, 0.9906, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9930, 0.9930, 0.9930, 0.9942, 0.9942, 0.9942, 0.9942,\n",
       "             0.9953, 0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9974e-01,\n",
       "             9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9970e-01, 9.9968e-01, 9.9968e-01, 9.9967e-01, 9.9966e-01,\n",
       "             9.9965e-01, 9.9965e-01, 9.9963e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01,\n",
       "             9.9961e-01, 9.9960e-01, 9.9959e-01, 9.9958e-01, 9.9958e-01, 9.9958e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9957e-01, 9.9956e-01, 9.9956e-01, 9.9951e-01,\n",
       "             9.9951e-01, 9.9949e-01, 9.9947e-01, 9.9946e-01, 9.9946e-01, 9.9946e-01,\n",
       "             9.9945e-01, 9.9942e-01, 9.9938e-01, 9.9936e-01, 9.9935e-01, 9.9935e-01,\n",
       "             9.9933e-01, 9.9929e-01, 9.9927e-01, 9.9926e-01, 9.9926e-01, 9.9925e-01,\n",
       "             9.9922e-01, 9.9916e-01, 9.9913e-01, 9.9913e-01, 9.9912e-01, 9.9911e-01,\n",
       "             9.9904e-01, 9.9903e-01, 9.9896e-01, 9.9895e-01, 9.9892e-01, 9.9882e-01,\n",
       "             9.9870e-01, 9.9867e-01, 9.9865e-01, 9.9863e-01, 9.9854e-01, 9.9848e-01,\n",
       "             9.9827e-01, 9.9767e-01, 9.9765e-01, 9.9758e-01, 9.9755e-01, 9.9743e-01,\n",
       "             9.9715e-01, 9.9684e-01, 9.9682e-01, 9.9681e-01, 9.9657e-01, 9.9653e-01,\n",
       "             9.9629e-01, 9.9555e-01, 9.9515e-01, 9.9460e-01, 9.9404e-01, 9.9332e-01,\n",
       "             9.9319e-01, 9.9169e-01, 9.9053e-01, 9.9018e-01, 9.9011e-01, 9.8918e-01,\n",
       "             9.8915e-01, 9.8904e-01, 9.8828e-01, 9.8754e-01, 9.8562e-01, 9.8426e-01,\n",
       "             9.8323e-01, 9.8316e-01, 9.8268e-01, 9.8104e-01, 9.8061e-01, 9.7881e-01,\n",
       "             9.7871e-01, 9.7648e-01, 9.7478e-01, 9.7269e-01, 9.7117e-01, 9.6875e-01,\n",
       "             9.5974e-01, 9.5775e-01, 9.5650e-01, 9.5525e-01, 9.5046e-01, 9.4791e-01,\n",
       "             9.4681e-01, 9.4679e-01, 9.3961e-01, 9.3188e-01, 9.2754e-01, 9.2580e-01,\n",
       "             9.1374e-01, 8.9594e-01, 8.9390e-01, 8.7414e-01, 8.5576e-01, 8.4683e-01,\n",
       "             8.4038e-01, 7.6522e-01, 7.3357e-01, 6.8654e-01, 6.6460e-01, 6.5654e-01,\n",
       "             5.9787e-01, 5.9110e-01, 5.8694e-01, 5.7571e-01, 5.6230e-01, 5.6139e-01,\n",
       "             5.4059e-01, 5.3975e-01, 4.3494e-01, 4.2379e-01, 4.1642e-01, 3.9378e-01,\n",
       "             3.9240e-01, 3.8397e-01, 3.4263e-01, 3.1778e-01, 3.0925e-01, 2.9305e-01,\n",
       "             2.5360e-01, 2.3998e-01, 2.3511e-01, 2.3166e-01, 2.2873e-01, 2.1781e-01,\n",
       "             1.6513e-01, 1.5591e-01, 1.3420e-01, 1.1281e-01, 1.0749e-01, 1.0260e-01,\n",
       "             1.0089e-01, 8.9059e-02, 8.4140e-02, 7.2579e-02, 5.6859e-02, 5.5044e-02,\n",
       "             5.1218e-02, 5.0626e-02, 4.7996e-02, 3.9026e-02, 3.1123e-02, 2.6023e-02,\n",
       "             2.5023e-02, 2.3283e-02, 2.2218e-02, 1.8872e-02, 1.6317e-02, 1.5813e-02,\n",
       "             1.5764e-02, 1.5745e-02, 1.4086e-02, 1.2520e-02, 1.1574e-02, 9.7933e-03,\n",
       "             9.4929e-03, 9.0557e-03, 6.6811e-03, 4.8944e-03, 4.8650e-03, 4.4537e-03,\n",
       "             3.8652e-03, 3.2489e-03, 2.8807e-03, 2.7353e-03, 2.5237e-03, 1.9850e-03,\n",
       "             1.9719e-03, 1.7987e-03, 1.6047e-03, 1.5936e-03, 1.5782e-03, 1.5606e-03,\n",
       "             1.3601e-03, 1.1404e-03, 9.9522e-04, 9.7936e-04, 9.3635e-04, 8.9686e-04,\n",
       "             6.9698e-04, 6.1655e-04, 4.4839e-04, 3.6302e-04, 3.0874e-04, 2.6727e-04,\n",
       "             2.0457e-04, 1.7484e-04, 1.5338e-04, 1.5258e-04, 1.4446e-04, 1.2564e-04,\n",
       "             9.0419e-05, 9.0044e-05, 8.8842e-05, 7.2270e-05, 6.9659e-05, 6.9315e-05,\n",
       "             6.7682e-05, 6.3464e-05, 6.3336e-05, 6.1306e-05, 5.9376e-05, 5.4764e-05,\n",
       "             4.9775e-05, 4.6604e-05, 3.9276e-05, 3.5562e-05, 3.4214e-05, 2.6691e-05,\n",
       "             2.4375e-05, 2.3943e-05, 2.3221e-05, 2.1677e-05, 2.1603e-05, 1.9819e-05,\n",
       "             1.8631e-05, 1.8408e-05, 1.5440e-05, 1.5098e-05, 1.4534e-05, 1.3794e-05,\n",
       "             1.2688e-05, 1.1600e-05, 1.0440e-05, 1.0011e-05, 9.8639e-06, 9.7714e-06,\n",
       "             8.1048e-06, 8.0513e-06, 7.5136e-06, 6.2176e-06, 6.0435e-06, 5.7502e-06,\n",
       "             5.6422e-06, 5.1519e-06, 5.0908e-06, 4.8295e-06, 4.2379e-06, 4.1290e-06,\n",
       "             3.5986e-06, 3.3542e-06, 3.3532e-06, 3.3526e-06, 3.2850e-06, 3.2155e-06,\n",
       "             2.9997e-06, 2.8038e-06, 2.7274e-06, 2.7265e-06, 2.6802e-06, 1.9901e-06,\n",
       "             1.8825e-06, 1.8282e-06, 1.8281e-06, 1.7869e-06, 1.7860e-06, 1.7742e-06,\n",
       "             1.7047e-06, 1.4645e-06, 1.3873e-06, 1.3810e-06, 1.3792e-06, 1.2444e-06,\n",
       "             1.2128e-06, 1.1780e-06, 1.1508e-06, 1.0353e-06, 7.7131e-07, 7.0021e-07,\n",
       "             6.3695e-07, 6.3510e-07, 6.0253e-07, 5.4878e-07, 5.4510e-07, 5.3300e-07,\n",
       "             5.3037e-07, 5.1898e-07, 4.8088e-07, 4.7701e-07, 4.6926e-07, 4.6654e-07,\n",
       "             4.5392e-07, 4.0313e-07, 3.8961e-07, 3.6986e-07, 3.6085e-07, 3.4583e-07,\n",
       "             3.3207e-07, 3.2348e-07, 2.7127e-07, 2.5781e-07, 2.4541e-07, 2.1174e-07,\n",
       "             1.9530e-07, 1.9090e-07, 1.8872e-07, 1.8820e-07, 1.5225e-07, 1.5052e-07,\n",
       "             1.4198e-07, 1.3928e-07, 1.3119e-07, 1.2141e-07, 1.1703e-07, 1.0713e-07,\n",
       "             1.0282e-07, 8.1098e-08, 7.8796e-08, 7.8205e-08, 7.5841e-08, 7.2074e-08,\n",
       "             7.0426e-08, 6.0368e-08, 5.8911e-08, 5.7925e-08, 5.6503e-08, 4.4927e-08,\n",
       "             4.1097e-08, 4.1084e-08, 4.0797e-08, 4.0070e-08, 3.8378e-08, 3.7102e-08,\n",
       "             3.1372e-08, 2.9787e-08, 2.9735e-08, 2.7959e-08, 2.7734e-08, 2.7386e-08,\n",
       "             2.7059e-08, 2.3895e-08, 2.2708e-08, 2.2144e-08, 1.9909e-08, 1.9069e-08,\n",
       "             1.8852e-08, 1.7549e-08, 1.7304e-08, 1.7018e-08, 1.6068e-08, 1.6063e-08,\n",
       "             1.5723e-08, 1.3419e-08, 1.1642e-08, 1.1133e-08, 1.0979e-08, 1.0454e-08,\n",
       "             1.0355e-08, 1.0176e-08, 1.0088e-08, 9.9727e-09, 9.9063e-09, 9.7593e-09,\n",
       "             9.4924e-09, 8.9815e-09, 8.9112e-09, 8.3503e-09, 7.8922e-09, 6.6900e-09,\n",
       "             6.3569e-09, 5.2363e-09, 4.3507e-09, 4.3313e-09, 4.1666e-09, 3.8315e-09,\n",
       "             3.2605e-09, 3.0608e-09, 2.6899e-09, 2.2469e-09, 2.0303e-09, 1.8684e-09,\n",
       "             1.8158e-09, 1.7971e-09, 1.6817e-09, 1.6068e-09, 1.3495e-09, 1.3391e-09,\n",
       "             1.3177e-09, 1.2932e-09, 1.2738e-09, 1.2274e-09, 6.0086e-10, 4.8605e-10,\n",
       "             4.2667e-10, 4.0971e-10, 3.9959e-10, 3.7145e-10, 3.5057e-10, 3.1851e-10,\n",
       "             3.1669e-10, 3.1209e-10, 2.9082e-10, 2.8049e-10, 1.7508e-10, 1.5819e-10,\n",
       "             1.5520e-10, 1.4708e-10, 1.3032e-10, 1.0714e-10, 9.1004e-11, 8.4280e-11,\n",
       "             6.5978e-11, 5.9665e-11, 5.4206e-11, 5.2972e-11, 5.2329e-11, 4.7457e-11,\n",
       "             4.5788e-11, 4.5277e-11, 3.9103e-11, 3.5957e-11, 2.9274e-11, 2.5356e-11,\n",
       "             1.9775e-11, 1.7755e-11, 1.3895e-11, 1.1464e-11, 1.1277e-11, 1.0338e-11,\n",
       "             4.4409e-12, 3.5993e-12, 1.6673e-12, 1.1832e-12, 2.8369e-13, 2.4564e-13,\n",
       "             5.5330e-14, 3.9045e-15])}},\n",
       "   {'fpr': np.float64(0.09316770186335403),\n",
       "    'tpr': np.float64(0.9847953216374269),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155, 0.0155, 0.0155, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0248, 0.0280, 0.0280,\n",
       "             0.0280, 0.0311, 0.0311, 0.0311, 0.0311, 0.0342, 0.0342, 0.0342, 0.0342,\n",
       "             0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342,\n",
       "             0.0342, 0.0342, 0.0342, 0.0342, 0.0373, 0.0373, 0.0373, 0.0373, 0.0373,\n",
       "             0.0435, 0.0435, 0.0435, 0.0435, 0.0466, 0.0497, 0.0528, 0.0559, 0.0590,\n",
       "             0.0621, 0.0621, 0.0621, 0.0621, 0.0652, 0.0683, 0.0714, 0.0714, 0.0745,\n",
       "             0.0745, 0.0745, 0.0745, 0.0776, 0.0776, 0.0776, 0.0776, 0.0807, 0.0807,\n",
       "             0.0839, 0.0870, 0.0870, 0.0901, 0.0932, 0.0963, 0.0963, 0.0994, 0.1025,\n",
       "             0.1056, 0.1087, 0.1118, 0.1149, 0.1180, 0.1211, 0.1211, 0.1242, 0.1273,\n",
       "             0.1273, 0.1273, 0.1273, 0.1304, 0.1304, 0.1335, 0.1366, 0.1366, 0.1398,\n",
       "             0.1398, 0.1398, 0.1429, 0.1460, 0.1491, 0.1491, 0.1522, 0.1553, 0.1584,\n",
       "             0.1615, 0.1646, 0.1677, 0.1708, 0.1739, 0.1770, 0.1801, 0.1801, 0.1832,\n",
       "             0.1863, 0.1894, 0.1925, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050, 0.2081,\n",
       "             0.2112, 0.2143, 0.2174, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329, 0.2360,\n",
       "             0.2391, 0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578, 0.2609, 0.2640,\n",
       "             0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857, 0.2888, 0.2919,\n",
       "             0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137, 0.3168, 0.3199,\n",
       "             0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416, 0.3447, 0.3478,\n",
       "             0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696, 0.3727, 0.3758,\n",
       "             0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975, 0.4006, 0.4037,\n",
       "             0.4068, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224, 0.4255, 0.4286, 0.4317,\n",
       "             0.4348, 0.4379, 0.4410, 0.4441, 0.4472, 0.4503, 0.4534, 0.4565, 0.4596,\n",
       "             0.4627, 0.4658, 0.4689, 0.4720, 0.4752, 0.4783, 0.4814, 0.4845, 0.4876,\n",
       "             0.4907, 0.4938, 0.4969, 0.5000, 0.5031, 0.5062, 0.5093, 0.5124, 0.5155,\n",
       "             0.5186, 0.5217, 0.5248, 0.5280, 0.5311, 0.5342, 0.5373, 0.5404, 0.5435,\n",
       "             0.5466, 0.5497, 0.5528, 0.5559, 0.5590, 0.5621, 0.5652, 0.5683, 0.5714,\n",
       "             0.5745, 0.5776, 0.5807, 0.5839, 0.5870, 0.5901, 0.5932, 0.5963, 0.5994,\n",
       "             0.6025, 0.6056, 0.6087, 0.6118, 0.6149, 0.6180, 0.6211, 0.6242, 0.6273,\n",
       "             0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429, 0.6460, 0.6491, 0.6522,\n",
       "             0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708, 0.6739, 0.6770, 0.6801,\n",
       "             0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988, 0.7019, 0.7050, 0.7081,\n",
       "             0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267, 0.7298, 0.7329, 0.7360,\n",
       "             0.7391, 0.7422, 0.7453, 0.7484, 0.7516, 0.7547, 0.7578, 0.7609, 0.7640,\n",
       "             0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826, 0.7857, 0.7888, 0.7919,\n",
       "             0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106, 0.8137, 0.8168, 0.8199,\n",
       "             0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385, 0.8416, 0.8447, 0.8478,\n",
       "             0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665, 0.8696, 0.8727, 0.8758,\n",
       "             0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944, 0.8975, 0.9006, 0.9037,\n",
       "             0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224, 0.9255, 0.9286, 0.9317,\n",
       "             0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503, 0.9534, 0.9565, 0.9596,\n",
       "             0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783, 0.9814, 0.9845, 0.9876,\n",
       "             0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7801, 0.8164, 0.8234, 0.8327, 0.8386, 0.8421, 0.8444, 0.8480,\n",
       "             0.8503, 0.8515, 0.8538, 0.8561, 0.8596, 0.8608, 0.8620, 0.8632, 0.8655,\n",
       "             0.8667, 0.8678, 0.8690, 0.8702, 0.8713, 0.8749, 0.8760, 0.8772, 0.8784,\n",
       "             0.8819, 0.8830, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912, 0.8912,\n",
       "             0.8936, 0.8947, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029,\n",
       "             0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135,\n",
       "             0.9146, 0.9158, 0.9170, 0.9181, 0.9193, 0.9193, 0.9205, 0.9216, 0.9216,\n",
       "             0.9228, 0.9240, 0.9251, 0.9251, 0.9263, 0.9275, 0.9287, 0.9298, 0.9310,\n",
       "             0.9322, 0.9333, 0.9345, 0.9357, 0.9368, 0.9380, 0.9380, 0.9380, 0.9392,\n",
       "             0.9404, 0.9404, 0.9415, 0.9427, 0.9439, 0.9439, 0.9450, 0.9462, 0.9474,\n",
       "             0.9485, 0.9497, 0.9509, 0.9520, 0.9532, 0.9544, 0.9556, 0.9567, 0.9579,\n",
       "             0.9591, 0.9602, 0.9614, 0.9626, 0.9626, 0.9637, 0.9649, 0.9661, 0.9673,\n",
       "             0.9673, 0.9684, 0.9696, 0.9708, 0.9708, 0.9708, 0.9708, 0.9708, 0.9708,\n",
       "             0.9708, 0.9719, 0.9731, 0.9743, 0.9743, 0.9743, 0.9743, 0.9754, 0.9754,\n",
       "             0.9766, 0.9778, 0.9789, 0.9789, 0.9801, 0.9813, 0.9825, 0.9825, 0.9836,\n",
       "             0.9836, 0.9836, 0.9848, 0.9848, 0.9848, 0.9848, 0.9860, 0.9860, 0.9860,\n",
       "             0.9860, 0.9860, 0.9860, 0.9860, 0.9860, 0.9860, 0.9871, 0.9871, 0.9871,\n",
       "             0.9883, 0.9895, 0.9906, 0.9906, 0.9918, 0.9918, 0.9918, 0.9930, 0.9930,\n",
       "             0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9986e-01, 9.9986e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9974e-01, 9.9970e-01, 9.9964e-01, 9.9963e-01, 9.9963e-01, 9.9961e-01,\n",
       "             9.9960e-01, 9.9947e-01, 9.9928e-01, 9.9920e-01, 9.9904e-01, 9.9895e-01,\n",
       "             9.9870e-01, 9.9848e-01, 9.9847e-01, 9.9842e-01, 9.9822e-01, 9.9792e-01,\n",
       "             9.9759e-01, 9.9756e-01, 9.9603e-01, 9.9532e-01, 9.9514e-01, 9.9508e-01,\n",
       "             9.9412e-01, 9.9362e-01, 9.9247e-01, 9.9172e-01, 9.9144e-01, 9.8978e-01,\n",
       "             9.8945e-01, 9.8631e-01, 9.8621e-01, 9.8562e-01, 9.8456e-01, 9.8126e-01,\n",
       "             9.7415e-01, 9.6739e-01, 9.6465e-01, 9.5946e-01, 9.5643e-01, 9.2045e-01,\n",
       "             9.1133e-01, 9.0409e-01, 8.9416e-01, 8.8065e-01, 8.7301e-01, 8.6537e-01,\n",
       "             8.6297e-01, 8.2097e-01, 7.6777e-01, 7.2511e-01, 6.4897e-01, 6.3145e-01,\n",
       "             6.2145e-01, 5.4173e-01, 4.9489e-01, 4.3679e-01, 4.0620e-01, 4.0258e-01,\n",
       "             3.1320e-01, 2.8717e-01, 2.2740e-01, 2.0463e-01, 1.7436e-01, 1.3094e-01,\n",
       "             1.2889e-01, 1.0356e-01, 1.0049e-01, 9.9163e-02, 7.8544e-02, 6.9117e-02,\n",
       "             6.6926e-02, 5.4271e-02, 5.0900e-02, 3.6344e-02, 3.4948e-02, 3.4099e-02,\n",
       "             3.1693e-02, 2.8171e-02, 2.6546e-02, 1.8426e-02, 1.7046e-02, 1.4114e-02,\n",
       "             1.3685e-02, 1.3678e-02, 1.2382e-02, 1.0706e-02, 9.4864e-03, 6.8747e-03,\n",
       "             3.0937e-03, 3.0170e-03, 2.5345e-03, 2.1557e-03, 2.1343e-03, 1.7282e-03,\n",
       "             1.5548e-03, 1.1460e-03, 1.0073e-03, 7.3169e-04, 6.4867e-04, 6.4480e-04,\n",
       "             6.0775e-04, 5.6530e-04, 5.2462e-04, 4.7387e-04, 4.3459e-04, 3.8681e-04,\n",
       "             2.8552e-04, 2.6634e-04, 2.5764e-04, 2.4083e-04, 1.9493e-04, 1.8878e-04,\n",
       "             1.8563e-04, 1.7560e-04, 1.7157e-04, 1.7053e-04, 1.3929e-04, 1.2784e-04,\n",
       "             1.1387e-04, 1.0162e-04, 9.4559e-05, 8.8382e-05, 8.7043e-05, 8.2056e-05,\n",
       "             7.4692e-05, 6.8827e-05, 6.1004e-05, 5.5532e-05, 5.5213e-05, 5.1485e-05,\n",
       "             4.8254e-05, 4.5171e-05, 3.9352e-05, 3.1170e-05, 3.1105e-05, 3.1024e-05,\n",
       "             2.6681e-05, 2.6371e-05, 2.3571e-05, 2.2055e-05, 1.4407e-05, 1.2640e-05,\n",
       "             1.1394e-05, 1.0412e-05, 9.9274e-06, 9.7592e-06, 8.9308e-06, 8.3266e-06,\n",
       "             7.9939e-06, 4.8042e-06, 4.0091e-06, 3.9986e-06, 3.9647e-06, 3.7029e-06,\n",
       "             3.2056e-06, 3.1047e-06, 2.8351e-06, 2.6484e-06, 2.1960e-06, 1.7630e-06,\n",
       "             1.5739e-06, 1.4356e-06, 1.4337e-06, 1.2508e-06, 1.2148e-06, 1.1706e-06,\n",
       "             1.1010e-06, 1.0543e-06, 9.5953e-07, 8.6402e-07, 7.3858e-07, 6.7851e-07,\n",
       "             6.1787e-07, 5.8617e-07, 5.6447e-07, 5.5444e-07, 5.5306e-07, 5.2112e-07,\n",
       "             4.9334e-07, 4.9070e-07, 4.8979e-07, 4.7245e-07, 4.4378e-07, 4.0346e-07,\n",
       "             3.9111e-07, 3.7299e-07, 3.4088e-07, 2.8016e-07, 2.7871e-07, 2.7194e-07,\n",
       "             2.6028e-07, 2.3129e-07, 2.2342e-07, 2.1575e-07, 2.0713e-07, 2.0095e-07,\n",
       "             1.7458e-07, 1.7245e-07, 1.7206e-07, 1.7072e-07, 1.7048e-07, 1.7035e-07,\n",
       "             1.5501e-07, 1.5388e-07, 1.4742e-07, 1.2580e-07, 1.2079e-07, 1.1474e-07,\n",
       "             1.1345e-07, 1.0921e-07, 9.7691e-08, 7.5275e-08, 7.5198e-08, 7.4485e-08,\n",
       "             7.0056e-08, 5.7016e-08, 5.5413e-08, 5.5353e-08, 5.1624e-08, 4.7474e-08,\n",
       "             4.7070e-08, 4.5684e-08, 4.5095e-08, 3.5425e-08, 3.5254e-08, 3.1353e-08,\n",
       "             3.1216e-08, 2.9876e-08, 2.8170e-08, 2.7054e-08, 2.6495e-08, 2.3217e-08,\n",
       "             2.2681e-08, 2.2675e-08, 1.9891e-08, 1.9801e-08, 1.5587e-08, 1.5536e-08,\n",
       "             1.2731e-08, 1.2553e-08, 1.0306e-08, 9.2562e-09, 8.6314e-09, 8.1738e-09,\n",
       "             7.4849e-09, 5.5705e-09, 4.6980e-09, 4.5901e-09, 4.5527e-09, 4.3414e-09,\n",
       "             3.6377e-09, 3.2895e-09, 3.2663e-09, 3.1705e-09, 3.1370e-09, 2.9094e-09,\n",
       "             2.6626e-09, 2.2804e-09, 2.1271e-09, 1.9659e-09, 1.9643e-09, 1.8952e-09,\n",
       "             1.7425e-09, 1.4847e-09, 1.4128e-09, 1.2881e-09, 1.2320e-09, 1.2119e-09,\n",
       "             1.1740e-09, 1.1303e-09, 7.8291e-10, 7.0888e-10, 6.7403e-10, 6.3173e-10,\n",
       "             4.6952e-10, 4.1640e-10, 3.4420e-10, 3.4085e-10, 3.3725e-10, 3.2089e-10,\n",
       "             3.1329e-10, 2.7871e-10, 2.7663e-10, 2.5127e-10, 2.5075e-10, 2.1168e-10,\n",
       "             1.4683e-10, 1.1747e-10, 1.1572e-10, 1.1099e-10, 1.0788e-10, 1.0187e-10,\n",
       "             7.3614e-11, 7.3367e-11, 6.7211e-11, 6.5142e-11, 5.7478e-11, 5.4840e-11,\n",
       "             3.9901e-11, 3.9378e-11, 3.5684e-11, 2.9241e-11, 2.7604e-11, 2.3350e-11,\n",
       "             2.0187e-11, 1.6790e-11, 1.5379e-11, 1.5348e-11, 1.3175e-11, 1.2195e-11,\n",
       "             1.2022e-11, 1.1100e-11, 1.0794e-11, 8.0653e-12, 7.7166e-12, 7.7034e-12,\n",
       "             7.4244e-12, 6.2023e-12, 4.7455e-12, 4.5878e-12, 3.8299e-12, 3.5114e-12,\n",
       "             3.4860e-12, 3.0375e-12, 2.8006e-12, 2.6465e-12, 2.1459e-12, 8.3182e-13,\n",
       "             7.3573e-13, 6.0612e-13, 5.7404e-13, 4.7201e-13, 2.5967e-13, 2.2770e-13,\n",
       "             2.2404e-13, 1.9268e-13, 1.8713e-13, 1.8669e-13, 1.5467e-13, 1.1924e-13,\n",
       "             6.6008e-14, 4.2140e-14, 4.1196e-14, 3.6550e-14, 3.2684e-14, 1.9403e-14,\n",
       "             1.0320e-14, 7.6686e-15, 7.5353e-15, 6.8435e-15, 6.1599e-15, 5.3941e-15,\n",
       "             5.2841e-15, 3.9905e-15, 1.1319e-15, 1.3452e-16, 3.5270e-18, 2.8903e-18,\n",
       "             6.4887e-20])}},\n",
       "   {'fpr': np.float64(0.08385093167701864),\n",
       "    'tpr': np.float64(0.9812865497076023),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0217, 0.0248,\n",
       "             0.0248, 0.0248, 0.0280, 0.0280, 0.0280, 0.0311, 0.0311, 0.0311, 0.0311,\n",
       "             0.0311, 0.0311, 0.0311, 0.0311, 0.0342, 0.0342, 0.0342, 0.0342, 0.0373,\n",
       "             0.0404, 0.0435, 0.0435, 0.0466, 0.0466, 0.0466, 0.0466, 0.0466, 0.0466,\n",
       "             0.0466, 0.0466, 0.0466, 0.0466, 0.0497, 0.0528, 0.0559, 0.0590, 0.0590,\n",
       "             0.0621, 0.0621, 0.0621, 0.0621, 0.0621, 0.0621, 0.0652, 0.0683, 0.0714,\n",
       "             0.0745, 0.0745, 0.0745, 0.0745, 0.0745, 0.0776, 0.0776, 0.0776, 0.0807,\n",
       "             0.0839, 0.0839, 0.0870, 0.0901, 0.0901, 0.0901, 0.0932, 0.0932, 0.0994,\n",
       "             0.0994, 0.1025, 0.1025, 0.1056, 0.1087, 0.1118, 0.1149, 0.1180, 0.1180,\n",
       "             0.1211, 0.1242, 0.1273, 0.1304, 0.1335, 0.1366, 0.1366, 0.1398, 0.1429,\n",
       "             0.1460, 0.1491, 0.1522, 0.1553, 0.1584, 0.1615, 0.1615, 0.1646, 0.1677,\n",
       "             0.1708, 0.1708, 0.1739, 0.1739, 0.1770, 0.1801, 0.1832, 0.1832, 0.1863,\n",
       "             0.1894, 0.1925, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050, 0.2081, 0.2112,\n",
       "             0.2143, 0.2143, 0.2174, 0.2205, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329,\n",
       "             0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578, 0.2609,\n",
       "             0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857, 0.2888,\n",
       "             0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137, 0.3168,\n",
       "             0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416, 0.3447,\n",
       "             0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696, 0.3727,\n",
       "             0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975, 0.4006,\n",
       "             0.4037, 0.4068, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224, 0.4255, 0.4286,\n",
       "             0.4317, 0.4348, 0.4379, 0.4410, 0.4441, 0.4472, 0.4503, 0.4534, 0.4565,\n",
       "             0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752, 0.4783, 0.4814, 0.4845,\n",
       "             0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031, 0.5062, 0.5093, 0.5124,\n",
       "             0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311, 0.5342, 0.5373, 0.5404,\n",
       "             0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590, 0.5621, 0.5652, 0.5683,\n",
       "             0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870, 0.5901, 0.5932, 0.5963,\n",
       "             0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149, 0.6180, 0.6180, 0.6211,\n",
       "             0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429, 0.6460, 0.6491,\n",
       "             0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708, 0.6739, 0.6770,\n",
       "             0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988, 0.7019, 0.7050,\n",
       "             0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267, 0.7298, 0.7329,\n",
       "             0.7360, 0.7391, 0.7422, 0.7453, 0.7484, 0.7516, 0.7547, 0.7578, 0.7609,\n",
       "             0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826, 0.7857, 0.7888,\n",
       "             0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106, 0.8137, 0.8168,\n",
       "             0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385, 0.8416, 0.8447,\n",
       "             0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665, 0.8696, 0.8727,\n",
       "             0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944, 0.8975, 0.9006,\n",
       "             0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224, 0.9255, 0.9286,\n",
       "             0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503, 0.9534, 0.9565,\n",
       "             0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783, 0.9814, 0.9845,\n",
       "             0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8058, 0.8468, 0.8550, 0.8620, 0.8667, 0.8713, 0.8737, 0.8749,\n",
       "             0.8760, 0.8784, 0.8807, 0.8830, 0.8842, 0.8865, 0.8877, 0.8889, 0.8912,\n",
       "             0.8924, 0.8936, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029, 0.9029,\n",
       "             0.9041, 0.9053, 0.9064, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9135,\n",
       "             0.9146, 0.9158, 0.9170, 0.9181, 0.9181, 0.9193, 0.9205, 0.9216, 0.9228,\n",
       "             0.9240, 0.9251, 0.9263, 0.9263, 0.9275, 0.9287, 0.9298, 0.9310, 0.9322,\n",
       "             0.9333, 0.9345, 0.9345, 0.9357, 0.9368, 0.9380, 0.9392, 0.9392, 0.9392,\n",
       "             0.9404, 0.9415, 0.9415, 0.9427, 0.9439, 0.9439, 0.9450, 0.9462, 0.9474,\n",
       "             0.9485, 0.9497, 0.9509, 0.9520, 0.9520, 0.9532, 0.9544, 0.9556, 0.9556,\n",
       "             0.9556, 0.9556, 0.9567, 0.9567, 0.9579, 0.9591, 0.9602, 0.9614, 0.9626,\n",
       "             0.9637, 0.9649, 0.9661, 0.9673, 0.9673, 0.9673, 0.9673, 0.9673, 0.9684,\n",
       "             0.9684, 0.9696, 0.9708, 0.9719, 0.9731, 0.9743, 0.9743, 0.9743, 0.9743,\n",
       "             0.9743, 0.9754, 0.9766, 0.9778, 0.9789, 0.9789, 0.9801, 0.9813, 0.9813,\n",
       "             0.9813, 0.9825, 0.9825, 0.9825, 0.9836, 0.9848, 0.9848, 0.9860, 0.9860,\n",
       "             0.9871, 0.9871, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9895,\n",
       "             0.9895, 0.9895, 0.9895, 0.9895, 0.9895, 0.9895, 0.9906, 0.9906, 0.9906,\n",
       "             0.9906, 0.9906, 0.9906, 0.9906, 0.9906, 0.9906, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9930, 0.9930, 0.9942, 0.9942, 0.9942, 0.9942, 0.9953, 0.9953,\n",
       "             0.9953, 0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9992e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9983e-01,\n",
       "             9.9978e-01, 9.9976e-01, 9.9972e-01, 9.9967e-01, 9.9965e-01, 9.9965e-01,\n",
       "             9.9932e-01, 9.9932e-01, 9.9919e-01, 9.9886e-01, 9.9878e-01, 9.9843e-01,\n",
       "             9.9762e-01, 9.9755e-01, 9.9736e-01, 9.9673e-01, 9.9514e-01, 9.9331e-01,\n",
       "             9.9168e-01, 9.9105e-01, 9.9095e-01, 9.9086e-01, 9.8620e-01, 9.8572e-01,\n",
       "             9.8391e-01, 9.8165e-01, 9.7696e-01, 9.7015e-01, 9.6908e-01, 9.6093e-01,\n",
       "             9.5334e-01, 9.5210e-01, 9.4990e-01, 9.4944e-01, 9.4743e-01, 9.3309e-01,\n",
       "             9.2961e-01, 9.2706e-01, 8.9564e-01, 8.8655e-01, 8.6148e-01, 8.5254e-01,\n",
       "             8.3796e-01, 8.3783e-01, 8.3530e-01, 8.0618e-01, 8.0123e-01, 7.8192e-01,\n",
       "             7.5934e-01, 7.0955e-01, 6.6221e-01, 6.1397e-01, 4.8420e-01, 4.2637e-01,\n",
       "             4.0318e-01, 3.1889e-01, 3.1660e-01, 2.8038e-01, 2.3820e-01, 1.9981e-01,\n",
       "             1.0221e-01, 8.1979e-02, 7.3678e-02, 6.4238e-02, 6.1261e-02, 5.8058e-02,\n",
       "             5.1044e-02, 4.7807e-02, 4.7450e-02, 4.1562e-02, 3.4676e-02, 3.0256e-02,\n",
       "             2.7546e-02, 2.6934e-02, 1.9995e-02, 1.9316e-02, 1.8481e-02, 1.3998e-02,\n",
       "             1.2985e-02, 1.2250e-02, 8.7663e-03, 7.7890e-03, 6.7868e-03, 5.3589e-03,\n",
       "             5.2696e-03, 4.4807e-03, 3.6576e-03, 3.5111e-03, 3.2479e-03, 2.9070e-03,\n",
       "             2.8364e-03, 1.4679e-03, 1.3025e-03, 1.2024e-03, 1.1658e-03, 8.3814e-04,\n",
       "             6.4906e-04, 5.3067e-04, 5.1563e-04, 4.2084e-04, 3.6098e-04, 3.5401e-04,\n",
       "             2.8493e-04, 2.7606e-04, 2.7205e-04, 2.5706e-04, 2.3720e-04, 1.7091e-04,\n",
       "             1.4135e-04, 1.3478e-04, 1.1851e-04, 1.1528e-04, 9.7758e-05, 9.4147e-05,\n",
       "             6.4507e-05, 5.3369e-05, 4.9678e-05, 4.4655e-05, 2.3512e-05, 1.8150e-05,\n",
       "             1.7660e-05, 1.6659e-05, 1.5505e-05, 1.2201e-05, 7.2255e-06, 6.9747e-06,\n",
       "             5.6848e-06, 5.6587e-06, 5.5841e-06, 3.6261e-06, 3.2292e-06, 2.4548e-06,\n",
       "             2.4416e-06, 2.1593e-06, 1.9839e-06, 1.8672e-06, 1.6805e-06, 1.3490e-06,\n",
       "             1.3197e-06, 1.2487e-06, 1.1499e-06, 1.1499e-06, 1.0262e-06, 1.0174e-06,\n",
       "             6.4327e-07, 6.1269e-07, 5.9379e-07, 5.5342e-07, 5.3835e-07, 5.2348e-07,\n",
       "             5.0125e-07, 3.8802e-07, 3.8780e-07, 3.1393e-07, 2.2876e-07, 2.0718e-07,\n",
       "             1.8050e-07, 1.5143e-07, 1.4242e-07, 1.4029e-07, 1.2957e-07, 9.7678e-08,\n",
       "             9.6642e-08, 9.2883e-08, 6.7276e-08, 5.7022e-08, 5.5618e-08, 5.0851e-08,\n",
       "             4.7183e-08, 4.5253e-08, 4.1970e-08, 4.0848e-08, 3.9362e-08, 3.8345e-08,\n",
       "             3.0957e-08, 2.7627e-08, 2.7496e-08, 2.6497e-08, 1.9845e-08, 1.7688e-08,\n",
       "             1.7344e-08, 1.6981e-08, 1.6278e-08, 1.6264e-08, 1.2383e-08, 1.2164e-08,\n",
       "             1.2005e-08, 1.1884e-08, 1.1129e-08, 9.5948e-09, 7.9753e-09, 6.3150e-09,\n",
       "             6.2451e-09, 5.3078e-09, 5.0197e-09, 3.3229e-09, 3.0949e-09, 2.8245e-09,\n",
       "             2.5422e-09, 2.1303e-09, 1.6813e-09, 1.6131e-09, 1.5603e-09, 1.3062e-09,\n",
       "             1.2539e-09, 1.1925e-09, 1.1503e-09, 9.7336e-10, 9.3987e-10, 8.3410e-10,\n",
       "             7.4847e-10, 7.0472e-10, 6.8478e-10, 6.6546e-10, 6.4704e-10, 5.9293e-10,\n",
       "             5.1954e-10, 5.1604e-10, 4.9321e-10, 4.7593e-10, 3.8548e-10, 3.4066e-10,\n",
       "             3.2813e-10, 3.0462e-10, 2.8342e-10, 2.6921e-10, 2.4916e-10, 2.4147e-10,\n",
       "             2.2007e-10, 1.7540e-10, 1.5564e-10, 1.1980e-10, 1.1883e-10, 6.2806e-11,\n",
       "             5.4755e-11, 5.2877e-11, 5.1174e-11, 4.7711e-11, 4.1144e-11, 4.1014e-11,\n",
       "             4.0614e-11, 3.1172e-11, 2.9304e-11, 2.8300e-11, 2.4753e-11, 1.9723e-11,\n",
       "             1.9681e-11, 1.9143e-11, 1.6715e-11, 1.5116e-11, 1.2795e-11, 1.0370e-11,\n",
       "             1.0113e-11, 9.8687e-12, 8.6615e-12, 7.6264e-12, 5.4488e-12, 4.6091e-12,\n",
       "             4.4513e-12, 4.0769e-12, 3.5615e-12, 2.9118e-12, 2.8379e-12, 2.6984e-12,\n",
       "             2.6681e-12, 2.5949e-12, 2.4565e-12, 2.4249e-12, 1.9007e-12, 1.6961e-12,\n",
       "             1.6767e-12, 1.5598e-12, 1.0883e-12, 9.7640e-13, 8.3941e-13, 8.0427e-13,\n",
       "             6.8360e-13, 6.3895e-13, 5.8586e-13, 2.8836e-13, 2.8520e-13, 1.5032e-13,\n",
       "             9.7554e-14, 9.2196e-14, 8.7452e-14, 8.4850e-14, 8.4839e-14, 6.8710e-14,\n",
       "             6.5212e-14, 6.2010e-14, 6.1670e-14, 5.1365e-14, 4.9966e-14, 4.8317e-14,\n",
       "             3.8208e-14, 3.6646e-14, 2.6163e-14, 2.5219e-14, 1.8087e-14, 1.7539e-14,\n",
       "             1.2126e-14, 1.2115e-14, 1.1689e-14, 1.0398e-14, 8.6621e-15, 7.6090e-15,\n",
       "             6.0257e-15, 5.5555e-15, 5.2393e-15, 4.9608e-15, 4.4371e-15, 3.9620e-15,\n",
       "             3.2477e-15, 2.1309e-15, 2.0552e-15, 1.6385e-15, 1.5309e-15, 1.4616e-15,\n",
       "             1.2701e-15, 7.6677e-16, 5.8988e-16, 4.9271e-16, 4.8618e-16, 4.6411e-16,\n",
       "             4.4265e-16, 3.1547e-16, 2.7249e-16, 2.1928e-16, 1.9539e-16, 1.7345e-16,\n",
       "             1.7270e-16, 1.7009e-16, 1.6924e-16, 1.2466e-16, 1.2366e-16, 8.8483e-17,\n",
       "             8.3273e-17, 5.8747e-17, 4.6490e-17, 4.1308e-17, 3.3506e-17, 2.7496e-17,\n",
       "             2.3214e-17, 1.7775e-17, 4.2948e-18, 3.6238e-18, 2.7307e-18, 2.1757e-18,\n",
       "             2.0221e-18, 1.3412e-18, 1.7796e-19, 1.5693e-19, 5.3715e-20, 4.1828e-20,\n",
       "             8.5080e-21, 3.3408e-21, 1.8983e-21, 7.2684e-22, 6.8294e-22, 7.5465e-23,\n",
       "             8.7096e-24, 1.8537e-25])}},\n",
       "   {'fpr': np.float64(0.32608695652173914),\n",
       "    'tpr': np.float64(0.9976608187134502),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.1056, 0.1087, 0.1149, 0.1211, 0.1242, 0.1304, 0.1304, 0.1335,\n",
       "             0.1366, 0.1366, 0.1366, 0.1398, 0.1429, 0.1460, 0.1491, 0.1522, 0.1553,\n",
       "             0.1584, 0.1615, 0.1646, 0.1677, 0.1708, 0.1739, 0.1770, 0.1801, 0.1832,\n",
       "             0.1863, 0.1894, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050, 0.2081, 0.2112,\n",
       "             0.2143, 0.2174, 0.2205, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329, 0.2360,\n",
       "             0.2391, 0.2422, 0.2453, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578, 0.2609,\n",
       "             0.2640, 0.2640, 0.2671, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826,\n",
       "             0.2857, 0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106,\n",
       "             0.3137, 0.3168, 0.3199, 0.3230, 0.3261, 0.3261, 0.3292, 0.3323, 0.3354,\n",
       "             0.3385, 0.3416, 0.3447, 0.3478, 0.3509, 0.3509, 0.3540, 0.3571, 0.3602,\n",
       "             0.3634, 0.3665, 0.3696, 0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882,\n",
       "             0.3913, 0.3944, 0.3975, 0.4006, 0.4037, 0.4068, 0.4099, 0.4130, 0.4161,\n",
       "             0.4193, 0.4224, 0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4410, 0.4441,\n",
       "             0.4472, 0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720,\n",
       "             0.4752, 0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000,\n",
       "             0.5031, 0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280,\n",
       "             0.5311, 0.5342, 0.5373, 0.5404, 0.5404, 0.5435, 0.5466, 0.5497, 0.5528,\n",
       "             0.5559, 0.5590, 0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776, 0.5807,\n",
       "             0.5839, 0.5870, 0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056, 0.6087,\n",
       "             0.6118, 0.6149, 0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335, 0.6366,\n",
       "             0.6398, 0.6429, 0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615, 0.6646,\n",
       "             0.6677, 0.6708, 0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894, 0.6925,\n",
       "             0.6957, 0.6988, 0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174, 0.7205,\n",
       "             0.7236, 0.7267, 0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453, 0.7484,\n",
       "             0.7516, 0.7547, 0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764,\n",
       "             0.7795, 0.7826, 0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043,\n",
       "             0.8075, 0.8106, 0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323,\n",
       "             0.8354, 0.8385, 0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602,\n",
       "             0.8634, 0.8665, 0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882,\n",
       "             0.8913, 0.8944, 0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161,\n",
       "             0.9193, 0.9224, 0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441,\n",
       "             0.9472, 0.9503, 0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720,\n",
       "             0.9752, 0.9783, 0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9848, 0.9871, 0.9883, 0.9883, 0.9883, 0.9883, 0.9895, 0.9895,\n",
       "             0.9895, 0.9906, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9918, 0.9930, 0.9930, 0.9930, 0.9930, 0.9930, 0.9930,\n",
       "             0.9930, 0.9930, 0.9930, 0.9942, 0.9942, 0.9942, 0.9942, 0.9942, 0.9942,\n",
       "             0.9942, 0.9953, 0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9988e-01, 9.9987e-01, 9.9976e-01, 9.9973e-01, 9.9971e-01,\n",
       "             9.9961e-01, 9.9959e-01, 9.9948e-01, 9.9941e-01, 9.9935e-01, 9.9935e-01,\n",
       "             9.9922e-01, 9.9874e-01, 9.9873e-01, 9.9851e-01, 9.9800e-01, 9.9770e-01,\n",
       "             9.9765e-01, 9.9651e-01, 9.9341e-01, 9.9041e-01, 9.9003e-01, 9.8966e-01,\n",
       "             9.8871e-01, 9.8835e-01, 9.8764e-01, 9.8506e-01, 9.8431e-01, 9.7369e-01,\n",
       "             9.7185e-01, 9.7052e-01, 9.6633e-01, 9.6224e-01, 9.3711e-01, 9.1429e-01,\n",
       "             9.1396e-01, 9.0290e-01, 8.9741e-01, 8.7448e-01, 8.4321e-01, 8.1718e-01,\n",
       "             7.1689e-01, 6.9231e-01, 6.8093e-01, 6.5880e-01, 6.4598e-01, 6.3359e-01,\n",
       "             6.3015e-01, 6.1000e-01, 5.8041e-01, 5.7097e-01, 5.5384e-01, 5.1107e-01,\n",
       "             4.7310e-01, 4.6307e-01, 3.9370e-01, 3.4249e-01, 2.4926e-01, 1.2969e-01,\n",
       "             1.1653e-01, 1.1503e-01, 1.0350e-01, 8.8735e-02, 8.2611e-02, 5.8413e-02,\n",
       "             5.6214e-02, 5.5117e-02, 3.7596e-02, 3.1057e-02, 3.0085e-02, 2.1304e-02,\n",
       "             1.8983e-02, 1.4684e-02, 1.3920e-02, 1.3721e-02, 1.1626e-02, 9.5057e-03,\n",
       "             8.8799e-03, 7.9458e-03, 6.0053e-03, 5.2737e-03, 5.2701e-03, 4.5554e-03,\n",
       "             2.9964e-03, 2.6346e-03, 2.3433e-03, 1.7318e-03, 1.5242e-03, 1.5004e-03,\n",
       "             1.4885e-03, 8.6824e-04, 7.8189e-04, 5.9473e-04, 4.6227e-04, 4.3505e-04,\n",
       "             3.9865e-04, 3.1952e-04, 2.7376e-04, 2.1427e-04, 1.9301e-04, 1.5010e-04,\n",
       "             1.2737e-04, 9.9608e-05, 7.1173e-05, 6.5225e-05, 6.4852e-05, 3.7387e-05,\n",
       "             3.3503e-05, 2.4758e-05, 2.3590e-05, 2.2455e-05, 2.1017e-05, 1.9796e-05,\n",
       "             1.5583e-05, 1.2649e-05, 1.2154e-05, 1.0929e-05, 1.0700e-05, 9.4701e-06,\n",
       "             9.3337e-06, 8.1069e-06, 7.7053e-06, 6.9737e-06, 5.9994e-06, 5.7345e-06,\n",
       "             5.6452e-06, 5.4064e-06, 5.1242e-06, 4.9790e-06, 4.0469e-06, 3.7948e-06,\n",
       "             3.4882e-06, 2.6549e-06, 2.5634e-06, 2.4011e-06, 2.0602e-06, 1.5935e-06,\n",
       "             1.4896e-06, 1.4785e-06, 9.7481e-07, 8.4032e-07, 8.2330e-07, 6.1266e-07,\n",
       "             3.2305e-07, 2.7316e-07, 2.7240e-07, 2.0246e-07, 1.7810e-07, 1.5911e-07,\n",
       "             1.5465e-07, 1.3565e-07, 1.1472e-07, 9.8920e-08, 8.1445e-08, 6.4686e-08,\n",
       "             5.6957e-08, 5.1807e-08, 4.2411e-08, 2.9935e-08, 2.9387e-08, 2.6815e-08,\n",
       "             2.1540e-08, 1.7808e-08, 1.7793e-08, 1.6605e-08, 1.5588e-08, 1.4327e-08,\n",
       "             1.4228e-08, 1.4140e-08, 1.3829e-08, 1.3133e-08, 9.2442e-09, 8.2227e-09,\n",
       "             8.0394e-09, 5.7966e-09, 5.7457e-09, 5.5544e-09, 4.7366e-09, 4.7359e-09,\n",
       "             4.6315e-09, 4.4006e-09, 4.2763e-09, 4.0770e-09, 3.4945e-09, 2.5141e-09,\n",
       "             2.4433e-09, 2.2627e-09, 2.0880e-09, 1.7019e-09, 1.0217e-09, 9.3294e-10,\n",
       "             8.8528e-10, 8.3056e-10, 5.9552e-10, 5.3968e-10, 5.2002e-10, 5.0427e-10,\n",
       "             4.2780e-10, 3.6892e-10, 3.5474e-10, 3.0384e-10, 3.0348e-10, 2.2387e-10,\n",
       "             1.7521e-10, 1.4499e-10, 1.4389e-10, 1.3990e-10, 1.2617e-10, 1.0779e-10,\n",
       "             9.6119e-11, 9.3841e-11, 9.3238e-11, 5.9814e-11, 3.7590e-11, 1.9130e-11,\n",
       "             1.2878e-11, 1.1631e-11, 1.0759e-11, 1.0743e-11, 9.1412e-12, 7.5731e-12,\n",
       "             7.2839e-12, 2.9458e-12, 2.8052e-12, 2.2655e-12, 1.3693e-12, 1.1810e-12,\n",
       "             9.0490e-13, 8.8672e-13, 7.0597e-13, 4.9461e-13, 4.6003e-13, 4.1339e-13,\n",
       "             3.6748e-13, 3.3477e-13, 2.7632e-13, 2.7431e-13, 2.4528e-13, 1.5832e-13,\n",
       "             1.4975e-13, 1.3941e-13, 7.4367e-14, 7.2630e-14, 6.9017e-14, 6.5100e-14,\n",
       "             5.5042e-14, 4.1141e-14, 3.3974e-14, 3.1612e-14, 1.8090e-14, 9.3151e-15,\n",
       "             7.8049e-15, 2.1157e-15, 1.9628e-15, 1.2713e-15, 1.0910e-15, 3.3358e-16,\n",
       "             2.4268e-16, 2.2475e-16, 1.4497e-16, 1.3840e-16, 1.0285e-16, 4.3161e-17,\n",
       "             3.7137e-17, 2.1802e-17, 1.4141e-17, 4.7474e-18, 2.2913e-18, 2.3045e-19,\n",
       "             6.4051e-20, 4.2582e-20, 2.7734e-23])}}],\n",
       "  [{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.005747126436781609),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 6.6341e-01, 6.4758e-01,  ..., 1.0260e-07, 1.8032e-08,\n",
       "             8.5904e-09])}},\n",
       "   {'fpr': np.float64(0.006514657980456026),\n",
       "    'tpr': np.float64(0.8528735632183908),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9993e-01, 9.9969e-01,  ..., 1.8457e-06, 1.7331e-06,\n",
       "             1.4368e-06])}},\n",
       "   {'fpr': np.float64(0.02280130293159609),\n",
       "    'tpr': np.float64(0.9091954022988505),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 9.9998e-01,  ..., 1.2266e-07, 9.7331e-08,\n",
       "             4.0143e-08])}},\n",
       "   {'fpr': np.float64(0.04234527687296417),\n",
       "    'tpr': np.float64(0.9517241379310345),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9989e-01,  ..., 2.3299e-07, 2.0987e-07,\n",
       "             7.9773e-08])}},\n",
       "   {'fpr': np.float64(0.03257328990228013),\n",
       "    'tpr': np.float64(0.9620689655172414),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9998e-01, 9.9991e-01,  ..., 3.3779e-07, 1.3030e-07,\n",
       "             1.9506e-08])}},\n",
       "   {'fpr': np.float64(0.03908794788273615),\n",
       "    'tpr': np.float64(0.9712643678160919),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.8149e-09, 7.8341e-10,\n",
       "             6.2051e-11])}},\n",
       "   {'fpr': np.float64(0.016286644951140065),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9959e-01, 9.9895e-01,  ..., 1.5764e-09, 1.3717e-09,\n",
       "             8.6240e-12])}},\n",
       "   {'fpr': np.float64(0.06188925081433225),\n",
       "    'tpr': np.float64(0.9770114942528736),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9996e-01, 9.9995e-01,  ..., 3.2982e-08, 2.1384e-08,\n",
       "             4.5861e-10])}},\n",
       "   {'fpr': np.float64(0.07166123778501629),\n",
       "    'tpr': np.float64(0.9862068965517241),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9998e-01, 9.9997e-01,  ..., 1.5995e-09, 5.1251e-10,\n",
       "             1.5140e-12])}},\n",
       "   {'fpr': np.float64(0.05863192182410423),\n",
       "    'tpr': np.float64(0.9816091954022989),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9999e-01,  ..., 1.0564e-11, 9.0585e-14,\n",
       "             6.5259e-16])}},\n",
       "   {'fpr': np.float64(0.08794788273615635),\n",
       "    'tpr': np.float64(0.9862068965517241),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0130, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
       "             0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
       "             0.0163, 0.0163, 0.0195, 0.0195, 0.0228, 0.0228, 0.0228, 0.0228, 0.0261,\n",
       "             0.0293, 0.0293, 0.0326, 0.0326, 0.0358, 0.0358, 0.0391, 0.0423, 0.0456,\n",
       "             0.0489, 0.0489, 0.0489, 0.0521, 0.0554, 0.0586, 0.0586, 0.0586, 0.0586,\n",
       "             0.0619, 0.0619, 0.0619, 0.0651, 0.0651, 0.0684, 0.0684, 0.0684, 0.0717,\n",
       "             0.0717, 0.0717, 0.0749, 0.0749, 0.0782, 0.0814, 0.0847, 0.0879, 0.0879,\n",
       "             0.0912, 0.0912, 0.0945, 0.0945, 0.0977, 0.1010, 0.1042, 0.1075, 0.1107,\n",
       "             0.1140, 0.1173, 0.1173, 0.1205, 0.1238, 0.1238, 0.1270, 0.1303, 0.1336,\n",
       "             0.1368, 0.1401, 0.1401, 0.1433, 0.1466, 0.1498, 0.1531, 0.1564, 0.1596,\n",
       "             0.1629, 0.1629, 0.1661, 0.1694, 0.1726, 0.1759, 0.1792, 0.1824, 0.1857,\n",
       "             0.1889, 0.1922, 0.1954, 0.1954, 0.1987, 0.2020, 0.2052, 0.2085, 0.2117,\n",
       "             0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345, 0.2378,\n",
       "             0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2606, 0.2638, 0.2671,\n",
       "             0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899, 0.2932,\n",
       "             0.2964, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160, 0.3192, 0.3225,\n",
       "             0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453, 0.3485, 0.3518,\n",
       "             0.3550, 0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779, 0.3811,\n",
       "             0.3844, 0.3876, 0.3909, 0.3941, 0.3974, 0.4007, 0.4039, 0.4072, 0.4072,\n",
       "             0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267, 0.4300, 0.4332, 0.4365,\n",
       "             0.4397, 0.4430, 0.4463, 0.4495, 0.4528, 0.4560, 0.4593, 0.4625, 0.4658,\n",
       "             0.4691, 0.4723, 0.4756, 0.4788, 0.4821, 0.4853, 0.4853, 0.4886, 0.4919,\n",
       "             0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114, 0.5147, 0.5179, 0.5212,\n",
       "             0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407, 0.5440, 0.5472, 0.5505,\n",
       "             0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700, 0.5733, 0.5765, 0.5798,\n",
       "             0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5993, 0.6026, 0.6059, 0.6091,\n",
       "             0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319, 0.6352, 0.6384,\n",
       "             0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612, 0.6645, 0.6678,\n",
       "             0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906, 0.6938, 0.6971,\n",
       "             0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199, 0.7231, 0.7264,\n",
       "             0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492, 0.7524, 0.7557,\n",
       "             0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785, 0.7818, 0.7850,\n",
       "             0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078, 0.8111, 0.8143,\n",
       "             0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371, 0.8404, 0.8436,\n",
       "             0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664, 0.8697, 0.8730,\n",
       "             0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958, 0.8990, 0.9023,\n",
       "             0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251, 0.9283, 0.9316,\n",
       "             0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544, 0.9577, 0.9609,\n",
       "             0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837, 0.9870, 0.9902,\n",
       "             0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0080, 0.0299, 0.0483, 0.0598, 0.0747, 0.0862, 0.0989, 0.1046,\n",
       "             0.1115, 0.1184, 0.1241, 0.1356, 0.1391, 0.1506, 0.1586, 0.1621, 0.1667,\n",
       "             0.1759, 0.1805, 0.1874, 0.1943, 0.1989, 0.2011, 0.2057, 0.2080, 0.2103,\n",
       "             0.2149, 0.2218, 0.2310, 0.2333, 0.2379, 0.2425, 0.2448, 0.2483, 0.2529,\n",
       "             0.2575, 0.2609, 0.2632, 0.2644, 0.2678, 0.2713, 0.2747, 0.2770, 0.2816,\n",
       "             0.2874, 0.2908, 0.2943, 0.3023, 0.3057, 0.3080, 0.3138, 0.3172, 0.3184,\n",
       "             0.3195, 0.3241, 0.3322, 0.3345, 0.3356, 0.3414, 0.3448, 0.3460, 0.3483,\n",
       "             0.3494, 0.3506, 0.3540, 0.3575, 0.3598, 0.3621, 0.3678, 0.3690, 0.3713,\n",
       "             0.3736, 0.3747, 0.3770, 0.3793, 0.3816, 0.3828, 0.3839, 0.3897, 0.3931,\n",
       "             0.3943, 0.3966, 0.3989, 0.4023, 0.4057, 0.4092, 0.4126, 0.4138, 0.4161,\n",
       "             0.4184, 0.4195, 0.4230, 0.4264, 0.4299, 0.4322, 0.4333, 0.4345, 0.4379,\n",
       "             0.4402, 0.4414, 0.4425, 0.4448, 0.4460, 0.4494, 0.4506, 0.4517, 0.4540,\n",
       "             0.4575, 0.4598, 0.4621, 0.4655, 0.4701, 0.4724, 0.4736, 0.4747, 0.4759,\n",
       "             0.4793, 0.4805, 0.4828, 0.4839, 0.4851, 0.4874, 0.4885, 0.4897, 0.4908,\n",
       "             0.4920, 0.4966, 0.5000, 0.5011, 0.5023, 0.5034, 0.5069, 0.5080, 0.5103,\n",
       "             0.5149, 0.5172, 0.5195, 0.5207, 0.5218, 0.5276, 0.5287, 0.5299, 0.5310,\n",
       "             0.5322, 0.5356, 0.5368, 0.5379, 0.5414, 0.5437, 0.5448, 0.5471, 0.5506,\n",
       "             0.5517, 0.5529, 0.5540, 0.5563, 0.5575, 0.5598, 0.5609, 0.5621, 0.5632,\n",
       "             0.5644, 0.5655, 0.5667, 0.5690, 0.5713, 0.5724, 0.5747, 0.5759, 0.5770,\n",
       "             0.5793, 0.5805, 0.5816, 0.5828, 0.5839, 0.5851, 0.5862, 0.5874, 0.5885,\n",
       "             0.5897, 0.5908, 0.5920, 0.5943, 0.5954, 0.5966, 0.5977, 0.6000, 0.6011,\n",
       "             0.6023, 0.6034, 0.6046, 0.6057, 0.6092, 0.6103, 0.6115, 0.6126, 0.6138,\n",
       "             0.6149, 0.6161, 0.6172, 0.6184, 0.6195, 0.6207, 0.6218, 0.6241, 0.6253,\n",
       "             0.6276, 0.6287, 0.6299, 0.6310, 0.6322, 0.6333, 0.6345, 0.6356, 0.6368,\n",
       "             0.6379, 0.6391, 0.6402, 0.6414, 0.6437, 0.6448, 0.6460, 0.6471, 0.6483,\n",
       "             0.6494, 0.6506, 0.6517, 0.6529, 0.6540, 0.6552, 0.6563, 0.6575, 0.6598,\n",
       "             0.6609, 0.6621, 0.6632, 0.6644, 0.6655, 0.6667, 0.6678, 0.6690, 0.6701,\n",
       "             0.6713, 0.6724, 0.6736, 0.6759, 0.6770, 0.6782, 0.6793, 0.6805, 0.6828,\n",
       "             0.6839, 0.6851, 0.6862, 0.6874, 0.6885, 0.6897, 0.6908, 0.6931, 0.6943,\n",
       "             0.6954, 0.6966, 0.6977, 0.6989, 0.7000, 0.7011, 0.7023, 0.7034, 0.7046,\n",
       "             0.7057, 0.7069, 0.7092, 0.7103, 0.7115, 0.7149, 0.7161, 0.7172, 0.7184,\n",
       "             0.7195, 0.7207, 0.7218, 0.7230, 0.7241, 0.7253, 0.7264, 0.7276, 0.7287,\n",
       "             0.7299, 0.7310, 0.7333, 0.7345, 0.7356, 0.7368, 0.7379, 0.7391, 0.7402,\n",
       "             0.7414, 0.7425, 0.7437, 0.7448, 0.7471, 0.7483, 0.7506, 0.7517, 0.7529,\n",
       "             0.7540, 0.7552, 0.7563, 0.7575, 0.7586, 0.7598, 0.7609, 0.7621, 0.7632,\n",
       "             0.7644, 0.7667, 0.7678, 0.7690, 0.7701, 0.7713, 0.7724, 0.7736, 0.7747,\n",
       "             0.7770, 0.7782, 0.7793, 0.7816, 0.7828, 0.7839, 0.7851, 0.7862, 0.7874,\n",
       "             0.7885, 0.7897, 0.7908, 0.7920, 0.7931, 0.7943, 0.7954, 0.7966, 0.7977,\n",
       "             0.7989, 0.8011, 0.8023, 0.8034, 0.8046, 0.8057, 0.8069, 0.8080, 0.8092,\n",
       "             0.8103, 0.8115, 0.8126, 0.8126, 0.8138, 0.8149, 0.8161, 0.8172, 0.8184,\n",
       "             0.8195, 0.8207, 0.8218, 0.8230, 0.8241, 0.8253, 0.8264, 0.8276, 0.8287,\n",
       "             0.8299, 0.8310, 0.8322, 0.8333, 0.8345, 0.8356, 0.8368, 0.8379, 0.8391,\n",
       "             0.8402, 0.8414, 0.8425, 0.8437, 0.8437, 0.8448, 0.8460, 0.8471, 0.8483,\n",
       "             0.8494, 0.8506, 0.8517, 0.8529, 0.8540, 0.8552, 0.8563, 0.8575, 0.8586,\n",
       "             0.8598, 0.8609, 0.8632, 0.8644, 0.8655, 0.8667, 0.8678, 0.8690, 0.8701,\n",
       "             0.8713, 0.8724, 0.8736, 0.8747, 0.8759, 0.8770, 0.8782, 0.8793, 0.8805,\n",
       "             0.8816, 0.8828, 0.8839, 0.8851, 0.8862, 0.8874, 0.8885, 0.8897, 0.8908,\n",
       "             0.8920, 0.8931, 0.8943, 0.8954, 0.8966, 0.8977, 0.8989, 0.9000, 0.9011,\n",
       "             0.9023, 0.9034, 0.9046, 0.9057, 0.9069, 0.9080, 0.9092, 0.9103, 0.9115,\n",
       "             0.9126, 0.9138, 0.9149, 0.9161, 0.9172, 0.9184, 0.9195, 0.9207, 0.9218,\n",
       "             0.9230, 0.9241, 0.9253, 0.9264, 0.9276, 0.9287, 0.9299, 0.9310, 0.9322,\n",
       "             0.9333, 0.9345, 0.9356, 0.9368, 0.9379, 0.9391, 0.9402, 0.9414, 0.9425,\n",
       "             0.9425, 0.9425, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483, 0.9494, 0.9506,\n",
       "             0.9517, 0.9529, 0.9540, 0.9552, 0.9563, 0.9575, 0.9586, 0.9598, 0.9609,\n",
       "             0.9621, 0.9632, 0.9632, 0.9644, 0.9644, 0.9655, 0.9667, 0.9678, 0.9678,\n",
       "             0.9678, 0.9690, 0.9690, 0.9701, 0.9701, 0.9713, 0.9713, 0.9713, 0.9713,\n",
       "             0.9713, 0.9724, 0.9736, 0.9736, 0.9736, 0.9736, 0.9747, 0.9759, 0.9770,\n",
       "             0.9770, 0.9782, 0.9793, 0.9793, 0.9805, 0.9805, 0.9816, 0.9828, 0.9828,\n",
       "             0.9839, 0.9851, 0.9851, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9874,\n",
       "             0.9874, 0.9885, 0.9885, 0.9897, 0.9897, 0.9897, 0.9897, 0.9897, 0.9897,\n",
       "             0.9897, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9968e-01, 9.9965e-01,\n",
       "             9.9965e-01, 9.9965e-01, 9.9965e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01,\n",
       "             9.9962e-01, 9.9960e-01, 9.9957e-01, 9.9955e-01, 9.9952e-01, 9.9950e-01,\n",
       "             9.9950e-01, 9.9949e-01, 9.9944e-01, 9.9943e-01, 9.9943e-01, 9.9941e-01,\n",
       "             9.9935e-01, 9.9935e-01, 9.9933e-01, 9.9933e-01, 9.9932e-01, 9.9929e-01,\n",
       "             9.9923e-01, 9.9920e-01, 9.9919e-01, 9.9917e-01, 9.9915e-01, 9.9904e-01,\n",
       "             9.9903e-01, 9.9902e-01, 9.9898e-01, 9.9898e-01, 9.9892e-01, 9.9891e-01,\n",
       "             9.9884e-01, 9.9883e-01, 9.9880e-01, 9.9870e-01, 9.9857e-01, 9.9843e-01,\n",
       "             9.9843e-01, 9.9819e-01, 9.9818e-01, 9.9816e-01, 9.9812e-01, 9.9811e-01,\n",
       "             9.9773e-01, 9.9770e-01, 9.9765e-01, 9.9763e-01, 9.9750e-01, 9.9730e-01,\n",
       "             9.9665e-01, 9.9621e-01, 9.9583e-01, 9.9568e-01, 9.9556e-01, 9.9553e-01,\n",
       "             9.9539e-01, 9.9498e-01, 9.9419e-01, 9.9394e-01, 9.9387e-01, 9.9363e-01,\n",
       "             9.9358e-01, 9.9282e-01, 9.9269e-01, 9.8922e-01, 9.8901e-01, 9.8709e-01,\n",
       "             9.8570e-01, 9.8372e-01, 9.8227e-01, 9.7905e-01, 9.7399e-01, 9.7332e-01,\n",
       "             9.7199e-01, 9.7134e-01, 9.6883e-01, 9.5658e-01, 9.5282e-01, 9.5203e-01,\n",
       "             9.5033e-01, 9.4726e-01, 9.4609e-01, 9.3831e-01, 9.3820e-01, 9.0950e-01,\n",
       "             9.0740e-01, 9.0683e-01, 9.0468e-01, 9.0239e-01, 9.0064e-01, 8.9594e-01,\n",
       "             8.8067e-01, 8.6048e-01, 8.4932e-01, 8.4895e-01, 8.4470e-01, 8.4183e-01,\n",
       "             8.4090e-01, 8.2322e-01, 8.2298e-01, 8.1869e-01, 7.9267e-01, 7.7822e-01,\n",
       "             7.5022e-01, 7.0970e-01, 6.8094e-01, 6.5522e-01, 6.3982e-01, 6.3887e-01,\n",
       "             5.9598e-01, 5.8128e-01, 5.7190e-01, 5.6596e-01, 5.5240e-01, 5.2526e-01,\n",
       "             5.2326e-01, 5.0350e-01, 4.8105e-01, 4.6043e-01, 4.4914e-01, 4.2718e-01,\n",
       "             4.2353e-01, 3.3186e-01, 3.2890e-01, 2.9456e-01, 2.9293e-01, 2.6535e-01,\n",
       "             2.6519e-01, 2.4431e-01, 2.3305e-01, 2.1277e-01, 2.0407e-01, 1.7358e-01,\n",
       "             1.6478e-01, 1.3434e-01, 1.0434e-01, 9.3784e-02, 9.3211e-02, 6.9953e-02,\n",
       "             6.4321e-02, 5.9418e-02, 5.0855e-02, 3.9550e-02, 3.9306e-02, 3.8529e-02,\n",
       "             2.8798e-02, 2.0844e-02, 2.0039e-02, 1.8982e-02, 1.6152e-02, 1.4864e-02,\n",
       "             1.4815e-02, 1.4468e-02, 1.4203e-02, 1.3499e-02, 1.2925e-02, 1.1250e-02,\n",
       "             1.1212e-02, 8.2794e-03, 7.8332e-03, 6.4686e-03, 5.8510e-03, 5.4187e-03,\n",
       "             5.1389e-03, 4.9220e-03, 4.8021e-03, 4.7165e-03, 4.3060e-03, 3.9611e-03,\n",
       "             3.7490e-03, 3.6051e-03, 3.3285e-03, 3.0394e-03, 2.8644e-03, 2.5307e-03,\n",
       "             2.2977e-03, 1.9712e-03, 1.8854e-03, 1.8735e-03, 1.8061e-03, 1.6939e-03,\n",
       "             1.5841e-03, 1.5164e-03, 1.4572e-03, 1.2735e-03, 1.1581e-03, 1.0796e-03,\n",
       "             9.0568e-04, 8.9472e-04, 8.4538e-04, 8.0034e-04, 7.3117e-04, 7.0359e-04,\n",
       "             6.2964e-04, 5.3917e-04, 4.9778e-04, 4.9354e-04, 4.2015e-04, 4.0565e-04,\n",
       "             3.9730e-04, 3.7460e-04, 3.6964e-04, 3.0380e-04, 2.6928e-04, 2.6096e-04,\n",
       "             2.4296e-04, 2.3559e-04, 2.3396e-04, 2.0434e-04, 2.0253e-04, 1.9602e-04,\n",
       "             1.8266e-04, 1.5106e-04, 1.5068e-04, 1.4721e-04, 1.4487e-04, 1.3582e-04,\n",
       "             1.3010e-04, 1.0812e-04, 1.0210e-04, 9.9881e-05, 9.2780e-05, 9.0828e-05,\n",
       "             5.5168e-05, 5.4297e-05, 5.1277e-05, 4.8369e-05, 3.8687e-05, 3.6103e-05,\n",
       "             3.4989e-05, 3.2002e-05, 2.9512e-05, 2.9228e-05, 2.9197e-05, 2.4989e-05,\n",
       "             2.2114e-05, 2.1645e-05, 2.1273e-05, 1.9667e-05, 1.9601e-05, 1.7037e-05,\n",
       "             1.6835e-05, 1.6195e-05, 1.5827e-05, 1.4139e-05, 1.3979e-05, 1.2451e-05,\n",
       "             1.2068e-05, 1.1532e-05, 1.0920e-05, 1.0163e-05, 9.5923e-06, 8.4884e-06,\n",
       "             8.2577e-06, 8.1191e-06, 7.5312e-06, 6.8969e-06, 5.4802e-06, 5.3313e-06,\n",
       "             4.6125e-06, 4.4836e-06, 4.3343e-06, 4.0860e-06, 3.9145e-06, 3.8564e-06,\n",
       "             3.2821e-06, 3.0162e-06, 2.6952e-06, 2.5683e-06, 2.4746e-06, 2.4252e-06,\n",
       "             2.3089e-06, 2.2232e-06, 2.1977e-06, 2.1027e-06, 2.0070e-06, 1.7779e-06,\n",
       "             1.5348e-06, 1.5162e-06, 1.2301e-06, 1.1961e-06, 1.1494e-06, 1.0960e-06,\n",
       "             9.9898e-07, 7.8528e-07, 7.8287e-07, 7.6818e-07, 7.6231e-07, 7.3440e-07,\n",
       "             7.2557e-07, 6.5241e-07, 5.3437e-07, 5.1865e-07, 5.1723e-07, 4.9314e-07,\n",
       "             4.8839e-07, 4.1197e-07, 3.8418e-07, 3.8016e-07, 3.6283e-07, 2.9303e-07,\n",
       "             2.4306e-07, 1.9523e-07, 1.7260e-07, 1.6365e-07, 1.6365e-07, 1.5656e-07,\n",
       "             1.5430e-07, 1.0911e-07, 1.0609e-07, 1.0337e-07, 7.7484e-08, 6.3685e-08,\n",
       "             6.1099e-08, 5.8518e-08, 5.7172e-08, 5.5387e-08, 5.5380e-08, 5.4188e-08,\n",
       "             5.1931e-08, 2.2741e-08, 2.2692e-08, 2.2675e-08, 2.1699e-08, 1.5399e-08,\n",
       "             1.4894e-08, 1.4719e-08, 1.3455e-08, 1.3261e-08, 1.1600e-08, 1.1580e-08,\n",
       "             9.2946e-09, 7.6039e-09, 7.5820e-09, 6.9398e-09, 6.2841e-09, 6.2553e-09,\n",
       "             6.1933e-09, 5.7231e-09, 5.5901e-09, 5.5242e-09, 5.5213e-09, 4.6268e-09,\n",
       "             4.1640e-09, 3.9599e-09, 3.1197e-09, 3.0882e-09, 2.7754e-09, 2.6787e-09,\n",
       "             2.5860e-09, 2.3616e-09, 1.8200e-09, 1.7108e-09, 1.5087e-09, 1.3598e-09,\n",
       "             1.1359e-09, 9.9370e-10, 3.2699e-10, 3.2549e-10, 2.8960e-10, 2.4566e-10,\n",
       "             2.3952e-10, 2.3266e-10, 2.2448e-10, 2.2055e-10, 2.1543e-10, 2.0714e-10,\n",
       "             1.9374e-10, 1.5197e-10, 1.3791e-10, 9.0903e-11, 8.1619e-11, 7.9870e-11,\n",
       "             6.8205e-11, 4.1550e-11, 3.9774e-11, 3.6577e-11, 2.9590e-11, 2.5180e-11,\n",
       "             1.8480e-11, 1.0701e-11, 6.2669e-12, 6.1410e-12, 5.9147e-12, 5.2881e-12,\n",
       "             4.6165e-12, 4.3445e-12, 2.4902e-12, 2.3470e-12, 1.7215e-12, 1.5485e-12,\n",
       "             8.8624e-13, 5.8804e-13, 4.8832e-13, 4.2099e-13, 3.2063e-13, 2.7400e-13,\n",
       "             2.4604e-13, 1.6772e-13, 1.1842e-13, 9.8197e-14, 7.7500e-14, 2.7754e-14,\n",
       "             1.9097e-14, 1.1242e-14, 4.2806e-15, 9.3274e-17, 1.2610e-17, 9.0356e-23])}},\n",
       "   {'fpr': np.float64(0.11400651465798045),\n",
       "    'tpr': np.float64(0.993103448275862),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0163, 0.0163, 0.0163, 0.0195, 0.0195, 0.0195, 0.0195, 0.0228, 0.0228,\n",
       "             0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228,\n",
       "             0.0228, 0.0228, 0.0228, 0.0261, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293,\n",
       "             0.0293, 0.0293, 0.0293, 0.0293, 0.0326, 0.0358, 0.0358, 0.0358, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0423, 0.0456, 0.0456, 0.0456, 0.0456, 0.0489,\n",
       "             0.0489, 0.0489, 0.0521, 0.0554, 0.0586, 0.0619, 0.0619, 0.0651, 0.0684,\n",
       "             0.0717, 0.0749, 0.0782, 0.0814, 0.0847, 0.0879, 0.0912, 0.0945, 0.0977,\n",
       "             0.0977, 0.1010, 0.1010, 0.1010, 0.1042, 0.1075, 0.1107, 0.1140, 0.1140,\n",
       "             0.1173, 0.1205, 0.1238, 0.1270, 0.1303, 0.1336, 0.1368, 0.1401, 0.1433,\n",
       "             0.1466, 0.1466, 0.1498, 0.1531, 0.1564, 0.1596, 0.1629, 0.1661, 0.1694,\n",
       "             0.1726, 0.1759, 0.1792, 0.1824, 0.1857, 0.1889, 0.1922, 0.1922, 0.1954,\n",
       "             0.1987, 0.2020, 0.2052, 0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2248,\n",
       "             0.2280, 0.2313, 0.2345, 0.2378, 0.2410, 0.2443, 0.2476, 0.2476, 0.2508,\n",
       "             0.2541, 0.2573, 0.2606, 0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801,\n",
       "             0.2834, 0.2866, 0.2899, 0.2932, 0.2964, 0.2964, 0.2997, 0.3029, 0.3062,\n",
       "             0.3094, 0.3127, 0.3127, 0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3322,\n",
       "             0.3355, 0.3388, 0.3420, 0.3453, 0.3453, 0.3485, 0.3518, 0.3550, 0.3583,\n",
       "             0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779, 0.3811, 0.3844, 0.3876,\n",
       "             0.3909, 0.3941, 0.3974, 0.4007, 0.4039, 0.4072, 0.4104, 0.4137, 0.4169,\n",
       "             0.4202, 0.4235, 0.4267, 0.4300, 0.4332, 0.4365, 0.4397, 0.4430, 0.4463,\n",
       "             0.4495, 0.4528, 0.4560, 0.4593, 0.4625, 0.4658, 0.4691, 0.4723, 0.4756,\n",
       "             0.4788, 0.4821, 0.4853, 0.4886, 0.4919, 0.4951, 0.4984, 0.5016, 0.5049,\n",
       "             0.5081, 0.5114, 0.5147, 0.5179, 0.5212, 0.5244, 0.5277, 0.5309, 0.5342,\n",
       "             0.5375, 0.5407, 0.5440, 0.5472, 0.5505, 0.5537, 0.5570, 0.5603, 0.5635,\n",
       "             0.5668, 0.5700, 0.5733, 0.5765, 0.5798, 0.5831, 0.5863, 0.5896, 0.5928,\n",
       "             0.5961, 0.5993, 0.6026, 0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221,\n",
       "             0.6254, 0.6287, 0.6319, 0.6352, 0.6384, 0.6417, 0.6450, 0.6482, 0.6515,\n",
       "             0.6547, 0.6580, 0.6612, 0.6645, 0.6678, 0.6710, 0.6743, 0.6775, 0.6808,\n",
       "             0.6840, 0.6873, 0.6906, 0.6938, 0.6971, 0.7003, 0.7036, 0.7068, 0.7101,\n",
       "             0.7134, 0.7166, 0.7199, 0.7231, 0.7264, 0.7296, 0.7329, 0.7362, 0.7394,\n",
       "             0.7427, 0.7459, 0.7492, 0.7524, 0.7557, 0.7590, 0.7622, 0.7655, 0.7687,\n",
       "             0.7720, 0.7752, 0.7785, 0.7818, 0.7850, 0.7883, 0.7915, 0.7948, 0.7980,\n",
       "             0.8013, 0.8046, 0.8078, 0.8111, 0.8143, 0.8176, 0.8208, 0.8241, 0.8274,\n",
       "             0.8306, 0.8339, 0.8371, 0.8404, 0.8436, 0.8469, 0.8502, 0.8534, 0.8567,\n",
       "             0.8599, 0.8632, 0.8664, 0.8697, 0.8730, 0.8762, 0.8795, 0.8827, 0.8860,\n",
       "             0.8893, 0.8925, 0.8958, 0.8990, 0.9023, 0.9055, 0.9088, 0.9121, 0.9153,\n",
       "             0.9186, 0.9218, 0.9251, 0.9283, 0.9316, 0.9349, 0.9381, 0.9414, 0.9446,\n",
       "             0.9479, 0.9511, 0.9544, 0.9577, 0.9609, 0.9642, 0.9674, 0.9707, 0.9739,\n",
       "             0.9772, 0.9805, 0.9837, 0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0414, 0.1034, 0.1391, 0.1678, 0.2034, 0.2276, 0.2506, 0.2759,\n",
       "             0.2966, 0.3149, 0.3310, 0.3437, 0.3540, 0.3678, 0.3759, 0.3839, 0.3920,\n",
       "             0.4023, 0.4080, 0.4161, 0.4241, 0.4299, 0.4356, 0.4494, 0.4563, 0.4621,\n",
       "             0.4667, 0.4678, 0.4724, 0.4770, 0.4874, 0.4954, 0.4977, 0.5023, 0.5103,\n",
       "             0.5149, 0.5218, 0.5241, 0.5299, 0.5322, 0.5379, 0.5471, 0.5506, 0.5517,\n",
       "             0.5563, 0.5609, 0.5644, 0.5678, 0.5690, 0.5724, 0.5759, 0.5805, 0.5839,\n",
       "             0.5851, 0.5874, 0.5897, 0.5920, 0.5943, 0.5989, 0.6000, 0.6034, 0.6057,\n",
       "             0.6080, 0.6103, 0.6126, 0.6149, 0.6161, 0.6172, 0.6207, 0.6253, 0.6264,\n",
       "             0.6287, 0.6322, 0.6333, 0.6368, 0.6379, 0.6402, 0.6414, 0.6448, 0.6460,\n",
       "             0.6471, 0.6483, 0.6506, 0.6529, 0.6552, 0.6575, 0.6586, 0.6598, 0.6632,\n",
       "             0.6655, 0.6678, 0.6701, 0.6713, 0.6736, 0.6759, 0.6770, 0.6782, 0.6793,\n",
       "             0.6816, 0.6874, 0.6885, 0.6897, 0.6908, 0.6931, 0.6943, 0.6954, 0.6977,\n",
       "             0.6989, 0.7000, 0.7011, 0.7023, 0.7034, 0.7057, 0.7069, 0.7080, 0.7092,\n",
       "             0.7115, 0.7115, 0.7138, 0.7149, 0.7161, 0.7172, 0.7184, 0.7195, 0.7207,\n",
       "             0.7218, 0.7230, 0.7241, 0.7264, 0.7287, 0.7310, 0.7333, 0.7345, 0.7379,\n",
       "             0.7391, 0.7402, 0.7414, 0.7437, 0.7448, 0.7460, 0.7471, 0.7483, 0.7506,\n",
       "             0.7540, 0.7552, 0.7563, 0.7575, 0.7598, 0.7609, 0.7621, 0.7632, 0.7644,\n",
       "             0.7655, 0.7667, 0.7678, 0.7690, 0.7713, 0.7724, 0.7736, 0.7747, 0.7759,\n",
       "             0.7770, 0.7782, 0.7793, 0.7805, 0.7828, 0.7839, 0.7851, 0.7862, 0.7885,\n",
       "             0.7897, 0.7908, 0.7920, 0.7931, 0.7943, 0.7954, 0.7977, 0.7989, 0.8000,\n",
       "             0.8011, 0.8023, 0.8034, 0.8046, 0.8057, 0.8069, 0.8080, 0.8092, 0.8103,\n",
       "             0.8115, 0.8126, 0.8138, 0.8149, 0.8161, 0.8172, 0.8184, 0.8207, 0.8218,\n",
       "             0.8230, 0.8241, 0.8253, 0.8264, 0.8276, 0.8287, 0.8299, 0.8310, 0.8333,\n",
       "             0.8345, 0.8356, 0.8368, 0.8379, 0.8391, 0.8402, 0.8414, 0.8425, 0.8437,\n",
       "             0.8448, 0.8460, 0.8471, 0.8483, 0.8494, 0.8506, 0.8517, 0.8529, 0.8540,\n",
       "             0.8563, 0.8575, 0.8586, 0.8598, 0.8609, 0.8621, 0.8632, 0.8644, 0.8655,\n",
       "             0.8667, 0.8678, 0.8690, 0.8701, 0.8713, 0.8724, 0.8736, 0.8747, 0.8759,\n",
       "             0.8770, 0.8782, 0.8793, 0.8805, 0.8816, 0.8828, 0.8839, 0.8851, 0.8862,\n",
       "             0.8874, 0.8885, 0.8897, 0.8908, 0.8920, 0.8943, 0.8954, 0.8966, 0.8977,\n",
       "             0.8989, 0.9000, 0.9011, 0.9023, 0.9034, 0.9046, 0.9057, 0.9069, 0.9080,\n",
       "             0.9092, 0.9103, 0.9115, 0.9126, 0.9138, 0.9149, 0.9161, 0.9172, 0.9184,\n",
       "             0.9195, 0.9207, 0.9218, 0.9230, 0.9241, 0.9241, 0.9253, 0.9264, 0.9276,\n",
       "             0.9276, 0.9287, 0.9299, 0.9310, 0.9322, 0.9333, 0.9345, 0.9356, 0.9368,\n",
       "             0.9379, 0.9391, 0.9402, 0.9402, 0.9414, 0.9425, 0.9437, 0.9448, 0.9460,\n",
       "             0.9460, 0.9471, 0.9483, 0.9483, 0.9494, 0.9506, 0.9517, 0.9517, 0.9529,\n",
       "             0.9540, 0.9552, 0.9563, 0.9575, 0.9586, 0.9598, 0.9609, 0.9621, 0.9632,\n",
       "             0.9644, 0.9655, 0.9667, 0.9667, 0.9667, 0.9678, 0.9690, 0.9701, 0.9713,\n",
       "             0.9724, 0.9736, 0.9747, 0.9759, 0.9759, 0.9759, 0.9770, 0.9782, 0.9782,\n",
       "             0.9793, 0.9805, 0.9816, 0.9816, 0.9816, 0.9828, 0.9839, 0.9851, 0.9851,\n",
       "             0.9862, 0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9897, 0.9897, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9978e-01, 9.9977e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9974e-01, 9.9971e-01, 9.9970e-01,\n",
       "             9.9967e-01, 9.9967e-01, 9.9963e-01, 9.9963e-01, 9.9961e-01, 9.9959e-01,\n",
       "             9.9955e-01, 9.9955e-01, 9.9949e-01, 9.9946e-01, 9.9945e-01, 9.9942e-01,\n",
       "             9.9942e-01, 9.9940e-01, 9.9938e-01, 9.9934e-01, 9.9931e-01, 9.9930e-01,\n",
       "             9.9925e-01, 9.9908e-01, 9.9908e-01, 9.9902e-01, 9.9884e-01, 9.9878e-01,\n",
       "             9.9870e-01, 9.9850e-01, 9.9834e-01, 9.9826e-01, 9.9819e-01, 9.9776e-01,\n",
       "             9.9765e-01, 9.9749e-01, 9.9743e-01, 9.9704e-01, 9.9679e-01, 9.9645e-01,\n",
       "             9.9576e-01, 9.9430e-01, 9.9178e-01, 9.9173e-01, 9.9135e-01, 9.9052e-01,\n",
       "             9.9003e-01, 9.8704e-01, 9.8402e-01, 9.8345e-01, 9.8303e-01, 9.8276e-01,\n",
       "             9.8165e-01, 9.7925e-01, 9.7797e-01, 9.7430e-01, 9.7104e-01, 9.7014e-01,\n",
       "             9.6956e-01, 9.6922e-01, 9.6888e-01, 9.6315e-01, 9.6270e-01, 9.5655e-01,\n",
       "             9.5448e-01, 9.5193e-01, 9.4720e-01, 9.4499e-01, 9.4447e-01, 9.4168e-01,\n",
       "             9.3243e-01, 9.2787e-01, 9.2650e-01, 9.1169e-01, 9.0856e-01, 9.0028e-01,\n",
       "             8.9174e-01, 8.8520e-01, 8.6301e-01, 8.3965e-01, 8.0003e-01, 7.8095e-01,\n",
       "             7.4096e-01, 7.2836e-01, 7.2497e-01, 6.8682e-01, 6.8677e-01, 6.4197e-01,\n",
       "             6.1810e-01, 6.0888e-01, 5.9492e-01, 5.5693e-01, 5.4905e-01, 5.1410e-01,\n",
       "             4.9463e-01, 4.1575e-01, 3.9382e-01, 3.8034e-01, 3.6208e-01, 3.0569e-01,\n",
       "             2.9485e-01, 2.6749e-01, 2.3734e-01, 1.7912e-01, 1.7223e-01, 1.4109e-01,\n",
       "             1.3426e-01, 1.2759e-01, 1.1836e-01, 1.1139e-01, 8.1937e-02, 6.5975e-02,\n",
       "             6.0604e-02, 5.5214e-02, 4.2179e-02, 4.2052e-02, 3.7957e-02, 3.6368e-02,\n",
       "             2.7580e-02, 2.0908e-02, 1.8605e-02, 1.8287e-02, 1.8064e-02, 1.7526e-02,\n",
       "             1.7271e-02, 1.6256e-02, 1.3934e-02, 1.1255e-02, 9.0980e-03, 7.9946e-03,\n",
       "             6.7570e-03, 6.5020e-03, 6.4250e-03, 5.9858e-03, 5.9789e-03, 5.7433e-03,\n",
       "             4.2883e-03, 3.9133e-03, 3.2235e-03, 3.0987e-03, 3.0286e-03, 2.9457e-03,\n",
       "             2.8193e-03, 2.5732e-03, 2.1708e-03, 1.9119e-03, 1.7915e-03, 1.7856e-03,\n",
       "             1.7459e-03, 1.6551e-03, 1.5382e-03, 1.3651e-03, 1.3052e-03, 1.1659e-03,\n",
       "             1.1624e-03, 1.1286e-03, 9.9338e-04, 9.0260e-04, 6.3523e-04, 5.4330e-04,\n",
       "             4.7457e-04, 4.6988e-04, 4.5831e-04, 4.3416e-04, 4.0147e-04, 3.6961e-04,\n",
       "             2.9542e-04, 2.8476e-04, 2.5666e-04, 1.9995e-04, 1.9075e-04, 1.8433e-04,\n",
       "             1.6947e-04, 1.2933e-04, 1.1749e-04, 1.1539e-04, 1.0836e-04, 1.0333e-04,\n",
       "             9.6401e-05, 9.3927e-05, 9.3026e-05, 8.9419e-05, 8.0269e-05, 7.2671e-05,\n",
       "             6.8377e-05, 6.8296e-05, 6.5055e-05, 6.4250e-05, 6.2272e-05, 6.1827e-05,\n",
       "             5.9885e-05, 5.7816e-05, 5.2432e-05, 5.1061e-05, 4.3870e-05, 4.2641e-05,\n",
       "             3.9425e-05, 3.4407e-05, 2.7487e-05, 1.9433e-05, 1.8544e-05, 1.8122e-05,\n",
       "             1.7546e-05, 1.6353e-05, 1.6235e-05, 1.6180e-05, 1.4853e-05, 1.4692e-05,\n",
       "             1.3828e-05, 1.3772e-05, 1.3731e-05, 1.2949e-05, 1.2538e-05, 1.1919e-05,\n",
       "             1.0748e-05, 1.0722e-05, 1.0620e-05, 9.6870e-06, 9.5094e-06, 9.4744e-06,\n",
       "             9.0815e-06, 8.7687e-06, 8.5640e-06, 6.3773e-06, 6.1972e-06, 5.6939e-06,\n",
       "             5.5007e-06, 5.0205e-06, 4.8978e-06, 4.3905e-06, 3.5950e-06, 3.5359e-06,\n",
       "             3.2909e-06, 2.5013e-06, 2.3130e-06, 2.3112e-06, 2.1007e-06, 2.0966e-06,\n",
       "             1.8978e-06, 1.5834e-06, 1.1126e-06, 1.0807e-06, 9.3839e-07, 8.7994e-07,\n",
       "             8.4179e-07, 7.7146e-07, 7.3514e-07, 7.2515e-07, 6.8576e-07, 6.5821e-07,\n",
       "             6.5802e-07, 5.8776e-07, 5.5537e-07, 5.4697e-07, 5.2649e-07, 4.8515e-07,\n",
       "             4.4073e-07, 4.3006e-07, 3.7501e-07, 3.5082e-07, 3.2897e-07, 3.1024e-07,\n",
       "             3.0352e-07, 2.8952e-07, 2.8777e-07, 2.6367e-07, 2.5735e-07, 2.4714e-07,\n",
       "             2.4015e-07, 2.1875e-07, 1.8564e-07, 1.7999e-07, 1.5567e-07, 1.5092e-07,\n",
       "             1.2433e-07, 1.1302e-07, 1.1046e-07, 1.0373e-07, 9.4705e-08, 9.3751e-08,\n",
       "             8.8966e-08, 8.5871e-08, 8.5581e-08, 6.3354e-08, 4.2257e-08, 4.0888e-08,\n",
       "             3.0968e-08, 2.5715e-08, 2.3036e-08, 2.2113e-08, 2.1378e-08, 1.9023e-08,\n",
       "             1.8280e-08, 1.6829e-08, 1.6494e-08, 1.5542e-08, 1.2835e-08, 1.2797e-08,\n",
       "             1.2017e-08, 1.1885e-08, 1.1376e-08, 1.1030e-08, 9.0075e-09, 8.8770e-09,\n",
       "             7.8891e-09, 7.2810e-09, 7.0868e-09, 6.6148e-09, 4.8167e-09, 4.2661e-09,\n",
       "             4.1877e-09, 3.7860e-09, 3.6308e-09, 3.4069e-09, 3.2547e-09, 2.5682e-09,\n",
       "             2.5567e-09, 1.9865e-09, 1.6306e-09, 1.4478e-09, 1.2483e-09, 1.1897e-09,\n",
       "             8.6711e-10, 5.9899e-10, 5.8071e-10, 4.9381e-10, 4.8197e-10, 4.5925e-10,\n",
       "             4.2629e-10, 3.8071e-10, 2.8899e-10, 2.6931e-10, 2.4642e-10, 2.1126e-10,\n",
       "             1.7416e-10, 1.6236e-10, 1.6168e-10, 1.0182e-10, 9.7824e-11, 7.1669e-11,\n",
       "             6.1804e-11, 5.7015e-11, 5.0758e-11, 3.1442e-11, 3.0097e-11, 2.0186e-11,\n",
       "             1.8574e-11, 1.4762e-11, 1.3685e-11, 4.3706e-12, 3.9533e-12, 2.9620e-12,\n",
       "             2.7596e-12, 2.6796e-12, 2.4872e-12, 2.2198e-12, 1.4835e-12, 6.0769e-13,\n",
       "             4.7270e-13, 4.2972e-13, 3.7506e-13, 1.2248e-13, 7.9074e-14, 5.0236e-14,\n",
       "             4.2574e-14, 1.6817e-14, 9.2434e-15, 7.8856e-15, 6.0922e-15, 7.7734e-16,\n",
       "             1.4407e-18, 7.6096e-21])}},\n",
       "   {'fpr': np.float64(0.04234527687296417),\n",
       "    'tpr': np.float64(0.9839080459770115),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0023, 0.0034,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.7643e-16, 2.7343e-16,\n",
       "             4.8041e-20])}},\n",
       "   {'fpr': np.float64(0.09771986970684039),\n",
       "    'tpr': np.float64(0.9873563218390805),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0163,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0228, 0.0228, 0.0261, 0.0293, 0.0293, 0.0326, 0.0326, 0.0358,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0423, 0.0423, 0.0423,\n",
       "             0.0423, 0.0423, 0.0423, 0.0423, 0.0456, 0.0456, 0.0489, 0.0489, 0.0489,\n",
       "             0.0521, 0.0521, 0.0554, 0.0554, 0.0586, 0.0586, 0.0619, 0.0651, 0.0651,\n",
       "             0.0651, 0.0651, 0.0684, 0.0717, 0.0749, 0.0782, 0.0814, 0.0814, 0.0814,\n",
       "             0.0847, 0.0879, 0.0912, 0.0945, 0.0977, 0.1010, 0.1042, 0.1042, 0.1075,\n",
       "             0.1107, 0.1107, 0.1140, 0.1140, 0.1173, 0.1205, 0.1205, 0.1238, 0.1270,\n",
       "             0.1303, 0.1336, 0.1368, 0.1401, 0.1433, 0.1466, 0.1466, 0.1498, 0.1531,\n",
       "             0.1564, 0.1596, 0.1629, 0.1661, 0.1694, 0.1726, 0.1726, 0.1759, 0.1759,\n",
       "             0.1792, 0.1824, 0.1857, 0.1889, 0.1922, 0.1954, 0.1987, 0.2020, 0.2052,\n",
       "             0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345,\n",
       "             0.2378, 0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2573, 0.2606,\n",
       "             0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899,\n",
       "             0.2932, 0.2964, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160, 0.3192,\n",
       "             0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453, 0.3485,\n",
       "             0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779,\n",
       "             0.3811, 0.3844, 0.3876, 0.3909, 0.3941, 0.3941, 0.3974, 0.4007, 0.4039,\n",
       "             0.4072, 0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267, 0.4300, 0.4332,\n",
       "             0.4365, 0.4397, 0.4430, 0.4463, 0.4495, 0.4528, 0.4528, 0.4560, 0.4593,\n",
       "             0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788, 0.4821, 0.4853, 0.4886,\n",
       "             0.4919, 0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114, 0.5147, 0.5179,\n",
       "             0.5212, 0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407, 0.5440, 0.5472,\n",
       "             0.5505, 0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700, 0.5733, 0.5765,\n",
       "             0.5798, 0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5961, 0.5993, 0.6026,\n",
       "             0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319,\n",
       "             0.6352, 0.6384, 0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612,\n",
       "             0.6645, 0.6678, 0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906,\n",
       "             0.6938, 0.6971, 0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199,\n",
       "             0.7231, 0.7264, 0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492,\n",
       "             0.7524, 0.7557, 0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785,\n",
       "             0.7818, 0.7850, 0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078,\n",
       "             0.8111, 0.8143, 0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371,\n",
       "             0.8404, 0.8436, 0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664,\n",
       "             0.8697, 0.8730, 0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958,\n",
       "             0.8990, 0.9023, 0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251,\n",
       "             0.9283, 0.9316, 0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544,\n",
       "             0.9577, 0.9609, 0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837,\n",
       "             0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.3782, 0.4690, 0.5080, 0.5368, 0.5586, 0.5736, 0.5874, 0.5977,\n",
       "             0.6046, 0.6126, 0.6253, 0.6345, 0.6414, 0.6471, 0.6517, 0.6575, 0.6621,\n",
       "             0.6678, 0.6701, 0.6747, 0.6759, 0.6805, 0.6851, 0.6897, 0.6943, 0.6977,\n",
       "             0.7000, 0.7057, 0.7069, 0.7103, 0.7126, 0.7138, 0.7172, 0.7195, 0.7207,\n",
       "             0.7218, 0.7230, 0.7241, 0.7276, 0.7310, 0.7345, 0.7356, 0.7379, 0.7414,\n",
       "             0.7425, 0.7448, 0.7460, 0.7494, 0.7517, 0.7540, 0.7552, 0.7563, 0.7575,\n",
       "             0.7586, 0.7598, 0.7632, 0.7644, 0.7655, 0.7678, 0.7690, 0.7713, 0.7736,\n",
       "             0.7747, 0.7770, 0.7793, 0.7805, 0.7816, 0.7839, 0.7851, 0.7862, 0.7874,\n",
       "             0.7885, 0.7897, 0.7908, 0.7920, 0.7931, 0.7966, 0.7977, 0.7989, 0.8000,\n",
       "             0.8011, 0.8011, 0.8023, 0.8057, 0.8069, 0.8080, 0.8092, 0.8103, 0.8115,\n",
       "             0.8126, 0.8138, 0.8149, 0.8161, 0.8172, 0.8184, 0.8195, 0.8207, 0.8218,\n",
       "             0.8230, 0.8241, 0.8253, 0.8264, 0.8276, 0.8287, 0.8299, 0.8310, 0.8322,\n",
       "             0.8333, 0.8345, 0.8356, 0.8379, 0.8391, 0.8402, 0.8414, 0.8437, 0.8448,\n",
       "             0.8460, 0.8460, 0.8471, 0.8483, 0.8494, 0.8506, 0.8517, 0.8529, 0.8540,\n",
       "             0.8552, 0.8563, 0.8575, 0.8598, 0.8609, 0.8621, 0.8632, 0.8644, 0.8655,\n",
       "             0.8667, 0.8678, 0.8690, 0.8701, 0.8713, 0.8724, 0.8736, 0.8747, 0.8759,\n",
       "             0.8770, 0.8782, 0.8793, 0.8805, 0.8816, 0.8828, 0.8839, 0.8862, 0.8874,\n",
       "             0.8885, 0.8897, 0.8908, 0.8920, 0.8931, 0.8943, 0.8954, 0.8966, 0.8977,\n",
       "             0.8989, 0.9000, 0.9011, 0.9023, 0.9034, 0.9046, 0.9057, 0.9069, 0.9080,\n",
       "             0.9092, 0.9103, 0.9115, 0.9126, 0.9138, 0.9149, 0.9161, 0.9172, 0.9184,\n",
       "             0.9195, 0.9207, 0.9218, 0.9230, 0.9241, 0.9253, 0.9264, 0.9276, 0.9287,\n",
       "             0.9299, 0.9310, 0.9322, 0.9333, 0.9345, 0.9356, 0.9368, 0.9379, 0.9391,\n",
       "             0.9402, 0.9414, 0.9425, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483, 0.9483,\n",
       "             0.9483, 0.9494, 0.9506, 0.9517, 0.9529, 0.9540, 0.9552, 0.9563, 0.9575,\n",
       "             0.9586, 0.9586, 0.9598, 0.9598, 0.9598, 0.9609, 0.9609, 0.9621, 0.9621,\n",
       "             0.9621, 0.9632, 0.9644, 0.9655, 0.9667, 0.9678, 0.9678, 0.9690, 0.9701,\n",
       "             0.9713, 0.9724, 0.9736, 0.9747, 0.9747, 0.9759, 0.9759, 0.9770, 0.9782,\n",
       "             0.9782, 0.9793, 0.9793, 0.9805, 0.9805, 0.9816, 0.9816, 0.9816, 0.9828,\n",
       "             0.9839, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9862, 0.9874,\n",
       "             0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9885, 0.9885,\n",
       "             0.9885, 0.9897, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9977e-01, 9.9976e-01, 9.9975e-01, 9.9974e-01,\n",
       "             9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9970e-01,\n",
       "             9.9970e-01, 9.9968e-01, 9.9967e-01, 9.9966e-01, 9.9965e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9961e-01, 9.9959e-01, 9.9958e-01, 9.9952e-01, 9.9944e-01,\n",
       "             9.9939e-01, 9.9939e-01, 9.9936e-01, 9.9936e-01, 9.9926e-01, 9.9922e-01,\n",
       "             9.9920e-01, 9.9899e-01, 9.9893e-01, 9.9884e-01, 9.9884e-01, 9.9881e-01,\n",
       "             9.9872e-01, 9.9847e-01, 9.9847e-01, 9.9844e-01, 9.9813e-01, 9.9813e-01,\n",
       "             9.9800e-01, 9.9790e-01, 9.9696e-01, 9.9625e-01, 9.9613e-01, 9.9600e-01,\n",
       "             9.9559e-01, 9.9346e-01, 9.9304e-01, 9.9199e-01, 9.9167e-01, 9.9084e-01,\n",
       "             9.9022e-01, 9.8951e-01, 9.8804e-01, 9.8243e-01, 9.8223e-01, 9.8222e-01,\n",
       "             9.8056e-01, 9.7723e-01, 9.7150e-01, 9.7058e-01, 9.6984e-01, 9.6625e-01,\n",
       "             9.6345e-01, 9.6291e-01, 9.5242e-01, 9.4761e-01, 9.4452e-01, 9.2851e-01,\n",
       "             9.1617e-01, 9.1141e-01, 9.0435e-01, 9.0412e-01, 9.0029e-01, 8.9675e-01,\n",
       "             8.9502e-01, 8.9188e-01, 8.8051e-01, 8.7633e-01, 8.6376e-01, 8.6185e-01,\n",
       "             8.0444e-01, 7.9866e-01, 7.8598e-01, 6.6012e-01, 5.8413e-01, 5.7297e-01,\n",
       "             5.4604e-01, 5.4162e-01, 4.5494e-01, 4.5164e-01, 3.8360e-01, 3.4572e-01,\n",
       "             3.0826e-01, 2.6988e-01, 2.6882e-01, 2.4493e-01, 2.2866e-01, 2.0006e-01,\n",
       "             1.2812e-01, 1.2120e-01, 1.0324e-01, 7.0203e-02, 6.7474e-02, 4.3949e-02,\n",
       "             4.3364e-02, 3.3114e-02, 2.4396e-02, 2.4393e-02, 1.5633e-02, 1.2673e-02,\n",
       "             1.1630e-02, 9.8748e-03, 9.0113e-03, 4.7637e-03, 4.1516e-03, 3.9449e-03,\n",
       "             3.6723e-03, 3.2913e-03, 2.5499e-03, 1.8347e-03, 1.7068e-03, 1.3341e-03,\n",
       "             1.2995e-03, 1.2421e-03, 1.1279e-03, 9.9742e-04, 8.7402e-04, 8.6474e-04,\n",
       "             8.2781e-04, 7.2728e-04, 6.5964e-04, 6.3613e-04, 6.1952e-04, 5.0152e-04,\n",
       "             5.0070e-04, 4.3525e-04, 3.9762e-04, 3.9752e-04, 3.7626e-04, 3.6085e-04,\n",
       "             2.1716e-04, 1.9346e-04, 1.6025e-04, 1.4757e-04, 1.3378e-04, 1.2942e-04,\n",
       "             1.2564e-04, 8.5708e-05, 6.0053e-05, 5.6658e-05, 5.0631e-05, 4.9361e-05,\n",
       "             4.5395e-05, 4.3919e-05, 4.1840e-05, 4.0268e-05, 3.4960e-05, 3.4187e-05,\n",
       "             2.7266e-05, 2.5679e-05, 2.0943e-05, 2.0387e-05, 2.0243e-05, 1.9505e-05,\n",
       "             1.8159e-05, 1.6932e-05, 1.3129e-05, 1.1180e-05, 1.0407e-05, 1.0010e-05,\n",
       "             9.5320e-06, 8.5665e-06, 6.6798e-06, 5.2199e-06, 3.8980e-06, 3.8857e-06,\n",
       "             2.9071e-06, 2.5078e-06, 2.4102e-06, 1.5709e-06, 1.4707e-06, 1.4239e-06,\n",
       "             1.2684e-06, 1.1389e-06, 1.0304e-06, 1.0012e-06, 8.4316e-07, 8.0709e-07,\n",
       "             7.3860e-07, 5.0078e-07, 4.6623e-07, 4.0567e-07, 4.0426e-07, 2.8575e-07,\n",
       "             2.7209e-07, 2.5882e-07, 2.5422e-07, 2.4575e-07, 2.2051e-07, 1.7803e-07,\n",
       "             1.7609e-07, 1.7167e-07, 1.6840e-07, 1.6544e-07, 1.6130e-07, 1.5380e-07,\n",
       "             1.4783e-07, 1.4187e-07, 1.3679e-07, 1.2008e-07, 9.0872e-08, 8.5406e-08,\n",
       "             6.7498e-08, 6.7251e-08, 6.6230e-08, 6.3096e-08, 5.7148e-08, 5.2042e-08,\n",
       "             4.3851e-08, 4.2255e-08, 3.3636e-08, 3.1706e-08, 2.9307e-08, 2.8543e-08,\n",
       "             2.6966e-08, 2.4296e-08, 1.8526e-08, 1.3419e-08, 1.2795e-08, 1.1372e-08,\n",
       "             1.0730e-08, 1.0549e-08, 1.0250e-08, 6.2524e-09, 5.9015e-09, 5.8225e-09,\n",
       "             5.5102e-09, 5.4801e-09, 5.2137e-09, 5.2023e-09, 4.6065e-09, 3.9554e-09,\n",
       "             3.5804e-09, 3.5621e-09, 3.5331e-09, 3.2667e-09, 3.0359e-09, 2.8941e-09,\n",
       "             2.7788e-09, 2.7685e-09, 2.3736e-09, 2.2246e-09, 2.0986e-09, 1.9920e-09,\n",
       "             1.7729e-09, 1.5649e-09, 1.5582e-09, 1.3348e-09, 1.2802e-09, 9.3261e-10,\n",
       "             8.4614e-10, 7.4867e-10, 7.4040e-10, 6.6483e-10, 6.3694e-10, 5.9691e-10,\n",
       "             4.3975e-10, 4.2840e-10, 3.7391e-10, 3.6270e-10, 2.4987e-10, 2.4387e-10,\n",
       "             2.3170e-10, 2.0532e-10, 2.0060e-10, 1.9692e-10, 1.6991e-10, 1.2221e-10,\n",
       "             1.2013e-10, 1.0222e-10, 1.0163e-10, 9.1884e-11, 8.8632e-11, 6.4194e-11,\n",
       "             5.8958e-11, 5.5513e-11, 5.4055e-11, 4.1409e-11, 3.9722e-11, 3.3512e-11,\n",
       "             2.9758e-11, 2.5905e-11, 2.4786e-11, 2.3457e-11, 2.0936e-11, 1.4142e-11,\n",
       "             9.2129e-12, 8.6717e-12, 8.5043e-12, 7.9353e-12, 7.0145e-12, 5.7313e-12,\n",
       "             5.6063e-12, 4.7422e-12, 3.7463e-12, 2.7882e-12, 2.5217e-12, 2.0905e-12,\n",
       "             2.0764e-12, 1.5886e-12, 1.5250e-12, 1.4539e-12, 9.0989e-13, 8.0342e-13,\n",
       "             7.7908e-13, 7.4713e-13, 7.0434e-13, 6.4507e-13, 6.4426e-13, 5.8715e-13,\n",
       "             5.2868e-13, 3.9695e-13, 3.7366e-13, 3.5262e-13, 3.5014e-13, 3.1289e-13,\n",
       "             2.7373e-13, 2.5930e-13, 1.9469e-13, 1.8037e-13, 1.7459e-13, 1.5266e-13,\n",
       "             1.0141e-13, 7.9443e-14, 6.7057e-14, 6.6785e-14, 5.5097e-14, 4.7392e-14,\n",
       "             4.3886e-14, 4.3481e-14, 4.3187e-14, 3.8722e-14, 3.6286e-14, 1.8950e-14,\n",
       "             1.8386e-14, 1.5228e-14, 1.1074e-14, 5.7466e-15, 4.4986e-15, 2.4932e-15,\n",
       "             1.7114e-15, 9.2564e-16, 8.7869e-16, 5.7563e-16, 4.1172e-16, 3.3619e-16,\n",
       "             3.0930e-16, 2.1111e-16, 1.9483e-16, 6.9953e-17, 2.9938e-17, 2.6411e-17,\n",
       "             1.5826e-17, 1.3799e-17, 8.6584e-18, 6.9443e-18, 3.8188e-18, 3.5587e-18,\n",
       "             1.3885e-18, 5.9354e-19, 1.1461e-19, 1.6392e-20, 4.7275e-21, 1.5034e-21,\n",
       "             1.9517e-23, 3.6825e-27])}},\n",
       "   {'fpr': np.float64(0.10097719869706841),\n",
       "    'tpr': np.float64(0.9919540229885058),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0130, 0.0130, 0.0130, 0.0130, 0.0163, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0261,\n",
       "             0.0261, 0.0293, 0.0293, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0358,\n",
       "             0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0391, 0.0391, 0.0391,\n",
       "             0.0423, 0.0423, 0.0423, 0.0423, 0.0423, 0.0456, 0.0489, 0.0489, 0.0489,\n",
       "             0.0521, 0.0521, 0.0554, 0.0554, 0.0586, 0.0619, 0.0651, 0.0684, 0.0684,\n",
       "             0.0717, 0.0717, 0.0717, 0.0749, 0.0782, 0.0782, 0.0814, 0.0847, 0.0847,\n",
       "             0.0879, 0.0912, 0.0945, 0.0977, 0.1010, 0.1042, 0.1075, 0.1107, 0.1107,\n",
       "             0.1140, 0.1173, 0.1205, 0.1238, 0.1270, 0.1303, 0.1336, 0.1368, 0.1401,\n",
       "             0.1433, 0.1466, 0.1498, 0.1498, 0.1531, 0.1564, 0.1596, 0.1629, 0.1629,\n",
       "             0.1629, 0.1661, 0.1694, 0.1726, 0.1759, 0.1792, 0.1792, 0.1824, 0.1857,\n",
       "             0.1889, 0.1922, 0.1954, 0.1954, 0.1954, 0.1987, 0.2020, 0.2052, 0.2085,\n",
       "             0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345, 0.2378,\n",
       "             0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2606, 0.2638, 0.2671,\n",
       "             0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899, 0.2932, 0.2964,\n",
       "             0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160, 0.3192, 0.3225, 0.3257,\n",
       "             0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453, 0.3485, 0.3518, 0.3550,\n",
       "             0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779, 0.3811, 0.3844,\n",
       "             0.3876, 0.3909, 0.3941, 0.3974, 0.4007, 0.4039, 0.4072, 0.4104, 0.4137,\n",
       "             0.4169, 0.4202, 0.4235, 0.4267, 0.4300, 0.4332, 0.4365, 0.4397, 0.4430,\n",
       "             0.4463, 0.4495, 0.4528, 0.4560, 0.4593, 0.4625, 0.4658, 0.4691, 0.4723,\n",
       "             0.4756, 0.4788, 0.4821, 0.4853, 0.4886, 0.4919, 0.4951, 0.4984, 0.5016,\n",
       "             0.5049, 0.5081, 0.5114, 0.5147, 0.5179, 0.5212, 0.5244, 0.5277, 0.5309,\n",
       "             0.5342, 0.5375, 0.5407, 0.5440, 0.5472, 0.5505, 0.5537, 0.5570, 0.5603,\n",
       "             0.5635, 0.5668, 0.5700, 0.5733, 0.5765, 0.5798, 0.5831, 0.5863, 0.5896,\n",
       "             0.5928, 0.5961, 0.5993, 0.6026, 0.6059, 0.6091, 0.6124, 0.6156, 0.6189,\n",
       "             0.6221, 0.6254, 0.6287, 0.6319, 0.6352, 0.6384, 0.6417, 0.6450, 0.6482,\n",
       "             0.6515, 0.6547, 0.6580, 0.6612, 0.6645, 0.6678, 0.6710, 0.6743, 0.6775,\n",
       "             0.6808, 0.6840, 0.6873, 0.6906, 0.6938, 0.6971, 0.7003, 0.7036, 0.7068,\n",
       "             0.7101, 0.7134, 0.7166, 0.7199, 0.7231, 0.7264, 0.7296, 0.7329, 0.7362,\n",
       "             0.7394, 0.7427, 0.7459, 0.7492, 0.7524, 0.7557, 0.7590, 0.7622, 0.7655,\n",
       "             0.7687, 0.7720, 0.7752, 0.7785, 0.7818, 0.7850, 0.7883, 0.7915, 0.7948,\n",
       "             0.7980, 0.8013, 0.8046, 0.8078, 0.8111, 0.8143, 0.8176, 0.8208, 0.8241,\n",
       "             0.8274, 0.8306, 0.8339, 0.8371, 0.8404, 0.8436, 0.8469, 0.8502, 0.8534,\n",
       "             0.8567, 0.8599, 0.8632, 0.8664, 0.8697, 0.8730, 0.8762, 0.8795, 0.8827,\n",
       "             0.8860, 0.8893, 0.8925, 0.8958, 0.8990, 0.9023, 0.9055, 0.9088, 0.9121,\n",
       "             0.9153, 0.9186, 0.9218, 0.9251, 0.9283, 0.9316, 0.9349, 0.9381, 0.9414,\n",
       "             0.9446, 0.9479, 0.9511, 0.9544, 0.9577, 0.9609, 0.9642, 0.9674, 0.9707,\n",
       "             0.9739, 0.9772, 0.9805, 0.9837, 0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0678, 0.1149, 0.1471, 0.1759, 0.1943, 0.2069, 0.2264, 0.2368,\n",
       "             0.2529, 0.2644, 0.2747, 0.2885, 0.3023, 0.3115, 0.3195, 0.3218, 0.3276,\n",
       "             0.3356, 0.3402, 0.3460, 0.3552, 0.3632, 0.3736, 0.3759, 0.3793, 0.3897,\n",
       "             0.3966, 0.4057, 0.4092, 0.4149, 0.4195, 0.4230, 0.4310, 0.4368, 0.4391,\n",
       "             0.4460, 0.4506, 0.4529, 0.4563, 0.4609, 0.4644, 0.4713, 0.4724, 0.4736,\n",
       "             0.4770, 0.4816, 0.4862, 0.4874, 0.4908, 0.4954, 0.4966, 0.5023, 0.5034,\n",
       "             0.5057, 0.5080, 0.5092, 0.5103, 0.5149, 0.5195, 0.5230, 0.5264, 0.5287,\n",
       "             0.5299, 0.5322, 0.5333, 0.5356, 0.5402, 0.5425, 0.5437, 0.5448, 0.5471,\n",
       "             0.5506, 0.5529, 0.5540, 0.5552, 0.5563, 0.5586, 0.5598, 0.5609, 0.5621,\n",
       "             0.5655, 0.5667, 0.5701, 0.5736, 0.5782, 0.5793, 0.5805, 0.5816, 0.5828,\n",
       "             0.5851, 0.5885, 0.5897, 0.5920, 0.5931, 0.5943, 0.5954, 0.5989, 0.6011,\n",
       "             0.6046, 0.6092, 0.6115, 0.6126, 0.6138, 0.6149, 0.6161, 0.6172, 0.6184,\n",
       "             0.6207, 0.6230, 0.6241, 0.6264, 0.6276, 0.6287, 0.6310, 0.6322, 0.6356,\n",
       "             0.6368, 0.6379, 0.6391, 0.6402, 0.6414, 0.6425, 0.6437, 0.6448, 0.6460,\n",
       "             0.6471, 0.6483, 0.6494, 0.6517, 0.6552, 0.6563, 0.6598, 0.6609, 0.6621,\n",
       "             0.6644, 0.6655, 0.6678, 0.6690, 0.6713, 0.6724, 0.6736, 0.6747, 0.6759,\n",
       "             0.6770, 0.6782, 0.6793, 0.6816, 0.6828, 0.6839, 0.6851, 0.6874, 0.6908,\n",
       "             0.6920, 0.6931, 0.6943, 0.6954, 0.6966, 0.6977, 0.7000, 0.7011, 0.7023,\n",
       "             0.7034, 0.7046, 0.7069, 0.7092, 0.7103, 0.7115, 0.7126, 0.7138, 0.7161,\n",
       "             0.7184, 0.7195, 0.7207, 0.7218, 0.7230, 0.7241, 0.7253, 0.7264, 0.7276,\n",
       "             0.7287, 0.7299, 0.7310, 0.7322, 0.7333, 0.7345, 0.7368, 0.7379, 0.7391,\n",
       "             0.7402, 0.7414, 0.7425, 0.7437, 0.7460, 0.7471, 0.7483, 0.7494, 0.7506,\n",
       "             0.7517, 0.7529, 0.7540, 0.7552, 0.7563, 0.7586, 0.7609, 0.7621, 0.7632,\n",
       "             0.7644, 0.7655, 0.7667, 0.7678, 0.7690, 0.7701, 0.7713, 0.7724, 0.7736,\n",
       "             0.7747, 0.7759, 0.7770, 0.7793, 0.7805, 0.7816, 0.7828, 0.7839, 0.7851,\n",
       "             0.7862, 0.7874, 0.7885, 0.7897, 0.7908, 0.7920, 0.7943, 0.7954, 0.7966,\n",
       "             0.7977, 0.7989, 0.8000, 0.8011, 0.8023, 0.8034, 0.8057, 0.8069, 0.8080,\n",
       "             0.8092, 0.8103, 0.8115, 0.8126, 0.8138, 0.8149, 0.8161, 0.8172, 0.8184,\n",
       "             0.8195, 0.8207, 0.8218, 0.8230, 0.8241, 0.8253, 0.8264, 0.8287, 0.8299,\n",
       "             0.8310, 0.8322, 0.8333, 0.8345, 0.8356, 0.8368, 0.8391, 0.8402, 0.8414,\n",
       "             0.8425, 0.8437, 0.8448, 0.8460, 0.8471, 0.8483, 0.8494, 0.8506, 0.8517,\n",
       "             0.8529, 0.8540, 0.8552, 0.8563, 0.8575, 0.8586, 0.8598, 0.8609, 0.8621,\n",
       "             0.8632, 0.8644, 0.8667, 0.8678, 0.8690, 0.8701, 0.8713, 0.8724, 0.8736,\n",
       "             0.8747, 0.8759, 0.8770, 0.8782, 0.8793, 0.8805, 0.8816, 0.8828, 0.8839,\n",
       "             0.8851, 0.8851, 0.8862, 0.8874, 0.8885, 0.8897, 0.8908, 0.8920, 0.8931,\n",
       "             0.8943, 0.8954, 0.8966, 0.8977, 0.8989, 0.9000, 0.9011, 0.9023, 0.9034,\n",
       "             0.9046, 0.9057, 0.9069, 0.9080, 0.9092, 0.9103, 0.9115, 0.9126, 0.9138,\n",
       "             0.9149, 0.9161, 0.9172, 0.9184, 0.9195, 0.9207, 0.9218, 0.9230, 0.9241,\n",
       "             0.9253, 0.9264, 0.9276, 0.9299, 0.9310, 0.9322, 0.9333, 0.9345, 0.9356,\n",
       "             0.9368, 0.9379, 0.9391, 0.9402, 0.9414, 0.9425, 0.9437, 0.9448, 0.9460,\n",
       "             0.9471, 0.9483, 0.9494, 0.9494, 0.9506, 0.9517, 0.9529, 0.9529, 0.9529,\n",
       "             0.9540, 0.9552, 0.9563, 0.9563, 0.9575, 0.9586, 0.9598, 0.9609, 0.9609,\n",
       "             0.9621, 0.9621, 0.9632, 0.9632, 0.9644, 0.9655, 0.9667, 0.9678, 0.9678,\n",
       "             0.9690, 0.9701, 0.9713, 0.9724, 0.9736, 0.9747, 0.9747, 0.9759, 0.9770,\n",
       "             0.9770, 0.9782, 0.9793, 0.9805, 0.9816, 0.9816, 0.9816, 0.9828, 0.9839,\n",
       "             0.9839, 0.9851, 0.9851, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9874,\n",
       "             0.9874, 0.9885, 0.9897, 0.9897, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9967e-01, 9.9967e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9964e-01, 9.9962e-01, 9.9961e-01, 9.9960e-01, 9.9959e-01,\n",
       "             9.9956e-01, 9.9950e-01, 9.9945e-01, 9.9945e-01, 9.9940e-01, 9.9940e-01,\n",
       "             9.9937e-01, 9.9937e-01, 9.9936e-01, 9.9935e-01, 9.9932e-01, 9.9932e-01,\n",
       "             9.9925e-01, 9.9918e-01, 9.9917e-01, 9.9915e-01, 9.9907e-01, 9.9904e-01,\n",
       "             9.9891e-01, 9.9889e-01, 9.9875e-01, 9.9871e-01, 9.9870e-01, 9.9856e-01,\n",
       "             9.9814e-01, 9.9805e-01, 9.9804e-01, 9.9789e-01, 9.9784e-01, 9.9758e-01,\n",
       "             9.9741e-01, 9.9732e-01, 9.9706e-01, 9.9696e-01, 9.9593e-01, 9.9590e-01,\n",
       "             9.9586e-01, 9.9547e-01, 9.9416e-01, 9.9297e-01, 9.9219e-01, 9.9105e-01,\n",
       "             9.9070e-01, 9.9054e-01, 9.9016e-01, 9.8956e-01, 9.8940e-01, 9.8777e-01,\n",
       "             9.8438e-01, 9.8241e-01, 9.8156e-01, 9.8029e-01, 9.7996e-01, 9.7422e-01,\n",
       "             9.7166e-01, 9.6590e-01, 9.6134e-01, 9.6075e-01, 9.5926e-01, 9.5035e-01,\n",
       "             9.4447e-01, 9.3930e-01, 9.2147e-01, 9.1811e-01, 8.9488e-01, 8.9041e-01,\n",
       "             8.8100e-01, 8.7280e-01, 8.6244e-01, 8.0915e-01, 8.0055e-01, 7.9973e-01,\n",
       "             6.5120e-01, 5.8015e-01, 4.8745e-01, 4.6553e-01, 4.5896e-01, 4.4619e-01,\n",
       "             4.3885e-01, 3.9628e-01, 3.6930e-01, 3.5925e-01, 3.5414e-01, 3.3104e-01,\n",
       "             2.1664e-01, 1.9269e-01, 1.9219e-01, 1.6442e-01, 1.5866e-01, 1.4438e-01,\n",
       "             1.3321e-01, 1.2527e-01, 1.1452e-01, 1.1264e-01, 8.5215e-02, 6.8434e-02,\n",
       "             4.3517e-02, 3.7875e-02, 3.5798e-02, 3.2913e-02, 2.6401e-02, 2.6258e-02,\n",
       "             2.1116e-02, 1.4696e-02, 1.4442e-02, 1.3120e-02, 1.2718e-02, 1.2386e-02,\n",
       "             1.2121e-02, 1.0369e-02, 8.4407e-03, 7.1779e-03, 6.3945e-03, 6.1934e-03,\n",
       "             6.0073e-03, 5.7138e-03, 5.2611e-03, 5.1392e-03, 2.8693e-03, 2.8394e-03,\n",
       "             2.2969e-03, 1.9845e-03, 1.9263e-03, 1.8695e-03, 1.8611e-03, 1.7550e-03,\n",
       "             1.7333e-03, 1.7121e-03, 1.3448e-03, 1.3214e-03, 1.1134e-03, 9.6958e-04,\n",
       "             5.7092e-04, 4.7745e-04, 4.7075e-04, 4.5386e-04, 4.3645e-04, 4.2557e-04,\n",
       "             4.2455e-04, 3.6097e-04, 3.5109e-04, 3.3412e-04, 3.3260e-04, 3.1376e-04,\n",
       "             3.0379e-04, 3.0129e-04, 2.8897e-04, 2.6410e-04, 1.5190e-04, 1.1697e-04,\n",
       "             1.1144e-04, 9.7797e-05, 9.0482e-05, 8.4457e-05, 8.3666e-05, 6.4592e-05,\n",
       "             6.4115e-05, 6.3919e-05, 6.3387e-05, 6.1487e-05, 5.4799e-05, 4.9111e-05,\n",
       "             4.6658e-05, 3.9171e-05, 3.1601e-05, 3.1303e-05, 3.0248e-05, 2.9544e-05,\n",
       "             2.7041e-05, 2.6639e-05, 2.5154e-05, 2.5073e-05, 2.2353e-05, 2.0154e-05,\n",
       "             1.7645e-05, 1.7559e-05, 1.7369e-05, 1.6191e-05, 1.5536e-05, 1.4340e-05,\n",
       "             1.3882e-05, 1.3344e-05, 1.1909e-05, 1.0818e-05, 9.5869e-06, 9.5774e-06,\n",
       "             9.0463e-06, 8.8431e-06, 6.6371e-06, 5.0889e-06, 4.6608e-06, 4.5413e-06,\n",
       "             4.3050e-06, 3.6752e-06, 3.6336e-06, 2.8554e-06, 2.6596e-06, 2.6384e-06,\n",
       "             2.5252e-06, 2.4992e-06, 2.4393e-06, 2.2802e-06, 2.2481e-06, 2.1881e-06,\n",
       "             1.8388e-06, 1.6673e-06, 1.4584e-06, 1.3513e-06, 1.1960e-06, 1.1882e-06,\n",
       "             9.7368e-07, 9.1552e-07, 8.6922e-07, 7.5976e-07, 6.6324e-07, 6.4426e-07,\n",
       "             5.7016e-07, 5.2572e-07, 4.8927e-07, 4.7348e-07, 4.6771e-07, 4.6328e-07,\n",
       "             4.5226e-07, 4.4374e-07, 4.4363e-07, 3.5775e-07, 3.3208e-07, 2.9304e-07,\n",
       "             2.8233e-07, 2.5940e-07, 2.2791e-07, 2.1451e-07, 1.9918e-07, 1.9752e-07,\n",
       "             1.7329e-07, 1.6964e-07, 1.4778e-07, 1.3140e-07, 9.8608e-08, 9.6355e-08,\n",
       "             9.3034e-08, 7.3228e-08, 6.6734e-08, 5.9012e-08, 5.6352e-08, 5.4704e-08,\n",
       "             5.0422e-08, 4.9995e-08, 4.6170e-08, 3.5381e-08, 3.3738e-08, 3.1314e-08,\n",
       "             3.0089e-08, 2.8853e-08, 2.7305e-08, 2.6898e-08, 2.4658e-08, 2.1200e-08,\n",
       "             1.9117e-08, 1.8527e-08, 1.7402e-08, 1.3941e-08, 1.1524e-08, 1.1042e-08,\n",
       "             9.7984e-09, 9.6867e-09, 9.3474e-09, 9.1447e-09, 8.8996e-09, 8.4158e-09,\n",
       "             8.3996e-09, 5.3876e-09, 4.9513e-09, 4.2444e-09, 4.1772e-09, 3.7363e-09,\n",
       "             3.4097e-09, 3.2546e-09, 2.9561e-09, 2.2005e-09, 1.8214e-09, 1.5845e-09,\n",
       "             1.5010e-09, 1.1694e-09, 1.1173e-09, 8.9433e-10, 8.8256e-10, 8.5948e-10,\n",
       "             8.5218e-10, 7.8654e-10, 7.1001e-10, 6.5226e-10, 5.9843e-10, 4.9850e-10,\n",
       "             4.5118e-10, 4.1799e-10, 3.9940e-10, 3.4257e-10, 3.1786e-10, 2.9726e-10,\n",
       "             2.8368e-10, 2.2069e-10, 2.1015e-10, 2.0713e-10, 2.0443e-10, 2.0400e-10,\n",
       "             1.8765e-10, 1.3487e-10, 1.2096e-10, 8.8842e-11, 8.2925e-11, 8.2289e-11,\n",
       "             7.1906e-11, 7.1142e-11, 5.4525e-11, 5.0830e-11, 2.9036e-11, 2.7332e-11,\n",
       "             2.1944e-11, 2.0110e-11, 1.6916e-11, 1.5131e-11, 1.3974e-11, 8.6244e-12,\n",
       "             8.4482e-12, 7.6893e-12, 4.5732e-12, 3.1500e-12, 2.6301e-12, 1.1981e-12,\n",
       "             1.1711e-12, 1.0194e-12, 6.3526e-13, 4.9402e-13, 4.3088e-13, 3.7284e-13,\n",
       "             2.6769e-13, 2.2911e-13, 2.2336e-13, 2.2317e-13, 8.1962e-14, 1.9042e-14,\n",
       "             1.0246e-14, 7.5566e-15, 1.4230e-15, 1.3032e-15, 8.3465e-16, 7.0545e-16,\n",
       "             4.6836e-16, 4.3261e-16, 3.3479e-16, 6.8222e-17, 5.6809e-17, 6.6368e-18,\n",
       "             4.0896e-19, 8.8062e-21, 3.9739e-24])}},\n",
       "   {'fpr': np.float64(0.20846905537459284),\n",
       "    'tpr': np.float64(0.9942528735632183),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0033, 0.0098, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
       "             0.0163, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228,\n",
       "             0.0228, 0.0228, 0.0228, 0.0228, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293,\n",
       "             0.0293, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0358, 0.0358,\n",
       "             0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0391, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0423, 0.0423, 0.0456,\n",
       "             0.0456, 0.0456, 0.0456, 0.0456, 0.0489, 0.0489, 0.0489, 0.0489, 0.0489,\n",
       "             0.0489, 0.0489, 0.0489, 0.0489, 0.0489, 0.0489, 0.0489, 0.0489, 0.0521,\n",
       "             0.0521, 0.0521, 0.0521, 0.0554, 0.0586, 0.0619, 0.0619, 0.0651, 0.0651,\n",
       "             0.0651, 0.0651, 0.0651, 0.0684, 0.0717, 0.0749, 0.0749, 0.0749, 0.0749,\n",
       "             0.0749, 0.0749, 0.0749, 0.0749, 0.0749, 0.0749, 0.0749, 0.0782, 0.0782,\n",
       "             0.0814, 0.0814, 0.0847, 0.0847, 0.0879, 0.0879, 0.0912, 0.0945, 0.0945,\n",
       "             0.0945, 0.0945, 0.0945, 0.0945, 0.0977, 0.1010, 0.1042, 0.1075, 0.1107,\n",
       "             0.1140, 0.1173, 0.1173, 0.1205, 0.1205, 0.1205, 0.1205, 0.1205, 0.1238,\n",
       "             0.1270, 0.1303, 0.1303, 0.1303, 0.1336, 0.1336, 0.1336, 0.1368, 0.1401,\n",
       "             0.1433, 0.1466, 0.1498, 0.1531, 0.1531, 0.1564, 0.1564, 0.1596, 0.1629,\n",
       "             0.1629, 0.1661, 0.1694, 0.1726, 0.1759, 0.1792, 0.1824, 0.1857, 0.1889,\n",
       "             0.1922, 0.1954, 0.1987, 0.2020, 0.2052, 0.2085, 0.2117, 0.2150, 0.2182,\n",
       "             0.2215, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345, 0.2378, 0.2410, 0.2443,\n",
       "             0.2476, 0.2508, 0.2541, 0.2573, 0.2606, 0.2638, 0.2671, 0.2704, 0.2736,\n",
       "             0.2769, 0.2801, 0.2834, 0.2866, 0.2899, 0.2932, 0.2964, 0.2997, 0.3029,\n",
       "             0.3062, 0.3094, 0.3127, 0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3290,\n",
       "             0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453, 0.3485, 0.3518, 0.3550,\n",
       "             0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779, 0.3811, 0.3844,\n",
       "             0.3876, 0.3909, 0.3941, 0.3974, 0.4007, 0.4039, 0.4072, 0.4104, 0.4137,\n",
       "             0.4169, 0.4202, 0.4235, 0.4267, 0.4300, 0.4332, 0.4365, 0.4397, 0.4430,\n",
       "             0.4463, 0.4495, 0.4528, 0.4560, 0.4593, 0.4593, 0.4625, 0.4658, 0.4691,\n",
       "             0.4723, 0.4756, 0.4788, 0.4821, 0.4853, 0.4886, 0.4919, 0.4951, 0.4984,\n",
       "             0.5016, 0.5049, 0.5081, 0.5114, 0.5147, 0.5179, 0.5212, 0.5244, 0.5277,\n",
       "             0.5309, 0.5342, 0.5375, 0.5407, 0.5440, 0.5472, 0.5505, 0.5537, 0.5570,\n",
       "             0.5603, 0.5635, 0.5668, 0.5700, 0.5733, 0.5765, 0.5798, 0.5831, 0.5831,\n",
       "             0.5863, 0.5896, 0.5928, 0.5961, 0.5993, 0.6026, 0.6059, 0.6091, 0.6124,\n",
       "             0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319, 0.6352, 0.6384, 0.6417,\n",
       "             0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612, 0.6645, 0.6678, 0.6710,\n",
       "             0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906, 0.6938, 0.6971, 0.7003,\n",
       "             0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199, 0.7231, 0.7264, 0.7296,\n",
       "             0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492, 0.7524, 0.7557, 0.7590,\n",
       "             0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785, 0.7818, 0.7850, 0.7883,\n",
       "             0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078, 0.8111, 0.8143, 0.8176,\n",
       "             0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371, 0.8404, 0.8436, 0.8469,\n",
       "             0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664, 0.8697, 0.8730, 0.8762,\n",
       "             0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958, 0.8990, 0.9023, 0.9055,\n",
       "             0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251, 0.9283, 0.9316, 0.9349,\n",
       "             0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544, 0.9577, 0.9609, 0.9642,\n",
       "             0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837, 0.9870, 0.9902, 0.9935,\n",
       "             0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.5621, 0.6356, 0.6736, 0.7000, 0.7115, 0.7253, 0.7345, 0.7379,\n",
       "             0.7460, 0.7552, 0.7563, 0.7598, 0.7655, 0.7678, 0.7690, 0.7713, 0.7724,\n",
       "             0.7736, 0.7782, 0.7816, 0.7851, 0.7874, 0.7897, 0.7920, 0.7966, 0.7977,\n",
       "             0.8000, 0.8023, 0.8069, 0.8080, 0.8103, 0.8126, 0.8138, 0.8149, 0.8161,\n",
       "             0.8207, 0.8218, 0.8230, 0.8264, 0.8287, 0.8299, 0.8310, 0.8322, 0.8345,\n",
       "             0.8356, 0.8379, 0.8391, 0.8402, 0.8414, 0.8425, 0.8460, 0.8483, 0.8494,\n",
       "             0.8506, 0.8506, 0.8517, 0.8540, 0.8552, 0.8563, 0.8575, 0.8586, 0.8598,\n",
       "             0.8609, 0.8621, 0.8632, 0.8655, 0.8655, 0.8678, 0.8690, 0.8701, 0.8724,\n",
       "             0.8736, 0.8759, 0.8770, 0.8782, 0.8805, 0.8816, 0.8828, 0.8839, 0.8851,\n",
       "             0.8862, 0.8862, 0.8874, 0.8885, 0.8897, 0.8908, 0.8920, 0.8931, 0.8943,\n",
       "             0.8954, 0.8954, 0.8966, 0.8977, 0.8989, 0.9000, 0.9011, 0.9023, 0.9034,\n",
       "             0.9046, 0.9057, 0.9069, 0.9080, 0.9092, 0.9103, 0.9126, 0.9126, 0.9138,\n",
       "             0.9149, 0.9161, 0.9172, 0.9184, 0.9195, 0.9207, 0.9218, 0.9218, 0.9230,\n",
       "             0.9241, 0.9253, 0.9264, 0.9276, 0.9287, 0.9299, 0.9299, 0.9310, 0.9310,\n",
       "             0.9322, 0.9333, 0.9345, 0.9356, 0.9356, 0.9368, 0.9379, 0.9391, 0.9402,\n",
       "             0.9414, 0.9425, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483, 0.9494, 0.9494,\n",
       "             0.9506, 0.9517, 0.9529, 0.9529, 0.9529, 0.9529, 0.9540, 0.9540, 0.9552,\n",
       "             0.9563, 0.9575, 0.9586, 0.9586, 0.9586, 0.9586, 0.9598, 0.9609, 0.9621,\n",
       "             0.9632, 0.9644, 0.9655, 0.9667, 0.9678, 0.9690, 0.9701, 0.9701, 0.9713,\n",
       "             0.9713, 0.9724, 0.9724, 0.9736, 0.9736, 0.9747, 0.9747, 0.9747, 0.9759,\n",
       "             0.9770, 0.9782, 0.9793, 0.9805, 0.9805, 0.9805, 0.9805, 0.9805, 0.9805,\n",
       "             0.9805, 0.9805, 0.9816, 0.9816, 0.9828, 0.9839, 0.9851, 0.9862, 0.9862,\n",
       "             0.9862, 0.9862, 0.9874, 0.9885, 0.9885, 0.9897, 0.9908, 0.9908, 0.9908,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9986e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9976e-01, 9.9976e-01, 9.9974e-01, 9.9972e-01, 9.9971e-01,\n",
       "             9.9970e-01, 9.9969e-01, 9.9966e-01, 9.9966e-01, 9.9964e-01, 9.9961e-01,\n",
       "             9.9961e-01, 9.9960e-01, 9.9959e-01, 9.9958e-01, 9.9956e-01, 9.9953e-01,\n",
       "             9.9952e-01, 9.9949e-01, 9.9946e-01, 9.9945e-01, 9.9943e-01, 9.9942e-01,\n",
       "             9.9933e-01, 9.9927e-01, 9.9917e-01, 9.9908e-01, 9.9907e-01, 9.9906e-01,\n",
       "             9.9903e-01, 9.9900e-01, 9.9896e-01, 9.9885e-01, 9.9884e-01, 9.9871e-01,\n",
       "             9.9833e-01, 9.9832e-01, 9.9814e-01, 9.9784e-01, 9.9775e-01, 9.9735e-01,\n",
       "             9.9699e-01, 9.9696e-01, 9.9688e-01, 9.9653e-01, 9.9647e-01, 9.9623e-01,\n",
       "             9.9593e-01, 9.9509e-01, 9.9495e-01, 9.9255e-01, 9.9150e-01, 9.9083e-01,\n",
       "             9.9082e-01, 9.8938e-01, 9.8925e-01, 9.8468e-01, 9.7918e-01, 9.7377e-01,\n",
       "             9.5849e-01, 9.5236e-01, 9.5005e-01, 9.4491e-01, 9.4386e-01, 9.3059e-01,\n",
       "             9.0952e-01, 8.8872e-01, 8.7954e-01, 8.7885e-01, 7.9442e-01, 7.8353e-01,\n",
       "             7.2923e-01, 6.8456e-01, 6.5707e-01, 6.3851e-01, 6.3454e-01, 6.3096e-01,\n",
       "             6.1735e-01, 5.9830e-01, 5.0963e-01, 4.5604e-01, 4.4624e-01, 3.7371e-01,\n",
       "             3.4022e-01, 3.4013e-01, 2.7035e-01, 2.6710e-01, 2.5744e-01, 2.2983e-01,\n",
       "             1.6923e-01, 1.5086e-01, 1.4258e-01, 1.3807e-01, 1.3772e-01, 1.3254e-01,\n",
       "             1.2611e-01, 9.6876e-02, 8.3695e-02, 7.4410e-02, 7.0988e-02, 7.0585e-02,\n",
       "             6.5298e-02, 6.4465e-02, 5.8346e-02, 5.7960e-02, 4.2553e-02, 3.7947e-02,\n",
       "             3.1184e-02, 3.0900e-02, 3.0783e-02, 2.8084e-02, 2.4839e-02, 2.2210e-02,\n",
       "             2.1511e-02, 1.6491e-02, 1.4746e-02, 1.2787e-02, 1.0912e-02, 6.9557e-03,\n",
       "             6.9308e-03, 6.0647e-03, 5.8777e-03, 5.4257e-03, 5.3070e-03, 5.1680e-03,\n",
       "             5.0790e-03, 5.0017e-03, 4.8503e-03, 4.7492e-03, 4.7235e-03, 4.3482e-03,\n",
       "             4.0264e-03, 3.3488e-03, 3.2814e-03, 3.2128e-03, 3.1788e-03, 2.9378e-03,\n",
       "             2.6830e-03, 1.9643e-03, 1.5205e-03, 1.2437e-03, 1.1977e-03, 1.1404e-03,\n",
       "             1.0681e-03, 1.0065e-03, 1.0049e-03, 9.1757e-04, 6.1062e-04, 5.7359e-04,\n",
       "             5.5720e-04, 3.9851e-04, 3.6792e-04, 3.4734e-04, 3.4660e-04, 2.7078e-04,\n",
       "             2.6870e-04, 2.6509e-04, 2.0467e-04, 1.9947e-04, 1.6277e-04, 1.5595e-04,\n",
       "             1.3315e-04, 1.2260e-04, 1.0730e-04, 1.0430e-04, 1.0353e-04, 9.9451e-05,\n",
       "             9.8087e-05, 9.6650e-05, 7.6931e-05, 7.4195e-05, 6.3591e-05, 6.0663e-05,\n",
       "             5.0502e-05, 4.4489e-05, 3.9446e-05, 3.6218e-05, 2.8952e-05, 2.6788e-05,\n",
       "             2.5811e-05, 2.5621e-05, 2.3994e-05, 2.3826e-05, 2.3634e-05, 2.2984e-05,\n",
       "             1.9851e-05, 1.9644e-05, 1.7758e-05, 1.6222e-05, 1.4801e-05, 1.4474e-05,\n",
       "             1.1878e-05, 1.1378e-05, 1.0080e-05, 7.2973e-06, 7.0820e-06, 6.5124e-06,\n",
       "             5.5078e-06, 5.0537e-06, 5.0033e-06, 4.8259e-06, 4.7052e-06, 4.6146e-06,\n",
       "             4.4585e-06, 4.3489e-06, 4.1851e-06, 4.0401e-06, 3.5837e-06, 2.8528e-06,\n",
       "             2.4528e-06, 2.2173e-06, 1.9498e-06, 1.8038e-06, 1.7024e-06, 1.6679e-06,\n",
       "             1.3182e-06, 1.3094e-06, 1.2541e-06, 1.1980e-06, 1.1728e-06, 1.1392e-06,\n",
       "             1.1380e-06, 1.0668e-06, 8.3981e-07, 8.0885e-07, 7.6587e-07, 7.2869e-07,\n",
       "             6.6392e-07, 6.0346e-07, 5.8463e-07, 3.7639e-07, 2.9783e-07, 2.7229e-07,\n",
       "             2.6104e-07, 2.0873e-07, 1.9805e-07, 1.8794e-07, 1.0403e-07, 9.8438e-08,\n",
       "             9.8103e-08, 9.4928e-08, 5.7001e-08, 4.1063e-08, 3.6202e-08, 3.1694e-08,\n",
       "             2.7171e-08, 2.2777e-08, 2.0737e-08, 2.0388e-08, 1.5952e-08, 1.5751e-08,\n",
       "             1.5673e-08, 1.4379e-08, 1.2790e-08, 1.0543e-08, 9.8614e-09, 8.8342e-09,\n",
       "             8.7384e-09, 6.5847e-09, 5.6997e-09, 5.0201e-09, 3.3569e-09, 3.3204e-09,\n",
       "             2.3249e-09, 1.8678e-09, 1.7243e-09, 1.6658e-09, 1.5647e-09, 1.5224e-09,\n",
       "             1.4592e-09, 1.3735e-09, 1.2087e-09, 1.1273e-09, 1.1071e-09, 1.0575e-09,\n",
       "             8.7010e-10, 8.0894e-10, 5.6920e-10, 5.4262e-10, 4.9698e-10, 4.1253e-10,\n",
       "             3.9268e-10, 3.8985e-10, 3.4921e-10, 3.0756e-10, 2.6552e-10, 1.6983e-10,\n",
       "             1.4866e-10, 1.1630e-10, 1.0039e-10, 9.5524e-11, 9.3483e-11, 8.2043e-11,\n",
       "             6.4554e-11, 3.4682e-11, 3.0827e-11, 2.7957e-11, 2.4481e-11, 1.7109e-11,\n",
       "             1.5954e-11, 1.2754e-11, 1.2174e-11, 1.1247e-11, 9.3912e-12, 3.8416e-12,\n",
       "             3.3687e-12, 2.4319e-12, 6.5457e-13, 3.4537e-13, 3.3186e-13, 2.8967e-13,\n",
       "             1.2895e-13, 6.5780e-14, 3.3433e-14, 2.1069e-14, 1.9780e-14, 1.6048e-14,\n",
       "             1.0202e-14, 4.5890e-15, 4.2446e-15, 3.6450e-15, 2.1319e-15, 7.0985e-16,\n",
       "             1.3808e-16, 2.7030e-17, 9.6205e-19, 1.1306e-19, 6.7570e-24])}},\n",
       "   {'fpr': np.float64(0.09446254071661238),\n",
       "    'tpr': np.float64(0.9850574712643678),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0130, 0.0163, 0.0163, 0.0163, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228,\n",
       "             0.0261, 0.0293, 0.0326, 0.0326, 0.0326, 0.0358, 0.0391, 0.0391, 0.0423,\n",
       "             0.0456, 0.0456, 0.0456, 0.0489, 0.0521, 0.0521, 0.0521, 0.0521, 0.0554,\n",
       "             0.0554, 0.0586, 0.0619, 0.0619, 0.0619, 0.0651, 0.0651, 0.0684, 0.0717,\n",
       "             0.0749, 0.0749, 0.0749, 0.0782, 0.0782, 0.0814, 0.0847, 0.0879, 0.0912,\n",
       "             0.0945, 0.0977, 0.0977, 0.1010, 0.1042, 0.1075, 0.1107, 0.1140, 0.1173,\n",
       "             0.1173, 0.1173, 0.1205, 0.1238, 0.1270, 0.1303, 0.1336, 0.1368, 0.1401,\n",
       "             0.1433, 0.1466, 0.1498, 0.1531, 0.1531, 0.1564, 0.1596, 0.1629, 0.1661,\n",
       "             0.1661, 0.1694, 0.1726, 0.1759, 0.1759, 0.1792, 0.1792, 0.1824, 0.1857,\n",
       "             0.1857, 0.1889, 0.1922, 0.1954, 0.1987, 0.2020, 0.2020, 0.2052, 0.2085,\n",
       "             0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345, 0.2378,\n",
       "             0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2606, 0.2638, 0.2671,\n",
       "             0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899, 0.2899, 0.2932,\n",
       "             0.2932, 0.2964, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160, 0.3192,\n",
       "             0.3192, 0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453,\n",
       "             0.3485, 0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746,\n",
       "             0.3746, 0.3779, 0.3811, 0.3844, 0.3876, 0.3909, 0.3941, 0.3974, 0.4007,\n",
       "             0.4039, 0.4072, 0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267, 0.4300,\n",
       "             0.4332, 0.4365, 0.4397, 0.4430, 0.4463, 0.4495, 0.4528, 0.4560, 0.4593,\n",
       "             0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788, 0.4821, 0.4853, 0.4886,\n",
       "             0.4919, 0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114, 0.5147, 0.5179,\n",
       "             0.5212, 0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407, 0.5440, 0.5472,\n",
       "             0.5505, 0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700, 0.5733, 0.5765,\n",
       "             0.5798, 0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5993, 0.6026, 0.6059,\n",
       "             0.6091, 0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319, 0.6352,\n",
       "             0.6384, 0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612, 0.6645,\n",
       "             0.6678, 0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906, 0.6938,\n",
       "             0.6971, 0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199, 0.7231,\n",
       "             0.7264, 0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492, 0.7524,\n",
       "             0.7557, 0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785, 0.7818,\n",
       "             0.7850, 0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078, 0.8111,\n",
       "             0.8143, 0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371, 0.8404,\n",
       "             0.8436, 0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664, 0.8697,\n",
       "             0.8730, 0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958, 0.8990,\n",
       "             0.9023, 0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251, 0.9283,\n",
       "             0.9316, 0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544, 0.9577,\n",
       "             0.9609, 0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837, 0.9870,\n",
       "             0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7356, 0.7931, 0.8161, 0.8333, 0.8414, 0.8494, 0.8540, 0.8586,\n",
       "             0.8644, 0.8690, 0.8701, 0.8747, 0.8793, 0.8816, 0.8828, 0.8839, 0.8862,\n",
       "             0.8874, 0.8920, 0.8931, 0.8966, 0.8989, 0.9011, 0.9023, 0.9034, 0.9046,\n",
       "             0.9057, 0.9069, 0.9080, 0.9115, 0.9149, 0.9161, 0.9172, 0.9184, 0.9195,\n",
       "             0.9207, 0.9218, 0.9241, 0.9253, 0.9264, 0.9276, 0.9287, 0.9299, 0.9310,\n",
       "             0.9322, 0.9333, 0.9345, 0.9356, 0.9368, 0.9379, 0.9391, 0.9402, 0.9414,\n",
       "             0.9425, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483, 0.9494, 0.9506, 0.9517,\n",
       "             0.9529, 0.9540, 0.9540, 0.9540, 0.9552, 0.9563, 0.9563, 0.9575, 0.9586,\n",
       "             0.9598, 0.9609, 0.9621, 0.9632, 0.9632, 0.9644, 0.9655, 0.9667, 0.9678,\n",
       "             0.9678, 0.9678, 0.9678, 0.9690, 0.9701, 0.9701, 0.9701, 0.9713, 0.9713,\n",
       "             0.9713, 0.9724, 0.9736, 0.9736, 0.9736, 0.9747, 0.9759, 0.9770, 0.9770,\n",
       "             0.9782, 0.9782, 0.9782, 0.9793, 0.9805, 0.9805, 0.9816, 0.9816, 0.9816,\n",
       "             0.9816, 0.9828, 0.9839, 0.9839, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851,\n",
       "             0.9851, 0.9851, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862,\n",
       "             0.9874, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9897, 0.9897, 0.9897, 0.9897, 0.9897,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9993e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9987e-01, 9.9985e-01, 9.9985e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9980e-01, 9.9979e-01, 9.9975e-01, 9.9959e-01,\n",
       "             9.9956e-01, 9.9949e-01, 9.9935e-01, 9.9925e-01, 9.9924e-01, 9.9917e-01,\n",
       "             9.9916e-01, 9.9853e-01, 9.9838e-01, 9.9764e-01, 9.9734e-01, 9.9712e-01,\n",
       "             9.9666e-01, 9.9641e-01, 9.9482e-01, 9.9478e-01, 9.9417e-01, 9.9357e-01,\n",
       "             9.9259e-01, 9.9152e-01, 9.8696e-01, 9.8611e-01, 9.8424e-01, 9.8289e-01,\n",
       "             9.8283e-01, 9.8115e-01, 9.5179e-01, 9.5145e-01, 9.4344e-01, 9.4127e-01,\n",
       "             9.2820e-01, 9.1886e-01, 8.9308e-01, 7.9808e-01, 6.2093e-01, 6.1160e-01,\n",
       "             5.6105e-01, 5.5338e-01, 5.2386e-01, 5.1969e-01, 4.3575e-01, 3.1544e-01,\n",
       "             3.0164e-01, 2.6431e-01, 2.2007e-01, 1.8099e-01, 1.6598e-01, 1.4001e-01,\n",
       "             1.2461e-01, 1.0268e-01, 1.0095e-01, 9.1635e-02, 9.0850e-02, 8.2867e-02,\n",
       "             5.5068e-02, 5.4999e-02, 3.0039e-02, 2.7696e-02, 2.2663e-02, 1.0594e-02,\n",
       "             7.3405e-03, 7.2553e-03, 2.8978e-03, 2.5010e-03, 2.1528e-03, 2.1473e-03,\n",
       "             1.2381e-03, 9.7286e-04, 4.7287e-04, 4.0888e-04, 3.7055e-04, 3.0720e-04,\n",
       "             2.4643e-04, 2.1792e-04, 2.1599e-04, 2.0821e-04, 1.9169e-04, 8.7627e-05,\n",
       "             8.3945e-05, 8.2009e-05, 6.0969e-05, 3.2406e-05, 3.0532e-05, 2.9065e-05,\n",
       "             2.0450e-05, 1.5365e-05, 1.5250e-05, 1.3238e-05, 6.1569e-06, 5.8838e-06,\n",
       "             3.7548e-06, 3.7525e-06, 2.7044e-06, 2.6369e-06, 2.1730e-06, 2.0836e-06,\n",
       "             1.4395e-06, 1.4294e-06, 1.3482e-06, 1.2987e-06, 1.2475e-06, 9.9127e-07,\n",
       "             9.1319e-07, 8.1599e-07, 8.0122e-07, 7.2886e-07, 5.9771e-07, 4.8109e-07,\n",
       "             4.1633e-07, 4.1191e-07, 4.0159e-07, 2.2885e-07, 2.1633e-07, 2.0908e-07,\n",
       "             1.8789e-07, 1.8095e-07, 1.4833e-07, 1.4174e-07, 1.4152e-07, 6.6249e-08,\n",
       "             6.2068e-08, 5.5187e-08, 5.1371e-08, 4.9582e-08, 4.7295e-08, 2.9183e-08,\n",
       "             2.6120e-08, 2.4226e-08, 2.2739e-08, 1.9921e-08, 1.7347e-08, 6.5988e-09,\n",
       "             6.3325e-09, 5.4389e-09, 4.1349e-09, 2.9335e-09, 2.6634e-09, 2.3236e-09,\n",
       "             2.1568e-09, 2.0742e-09, 1.8410e-09, 1.8238e-09, 1.3317e-09, 1.2450e-09,\n",
       "             1.1707e-09, 1.1430e-09, 1.1090e-09, 1.0630e-09, 9.7212e-10, 9.1859e-10,\n",
       "             7.0277e-10, 4.1891e-10, 2.8028e-10, 2.6980e-10, 2.5590e-10, 1.8438e-10,\n",
       "             1.1931e-10, 1.0814e-10, 1.0454e-10, 1.0396e-10, 1.0226e-10, 9.8571e-11,\n",
       "             9.1167e-11, 7.3830e-11, 5.9585e-11, 5.0946e-11, 4.7878e-11, 4.0557e-11,\n",
       "             3.3918e-11, 2.9224e-11, 2.7224e-11, 2.5045e-11, 1.9918e-11, 1.6303e-11,\n",
       "             1.5515e-11, 1.5329e-11, 1.2340e-11, 1.1793e-11, 1.0365e-11, 9.7660e-12,\n",
       "             7.7079e-12, 6.2304e-12, 4.8092e-12, 4.6527e-12, 4.4024e-12, 4.3152e-12,\n",
       "             4.1148e-12, 3.9991e-12, 3.4117e-12, 2.9936e-12, 2.3279e-12, 2.1243e-12,\n",
       "             2.0354e-12, 2.0215e-12, 1.7091e-12, 1.7035e-12, 1.4912e-12, 1.3344e-12,\n",
       "             1.2872e-12, 6.3213e-13, 5.1462e-13, 4.8842e-13, 4.4325e-13, 3.7336e-13,\n",
       "             3.2761e-13, 2.9487e-13, 2.5169e-13, 2.3418e-13, 2.1339e-13, 2.0958e-13,\n",
       "             1.6949e-13, 1.6710e-13, 1.6445e-13, 1.2224e-13, 1.0236e-13, 9.1409e-14,\n",
       "             9.0733e-14, 7.7185e-14, 7.1486e-14, 6.7730e-14, 6.6825e-14, 6.4102e-14,\n",
       "             5.6708e-14, 3.2112e-14, 2.7952e-14, 2.6493e-14, 2.3905e-14, 2.3463e-14,\n",
       "             1.9860e-14, 1.8544e-14, 1.7985e-14, 1.5269e-14, 1.0344e-14, 9.2552e-15,\n",
       "             6.5740e-15, 5.6427e-15, 5.5860e-15, 4.8980e-15, 4.4371e-15, 4.4017e-15,\n",
       "             3.2372e-15, 2.9514e-15, 2.6892e-15, 1.3195e-15, 1.1014e-15, 7.9386e-16,\n",
       "             7.6618e-16, 7.3825e-16, 1.7176e-16, 1.2831e-16, 8.7073e-17, 6.7629e-17,\n",
       "             6.2795e-17, 4.2479e-17, 3.1567e-17, 2.5905e-17, 2.1273e-17, 2.0894e-17,\n",
       "             1.6885e-17, 1.5113e-17, 1.3435e-17, 1.1439e-17, 8.0810e-18, 4.7424e-18,\n",
       "             4.2020e-18, 2.4571e-18, 2.1729e-18, 1.9755e-18, 1.6228e-18, 1.5842e-18,\n",
       "             1.5690e-18, 1.3528e-18, 1.0544e-18, 8.3467e-19, 7.7236e-19, 7.2577e-19,\n",
       "             5.4635e-19, 4.9469e-19, 3.1001e-19, 2.4634e-19, 2.0788e-19, 1.3943e-19,\n",
       "             1.1409e-19, 1.0841e-19, 9.3430e-20, 5.1271e-20, 4.0426e-20, 3.5253e-20,\n",
       "             3.2353e-20, 3.1462e-20, 2.7464e-20, 2.6640e-20, 1.8321e-20, 1.5883e-20,\n",
       "             1.3802e-20, 8.0419e-21, 5.5923e-21, 4.0539e-21, 2.0005e-21, 5.2275e-22,\n",
       "             2.6590e-22, 2.1600e-22, 1.7674e-22, 5.1101e-23, 4.3970e-23, 2.8455e-23,\n",
       "             2.8127e-23, 2.5190e-23, 2.1442e-23, 1.7988e-23, 7.5777e-24, 6.1469e-24,\n",
       "             3.3827e-24, 2.8136e-24, 2.2353e-24, 1.8796e-24, 4.3283e-25, 3.3768e-25,\n",
       "             1.5347e-25, 3.5331e-26, 1.3879e-26, 6.3339e-27, 4.2232e-27, 3.1270e-27,\n",
       "             7.3338e-28, 3.4301e-28, 8.3740e-29, 4.9000e-29, 2.5588e-33, 1.8654e-34,\n",
       "             0.0000e+00])}},\n",
       "   {'fpr': np.float64(0.06840390879478828),\n",
       "    'tpr': np.float64(0.9827586206896551),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
       "             0.0163, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0228, 0.0228, 0.0228, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0293, 0.0293, 0.0326, 0.0358, 0.0358, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0423, 0.0423, 0.0456,\n",
       "             0.0489, 0.0521, 0.0554, 0.0586, 0.0619, 0.0619, 0.0651, 0.0684, 0.0684,\n",
       "             0.0684, 0.0717, 0.0717, 0.0749, 0.0749, 0.0782, 0.0814, 0.0814, 0.0847,\n",
       "             0.0879, 0.0912, 0.0945, 0.0945, 0.0977, 0.1010, 0.1042, 0.1075, 0.1075,\n",
       "             0.1075, 0.1107, 0.1107, 0.1140, 0.1140, 0.1173, 0.1205, 0.1238, 0.1270,\n",
       "             0.1303, 0.1336, 0.1368, 0.1368, 0.1368, 0.1401, 0.1433, 0.1466, 0.1498,\n",
       "             0.1531, 0.1564, 0.1564, 0.1596, 0.1629, 0.1661, 0.1661, 0.1694, 0.1726,\n",
       "             0.1759, 0.1792, 0.1824, 0.1857, 0.1889, 0.1889, 0.1922, 0.1922, 0.1954,\n",
       "             0.1987, 0.2020, 0.2052, 0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2248,\n",
       "             0.2280, 0.2313, 0.2345, 0.2378, 0.2410, 0.2443, 0.2476, 0.2508, 0.2541,\n",
       "             0.2573, 0.2606, 0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834,\n",
       "             0.2834, 0.2866, 0.2899, 0.2932, 0.2964, 0.2997, 0.3029, 0.3062, 0.3094,\n",
       "             0.3127, 0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388,\n",
       "             0.3420, 0.3453, 0.3485, 0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3681,\n",
       "             0.3713, 0.3746, 0.3779, 0.3811, 0.3844, 0.3876, 0.3909, 0.3941, 0.3974,\n",
       "             0.4007, 0.4039, 0.4072, 0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267,\n",
       "             0.4300, 0.4332, 0.4365, 0.4397, 0.4430, 0.4463, 0.4495, 0.4528, 0.4560,\n",
       "             0.4593, 0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788, 0.4821, 0.4853,\n",
       "             0.4886, 0.4919, 0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114, 0.5147,\n",
       "             0.5179, 0.5212, 0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407, 0.5440,\n",
       "             0.5472, 0.5505, 0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700, 0.5733,\n",
       "             0.5765, 0.5798, 0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5993, 0.6026,\n",
       "             0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319,\n",
       "             0.6352, 0.6384, 0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612,\n",
       "             0.6645, 0.6678, 0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906,\n",
       "             0.6938, 0.6971, 0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199,\n",
       "             0.7231, 0.7264, 0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492,\n",
       "             0.7524, 0.7557, 0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785,\n",
       "             0.7818, 0.7850, 0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078,\n",
       "             0.8111, 0.8143, 0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371,\n",
       "             0.8404, 0.8436, 0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664,\n",
       "             0.8697, 0.8730, 0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958,\n",
       "             0.8990, 0.9023, 0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251,\n",
       "             0.9283, 0.9316, 0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544,\n",
       "             0.9577, 0.9609, 0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837,\n",
       "             0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.6655, 0.7414, 0.7724, 0.7966, 0.8057, 0.8207, 0.8287, 0.8345,\n",
       "             0.8414, 0.8471, 0.8506, 0.8563, 0.8632, 0.8655, 0.8701, 0.8724, 0.8747,\n",
       "             0.8759, 0.8782, 0.8793, 0.8805, 0.8851, 0.8862, 0.8874, 0.8885, 0.8897,\n",
       "             0.8908, 0.8920, 0.8931, 0.8943, 0.8966, 0.8977, 0.8989, 0.9000, 0.9011,\n",
       "             0.9023, 0.9034, 0.9046, 0.9057, 0.9069, 0.9080, 0.9092, 0.9103, 0.9115,\n",
       "             0.9126, 0.9138, 0.9149, 0.9161, 0.9172, 0.9184, 0.9195, 0.9207, 0.9218,\n",
       "             0.9230, 0.9241, 0.9253, 0.9264, 0.9264, 0.9276, 0.9287, 0.9299, 0.9310,\n",
       "             0.9322, 0.9322, 0.9333, 0.9345, 0.9356, 0.9368, 0.9368, 0.9379, 0.9391,\n",
       "             0.9402, 0.9414, 0.9425, 0.9437, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483,\n",
       "             0.9494, 0.9494, 0.9506, 0.9517, 0.9529, 0.9540, 0.9552, 0.9563, 0.9575,\n",
       "             0.9586, 0.9586, 0.9598, 0.9609, 0.9609, 0.9621, 0.9632, 0.9644, 0.9655,\n",
       "             0.9667, 0.9678, 0.9690, 0.9690, 0.9701, 0.9701, 0.9701, 0.9713, 0.9713,\n",
       "             0.9724, 0.9736, 0.9747, 0.9759, 0.9770, 0.9782, 0.9782, 0.9793, 0.9793,\n",
       "             0.9793, 0.9793, 0.9793, 0.9793, 0.9793, 0.9805, 0.9805, 0.9805, 0.9816,\n",
       "             0.9828, 0.9828, 0.9839, 0.9839, 0.9851, 0.9851, 0.9851, 0.9862, 0.9862,\n",
       "             0.9862, 0.9862, 0.9862, 0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9885,\n",
       "             0.9897, 0.9897, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9986e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9982e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9976e-01, 9.9968e-01, 9.9957e-01, 9.9955e-01,\n",
       "             9.9930e-01, 9.9890e-01, 9.9871e-01, 9.9864e-01, 9.9842e-01, 9.9840e-01,\n",
       "             9.9785e-01, 9.9767e-01, 9.9733e-01, 9.9695e-01, 9.9616e-01, 9.9612e-01,\n",
       "             9.9598e-01, 9.9483e-01, 9.9428e-01, 9.9407e-01, 9.9358e-01, 9.9289e-01,\n",
       "             9.9013e-01, 9.8932e-01, 9.8921e-01, 9.8820e-01, 9.8745e-01, 9.8638e-01,\n",
       "             9.8583e-01, 9.8538e-01, 9.6129e-01, 9.6004e-01, 9.4788e-01, 9.4302e-01,\n",
       "             9.3932e-01, 9.2895e-01, 9.2776e-01, 9.2110e-01, 9.1496e-01, 8.7573e-01,\n",
       "             8.6701e-01, 8.4449e-01, 8.4260e-01, 8.2154e-01, 7.6653e-01, 7.0869e-01,\n",
       "             7.0329e-01, 6.2059e-01, 5.9791e-01, 5.7379e-01, 5.2947e-01, 5.1992e-01,\n",
       "             5.1456e-01, 3.3774e-01, 2.9821e-01, 2.1233e-01, 1.5439e-01, 1.4770e-01,\n",
       "             1.3627e-01, 1.1635e-01, 1.1124e-01, 6.4294e-02, 5.4337e-02, 5.0501e-02,\n",
       "             3.3851e-02, 2.1637e-02, 2.0141e-02, 1.5159e-02, 1.2776e-02, 1.2122e-02,\n",
       "             1.1488e-02, 1.1167e-02, 6.5232e-03, 6.1918e-03, 3.8769e-03, 3.6271e-03,\n",
       "             3.5168e-03, 3.3544e-03, 3.2552e-03, 2.6778e-03, 2.6074e-03, 2.0163e-03,\n",
       "             5.2843e-04, 4.1341e-04, 4.1072e-04, 3.4129e-04, 2.2735e-04, 2.1528e-04,\n",
       "             2.1471e-04, 1.8346e-04, 1.5900e-04, 1.5055e-04, 1.2163e-04, 9.3927e-05,\n",
       "             8.6921e-05, 7.9783e-05, 6.6019e-05, 4.7074e-05, 4.5113e-05, 4.3993e-05,\n",
       "             3.4666e-05, 2.3408e-05, 1.3425e-05, 1.2318e-05, 9.3917e-06, 9.2275e-06,\n",
       "             8.7014e-06, 8.3585e-06, 7.3356e-06, 7.2509e-06, 5.5478e-06, 3.8667e-06,\n",
       "             3.4837e-06, 2.4438e-06, 2.0464e-06, 2.0311e-06, 1.8528e-06, 1.7409e-06,\n",
       "             1.3811e-06, 1.2344e-06, 1.2248e-06, 1.0384e-06, 6.5072e-07, 6.1570e-07,\n",
       "             5.7847e-07, 4.8344e-07, 3.4989e-07, 2.8710e-07, 1.8488e-07, 1.8320e-07,\n",
       "             1.7959e-07, 1.4572e-07, 1.3850e-07, 1.1931e-07, 1.0074e-07, 8.8557e-08,\n",
       "             7.1705e-08, 6.7372e-08, 5.4024e-08, 3.7527e-08, 3.6577e-08, 2.6859e-08,\n",
       "             2.1959e-08, 2.0383e-08, 1.9938e-08, 1.9460e-08, 1.8361e-08, 1.8298e-08,\n",
       "             1.7579e-08, 1.6302e-08, 9.3292e-09, 8.9891e-09, 7.3778e-09, 6.1539e-09,\n",
       "             5.0324e-09, 4.5596e-09, 4.1034e-09, 3.7560e-09, 3.4965e-09, 3.3299e-09,\n",
       "             3.1235e-09, 3.0079e-09, 3.0004e-09, 2.4804e-09, 2.4243e-09, 2.3877e-09,\n",
       "             2.2522e-09, 2.1100e-09, 2.0058e-09, 1.2313e-09, 1.1983e-09, 1.1494e-09,\n",
       "             1.0114e-09, 8.2670e-10, 7.5450e-10, 6.7028e-10, 5.8021e-10, 4.8983e-10,\n",
       "             4.5678e-10, 4.2670e-10, 3.0805e-10, 2.9643e-10, 2.8812e-10, 2.5987e-10,\n",
       "             2.1532e-10, 1.4824e-10, 1.4509e-10, 1.4428e-10, 1.0511e-10, 9.6492e-11,\n",
       "             9.0594e-11, 8.6170e-11, 7.1940e-11, 4.8938e-11, 4.6528e-11, 4.5179e-11,\n",
       "             4.4847e-11, 3.6926e-11, 3.5897e-11, 3.4540e-11, 3.2873e-11, 3.1008e-11,\n",
       "             2.3286e-11, 2.3035e-11, 2.1290e-11, 2.1219e-11, 1.9232e-11, 1.4501e-11,\n",
       "             1.2512e-11, 1.2248e-11, 1.1830e-11, 1.0997e-11, 1.0057e-11, 9.9713e-12,\n",
       "             9.7737e-12, 8.6813e-12, 5.2808e-12, 5.0710e-12, 4.7520e-12, 3.6144e-12,\n",
       "             3.1001e-12, 2.8417e-12, 2.7444e-12, 2.6992e-12, 2.2741e-12, 1.9977e-12,\n",
       "             1.2731e-12, 1.1400e-12, 1.0980e-12, 7.1180e-13, 6.8278e-13, 6.7797e-13,\n",
       "             5.1015e-13, 4.9449e-13, 4.7292e-13, 4.3120e-13, 4.0441e-13, 3.9146e-13,\n",
       "             3.6110e-13, 3.5790e-13, 3.2694e-13, 2.8417e-13, 2.7784e-13, 2.0879e-13,\n",
       "             1.9323e-13, 1.8742e-13, 1.3672e-13, 1.3370e-13, 1.0026e-13, 7.3266e-14,\n",
       "             7.1593e-14, 5.4078e-14, 4.7845e-14, 4.5167e-14, 4.2258e-14, 4.0426e-14,\n",
       "             4.0425e-14, 2.6634e-14, 2.1248e-14, 1.9981e-14, 1.4711e-14, 1.0989e-14,\n",
       "             1.0830e-14, 9.8509e-15, 8.6221e-15, 8.4930e-15, 8.0727e-15, 5.6963e-15,\n",
       "             5.2460e-15, 5.1935e-15, 4.7994e-15, 4.1154e-15, 2.7451e-15, 2.5113e-15,\n",
       "             2.2105e-15, 2.0769e-15, 2.0383e-15, 1.6802e-15, 1.4845e-15, 1.4255e-15,\n",
       "             1.4183e-15, 9.5505e-16, 9.0747e-16, 8.5272e-16, 7.6340e-16, 6.8301e-16,\n",
       "             5.4103e-16, 5.1624e-16, 5.1155e-16, 4.6368e-16, 4.5151e-16, 4.5089e-16,\n",
       "             4.3956e-16, 4.1056e-16, 3.4599e-16, 3.1967e-16, 1.9382e-16, 1.8607e-16,\n",
       "             1.4825e-16, 1.2409e-16, 1.1870e-16, 1.1023e-16, 8.7174e-17, 8.5597e-17,\n",
       "             7.9766e-17, 5.9819e-17, 5.1963e-17, 4.0572e-17, 3.5978e-17, 2.7378e-17,\n",
       "             2.3210e-17, 1.9177e-17, 1.4316e-17, 1.3378e-17, 1.0371e-17, 9.5646e-18,\n",
       "             7.9559e-18, 6.6291e-18, 5.3607e-18, 5.3301e-18, 4.9016e-18, 4.2437e-18,\n",
       "             2.3061e-18, 1.6202e-18, 6.2383e-19, 3.0402e-19, 2.4660e-19, 1.6475e-19,\n",
       "             1.5095e-19, 1.3178e-19, 8.1108e-20, 7.5320e-20, 6.3488e-20, 3.1774e-20,\n",
       "             2.9888e-20, 2.9109e-20, 2.4416e-20, 1.9905e-20, 1.9738e-20, 1.7946e-20,\n",
       "             7.5062e-21, 7.0796e-21, 6.0626e-21, 2.6736e-21, 7.5894e-22, 3.6925e-23,\n",
       "             1.9091e-23, 7.5897e-24, 5.4552e-24, 1.8391e-24, 4.1704e-27, 3.5881e-27,\n",
       "             2.1941e-27, 1.3004e-34])}},\n",
       "   {'fpr': np.float64(0.15960912052117263),\n",
       "    'tpr': np.float64(0.993103448275862),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0195, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0293, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358,\n",
       "             0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358,\n",
       "             0.0358, 0.0391, 0.0391, 0.0391, 0.0391, 0.0423, 0.0423, 0.0423, 0.0423,\n",
       "             0.0423, 0.0423, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456,\n",
       "             0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0489,\n",
       "             0.0521, 0.0521, 0.0554, 0.0554, 0.0554, 0.0554, 0.0586, 0.0586, 0.0586,\n",
       "             0.0586, 0.0586, 0.0619, 0.0619, 0.0651, 0.0684, 0.0717, 0.0717, 0.0717,\n",
       "             0.0749, 0.0782, 0.0814, 0.0814, 0.0814, 0.0814, 0.0814, 0.0847, 0.0879,\n",
       "             0.0912, 0.0945, 0.0945, 0.0977, 0.1010, 0.1010, 0.1010, 0.1010, 0.1010,\n",
       "             0.1010, 0.1042, 0.1075, 0.1107, 0.1140, 0.1140, 0.1173, 0.1173, 0.1205,\n",
       "             0.1238, 0.1270, 0.1303, 0.1336, 0.1368, 0.1401, 0.1433, 0.1466, 0.1498,\n",
       "             0.1531, 0.1564, 0.1596, 0.1629, 0.1629, 0.1661, 0.1694, 0.1726, 0.1759,\n",
       "             0.1792, 0.1824, 0.1857, 0.1889, 0.1922, 0.1954, 0.1954, 0.1987, 0.2020,\n",
       "             0.2052, 0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313,\n",
       "             0.2345, 0.2378, 0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2606,\n",
       "             0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899,\n",
       "             0.2932, 0.2964, 0.2997, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160,\n",
       "             0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420,\n",
       "             0.3453, 0.3485, 0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3681, 0.3713,\n",
       "             0.3746, 0.3779, 0.3779, 0.3811, 0.3844, 0.3876, 0.3909, 0.3941, 0.3974,\n",
       "             0.4007, 0.4039, 0.4072, 0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267,\n",
       "             0.4300, 0.4332, 0.4332, 0.4365, 0.4397, 0.4430, 0.4463, 0.4495, 0.4528,\n",
       "             0.4560, 0.4593, 0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788, 0.4821,\n",
       "             0.4853, 0.4886, 0.4919, 0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114,\n",
       "             0.5147, 0.5179, 0.5212, 0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407,\n",
       "             0.5440, 0.5472, 0.5505, 0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700,\n",
       "             0.5733, 0.5765, 0.5798, 0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5993,\n",
       "             0.6026, 0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287,\n",
       "             0.6319, 0.6352, 0.6384, 0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580,\n",
       "             0.6612, 0.6645, 0.6678, 0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873,\n",
       "             0.6906, 0.6938, 0.6971, 0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166,\n",
       "             0.7199, 0.7231, 0.7264, 0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459,\n",
       "             0.7492, 0.7524, 0.7557, 0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752,\n",
       "             0.7785, 0.7818, 0.7850, 0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046,\n",
       "             0.8078, 0.8111, 0.8143, 0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339,\n",
       "             0.8371, 0.8404, 0.8436, 0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632,\n",
       "             0.8664, 0.8697, 0.8730, 0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925,\n",
       "             0.8958, 0.8990, 0.9023, 0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218,\n",
       "             0.9251, 0.9283, 0.9316, 0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511,\n",
       "             0.9544, 0.9577, 0.9609, 0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805,\n",
       "             0.9837, 0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7425, 0.7908, 0.8161, 0.8287, 0.8356, 0.8483, 0.8563, 0.8609,\n",
       "             0.8644, 0.8690, 0.8713, 0.8724, 0.8770, 0.8782, 0.8839, 0.8874, 0.8920,\n",
       "             0.8931, 0.8943, 0.8954, 0.8966, 0.8977, 0.8989, 0.9011, 0.9046, 0.9057,\n",
       "             0.9080, 0.9103, 0.9103, 0.9115, 0.9126, 0.9138, 0.9161, 0.9172, 0.9184,\n",
       "             0.9184, 0.9195, 0.9207, 0.9218, 0.9230, 0.9241, 0.9253, 0.9264, 0.9276,\n",
       "             0.9299, 0.9310, 0.9322, 0.9333, 0.9345, 0.9356, 0.9368, 0.9379, 0.9391,\n",
       "             0.9402, 0.9402, 0.9414, 0.9425, 0.9437, 0.9437, 0.9448, 0.9460, 0.9483,\n",
       "             0.9494, 0.9506, 0.9506, 0.9517, 0.9529, 0.9540, 0.9552, 0.9563, 0.9575,\n",
       "             0.9586, 0.9598, 0.9609, 0.9621, 0.9632, 0.9644, 0.9655, 0.9667, 0.9667,\n",
       "             0.9667, 0.9678, 0.9678, 0.9690, 0.9701, 0.9713, 0.9713, 0.9724, 0.9736,\n",
       "             0.9747, 0.9759, 0.9759, 0.9770, 0.9770, 0.9770, 0.9770, 0.9782, 0.9793,\n",
       "             0.9793, 0.9793, 0.9793, 0.9805, 0.9816, 0.9828, 0.9839, 0.9839, 0.9839,\n",
       "             0.9839, 0.9839, 0.9851, 0.9851, 0.9851, 0.9862, 0.9874, 0.9885, 0.9897,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9985e-01, 9.9982e-01, 9.9980e-01,\n",
       "             9.9977e-01, 9.9976e-01, 9.9974e-01, 9.9970e-01, 9.9968e-01, 9.9964e-01,\n",
       "             9.9953e-01, 9.9935e-01, 9.9925e-01, 9.9924e-01, 9.9922e-01, 9.9921e-01,\n",
       "             9.9918e-01, 9.9914e-01, 9.9908e-01, 9.9908e-01, 9.9895e-01, 9.9892e-01,\n",
       "             9.9879e-01, 9.9867e-01, 9.9847e-01, 9.9846e-01, 9.9805e-01, 9.9779e-01,\n",
       "             9.9779e-01, 9.9577e-01, 9.9441e-01, 9.9414e-01, 9.9337e-01, 9.9253e-01,\n",
       "             9.9169e-01, 9.9010e-01, 9.8885e-01, 9.8869e-01, 9.8795e-01, 9.8507e-01,\n",
       "             9.8384e-01, 9.7328e-01, 9.6303e-01, 9.5997e-01, 9.5856e-01, 9.5572e-01,\n",
       "             9.3265e-01, 8.5692e-01, 8.4325e-01, 8.2899e-01, 8.2545e-01, 7.9275e-01,\n",
       "             7.6400e-01, 7.5707e-01, 7.2108e-01, 7.1102e-01, 7.0883e-01, 6.7651e-01,\n",
       "             6.2695e-01, 6.0817e-01, 5.9350e-01, 5.7084e-01, 5.2183e-01, 5.0496e-01,\n",
       "             4.5393e-01, 3.8462e-01, 2.1205e-01, 1.8964e-01, 1.8762e-01, 1.1832e-01,\n",
       "             1.0680e-01, 6.7432e-02, 6.5909e-02, 6.3582e-02, 4.3201e-02, 3.9536e-02,\n",
       "             3.4225e-02, 3.3987e-02, 2.8810e-02, 2.5951e-02, 2.5095e-02, 2.4362e-02,\n",
       "             2.2662e-02, 2.2653e-02, 2.1948e-02, 9.4462e-03, 8.0601e-03, 7.1898e-03,\n",
       "             6.4778e-03, 4.6582e-03, 3.2193e-03, 3.1534e-03, 2.7494e-03, 2.3411e-03,\n",
       "             1.9837e-03, 1.5752e-03, 1.3174e-03, 1.1768e-03, 1.0687e-03, 9.8491e-04,\n",
       "             9.6335e-04, 5.6580e-04, 3.6130e-04, 3.1098e-04, 2.7902e-04, 2.7454e-04,\n",
       "             2.5719e-04, 2.2809e-04, 1.8952e-04, 1.7546e-04, 1.7001e-04, 1.6285e-04,\n",
       "             1.4447e-04, 1.3710e-04, 1.1860e-04, 9.1754e-05, 8.9395e-05, 8.7689e-05,\n",
       "             8.7185e-05, 8.4823e-05, 7.6336e-05, 7.1234e-05, 5.9460e-05, 5.4121e-05,\n",
       "             5.3539e-05, 4.4529e-05, 2.2845e-05, 2.2134e-05, 2.1774e-05, 1.8545e-05,\n",
       "             1.7888e-05, 1.5043e-05, 1.4775e-05, 1.2480e-05, 1.1340e-05, 1.0923e-05,\n",
       "             8.0282e-06, 7.1642e-06, 5.2478e-06, 4.1648e-06, 2.8990e-06, 2.2091e-06,\n",
       "             2.1258e-06, 1.9839e-06, 1.8253e-06, 1.8086e-06, 1.7536e-06, 1.6707e-06,\n",
       "             1.5561e-06, 1.4569e-06, 1.3938e-06, 1.2972e-06, 1.1090e-06, 1.0759e-06,\n",
       "             1.0436e-06, 8.5320e-07, 5.5240e-07, 5.2340e-07, 4.9782e-07, 3.8149e-07,\n",
       "             3.0043e-07, 2.7923e-07, 2.7690e-07, 2.6282e-07, 2.4495e-07, 1.9411e-07,\n",
       "             1.7465e-07, 1.6987e-07, 1.6394e-07, 1.4021e-07, 1.3958e-07, 1.3048e-07,\n",
       "             1.2994e-07, 9.6303e-08, 9.4302e-08, 8.9192e-08, 8.2356e-08, 7.9686e-08,\n",
       "             7.3251e-08, 6.0910e-08, 5.5149e-08, 4.4391e-08, 4.0560e-08, 4.0204e-08,\n",
       "             2.6459e-08, 2.4966e-08, 2.0831e-08, 1.6668e-08, 1.0609e-08, 1.0360e-08,\n",
       "             6.1072e-09, 5.8135e-09, 5.2381e-09, 5.1125e-09, 4.8441e-09, 4.8154e-09,\n",
       "             4.7863e-09, 3.4848e-09, 3.4602e-09, 3.4253e-09, 2.6125e-09, 2.4310e-09,\n",
       "             2.3434e-09, 1.8947e-09, 1.7804e-09, 1.7501e-09, 1.4000e-09, 1.3502e-09,\n",
       "             1.0932e-09, 1.0903e-09, 9.1380e-10, 9.0544e-10, 8.2607e-10, 7.7347e-10,\n",
       "             5.3994e-10, 5.1653e-10, 4.9575e-10, 4.6881e-10, 4.1377e-10, 3.6720e-10,\n",
       "             3.3597e-10, 2.6479e-10, 2.5414e-10, 2.5260e-10, 2.4447e-10, 2.4293e-10,\n",
       "             1.9787e-10, 1.8508e-10, 1.6813e-10, 1.3958e-10, 1.1860e-10, 9.4427e-11,\n",
       "             7.8205e-11, 7.7415e-11, 6.6333e-11, 6.5492e-11, 6.4304e-11, 6.1665e-11,\n",
       "             5.7720e-11, 4.8596e-11, 4.5117e-11, 3.4806e-11, 2.0648e-11, 1.9983e-11,\n",
       "             1.8525e-11, 1.7647e-11, 1.3070e-11, 1.2019e-11, 1.0430e-11, 8.3160e-12,\n",
       "             5.8564e-12, 5.7655e-12, 5.5720e-12, 4.4204e-12, 4.1323e-12, 4.1215e-12,\n",
       "             3.0328e-12, 2.9104e-12, 2.0840e-12, 1.9771e-12, 1.8563e-12, 1.7485e-12,\n",
       "             1.7022e-12, 1.5755e-12, 1.4707e-12, 9.5939e-13, 9.2879e-13, 8.8693e-13,\n",
       "             3.8595e-13, 3.7112e-13, 2.9221e-13, 2.4889e-13, 2.3442e-13, 2.1113e-13,\n",
       "             2.0448e-13, 1.6805e-13, 7.1670e-14, 6.9970e-14, 5.0736e-14, 3.1179e-14,\n",
       "             2.8935e-14, 2.8915e-14, 2.2269e-14, 1.1759e-14, 9.1897e-15, 8.2944e-15,\n",
       "             8.0583e-15, 7.3183e-15, 5.2710e-15, 1.9841e-15, 1.9160e-15, 1.4322e-15,\n",
       "             1.3398e-15, 1.1168e-15, 1.0929e-15, 9.1189e-16, 8.1115e-16, 7.9751e-16,\n",
       "             6.2058e-16, 5.5254e-16, 5.3329e-16, 3.7211e-16, 3.7116e-16, 8.8680e-17,\n",
       "             8.1609e-17, 7.1219e-17, 6.2794e-17, 5.4551e-17, 1.3357e-17, 1.0555e-17,\n",
       "             7.6948e-18, 1.6830e-18, 3.6178e-19, 2.6185e-19, 1.4097e-19, 6.8708e-20,\n",
       "             3.8379e-20, 3.7367e-20, 1.0676e-20, 9.4013e-21, 6.7412e-21, 5.1275e-21,\n",
       "             1.3166e-21, 7.6582e-22, 9.8185e-23, 4.7800e-23, 1.1491e-31, 5.5580e-32])}},\n",
       "   {'fpr': np.float64(0.40716612377850164),\n",
       "    'tpr': np.float64(0.9942528735632183),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0847, 0.0977, 0.0977, 0.0977, 0.1010, 0.1010, 0.1042, 0.1075,\n",
       "             0.1107, 0.1107, 0.1107, 0.1107, 0.1107, 0.1140, 0.1173, 0.1173, 0.1173,\n",
       "             0.1173, 0.1205, 0.1205, 0.1205, 0.1238, 0.1270, 0.1303, 0.1336, 0.1336,\n",
       "             0.1368, 0.1401, 0.1433, 0.1466, 0.1466, 0.1498, 0.1531, 0.1531, 0.1564,\n",
       "             0.1629, 0.1629, 0.1661, 0.1694, 0.1726, 0.1759, 0.1792, 0.1792, 0.1792,\n",
       "             0.1792, 0.1824, 0.1857, 0.1889, 0.1922, 0.1954, 0.1987, 0.2020, 0.2052,\n",
       "             0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2215, 0.2248, 0.2280, 0.2313,\n",
       "             0.2345, 0.2345, 0.2378, 0.2378, 0.2410, 0.2443, 0.2476, 0.2508, 0.2541,\n",
       "             0.2573, 0.2606, 0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834,\n",
       "             0.2866, 0.2899, 0.2932, 0.2964, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127,\n",
       "             0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420,\n",
       "             0.3420, 0.3453, 0.3485, 0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3648,\n",
       "             0.3681, 0.3713, 0.3746, 0.3779, 0.3811, 0.3844, 0.3876, 0.3909, 0.3941,\n",
       "             0.3974, 0.4007, 0.4039, 0.4072, 0.4104, 0.4137, 0.4137, 0.4169, 0.4202,\n",
       "             0.4235, 0.4267, 0.4300, 0.4332, 0.4365, 0.4397, 0.4430, 0.4463, 0.4495,\n",
       "             0.4528, 0.4560, 0.4593, 0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788,\n",
       "             0.4821, 0.4821, 0.4853, 0.4886, 0.4919, 0.4951, 0.4984, 0.5016, 0.5049,\n",
       "             0.5081, 0.5114, 0.5147, 0.5179, 0.5212, 0.5244, 0.5277, 0.5309, 0.5342,\n",
       "             0.5375, 0.5407, 0.5440, 0.5472, 0.5505, 0.5537, 0.5570, 0.5603, 0.5635,\n",
       "             0.5668, 0.5700, 0.5733, 0.5765, 0.5798, 0.5831, 0.5863, 0.5896, 0.5928,\n",
       "             0.5961, 0.5993, 0.6026, 0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221,\n",
       "             0.6254, 0.6287, 0.6319, 0.6319, 0.6352, 0.6352, 0.6352, 0.6384, 0.6417,\n",
       "             0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612, 0.6645, 0.6678, 0.6710,\n",
       "             0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906, 0.6938, 0.6971, 0.7003,\n",
       "             0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199, 0.7231, 0.7264, 0.7296,\n",
       "             0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492, 0.7524, 0.7557, 0.7590,\n",
       "             0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785, 0.7818, 0.7850, 0.7883,\n",
       "             0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078, 0.8111, 0.8143, 0.8176,\n",
       "             0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371, 0.8404, 0.8436, 0.8469,\n",
       "             0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664, 0.8697, 0.8730, 0.8762,\n",
       "             0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958, 0.8990, 0.9023, 0.9055,\n",
       "             0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251, 0.9283, 0.9316, 0.9349,\n",
       "             0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544, 0.9577, 0.9609, 0.9642,\n",
       "             0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837, 0.9870, 0.9902, 0.9935,\n",
       "             0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9437, 0.9552, 0.9598, 0.9609, 0.9632, 0.9667, 0.9678, 0.9678,\n",
       "             0.9701, 0.9713, 0.9724, 0.9736, 0.9747, 0.9747, 0.9747, 0.9759, 0.9770,\n",
       "             0.9782, 0.9782, 0.9793, 0.9805, 0.9805, 0.9805, 0.9805, 0.9805, 0.9816,\n",
       "             0.9816, 0.9816, 0.9816, 0.9816, 0.9828, 0.9828, 0.9828, 0.9839, 0.9839,\n",
       "             0.9839, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9862, 0.9874,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9897, 0.9897, 0.9897, 0.9897,\n",
       "             0.9897, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9985e-01, 9.9984e-01, 9.9979e-01, 9.9979e-01, 9.9976e-01,\n",
       "             9.9973e-01, 9.9968e-01, 9.9967e-01, 9.9956e-01, 9.9952e-01, 9.9946e-01,\n",
       "             9.9937e-01, 9.9930e-01, 9.9928e-01, 9.9910e-01, 9.9883e-01, 9.9879e-01,\n",
       "             9.9842e-01, 9.9788e-01, 9.9752e-01, 9.9585e-01, 9.9456e-01, 9.9432e-01,\n",
       "             9.9391e-01, 9.9327e-01, 9.9277e-01, 9.9264e-01, 9.9155e-01, 9.9101e-01,\n",
       "             9.9100e-01, 9.9096e-01, 9.8926e-01, 9.8896e-01, 9.8718e-01, 9.8712e-01,\n",
       "             9.8687e-01, 9.8655e-01, 9.8584e-01, 9.8177e-01, 9.7785e-01, 9.7769e-01,\n",
       "             9.7617e-01, 9.7391e-01, 9.6645e-01, 9.6036e-01, 9.5843e-01, 9.5091e-01,\n",
       "             9.4750e-01, 9.4186e-01, 9.3837e-01, 9.3521e-01, 9.3325e-01, 9.2647e-01,\n",
       "             9.1827e-01, 9.1819e-01, 9.1018e-01, 8.7431e-01, 8.4068e-01, 8.3329e-01,\n",
       "             8.1714e-01, 8.0881e-01, 7.9513e-01, 7.8239e-01, 7.8050e-01, 7.3406e-01,\n",
       "             7.2542e-01, 7.2070e-01, 7.1849e-01, 6.6646e-01, 6.5411e-01, 6.3450e-01,\n",
       "             5.2118e-01, 4.8745e-01, 4.8398e-01, 4.7280e-01, 4.1788e-01, 3.7551e-01,\n",
       "             3.5434e-01, 3.4506e-01, 3.2689e-01, 3.1080e-01, 3.0322e-01, 3.0105e-01,\n",
       "             2.9921e-01, 2.4211e-01, 1.5474e-01, 1.5194e-01, 1.1454e-01, 1.1445e-01,\n",
       "             1.1013e-01, 9.4962e-02, 8.8094e-02, 8.3396e-02, 8.0044e-02, 7.3838e-02,\n",
       "             7.0535e-02, 5.8181e-02, 4.0192e-02, 3.1473e-02, 2.7407e-02, 1.2826e-02,\n",
       "             1.2694e-02, 1.1758e-02, 9.7902e-03, 9.4823e-03, 9.2022e-03, 8.7313e-03,\n",
       "             8.2645e-03, 7.3985e-03, 7.3048e-03, 7.1701e-03, 6.8347e-03, 6.6492e-03,\n",
       "             6.0072e-03, 5.2545e-03, 4.9417e-03, 4.2529e-03, 4.0833e-03, 4.0648e-03,\n",
       "             3.9032e-03, 3.7305e-03, 3.4819e-03, 2.9932e-03, 2.5155e-03, 2.4943e-03,\n",
       "             1.9667e-03, 1.2680e-03, 1.2318e-03, 9.9768e-04, 7.4822e-04, 7.4595e-04,\n",
       "             7.2377e-04, 6.5367e-04, 6.2651e-04, 6.0220e-04, 5.6899e-04, 5.3781e-04,\n",
       "             5.0099e-04, 4.2006e-04, 4.0219e-04, 3.7094e-04, 3.2791e-04, 3.1573e-04,\n",
       "             2.2279e-04, 2.0893e-04, 1.7960e-04, 1.6014e-04, 1.4052e-04, 1.3287e-04,\n",
       "             1.2861e-04, 1.2714e-04, 1.2502e-04, 9.9260e-05, 9.2601e-05, 8.5057e-05,\n",
       "             6.6717e-05, 4.8418e-05, 4.7421e-05, 4.2707e-05, 4.0536e-05, 3.8809e-05,\n",
       "             3.6525e-05, 3.3944e-05, 3.2164e-05, 3.1970e-05, 2.5503e-05, 2.4780e-05,\n",
       "             1.9836e-05, 1.7397e-05, 1.5594e-05, 1.1366e-05, 1.1112e-05, 9.4280e-06,\n",
       "             9.2554e-06, 9.1537e-06, 8.9136e-06, 8.1254e-06, 7.4271e-06, 7.2983e-06,\n",
       "             6.9246e-06, 6.8325e-06, 5.8510e-06, 5.6030e-06, 4.7942e-06, 4.5754e-06,\n",
       "             4.3928e-06, 3.8641e-06, 3.2708e-06, 3.0985e-06, 3.0677e-06, 2.3131e-06,\n",
       "             2.1758e-06, 2.1704e-06, 2.1527e-06, 1.5429e-06, 1.4189e-06, 1.3976e-06,\n",
       "             1.3714e-06, 1.3658e-06, 1.1497e-06, 7.2238e-07, 6.8555e-07, 6.5168e-07,\n",
       "             3.6477e-07, 2.1237e-07, 1.9015e-07, 1.6439e-07, 1.1797e-07, 8.9651e-08,\n",
       "             5.9813e-08, 5.3437e-08, 3.8448e-08, 3.7692e-08, 3.7374e-08, 3.1534e-08,\n",
       "             3.1251e-08, 1.3049e-08, 1.2371e-08, 1.2297e-08, 1.1574e-08, 9.2022e-09,\n",
       "             7.5066e-09, 7.3332e-09, 4.4305e-09, 4.0866e-09, 4.0268e-09, 2.5576e-09,\n",
       "             2.3644e-09, 1.2105e-09, 8.9201e-10, 8.7321e-10, 6.7055e-10, 5.9502e-10,\n",
       "             5.1302e-10, 2.6676e-10, 2.3212e-10, 1.9525e-10, 1.0652e-10, 2.2187e-11,\n",
       "             9.2430e-12, 3.7843e-12, 3.6776e-12, 1.5720e-12, 1.4861e-12, 1.3634e-12,\n",
       "             1.0913e-12, 9.7881e-13, 5.7826e-13, 5.0384e-13, 4.9256e-13, 3.3515e-13,\n",
       "             6.2697e-14, 2.5890e-14, 1.8232e-14, 3.2363e-15, 8.9638e-17, 6.3170e-17,\n",
       "             2.9467e-19, 2.9337e-19])}}],\n",
       "  [{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 3.9753e-01, 2.8787e-01,  ..., 2.6765e-11, 1.8997e-11,\n",
       "             1.5550e-11])}},\n",
       "   {'fpr': np.float64(0.0035587188612099642),\n",
       "    'tpr': np.float64(0.71875),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9122e-01, 9.8924e-01,  ..., 2.3982e-05, 1.1975e-05,\n",
       "             9.1302e-06])}},\n",
       "   {'fpr': np.float64(0.046263345195729534),\n",
       "    'tpr': np.float64(0.9453125),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9841e-01, 9.9770e-01,  ..., 1.2020e-04, 4.8357e-05,\n",
       "             4.8183e-05])}},\n",
       "   {'fpr': np.float64(0.017793594306049824),\n",
       "    'tpr': np.float64(0.9129464285714286),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0033,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9995e-01, 9.9993e-01,  ..., 1.0187e-07, 1.4640e-08,\n",
       "             5.6864e-09])}},\n",
       "   {'fpr': np.float64(0.028469750889679714),\n",
       "    'tpr': np.float64(0.9631696428571429),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0033, 0.0045,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 9.9999e-01,  ..., 1.0170e-08, 4.7741e-09,\n",
       "             2.5028e-09])}},\n",
       "   {'fpr': np.float64(0.02491103202846975),\n",
       "    'tpr': np.float64(0.9375),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 4.6949e-11, 1.8428e-11,\n",
       "             2.9440e-12])}},\n",
       "   {'fpr': np.float64(0.028469750889679714),\n",
       "    'tpr': np.float64(0.9665178571428571),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0033,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0119e-10, 2.8004e-10,\n",
       "             7.8818e-11])}},\n",
       "   {'fpr': np.float64(0.03558718861209965),\n",
       "    'tpr': np.float64(0.9709821428571429),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0033,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0863e-08, 3.7967e-08,\n",
       "             1.2946e-08])}},\n",
       "   {'fpr': np.float64(0.03558718861209965),\n",
       "    'tpr': np.float64(0.9821428571428571),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.3752e-08, 7.0870e-09,\n",
       "             4.9858e-09])}},\n",
       "   {'fpr': np.float64(0.021352313167259787),\n",
       "    'tpr': np.float64(0.9609375),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 7.5283e-11, 6.6510e-11,\n",
       "             2.9411e-11])}},\n",
       "   {'fpr': np.float64(0.0498220640569395),\n",
       "    'tpr': np.float64(0.9776785714285714),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0033, 0.0078,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 7.6604e-10, 7.2220e-10,\n",
       "             3.3287e-10])}},\n",
       "   {'fpr': np.float64(0.05693950177935943),\n",
       "    'tpr': np.float64(0.9899553571428571),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0285, 0.0285, 0.0285,\n",
       "             0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0320, 0.0356, 0.0391, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427,\n",
       "             0.0427, 0.0427, 0.0427, 0.0463, 0.0463, 0.0463, 0.0463, 0.0463, 0.0463,\n",
       "             0.0463, 0.0498, 0.0534, 0.0534, 0.0534, 0.0534, 0.0534, 0.0569, 0.0569,\n",
       "             0.0569, 0.0569, 0.0605, 0.0641, 0.0676, 0.0712, 0.0747, 0.0783, 0.0819,\n",
       "             0.0854, 0.0890, 0.0925, 0.0961, 0.0996, 0.0996, 0.1032, 0.1068, 0.1068,\n",
       "             0.1103, 0.1103, 0.1139, 0.1174, 0.1210, 0.1246, 0.1281, 0.1281, 0.1317,\n",
       "             0.1352, 0.1388, 0.1423, 0.1459, 0.1495, 0.1530, 0.1566, 0.1601, 0.1637,\n",
       "             0.1673, 0.1708, 0.1744, 0.1779, 0.1815, 0.1815, 0.1851, 0.1886, 0.1922,\n",
       "             0.1957, 0.1993, 0.2028, 0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242,\n",
       "             0.2278, 0.2313, 0.2349, 0.2384, 0.2420, 0.2456, 0.2491, 0.2527, 0.2562,\n",
       "             0.2598, 0.2633, 0.2669, 0.2705, 0.2740, 0.2776, 0.2811, 0.2847, 0.2883,\n",
       "             0.2918, 0.2954, 0.2989, 0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203,\n",
       "             0.3238, 0.3274, 0.3310, 0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523,\n",
       "             0.3559, 0.3559, 0.3594, 0.3630, 0.3665, 0.3701, 0.3737, 0.3772, 0.3808,\n",
       "             0.3843, 0.3879, 0.3915, 0.3950, 0.3986, 0.4021, 0.4057, 0.4093, 0.4128,\n",
       "             0.4164, 0.4199, 0.4235, 0.4270, 0.4306, 0.4342, 0.4377, 0.4377, 0.4413,\n",
       "             0.4448, 0.4484, 0.4520, 0.4555, 0.4591, 0.4626, 0.4662, 0.4698, 0.4733,\n",
       "             0.4769, 0.4804, 0.4840, 0.4875, 0.4911, 0.4947, 0.4982, 0.5018, 0.5053,\n",
       "             0.5089, 0.5125, 0.5160, 0.5196, 0.5231, 0.5267, 0.5302, 0.5338, 0.5374,\n",
       "             0.5409, 0.5445, 0.5480, 0.5516, 0.5552, 0.5587, 0.5623, 0.5658, 0.5694,\n",
       "             0.5730, 0.5765, 0.5801, 0.5836, 0.5872, 0.5907, 0.5943, 0.5979, 0.6014,\n",
       "             0.6050, 0.6085, 0.6121, 0.6157, 0.6192, 0.6228, 0.6263, 0.6299, 0.6335,\n",
       "             0.6370, 0.6406, 0.6441, 0.6477, 0.6512, 0.6548, 0.6584, 0.6619, 0.6655,\n",
       "             0.6690, 0.6726, 0.6762, 0.6797, 0.6833, 0.6868, 0.6904, 0.6940, 0.6975,\n",
       "             0.7011, 0.7046, 0.7082, 0.7117, 0.7153, 0.7189, 0.7224, 0.7260, 0.7295,\n",
       "             0.7331, 0.7367, 0.7402, 0.7438, 0.7473, 0.7509, 0.7544, 0.7580, 0.7616,\n",
       "             0.7651, 0.7687, 0.7722, 0.7758, 0.7794, 0.7829, 0.7865, 0.7900, 0.7936,\n",
       "             0.7972, 0.8007, 0.8043, 0.8078, 0.8114, 0.8149, 0.8185, 0.8221, 0.8256,\n",
       "             0.8292, 0.8327, 0.8363, 0.8399, 0.8434, 0.8470, 0.8505, 0.8541, 0.8577,\n",
       "             0.8612, 0.8648, 0.8683, 0.8719, 0.8754, 0.8790, 0.8826, 0.8861, 0.8897,\n",
       "             0.8932, 0.8968, 0.9004, 0.9039, 0.9075, 0.9110, 0.9146, 0.9181, 0.9217,\n",
       "             0.9253, 0.9288, 0.9324, 0.9359, 0.9395, 0.9431, 0.9466, 0.9502, 0.9537,\n",
       "             0.9573, 0.9609, 0.9644, 0.9680, 0.9715, 0.9751, 0.9786, 0.9822, 0.9858,\n",
       "             0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0446, 0.0792, 0.1049, 0.1295, 0.1462, 0.1618, 0.1730, 0.1819,\n",
       "             0.1853, 0.1897, 0.1964, 0.2076, 0.2109, 0.2188, 0.2310, 0.2388, 0.2444,\n",
       "             0.2511, 0.2545, 0.2589, 0.2701, 0.2779, 0.2812, 0.2835, 0.2846, 0.2913,\n",
       "             0.2935, 0.2969, 0.3013, 0.3047, 0.3092, 0.3136, 0.3147, 0.3170, 0.3181,\n",
       "             0.3214, 0.3225, 0.3248, 0.3259, 0.3304, 0.3348, 0.3382, 0.3415, 0.3438,\n",
       "             0.3449, 0.3471, 0.3504, 0.3538, 0.3560, 0.3605, 0.3616, 0.3638, 0.3661,\n",
       "             0.3672, 0.3683, 0.3694, 0.3717, 0.3750, 0.3761, 0.3772, 0.3783, 0.3795,\n",
       "             0.3806, 0.3828, 0.3850, 0.3873, 0.3884, 0.3917, 0.3929, 0.3940, 0.3951,\n",
       "             0.3962, 0.3984, 0.3996, 0.4007, 0.4018, 0.4029, 0.4051, 0.4062, 0.4096,\n",
       "             0.4118, 0.4129, 0.4141, 0.4163, 0.4185, 0.4230, 0.4252, 0.4263, 0.4286,\n",
       "             0.4297, 0.4308, 0.4342, 0.4375, 0.4420, 0.4442, 0.4475, 0.4487, 0.4498,\n",
       "             0.4509, 0.4520, 0.4542, 0.4554, 0.4609, 0.4621, 0.4654, 0.4665, 0.4676,\n",
       "             0.4688, 0.4699, 0.4710, 0.4732, 0.4754, 0.4766, 0.4788, 0.4810, 0.4821,\n",
       "             0.4833, 0.4855, 0.4866, 0.4888, 0.4911, 0.4922, 0.4933, 0.4944, 0.4955,\n",
       "             0.4978, 0.4989, 0.5000, 0.5011, 0.5033, 0.5045, 0.5056, 0.5067, 0.5078,\n",
       "             0.5089, 0.5100, 0.5123, 0.5134, 0.5145, 0.5156, 0.5167, 0.5190, 0.5201,\n",
       "             0.5212, 0.5223, 0.5246, 0.5257, 0.5268, 0.5279, 0.5290, 0.5301, 0.5312,\n",
       "             0.5324, 0.5335, 0.5346, 0.5357, 0.5368, 0.5379, 0.5391, 0.5402, 0.5413,\n",
       "             0.5424, 0.5435, 0.5446, 0.5458, 0.5469, 0.5480, 0.5513, 0.5525, 0.5536,\n",
       "             0.5558, 0.5569, 0.5580, 0.5592, 0.5603, 0.5625, 0.5636, 0.5647, 0.5658,\n",
       "             0.5670, 0.5681, 0.5703, 0.5714, 0.5725, 0.5737, 0.5748, 0.5759, 0.5770,\n",
       "             0.5781, 0.5792, 0.5804, 0.5815, 0.5826, 0.5837, 0.5848, 0.5859, 0.5871,\n",
       "             0.5882, 0.5893, 0.5904, 0.5915, 0.5938, 0.5949, 0.5960, 0.5971, 0.5982,\n",
       "             0.5993, 0.6004, 0.6038, 0.6049, 0.6060, 0.6071, 0.6083, 0.6094, 0.6105,\n",
       "             0.6116, 0.6127, 0.6138, 0.6150, 0.6194, 0.6205, 0.6217, 0.6228, 0.6239,\n",
       "             0.6250, 0.6261, 0.6272, 0.6283, 0.6295, 0.6306, 0.6317, 0.6328, 0.6339,\n",
       "             0.6350, 0.6362, 0.6373, 0.6384, 0.6395, 0.6406, 0.6417, 0.6429, 0.6440,\n",
       "             0.6451, 0.6462, 0.6473, 0.6496, 0.6518, 0.6529, 0.6540, 0.6551, 0.6562,\n",
       "             0.6574, 0.6585, 0.6596, 0.6607, 0.6618, 0.6629, 0.6641, 0.6652, 0.6663,\n",
       "             0.6674, 0.6685, 0.6708, 0.6719, 0.6730, 0.6741, 0.6752, 0.6763, 0.6775,\n",
       "             0.6786, 0.6797, 0.6808, 0.6819, 0.6830, 0.6842, 0.6853, 0.6864, 0.6875,\n",
       "             0.6886, 0.6897, 0.6908, 0.6920, 0.6931, 0.6942, 0.6953, 0.6964, 0.6975,\n",
       "             0.6987, 0.7009, 0.7020, 0.7031, 0.7042, 0.7065, 0.7076, 0.7087, 0.7098,\n",
       "             0.7109, 0.7121, 0.7132, 0.7143, 0.7154, 0.7165, 0.7176, 0.7188, 0.7199,\n",
       "             0.7210, 0.7221, 0.7232, 0.7254, 0.7266, 0.7277, 0.7288, 0.7299, 0.7310,\n",
       "             0.7321, 0.7333, 0.7344, 0.7355, 0.7366, 0.7377, 0.7388, 0.7411, 0.7422,\n",
       "             0.7433, 0.7444, 0.7455, 0.7467, 0.7478, 0.7489, 0.7500, 0.7511, 0.7522,\n",
       "             0.7533, 0.7556, 0.7567, 0.7578, 0.7589, 0.7600, 0.7612, 0.7623, 0.7634,\n",
       "             0.7645, 0.7656, 0.7667, 0.7679, 0.7690, 0.7701, 0.7712, 0.7723, 0.7734,\n",
       "             0.7746, 0.7757, 0.7768, 0.7779, 0.7790, 0.7801, 0.7812, 0.7824, 0.7835,\n",
       "             0.7846, 0.7857, 0.7868, 0.7879, 0.7891, 0.7902, 0.7913, 0.7924, 0.7935,\n",
       "             0.7946, 0.7958, 0.7969, 0.7980, 0.7991, 0.8002, 0.8013, 0.8025, 0.8036,\n",
       "             0.8047, 0.8058, 0.8069, 0.8069, 0.8080, 0.8092, 0.8103, 0.8114, 0.8125,\n",
       "             0.8136, 0.8147, 0.8147, 0.8158, 0.8170, 0.8181, 0.8192, 0.8203, 0.8214,\n",
       "             0.8225, 0.8237, 0.8248, 0.8259, 0.8270, 0.8281, 0.8292, 0.8304, 0.8315,\n",
       "             0.8326, 0.8337, 0.8348, 0.8359, 0.8371, 0.8382, 0.8393, 0.8404, 0.8415,\n",
       "             0.8426, 0.8438, 0.8438, 0.8449, 0.8460, 0.8471, 0.8482, 0.8493, 0.8504,\n",
       "             0.8516, 0.8527, 0.8538, 0.8549, 0.8560, 0.8571, 0.8583, 0.8594, 0.8605,\n",
       "             0.8616, 0.8627, 0.8638, 0.8650, 0.8661, 0.8672, 0.8683, 0.8694, 0.8705,\n",
       "             0.8717, 0.8728, 0.8739, 0.8750, 0.8761, 0.8772, 0.8783, 0.8795, 0.8806,\n",
       "             0.8817, 0.8828, 0.8839, 0.8850, 0.8862, 0.8873, 0.8884, 0.8895, 0.8906,\n",
       "             0.8917, 0.8929, 0.8940, 0.8962, 0.8973, 0.8984, 0.8996, 0.9007, 0.9018,\n",
       "             0.9029, 0.9040, 0.9051, 0.9062, 0.9074, 0.9085, 0.9096, 0.9107, 0.9118,\n",
       "             0.9129, 0.9141, 0.9152, 0.9163, 0.9174, 0.9185, 0.9196, 0.9208, 0.9219,\n",
       "             0.9219, 0.9230, 0.9241, 0.9252, 0.9263, 0.9275, 0.9286, 0.9297, 0.9308,\n",
       "             0.9319, 0.9330, 0.9342, 0.9353, 0.9364, 0.9375, 0.9386, 0.9397, 0.9408,\n",
       "             0.9420, 0.9431, 0.9442, 0.9453, 0.9464, 0.9464, 0.9464, 0.9475, 0.9487,\n",
       "             0.9487, 0.9498, 0.9509, 0.9520, 0.9531, 0.9542, 0.9554, 0.9565, 0.9576,\n",
       "             0.9587, 0.9598, 0.9609, 0.9621, 0.9632, 0.9643, 0.9654, 0.9665, 0.9676,\n",
       "             0.9688, 0.9699, 0.9699, 0.9699, 0.9699, 0.9710, 0.9721, 0.9732, 0.9743,\n",
       "             0.9754, 0.9766, 0.9777, 0.9777, 0.9788, 0.9799, 0.9810, 0.9821, 0.9833,\n",
       "             0.9844, 0.9844, 0.9844, 0.9855, 0.9866, 0.9877, 0.9888, 0.9888, 0.9900,\n",
       "             0.9911, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9944,\n",
       "             0.9944, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9964e-01, 9.9964e-01,\n",
       "             9.9963e-01, 9.9963e-01, 9.9962e-01, 9.9961e-01, 9.9961e-01, 9.9961e-01,\n",
       "             9.9961e-01, 9.9961e-01, 9.9959e-01, 9.9957e-01, 9.9956e-01, 9.9954e-01,\n",
       "             9.9954e-01, 9.9953e-01, 9.9952e-01, 9.9952e-01, 9.9951e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9948e-01, 9.9947e-01, 9.9947e-01, 9.9947e-01, 9.9946e-01,\n",
       "             9.9946e-01, 9.9944e-01, 9.9944e-01, 9.9940e-01, 9.9939e-01, 9.9937e-01,\n",
       "             9.9936e-01, 9.9936e-01, 9.9933e-01, 9.9932e-01, 9.9930e-01, 9.9930e-01,\n",
       "             9.9925e-01, 9.9924e-01, 9.9923e-01, 9.9923e-01, 9.9920e-01, 9.9918e-01,\n",
       "             9.9917e-01, 9.9917e-01, 9.9916e-01, 9.9915e-01, 9.9913e-01, 9.9912e-01,\n",
       "             9.9906e-01, 9.9906e-01, 9.9905e-01, 9.9904e-01, 9.9901e-01, 9.9900e-01,\n",
       "             9.9899e-01, 9.9898e-01, 9.9895e-01, 9.9889e-01, 9.9889e-01, 9.9887e-01,\n",
       "             9.9885e-01, 9.9884e-01, 9.9875e-01, 9.9875e-01, 9.9874e-01, 9.9847e-01,\n",
       "             9.9847e-01, 9.9845e-01, 9.9844e-01, 9.9843e-01, 9.9831e-01, 9.9829e-01,\n",
       "             9.9821e-01, 9.9821e-01, 9.9809e-01, 9.9800e-01, 9.9787e-01, 9.9771e-01,\n",
       "             9.9757e-01, 9.9756e-01, 9.9751e-01, 9.9730e-01, 9.9727e-01, 9.9695e-01,\n",
       "             9.9688e-01, 9.9684e-01, 9.9666e-01, 9.9652e-01, 9.9639e-01, 9.9615e-01,\n",
       "             9.9569e-01, 9.9544e-01, 9.9543e-01, 9.9534e-01, 9.9529e-01, 9.9510e-01,\n",
       "             9.9489e-01, 9.9489e-01, 9.9471e-01, 9.9464e-01, 9.9455e-01, 9.9446e-01,\n",
       "             9.9423e-01, 9.9374e-01, 9.9352e-01, 9.9317e-01, 9.9285e-01, 9.9217e-01,\n",
       "             9.9172e-01, 9.9088e-01, 9.8980e-01, 9.8777e-01, 9.8770e-01, 9.8743e-01,\n",
       "             9.8729e-01, 9.8718e-01, 9.8610e-01, 9.8582e-01, 9.8572e-01, 9.8470e-01,\n",
       "             9.8247e-01, 9.8031e-01, 9.7798e-01, 9.7521e-01, 9.7404e-01, 9.7397e-01,\n",
       "             9.7367e-01, 9.7282e-01, 9.7230e-01, 9.7076e-01, 9.6781e-01, 9.6694e-01,\n",
       "             9.6470e-01, 9.6263e-01, 9.5933e-01, 9.5530e-01, 9.5085e-01, 9.4995e-01,\n",
       "             9.4147e-01, 9.3622e-01, 9.2866e-01, 9.2268e-01, 8.9495e-01, 8.8847e-01,\n",
       "             8.5501e-01, 8.5170e-01, 8.5105e-01, 8.3001e-01, 8.2601e-01, 7.8591e-01,\n",
       "             7.3120e-01, 7.3036e-01, 6.3808e-01, 5.9710e-01, 5.9262e-01, 5.6508e-01,\n",
       "             4.9506e-01, 3.5115e-01, 3.0836e-01, 3.0418e-01, 2.2584e-01, 2.0051e-01,\n",
       "             1.7964e-01, 1.7302e-01, 1.6168e-01, 1.5529e-01, 1.2456e-01, 1.2096e-01,\n",
       "             1.2032e-01, 1.1621e-01, 1.1101e-01, 1.0146e-01, 7.1432e-02, 5.9772e-02,\n",
       "             5.8132e-02, 5.7956e-02, 5.7701e-02, 3.3361e-02, 3.0034e-02, 2.7951e-02,\n",
       "             2.7004e-02, 2.5749e-02, 2.1589e-02, 2.1055e-02, 1.8995e-02, 1.8836e-02,\n",
       "             1.7030e-02, 1.6517e-02, 1.6317e-02, 1.5461e-02, 1.5183e-02, 1.3790e-02,\n",
       "             1.2630e-02, 1.2552e-02, 1.2407e-02, 1.2309e-02, 1.0082e-02, 9.1798e-03,\n",
       "             9.1220e-03, 8.0858e-03, 7.6139e-03, 7.2260e-03, 6.9967e-03, 6.6574e-03,\n",
       "             6.2824e-03, 5.6758e-03, 5.5277e-03, 5.4479e-03, 5.1052e-03, 4.9340e-03,\n",
       "             4.7000e-03, 4.6015e-03, 4.5323e-03, 4.4871e-03, 4.3407e-03, 4.1451e-03,\n",
       "             3.9100e-03, 3.5917e-03, 3.4790e-03, 3.3713e-03, 2.6565e-03, 2.5778e-03,\n",
       "             2.4029e-03, 2.3849e-03, 2.2025e-03, 1.7084e-03, 1.6144e-03, 1.4174e-03,\n",
       "             1.3793e-03, 1.2429e-03, 1.2045e-03, 9.8737e-04, 9.2268e-04, 8.9023e-04,\n",
       "             8.8844e-04, 8.4873e-04, 8.4758e-04, 7.0348e-04, 7.0097e-04, 6.2866e-04,\n",
       "             6.1674e-04, 6.1042e-04, 5.8368e-04, 5.8002e-04, 5.7042e-04, 5.5587e-04,\n",
       "             5.4538e-04, 5.4103e-04, 4.8709e-04, 4.8642e-04, 4.4817e-04, 4.4123e-04,\n",
       "             4.3016e-04, 4.0792e-04, 3.9888e-04, 3.9828e-04, 3.8337e-04, 3.7456e-04,\n",
       "             3.6539e-04, 3.4558e-04, 3.4391e-04, 2.8562e-04, 2.7682e-04, 2.6637e-04,\n",
       "             2.6042e-04, 2.5885e-04, 2.5739e-04, 2.0711e-04, 2.0483e-04, 2.0040e-04,\n",
       "             1.8767e-04, 1.8074e-04, 1.7550e-04, 1.6472e-04, 1.6399e-04, 1.5622e-04,\n",
       "             1.5133e-04, 1.4487e-04, 1.3856e-04, 1.2545e-04, 1.0630e-04, 1.0591e-04,\n",
       "             9.0410e-05, 8.9851e-05, 8.1950e-05, 7.7077e-05, 7.3645e-05, 7.0910e-05,\n",
       "             6.4753e-05, 6.4301e-05, 6.4225e-05, 6.0526e-05, 6.0117e-05, 4.6414e-05,\n",
       "             4.5704e-05, 4.4221e-05, 4.4205e-05, 4.2753e-05, 4.1343e-05, 3.9303e-05,\n",
       "             3.7621e-05, 3.4260e-05, 3.3448e-05, 3.1697e-05, 3.0360e-05, 2.8907e-05,\n",
       "             2.5518e-05, 2.4836e-05, 2.4266e-05, 2.3381e-05, 2.3285e-05, 2.2058e-05,\n",
       "             2.1968e-05, 1.8505e-05, 1.7494e-05, 1.6023e-05, 1.5004e-05, 1.1135e-05,\n",
       "             9.6336e-06, 9.4074e-06, 8.0966e-06, 7.2299e-06, 6.4664e-06, 6.4087e-06,\n",
       "             5.4784e-06, 5.4364e-06, 4.8888e-06, 4.6227e-06, 4.5351e-06, 4.3235e-06,\n",
       "             4.2689e-06, 4.2662e-06, 4.1479e-06, 3.9404e-06, 3.3311e-06, 3.2247e-06,\n",
       "             3.2162e-06, 2.8805e-06, 2.8088e-06, 2.3955e-06, 2.3256e-06, 2.1454e-06,\n",
       "             2.1039e-06, 2.0919e-06, 1.9683e-06, 1.9500e-06, 1.8032e-06, 1.7983e-06,\n",
       "             1.6808e-06, 1.6302e-06, 1.5985e-06, 1.5844e-06, 1.5824e-06, 1.5220e-06,\n",
       "             1.4555e-06, 1.4198e-06, 1.3067e-06, 1.2103e-06, 1.1178e-06, 1.1064e-06,\n",
       "             1.0706e-06, 1.0704e-06, 1.0278e-06, 9.4874e-07, 9.1204e-07, 8.8956e-07,\n",
       "             8.3841e-07, 8.3421e-07, 7.7957e-07, 6.6041e-07, 6.0924e-07, 5.4980e-07,\n",
       "             4.9452e-07, 4.8771e-07, 4.4973e-07, 4.4590e-07, 4.1380e-07, 3.8732e-07,\n",
       "             3.7590e-07, 3.4452e-07, 3.2878e-07, 3.2732e-07, 3.2504e-07, 3.1473e-07,\n",
       "             2.9113e-07, 2.2668e-07, 2.2613e-07, 2.0863e-07, 2.0457e-07, 1.8473e-07,\n",
       "             1.8415e-07, 1.7335e-07, 1.6324e-07, 1.5566e-07, 1.5153e-07, 1.3750e-07,\n",
       "             1.3250e-07, 1.3162e-07, 1.2141e-07, 8.4341e-08, 8.1199e-08, 6.8081e-08,\n",
       "             6.2675e-08, 5.4418e-08, 5.3043e-08, 5.1257e-08, 5.0921e-08, 4.7812e-08,\n",
       "             4.7470e-08, 2.7615e-08, 2.3060e-08, 2.1740e-08, 2.0753e-08, 1.4812e-08,\n",
       "             1.0997e-08, 9.7883e-09, 9.5975e-09, 8.0418e-09, 6.2846e-09, 5.0905e-09,\n",
       "             2.5797e-09, 1.6325e-09, 1.3171e-09, 1.2174e-09, 1.2068e-09, 1.2061e-09,\n",
       "             1.0669e-09, 9.7288e-10, 4.9730e-10, 1.6262e-10])}},\n",
       "   {'fpr': np.float64(0.03558718861209965),\n",
       "    'tpr': np.float64(0.9720982142857143),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0056, 0.0100,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 7.3463e-12, 5.5101e-12,\n",
       "             1.6504e-12])}},\n",
       "   {'fpr': np.float64(0.05693950177935943),\n",
       "    'tpr': np.float64(0.9799107142857143),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0285, 0.0285,\n",
       "             0.0285, 0.0285, 0.0285, 0.0285, 0.0320, 0.0320, 0.0320, 0.0320, 0.0356,\n",
       "             0.0356, 0.0356, 0.0356, 0.0391, 0.0427, 0.0463, 0.0463, 0.0463, 0.0463,\n",
       "             0.0463, 0.0463, 0.0463, 0.0498, 0.0498, 0.0498, 0.0534, 0.0569, 0.0569,\n",
       "             0.0605, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641,\n",
       "             0.0641, 0.0676, 0.0712, 0.0712, 0.0712, 0.0747, 0.0783, 0.0819, 0.0854,\n",
       "             0.0890, 0.0925, 0.0925, 0.0961, 0.0996, 0.1032, 0.1068, 0.1103, 0.1103,\n",
       "             0.1139, 0.1174, 0.1210, 0.1246, 0.1281, 0.1317, 0.1352, 0.1388, 0.1423,\n",
       "             0.1423, 0.1459, 0.1495, 0.1495, 0.1530, 0.1530, 0.1566, 0.1601, 0.1637,\n",
       "             0.1673, 0.1708, 0.1744, 0.1779, 0.1815, 0.1851, 0.1886, 0.1922, 0.1957,\n",
       "             0.1957, 0.1993, 0.2028, 0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242,\n",
       "             0.2278, 0.2313, 0.2349, 0.2384, 0.2420, 0.2456, 0.2491, 0.2527, 0.2562,\n",
       "             0.2598, 0.2633, 0.2669, 0.2705, 0.2740, 0.2776, 0.2811, 0.2847, 0.2883,\n",
       "             0.2918, 0.2954, 0.2989, 0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203,\n",
       "             0.3238, 0.3274, 0.3310, 0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523,\n",
       "             0.3559, 0.3594, 0.3630, 0.3665, 0.3701, 0.3737, 0.3772, 0.3808, 0.3843,\n",
       "             0.3879, 0.3915, 0.3950, 0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164,\n",
       "             0.4199, 0.4235, 0.4270, 0.4306, 0.4342, 0.4377, 0.4413, 0.4448, 0.4484,\n",
       "             0.4520, 0.4555, 0.4591, 0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804,\n",
       "             0.4840, 0.4875, 0.4911, 0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5089,\n",
       "             0.5125, 0.5160, 0.5196, 0.5231, 0.5267, 0.5302, 0.5338, 0.5374, 0.5409,\n",
       "             0.5445, 0.5480, 0.5516, 0.5552, 0.5587, 0.5623, 0.5658, 0.5694, 0.5730,\n",
       "             0.5765, 0.5801, 0.5836, 0.5872, 0.5907, 0.5943, 0.5979, 0.6014, 0.6050,\n",
       "             0.6085, 0.6121, 0.6157, 0.6192, 0.6228, 0.6263, 0.6299, 0.6335, 0.6370,\n",
       "             0.6406, 0.6441, 0.6477, 0.6512, 0.6548, 0.6584, 0.6619, 0.6655, 0.6690,\n",
       "             0.6726, 0.6762, 0.6797, 0.6833, 0.6868, 0.6904, 0.6940, 0.6975, 0.7011,\n",
       "             0.7046, 0.7082, 0.7117, 0.7153, 0.7189, 0.7224, 0.7260, 0.7295, 0.7331,\n",
       "             0.7367, 0.7402, 0.7438, 0.7473, 0.7509, 0.7544, 0.7580, 0.7616, 0.7651,\n",
       "             0.7687, 0.7722, 0.7758, 0.7794, 0.7829, 0.7865, 0.7900, 0.7936, 0.7972,\n",
       "             0.8007, 0.8043, 0.8078, 0.8114, 0.8149, 0.8185, 0.8221, 0.8256, 0.8292,\n",
       "             0.8327, 0.8363, 0.8399, 0.8434, 0.8470, 0.8505, 0.8541, 0.8577, 0.8612,\n",
       "             0.8648, 0.8683, 0.8719, 0.8754, 0.8790, 0.8826, 0.8861, 0.8897, 0.8932,\n",
       "             0.8968, 0.9004, 0.9039, 0.9075, 0.9110, 0.9146, 0.9181, 0.9217, 0.9253,\n",
       "             0.9288, 0.9324, 0.9359, 0.9395, 0.9431, 0.9466, 0.9502, 0.9537, 0.9573,\n",
       "             0.9609, 0.9644, 0.9680, 0.9715, 0.9751, 0.9786, 0.9822, 0.9858, 0.9893,\n",
       "             0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0179, 0.0279, 0.0491, 0.0636, 0.0792, 0.0904, 0.0993, 0.1071,\n",
       "             0.1138, 0.1205, 0.1272, 0.1395, 0.1440, 0.1551, 0.1596, 0.1652, 0.1730,\n",
       "             0.1786, 0.1808, 0.1886, 0.1920, 0.1987, 0.2031, 0.2087, 0.2132, 0.2176,\n",
       "             0.2188, 0.2199, 0.2221, 0.2266, 0.2299, 0.2344, 0.2366, 0.2411, 0.2444,\n",
       "             0.2478, 0.2500, 0.2511, 0.2533, 0.2545, 0.2556, 0.2578, 0.2589, 0.2600,\n",
       "             0.2612, 0.2645, 0.2690, 0.2712, 0.2734, 0.2790, 0.2812, 0.2846, 0.2913,\n",
       "             0.2924, 0.2935, 0.2958, 0.2969, 0.3002, 0.3036, 0.3058, 0.3069, 0.3080,\n",
       "             0.3114, 0.3125, 0.3147, 0.3181, 0.3203, 0.3237, 0.3248, 0.3259, 0.3304,\n",
       "             0.3315, 0.3337, 0.3359, 0.3382, 0.3393, 0.3404, 0.3415, 0.3438, 0.3449,\n",
       "             0.3460, 0.3482, 0.3493, 0.3516, 0.3538, 0.3560, 0.3571, 0.3583, 0.3616,\n",
       "             0.3661, 0.3672, 0.3683, 0.3694, 0.3705, 0.3728, 0.3739, 0.3750, 0.3761,\n",
       "             0.3772, 0.3795, 0.3817, 0.3828, 0.3850, 0.3884, 0.3906, 0.3929, 0.3973,\n",
       "             0.4007, 0.4029, 0.4051, 0.4074, 0.4085, 0.4096, 0.4107, 0.4118, 0.4129,\n",
       "             0.4141, 0.4152, 0.4174, 0.4185, 0.4219, 0.4230, 0.4241, 0.4263, 0.4275,\n",
       "             0.4286, 0.4308, 0.4319, 0.4342, 0.4353, 0.4364, 0.4375, 0.4386, 0.4420,\n",
       "             0.4431, 0.4442, 0.4453, 0.4464, 0.4487, 0.4498, 0.4509, 0.4520, 0.4542,\n",
       "             0.4554, 0.4565, 0.4576, 0.4598, 0.4609, 0.4621, 0.4643, 0.4676, 0.4688,\n",
       "             0.4699, 0.4710, 0.4721, 0.4743, 0.4754, 0.4766, 0.4777, 0.4788, 0.4799,\n",
       "             0.4821, 0.4833, 0.4844, 0.4855, 0.4866, 0.4888, 0.4900, 0.4911, 0.4922,\n",
       "             0.4944, 0.4955, 0.4967, 0.4978, 0.4989, 0.5000, 0.5011, 0.5022, 0.5033,\n",
       "             0.5045, 0.5056, 0.5078, 0.5089, 0.5100, 0.5112, 0.5123, 0.5134, 0.5145,\n",
       "             0.5156, 0.5167, 0.5179, 0.5190, 0.5212, 0.5234, 0.5246, 0.5257, 0.5279,\n",
       "             0.5290, 0.5301, 0.5312, 0.5324, 0.5346, 0.5357, 0.5368, 0.5379, 0.5391,\n",
       "             0.5413, 0.5424, 0.5435, 0.5458, 0.5469, 0.5480, 0.5502, 0.5513, 0.5536,\n",
       "             0.5558, 0.5569, 0.5580, 0.5592, 0.5603, 0.5614, 0.5625, 0.5636, 0.5647,\n",
       "             0.5658, 0.5670, 0.5681, 0.5692, 0.5703, 0.5714, 0.5725, 0.5737, 0.5748,\n",
       "             0.5759, 0.5770, 0.5781, 0.5792, 0.5804, 0.5815, 0.5826, 0.5837, 0.5848,\n",
       "             0.5859, 0.5871, 0.5882, 0.5893, 0.5904, 0.5915, 0.5926, 0.5938, 0.5949,\n",
       "             0.5960, 0.5971, 0.5982, 0.5993, 0.6004, 0.6016, 0.6027, 0.6038, 0.6049,\n",
       "             0.6060, 0.6071, 0.6083, 0.6094, 0.6105, 0.6116, 0.6127, 0.6138, 0.6150,\n",
       "             0.6161, 0.6194, 0.6205, 0.6217, 0.6228, 0.6239, 0.6250, 0.6272, 0.6295,\n",
       "             0.6306, 0.6317, 0.6328, 0.6339, 0.6350, 0.6362, 0.6373, 0.6384, 0.6395,\n",
       "             0.6406, 0.6417, 0.6429, 0.6440, 0.6462, 0.6473, 0.6484, 0.6496, 0.6507,\n",
       "             0.6518, 0.6529, 0.6540, 0.6551, 0.6562, 0.6574, 0.6585, 0.6596, 0.6607,\n",
       "             0.6618, 0.6629, 0.6641, 0.6652, 0.6663, 0.6674, 0.6685, 0.6696, 0.6708,\n",
       "             0.6719, 0.6730, 0.6752, 0.6763, 0.6775, 0.6786, 0.6797, 0.6808, 0.6819,\n",
       "             0.6830, 0.6842, 0.6853, 0.6864, 0.6875, 0.6886, 0.6897, 0.6920, 0.6931,\n",
       "             0.6942, 0.6964, 0.6975, 0.6987, 0.6998, 0.7009, 0.7020, 0.7031, 0.7042,\n",
       "             0.7054, 0.7065, 0.7076, 0.7087, 0.7098, 0.7109, 0.7121, 0.7132, 0.7143,\n",
       "             0.7154, 0.7165, 0.7176, 0.7188, 0.7199, 0.7210, 0.7221, 0.7232, 0.7243,\n",
       "             0.7254, 0.7266, 0.7277, 0.7288, 0.7299, 0.7310, 0.7321, 0.7333, 0.7355,\n",
       "             0.7366, 0.7377, 0.7388, 0.7400, 0.7411, 0.7422, 0.7433, 0.7444, 0.7455,\n",
       "             0.7467, 0.7478, 0.7489, 0.7500, 0.7500, 0.7511, 0.7522, 0.7533, 0.7545,\n",
       "             0.7556, 0.7567, 0.7578, 0.7589, 0.7600, 0.7612, 0.7623, 0.7634, 0.7645,\n",
       "             0.7656, 0.7667, 0.7679, 0.7690, 0.7701, 0.7712, 0.7723, 0.7734, 0.7746,\n",
       "             0.7757, 0.7768, 0.7779, 0.7790, 0.7801, 0.7812, 0.7824, 0.7835, 0.7846,\n",
       "             0.7857, 0.7879, 0.7891, 0.7902, 0.7913, 0.7924, 0.7935, 0.7946, 0.7958,\n",
       "             0.7969, 0.7980, 0.7991, 0.8002, 0.8013, 0.8013, 0.8025, 0.8036, 0.8047,\n",
       "             0.8058, 0.8069, 0.8080, 0.8092, 0.8103, 0.8114, 0.8125, 0.8136, 0.8147,\n",
       "             0.8158, 0.8170, 0.8181, 0.8192, 0.8203, 0.8214, 0.8225, 0.8237, 0.8248,\n",
       "             0.8259, 0.8270, 0.8281, 0.8292, 0.8304, 0.8315, 0.8326, 0.8337, 0.8348,\n",
       "             0.8359, 0.8371, 0.8382, 0.8393, 0.8404, 0.8415, 0.8426, 0.8438, 0.8449,\n",
       "             0.8460, 0.8471, 0.8471, 0.8482, 0.8493, 0.8504, 0.8516, 0.8527, 0.8538,\n",
       "             0.8549, 0.8560, 0.8571, 0.8583, 0.8594, 0.8605, 0.8616, 0.8627, 0.8638,\n",
       "             0.8650, 0.8661, 0.8672, 0.8683, 0.8694, 0.8705, 0.8717, 0.8728, 0.8739,\n",
       "             0.8750, 0.8761, 0.8772, 0.8783, 0.8795, 0.8806, 0.8817, 0.8828, 0.8839,\n",
       "             0.8850, 0.8862, 0.8873, 0.8884, 0.8895, 0.8906, 0.8917, 0.8929, 0.8940,\n",
       "             0.8951, 0.8962, 0.8973, 0.8984, 0.8996, 0.9007, 0.9018, 0.9029, 0.9040,\n",
       "             0.9051, 0.9062, 0.9074, 0.9074, 0.9085, 0.9096, 0.9107, 0.9118, 0.9129,\n",
       "             0.9141, 0.9152, 0.9163, 0.9174, 0.9185, 0.9196, 0.9208, 0.9219, 0.9230,\n",
       "             0.9241, 0.9252, 0.9263, 0.9263, 0.9275, 0.9286, 0.9297, 0.9308, 0.9319,\n",
       "             0.9330, 0.9342, 0.9353, 0.9364, 0.9375, 0.9375, 0.9386, 0.9397, 0.9408,\n",
       "             0.9420, 0.9431, 0.9442, 0.9453, 0.9464, 0.9475, 0.9487, 0.9498, 0.9509,\n",
       "             0.9520, 0.9531, 0.9542, 0.9554, 0.9565, 0.9576, 0.9587, 0.9587, 0.9598,\n",
       "             0.9609, 0.9621, 0.9632, 0.9643, 0.9643, 0.9654, 0.9665, 0.9676, 0.9676,\n",
       "             0.9688, 0.9699, 0.9710, 0.9710, 0.9710, 0.9710, 0.9721, 0.9732, 0.9743,\n",
       "             0.9754, 0.9766, 0.9777, 0.9777, 0.9788, 0.9799, 0.9799, 0.9799, 0.9810,\n",
       "             0.9810, 0.9810, 0.9821, 0.9833, 0.9844, 0.9855, 0.9866, 0.9877, 0.9888,\n",
       "             0.9900, 0.9900, 0.9900, 0.9911, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9944,\n",
       "             0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944,\n",
       "             0.9955, 0.9955, 0.9955, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01,\n",
       "             9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9971e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9970e-01, 9.9970e-01, 9.9968e-01,\n",
       "             9.9967e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01, 9.9965e-01, 9.9965e-01,\n",
       "             9.9965e-01, 9.9964e-01, 9.9963e-01, 9.9961e-01, 9.9961e-01, 9.9961e-01,\n",
       "             9.9961e-01, 9.9960e-01, 9.9960e-01, 9.9960e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9959e-01, 9.9959e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9956e-01, 9.9956e-01, 9.9954e-01, 9.9953e-01, 9.9953e-01, 9.9952e-01,\n",
       "             9.9951e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01, 9.9948e-01, 9.9948e-01,\n",
       "             9.9947e-01, 9.9947e-01, 9.9947e-01, 9.9946e-01, 9.9945e-01, 9.9944e-01,\n",
       "             9.9944e-01, 9.9943e-01, 9.9943e-01, 9.9943e-01, 9.9943e-01, 9.9942e-01,\n",
       "             9.9942e-01, 9.9940e-01, 9.9940e-01, 9.9940e-01, 9.9939e-01, 9.9939e-01,\n",
       "             9.9938e-01, 9.9937e-01, 9.9934e-01, 9.9934e-01, 9.9933e-01, 9.9931e-01,\n",
       "             9.9930e-01, 9.9928e-01, 9.9927e-01, 9.9922e-01, 9.9922e-01, 9.9921e-01,\n",
       "             9.9920e-01, 9.9920e-01, 9.9916e-01, 9.9915e-01, 9.9915e-01, 9.9912e-01,\n",
       "             9.9912e-01, 9.9911e-01, 9.9911e-01, 9.9911e-01, 9.9908e-01, 9.9907e-01,\n",
       "             9.9905e-01, 9.9903e-01, 9.9902e-01, 9.9899e-01, 9.9899e-01, 9.9894e-01,\n",
       "             9.9889e-01, 9.9887e-01, 9.9886e-01, 9.9884e-01, 9.9880e-01, 9.9879e-01,\n",
       "             9.9877e-01, 9.9876e-01, 9.9875e-01, 9.9873e-01, 9.9871e-01, 9.9864e-01,\n",
       "             9.9863e-01, 9.9862e-01, 9.9858e-01, 9.9853e-01, 9.9853e-01, 9.9852e-01,\n",
       "             9.9849e-01, 9.9848e-01, 9.9848e-01, 9.9848e-01, 9.9846e-01, 9.9845e-01,\n",
       "             9.9835e-01, 9.9834e-01, 9.9834e-01, 9.9834e-01, 9.9825e-01, 9.9825e-01,\n",
       "             9.9824e-01, 9.9815e-01, 9.9800e-01, 9.9798e-01, 9.9789e-01, 9.9788e-01,\n",
       "             9.9776e-01, 9.9772e-01, 9.9767e-01, 9.9761e-01, 9.9753e-01, 9.9753e-01,\n",
       "             9.9743e-01, 9.9718e-01, 9.9705e-01, 9.9703e-01, 9.9684e-01, 9.9671e-01,\n",
       "             9.9667e-01, 9.9652e-01, 9.9651e-01, 9.9650e-01, 9.9642e-01, 9.9595e-01,\n",
       "             9.9568e-01, 9.9567e-01, 9.9549e-01, 9.9549e-01, 9.9521e-01, 9.9501e-01,\n",
       "             9.9475e-01, 9.9470e-01, 9.9456e-01, 9.9443e-01, 9.9435e-01, 9.9434e-01,\n",
       "             9.9424e-01, 9.9422e-01, 9.9420e-01, 9.9418e-01, 9.9415e-01, 9.9402e-01,\n",
       "             9.9351e-01, 9.9342e-01, 9.9302e-01, 9.9274e-01, 9.9247e-01, 9.9242e-01,\n",
       "             9.9232e-01, 9.9188e-01, 9.9187e-01, 9.9164e-01, 9.9123e-01, 9.9069e-01,\n",
       "             9.9019e-01, 9.8972e-01, 9.8971e-01, 9.8935e-01, 9.8932e-01, 9.8929e-01,\n",
       "             9.8840e-01, 9.8740e-01, 9.8530e-01, 9.8453e-01, 9.8396e-01, 9.8344e-01,\n",
       "             9.8317e-01, 9.8213e-01, 9.7797e-01, 9.7796e-01, 9.7589e-01, 9.7395e-01,\n",
       "             9.7394e-01, 9.6431e-01, 9.6380e-01, 9.6102e-01, 9.5854e-01, 9.5710e-01,\n",
       "             9.5500e-01, 9.4909e-01, 9.4655e-01, 9.4325e-01, 9.4148e-01, 9.3695e-01,\n",
       "             9.2460e-01, 8.9581e-01, 8.8534e-01, 8.7613e-01, 8.4503e-01, 8.3395e-01,\n",
       "             8.2245e-01, 7.6875e-01, 7.5139e-01, 7.4658e-01, 7.4053e-01, 7.3798e-01,\n",
       "             6.9622e-01, 6.0380e-01, 6.0364e-01, 5.9104e-01, 5.8257e-01, 5.7359e-01,\n",
       "             5.5925e-01, 5.3822e-01, 4.8627e-01, 4.8487e-01, 4.1136e-01, 3.8424e-01,\n",
       "             3.4876e-01, 3.4390e-01, 3.1722e-01, 2.6418e-01, 2.3702e-01, 2.0173e-01,\n",
       "             1.3004e-01, 1.2351e-01, 1.2328e-01, 1.1651e-01, 9.4792e-02, 7.0036e-02,\n",
       "             5.2995e-02, 5.2134e-02, 3.6987e-02, 3.4729e-02, 3.1888e-02, 2.7691e-02,\n",
       "             1.7092e-02, 1.4087e-02, 1.3343e-02, 1.2619e-02, 1.1707e-02, 1.1590e-02,\n",
       "             1.0369e-02, 9.5719e-03, 9.3892e-03, 9.2111e-03, 8.7817e-03, 7.6489e-03,\n",
       "             7.2862e-03, 7.2419e-03, 6.9020e-03, 6.8702e-03, 6.7275e-03, 6.5801e-03,\n",
       "             6.4738e-03, 6.2286e-03, 5.3580e-03, 5.0736e-03, 4.8544e-03, 3.8306e-03,\n",
       "             3.2660e-03, 3.2640e-03, 2.9120e-03, 2.7369e-03, 2.4704e-03, 2.4267e-03,\n",
       "             2.1006e-03, 1.7828e-03, 1.6749e-03, 1.6557e-03, 1.5858e-03, 1.5841e-03,\n",
       "             1.4145e-03, 1.3223e-03, 1.2616e-03, 1.1022e-03, 8.0994e-04, 7.8246e-04,\n",
       "             6.8569e-04, 6.6971e-04, 5.7425e-04, 5.2396e-04, 5.1656e-04, 5.1413e-04,\n",
       "             5.1197e-04, 4.9322e-04, 4.9137e-04, 3.5890e-04, 3.2288e-04, 3.0996e-04,\n",
       "             2.7661e-04, 2.6708e-04, 2.5900e-04, 2.5175e-04, 2.3969e-04, 2.3939e-04,\n",
       "             2.3701e-04, 2.3237e-04, 2.0824e-04, 2.0113e-04, 1.4846e-04, 1.4475e-04,\n",
       "             1.3371e-04, 1.0772e-04, 1.0238e-04, 1.0112e-04, 9.3707e-05, 9.3673e-05,\n",
       "             8.9034e-05, 8.8358e-05, 7.9826e-05, 7.6712e-05, 7.4100e-05, 7.0946e-05,\n",
       "             6.5352e-05, 6.0653e-05, 5.9624e-05, 5.9365e-05, 5.7373e-05, 4.7390e-05,\n",
       "             4.4094e-05, 3.7209e-05, 3.1778e-05, 3.1576e-05, 3.1280e-05, 3.0553e-05,\n",
       "             2.7711e-05, 2.6725e-05, 2.2674e-05, 2.0759e-05, 1.9580e-05, 1.9449e-05,\n",
       "             1.8951e-05, 1.8345e-05, 1.8112e-05, 1.7702e-05, 1.7542e-05, 1.7055e-05,\n",
       "             1.5951e-05, 1.4569e-05, 1.4087e-05, 1.3195e-05, 1.2765e-05, 1.1773e-05,\n",
       "             1.0094e-05, 9.9348e-06, 9.5622e-06, 8.5332e-06, 8.3831e-06, 8.1951e-06,\n",
       "             8.1653e-06, 7.6242e-06, 7.4143e-06, 6.7475e-06, 6.1508e-06, 6.1433e-06,\n",
       "             5.3126e-06, 4.7562e-06, 4.5006e-06, 4.1896e-06, 4.0960e-06, 3.9322e-06,\n",
       "             3.7814e-06, 3.4008e-06, 2.8056e-06, 2.8014e-06, 2.5957e-06, 2.4203e-06,\n",
       "             2.0607e-06, 2.0598e-06, 2.0116e-06, 1.9203e-06, 1.8634e-06, 1.7681e-06,\n",
       "             1.5266e-06, 1.5118e-06, 1.4823e-06, 1.4526e-06, 1.3921e-06, 1.2921e-06,\n",
       "             1.1655e-06, 1.1407e-06, 1.0630e-06, 1.0525e-06, 9.9961e-07, 8.9678e-07,\n",
       "             7.8475e-07, 7.3636e-07, 7.2421e-07, 6.7257e-07, 6.4921e-07, 6.0842e-07,\n",
       "             5.7708e-07, 5.5123e-07, 5.2408e-07, 4.8374e-07, 4.0363e-07, 3.9253e-07,\n",
       "             3.9123e-07, 3.8846e-07, 3.8024e-07, 3.4687e-07, 3.4177e-07, 3.3099e-07,\n",
       "             2.7617e-07, 2.3165e-07, 1.9897e-07, 1.9553e-07, 1.7260e-07, 1.6412e-07,\n",
       "             1.5567e-07, 1.5558e-07, 1.4061e-07, 1.3772e-07, 1.3354e-07, 1.1111e-07,\n",
       "             1.0376e-07, 9.6838e-08, 9.3698e-08, 8.8930e-08, 8.7942e-08, 8.7110e-08,\n",
       "             8.2905e-08, 7.6368e-08, 7.6358e-08, 7.0507e-08, 6.9439e-08, 6.6706e-08,\n",
       "             6.4490e-08, 5.7859e-08, 5.3877e-08, 5.0678e-08, 4.9370e-08, 4.1159e-08,\n",
       "             4.0864e-08, 3.8283e-08, 3.6907e-08, 3.5810e-08, 3.5480e-08, 3.4494e-08,\n",
       "             3.3592e-08, 3.2835e-08, 3.1595e-08, 3.1211e-08, 2.3034e-08, 2.2843e-08,\n",
       "             2.1797e-08, 1.8073e-08, 1.5134e-08, 1.4870e-08, 1.4787e-08, 1.3985e-08,\n",
       "             1.2981e-08, 1.2894e-08, 1.2351e-08, 1.2039e-08, 1.2014e-08, 9.4368e-09,\n",
       "             9.4029e-09, 8.2015e-09, 8.1949e-09, 7.8216e-09, 5.6345e-09, 4.2971e-09,\n",
       "             2.8678e-09, 2.7007e-09, 2.6332e-09, 2.5034e-09, 2.1411e-09, 2.0251e-09,\n",
       "             1.9507e-09, 1.9243e-09, 1.4620e-09, 1.1050e-09, 9.9953e-10, 9.9481e-10,\n",
       "             9.3797e-10, 9.0629e-10, 8.9517e-10, 8.8209e-10, 6.4048e-10, 5.7835e-10,\n",
       "             5.3831e-10, 4.8296e-10, 4.7401e-10, 3.7255e-10, 2.1437e-10, 1.8772e-10,\n",
       "             1.4998e-10, 1.2983e-10, 1.2515e-10, 9.8088e-11, 5.5438e-11, 1.3972e-11,\n",
       "             9.4565e-12, 7.9558e-12, 6.9756e-12])}},\n",
       "   {'fpr': np.float64(0.2313167259786477),\n",
       "    'tpr': np.float64(0.9966517857142857),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0285, 0.0285, 0.0285,\n",
       "             0.0285, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0391, 0.0427, 0.0463,\n",
       "             0.0463, 0.0463, 0.0498, 0.0534, 0.0534, 0.0569, 0.0569, 0.0569, 0.0605,\n",
       "             0.0641, 0.0641, 0.0641, 0.0641, 0.0676, 0.0712, 0.0712, 0.0747, 0.0747,\n",
       "             0.0783, 0.0819, 0.0819, 0.0854, 0.0854, 0.0890, 0.0890, 0.0925, 0.0961,\n",
       "             0.0996, 0.1032, 0.1068, 0.1103, 0.1139, 0.1174, 0.1210, 0.1210, 0.1246,\n",
       "             0.1281, 0.1317, 0.1352, 0.1388, 0.1423, 0.1459, 0.1495, 0.1530, 0.1566,\n",
       "             0.1566, 0.1601, 0.1637, 0.1673, 0.1708, 0.1744, 0.1779, 0.1815, 0.1851,\n",
       "             0.1886, 0.1922, 0.1957, 0.1993, 0.2028, 0.2064, 0.2100, 0.2135, 0.2171,\n",
       "             0.2206, 0.2206, 0.2242, 0.2278, 0.2313, 0.2313, 0.2349, 0.2384, 0.2420,\n",
       "             0.2456, 0.2491, 0.2527, 0.2562, 0.2598, 0.2633, 0.2669, 0.2705, 0.2740,\n",
       "             0.2776, 0.2811, 0.2847, 0.2883, 0.2918, 0.2954, 0.2989, 0.3025, 0.3060,\n",
       "             0.3096, 0.3132, 0.3167, 0.3203, 0.3238, 0.3274, 0.3310, 0.3345, 0.3381,\n",
       "             0.3416, 0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630, 0.3665, 0.3665,\n",
       "             0.3701, 0.3737, 0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950,\n",
       "             0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270,\n",
       "             0.4306, 0.4342, 0.4377, 0.4413, 0.4448, 0.4484, 0.4520, 0.4555, 0.4591,\n",
       "             0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804, 0.4840, 0.4875, 0.4911,\n",
       "             0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5125, 0.5160, 0.5196, 0.5231,\n",
       "             0.5267, 0.5302, 0.5338, 0.5374, 0.5409, 0.5445, 0.5480, 0.5516, 0.5552,\n",
       "             0.5587, 0.5623, 0.5658, 0.5694, 0.5730, 0.5765, 0.5801, 0.5836, 0.5872,\n",
       "             0.5907, 0.5943, 0.5979, 0.6014, 0.6050, 0.6085, 0.6121, 0.6157, 0.6192,\n",
       "             0.6228, 0.6263, 0.6299, 0.6335, 0.6370, 0.6406, 0.6441, 0.6477, 0.6512,\n",
       "             0.6548, 0.6584, 0.6619, 0.6655, 0.6690, 0.6726, 0.6762, 0.6797, 0.6833,\n",
       "             0.6868, 0.6904, 0.6940, 0.6975, 0.7011, 0.7046, 0.7082, 0.7117, 0.7153,\n",
       "             0.7189, 0.7224, 0.7260, 0.7295, 0.7331, 0.7367, 0.7402, 0.7438, 0.7473,\n",
       "             0.7509, 0.7544, 0.7580, 0.7616, 0.7651, 0.7687, 0.7722, 0.7758, 0.7794,\n",
       "             0.7829, 0.7865, 0.7900, 0.7936, 0.7972, 0.8007, 0.8043, 0.8078, 0.8114,\n",
       "             0.8149, 0.8185, 0.8221, 0.8256, 0.8292, 0.8327, 0.8363, 0.8399, 0.8434,\n",
       "             0.8470, 0.8505, 0.8541, 0.8577, 0.8612, 0.8648, 0.8683, 0.8719, 0.8754,\n",
       "             0.8790, 0.8826, 0.8861, 0.8897, 0.8932, 0.8968, 0.9004, 0.9039, 0.9075,\n",
       "             0.9110, 0.9146, 0.9181, 0.9217, 0.9253, 0.9288, 0.9324, 0.9359, 0.9395,\n",
       "             0.9431, 0.9466, 0.9502, 0.9537, 0.9573, 0.9609, 0.9644, 0.9680, 0.9715,\n",
       "             0.9751, 0.9786, 0.9822, 0.9858, 0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8549, 0.8962, 0.9129, 0.9185, 0.9230, 0.9252, 0.9275, 0.9297,\n",
       "             0.9330, 0.9342, 0.9375, 0.9431, 0.9475, 0.9487, 0.9498, 0.9509, 0.9520,\n",
       "             0.9531, 0.9542, 0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621,\n",
       "             0.9632, 0.9643, 0.9654, 0.9665, 0.9676, 0.9688, 0.9699, 0.9710, 0.9721,\n",
       "             0.9732, 0.9743, 0.9754, 0.9766, 0.9777, 0.9788, 0.9788, 0.9788, 0.9788,\n",
       "             0.9799, 0.9810, 0.9810, 0.9810, 0.9821, 0.9821, 0.9833, 0.9844, 0.9844,\n",
       "             0.9844, 0.9855, 0.9866, 0.9877, 0.9877, 0.9877, 0.9888, 0.9888, 0.9900,\n",
       "             0.9900, 0.9900, 0.9911, 0.9911, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9944, 0.9944,\n",
       "             0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944,\n",
       "             0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9967, 0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9989,\n",
       "             0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9996e-01, 9.9996e-01, 9.9994e-01, 9.9994e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9988e-01, 9.9986e-01, 9.9982e-01,\n",
       "             9.9976e-01, 9.9974e-01, 9.9967e-01, 9.9966e-01, 9.9952e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9948e-01, 9.9928e-01, 9.9901e-01, 9.9877e-01, 9.9875e-01,\n",
       "             9.9825e-01, 9.9814e-01, 9.9764e-01, 9.9745e-01, 9.9580e-01, 9.9497e-01,\n",
       "             9.9378e-01, 9.9307e-01, 9.9252e-01, 9.9125e-01, 9.8900e-01, 9.8150e-01,\n",
       "             9.7888e-01, 9.7711e-01, 9.7551e-01, 9.7354e-01, 9.7202e-01, 9.7196e-01,\n",
       "             9.7121e-01, 9.6634e-01, 9.5645e-01, 9.1742e-01, 9.1031e-01, 9.0957e-01,\n",
       "             9.0661e-01, 9.0555e-01, 8.9268e-01, 8.8166e-01, 8.7929e-01, 8.5653e-01,\n",
       "             8.4727e-01, 8.4162e-01, 8.4037e-01, 8.3606e-01, 8.3121e-01, 8.1233e-01,\n",
       "             7.8604e-01, 7.7971e-01, 7.7501e-01, 7.5315e-01, 7.2394e-01, 7.1645e-01,\n",
       "             7.0262e-01, 6.9814e-01, 6.6713e-01, 6.5668e-01, 6.5310e-01, 6.4439e-01,\n",
       "             6.1031e-01, 5.7906e-01, 5.6998e-01, 5.2466e-01, 5.0994e-01, 4.8181e-01,\n",
       "             4.7295e-01, 4.2509e-01, 4.1910e-01, 3.3856e-01, 3.3761e-01, 3.2492e-01,\n",
       "             3.1462e-01, 2.9947e-01, 2.8069e-01, 2.7623e-01, 2.6231e-01, 2.5362e-01,\n",
       "             2.4934e-01, 2.3959e-01, 2.3473e-01, 2.1979e-01, 2.1303e-01, 2.0922e-01,\n",
       "             1.7248e-01, 1.4689e-01, 1.3831e-01, 1.1043e-01, 1.0240e-01, 9.8970e-02,\n",
       "             8.3644e-02, 8.1900e-02, 7.2692e-02, 6.6644e-02, 6.5383e-02, 6.2118e-02,\n",
       "             6.0112e-02, 5.4865e-02, 5.4428e-02, 4.9011e-02, 3.8518e-02, 3.8184e-02,\n",
       "             3.7486e-02, 3.5213e-02, 3.4614e-02, 2.2248e-02, 2.0778e-02, 1.7788e-02,\n",
       "             1.5773e-02, 1.4181e-02, 1.1745e-02, 1.1349e-02, 9.0859e-03, 8.7898e-03,\n",
       "             8.2561e-03, 7.5034e-03, 7.4250e-03, 6.1986e-03, 4.7698e-03, 4.6570e-03,\n",
       "             3.7227e-03, 3.4676e-03, 3.1373e-03, 2.8702e-03, 2.8245e-03, 2.4397e-03,\n",
       "             2.3183e-03, 2.2191e-03, 1.9428e-03, 1.8337e-03, 1.8081e-03, 1.7117e-03,\n",
       "             1.5110e-03, 1.5030e-03, 1.3401e-03, 1.2668e-03, 1.2651e-03, 1.2414e-03,\n",
       "             1.0726e-03, 1.0670e-03, 1.0383e-03, 8.0124e-04, 7.4977e-04, 6.5812e-04,\n",
       "             5.1535e-04, 5.1265e-04, 5.0602e-04, 4.7710e-04, 3.8067e-04, 3.6833e-04,\n",
       "             3.6043e-04, 2.6830e-04, 2.1626e-04, 2.0819e-04, 1.8934e-04, 1.8504e-04,\n",
       "             1.5312e-04, 9.6528e-05, 8.9718e-05, 8.1991e-05, 7.9810e-05, 7.8717e-05,\n",
       "             7.8602e-05, 7.8276e-05, 7.2916e-05, 7.1761e-05, 6.0197e-05, 5.2068e-05,\n",
       "             5.2038e-05, 4.4904e-05, 4.1607e-05, 4.0186e-05, 3.9405e-05, 3.8641e-05,\n",
       "             3.7633e-05, 3.1405e-05, 2.6547e-05, 2.5159e-05, 2.3756e-05, 2.2419e-05,\n",
       "             2.1704e-05, 1.6654e-05, 1.5067e-05, 1.4753e-05, 1.3160e-05, 1.2181e-05,\n",
       "             1.2009e-05, 1.0727e-05, 8.3315e-06, 7.9355e-06, 7.6220e-06, 7.5671e-06,\n",
       "             7.0692e-06, 6.6659e-06, 5.1732e-06, 5.1381e-06, 4.9377e-06, 4.6716e-06,\n",
       "             4.0942e-06, 3.9605e-06, 3.3897e-06, 3.1993e-06, 3.0392e-06, 2.8914e-06,\n",
       "             2.6322e-06, 2.5202e-06, 2.3770e-06, 2.1494e-06, 2.0150e-06, 1.7343e-06,\n",
       "             1.5434e-06, 1.5358e-06, 1.4956e-06, 1.4835e-06, 1.4615e-06, 1.3322e-06,\n",
       "             1.0610e-06, 1.0366e-06, 9.4503e-07, 9.0657e-07, 8.1625e-07, 8.0022e-07,\n",
       "             7.4468e-07, 7.2307e-07, 6.2909e-07, 6.0262e-07, 5.8139e-07, 3.1670e-07,\n",
       "             2.9202e-07, 2.4793e-07, 2.3612e-07, 2.1186e-07, 1.6600e-07, 1.5664e-07,\n",
       "             1.5228e-07, 1.2933e-07, 1.0943e-07, 1.0605e-07, 1.0459e-07, 1.0333e-07,\n",
       "             9.9596e-08, 7.6636e-08, 6.8763e-08, 6.7599e-08, 6.3918e-08, 6.2889e-08,\n",
       "             5.0493e-08, 4.5320e-08, 3.6431e-08, 2.9711e-08, 2.8610e-08, 2.6829e-08,\n",
       "             2.3602e-08, 2.0717e-08, 1.9106e-08, 1.8158e-08, 6.6971e-09, 6.4210e-09,\n",
       "             3.7667e-09, 3.7492e-09, 3.2286e-09, 2.9436e-09, 2.8975e-09, 1.9028e-09,\n",
       "             5.2192e-10, 4.5123e-10, 4.3307e-10, 3.2349e-10, 1.4923e-10, 1.1739e-10,\n",
       "             9.8953e-11, 8.9832e-11, 8.5752e-11, 5.7683e-11, 4.8827e-11, 4.6093e-11,\n",
       "             2.5136e-11, 2.3040e-11, 2.2330e-11, 2.0540e-11, 1.9224e-11, 1.8475e-11,\n",
       "             5.5155e-12, 4.8816e-12])}},\n",
       "   {'fpr': np.float64(0.042704626334519574),\n",
       "    'tpr': np.float64(0.9787946428571429),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0142, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249,\n",
       "             0.0285, 0.0285, 0.0285, 0.0285, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0320, 0.0356, 0.0356, 0.0356, 0.0356, 0.0391, 0.0391, 0.0391,\n",
       "             0.0427, 0.0427, 0.0463, 0.0463, 0.0498, 0.0534, 0.0534, 0.0534, 0.0534,\n",
       "             0.0534, 0.0534, 0.0534, 0.0534, 0.0569, 0.0569, 0.0605, 0.0641, 0.0676,\n",
       "             0.0712, 0.0747, 0.0783, 0.0819, 0.0819, 0.0854, 0.0854, 0.0890, 0.0890,\n",
       "             0.0925, 0.0961, 0.0996, 0.1032, 0.1068, 0.1103, 0.1139, 0.1174, 0.1210,\n",
       "             0.1246, 0.1246, 0.1281, 0.1317, 0.1352, 0.1352, 0.1388, 0.1423, 0.1459,\n",
       "             0.1495, 0.1530, 0.1566, 0.1601, 0.1601, 0.1637, 0.1673, 0.1708, 0.1744,\n",
       "             0.1779, 0.1815, 0.1851, 0.1886, 0.1922, 0.1922, 0.1957, 0.1993, 0.2028,\n",
       "             0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242, 0.2278, 0.2313, 0.2349,\n",
       "             0.2384, 0.2420, 0.2456, 0.2491, 0.2527, 0.2562, 0.2598, 0.2633, 0.2669,\n",
       "             0.2705, 0.2740, 0.2776, 0.2811, 0.2847, 0.2883, 0.2918, 0.2954, 0.2989,\n",
       "             0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203, 0.3238, 0.3274, 0.3310,\n",
       "             0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630,\n",
       "             0.3665, 0.3701, 0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950,\n",
       "             0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270,\n",
       "             0.4306, 0.4342, 0.4342, 0.4377, 0.4413, 0.4448, 0.4448, 0.4484, 0.4520,\n",
       "             0.4555, 0.4591, 0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804, 0.4840,\n",
       "             0.4875, 0.4911, 0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5125, 0.5160,\n",
       "             0.5196, 0.5231, 0.5267, 0.5302, 0.5338, 0.5374, 0.5409, 0.5445, 0.5480,\n",
       "             0.5516, 0.5552, 0.5587, 0.5623, 0.5658, 0.5694, 0.5730, 0.5765, 0.5801,\n",
       "             0.5836, 0.5872, 0.5907, 0.5943, 0.5979, 0.6014, 0.6050, 0.6085, 0.6121,\n",
       "             0.6157, 0.6192, 0.6228, 0.6263, 0.6299, 0.6335, 0.6370, 0.6406, 0.6441,\n",
       "             0.6477, 0.6512, 0.6548, 0.6584, 0.6619, 0.6655, 0.6690, 0.6726, 0.6762,\n",
       "             0.6797, 0.6833, 0.6868, 0.6904, 0.6940, 0.6975, 0.7011, 0.7046, 0.7082,\n",
       "             0.7117, 0.7153, 0.7189, 0.7224, 0.7260, 0.7295, 0.7331, 0.7367, 0.7402,\n",
       "             0.7438, 0.7473, 0.7509, 0.7544, 0.7580, 0.7616, 0.7651, 0.7687, 0.7722,\n",
       "             0.7758, 0.7794, 0.7829, 0.7865, 0.7900, 0.7936, 0.7972, 0.8007, 0.8043,\n",
       "             0.8078, 0.8114, 0.8149, 0.8185, 0.8221, 0.8256, 0.8292, 0.8327, 0.8363,\n",
       "             0.8399, 0.8434, 0.8470, 0.8505, 0.8541, 0.8577, 0.8612, 0.8648, 0.8683,\n",
       "             0.8719, 0.8754, 0.8790, 0.8826, 0.8861, 0.8897, 0.8932, 0.8968, 0.9004,\n",
       "             0.9039, 0.9075, 0.9110, 0.9146, 0.9181, 0.9217, 0.9253, 0.9288, 0.9324,\n",
       "             0.9359, 0.9395, 0.9431, 0.9466, 0.9502, 0.9537, 0.9573, 0.9609, 0.9644,\n",
       "             0.9680, 0.9715, 0.9751, 0.9786, 0.9822, 0.9858, 0.9893, 0.9929, 0.9964,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.3817, 0.4754, 0.5190, 0.5413, 0.5603, 0.5737, 0.5837, 0.5904,\n",
       "             0.6004, 0.6071, 0.6150, 0.6217, 0.6295, 0.6373, 0.6406, 0.6484, 0.6507,\n",
       "             0.6529, 0.6562, 0.6596, 0.6629, 0.6674, 0.6708, 0.6730, 0.6763, 0.6775,\n",
       "             0.6830, 0.6864, 0.6875, 0.6920, 0.6942, 0.6953, 0.6964, 0.6998, 0.7031,\n",
       "             0.7042, 0.7065, 0.7098, 0.7109, 0.7132, 0.7165, 0.7188, 0.7199, 0.7210,\n",
       "             0.7232, 0.7243, 0.7254, 0.7277, 0.7321, 0.7355, 0.7377, 0.7388, 0.7411,\n",
       "             0.7433, 0.7444, 0.7455, 0.7478, 0.7489, 0.7500, 0.7511, 0.7522, 0.7545,\n",
       "             0.7567, 0.7589, 0.7600, 0.7623, 0.7634, 0.7645, 0.7656, 0.7667, 0.7679,\n",
       "             0.7690, 0.7701, 0.7712, 0.7712, 0.7723, 0.7734, 0.7746, 0.7757, 0.7768,\n",
       "             0.7779, 0.7790, 0.7801, 0.7824, 0.7846, 0.7857, 0.7868, 0.7879, 0.7891,\n",
       "             0.7902, 0.7913, 0.7924, 0.7935, 0.7946, 0.7958, 0.7969, 0.7980, 0.7991,\n",
       "             0.8002, 0.8013, 0.8025, 0.8036, 0.8047, 0.8058, 0.8069, 0.8092, 0.8103,\n",
       "             0.8114, 0.8125, 0.8136, 0.8147, 0.8158, 0.8170, 0.8181, 0.8192, 0.8237,\n",
       "             0.8248, 0.8259, 0.8270, 0.8281, 0.8292, 0.8304, 0.8315, 0.8326, 0.8337,\n",
       "             0.8348, 0.8359, 0.8371, 0.8382, 0.8393, 0.8404, 0.8415, 0.8426, 0.8438,\n",
       "             0.8449, 0.8460, 0.8471, 0.8482, 0.8493, 0.8504, 0.8516, 0.8527, 0.8538,\n",
       "             0.8549, 0.8560, 0.8571, 0.8583, 0.8594, 0.8616, 0.8627, 0.8661, 0.8672,\n",
       "             0.8683, 0.8694, 0.8705, 0.8717, 0.8728, 0.8739, 0.8750, 0.8761, 0.8772,\n",
       "             0.8783, 0.8795, 0.8806, 0.8817, 0.8828, 0.8839, 0.8850, 0.8862, 0.8873,\n",
       "             0.8884, 0.8895, 0.8906, 0.8917, 0.8929, 0.8940, 0.8951, 0.8962, 0.8973,\n",
       "             0.8984, 0.8996, 0.9007, 0.9018, 0.9029, 0.9040, 0.9051, 0.9051, 0.9051,\n",
       "             0.9062, 0.9074, 0.9085, 0.9096, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152,\n",
       "             0.9163, 0.9174, 0.9174, 0.9185, 0.9196, 0.9208, 0.9219, 0.9230, 0.9241,\n",
       "             0.9252, 0.9263, 0.9275, 0.9286, 0.9297, 0.9308, 0.9319, 0.9330, 0.9342,\n",
       "             0.9353, 0.9364, 0.9375, 0.9386, 0.9397, 0.9408, 0.9420, 0.9431, 0.9442,\n",
       "             0.9453, 0.9464, 0.9475, 0.9487, 0.9498, 0.9509, 0.9520, 0.9531, 0.9542,\n",
       "             0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621, 0.9632, 0.9632,\n",
       "             0.9632, 0.9643, 0.9654, 0.9665, 0.9665, 0.9676, 0.9688, 0.9699, 0.9710,\n",
       "             0.9721, 0.9732, 0.9732, 0.9743, 0.9754, 0.9766, 0.9766, 0.9777, 0.9788,\n",
       "             0.9788, 0.9799, 0.9799, 0.9810, 0.9810, 0.9810, 0.9821, 0.9833, 0.9844,\n",
       "             0.9855, 0.9866, 0.9877, 0.9888, 0.9888, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9911, 0.9911, 0.9922, 0.9922, 0.9933,\n",
       "             0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9944, 0.9944, 0.9944, 0.9944, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9955, 0.9955, 0.9955, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9964e-01,\n",
       "             9.9957e-01, 9.9957e-01, 9.9957e-01, 9.9956e-01, 9.9953e-01, 9.9953e-01,\n",
       "             9.9952e-01, 9.9945e-01, 9.9940e-01, 9.9939e-01, 9.9936e-01, 9.9932e-01,\n",
       "             9.9926e-01, 9.9926e-01, 9.9925e-01, 9.9920e-01, 9.9916e-01, 9.9912e-01,\n",
       "             9.9910e-01, 9.9908e-01, 9.9907e-01, 9.9906e-01, 9.9890e-01, 9.9889e-01,\n",
       "             9.9867e-01, 9.9866e-01, 9.9858e-01, 9.9843e-01, 9.9842e-01, 9.9833e-01,\n",
       "             9.9832e-01, 9.9825e-01, 9.9822e-01, 9.9809e-01, 9.9804e-01, 9.9791e-01,\n",
       "             9.9783e-01, 9.9707e-01, 9.9697e-01, 9.9680e-01, 9.9610e-01, 9.9557e-01,\n",
       "             9.9553e-01, 9.9546e-01, 9.9538e-01, 9.9517e-01, 9.9497e-01, 9.9480e-01,\n",
       "             9.9345e-01, 9.9265e-01, 9.9259e-01, 9.9221e-01, 9.9132e-01, 9.9064e-01,\n",
       "             9.9012e-01, 9.8675e-01, 9.8608e-01, 9.8559e-01, 9.8338e-01, 9.7554e-01,\n",
       "             9.7376e-01, 9.6778e-01, 9.6768e-01, 9.6104e-01, 9.6035e-01, 9.5183e-01,\n",
       "             9.4876e-01, 9.4142e-01, 9.3591e-01, 9.2680e-01, 9.2305e-01, 9.2164e-01,\n",
       "             8.8036e-01, 8.7898e-01, 8.5866e-01, 8.5567e-01, 7.6692e-01, 7.1936e-01,\n",
       "             6.4511e-01, 6.3631e-01, 5.7232e-01, 5.4027e-01, 4.1508e-01, 3.2391e-01,\n",
       "             3.0701e-01, 2.5037e-01, 2.4007e-01, 2.3098e-01, 1.2085e-01, 1.1932e-01,\n",
       "             9.1048e-02, 7.2720e-02, 7.1578e-02, 6.7341e-02, 5.4102e-02, 3.0249e-02,\n",
       "             2.8995e-02, 2.7830e-02, 2.7248e-02, 2.6495e-02, 2.5855e-02, 2.2845e-02,\n",
       "             1.8907e-02, 1.7634e-02, 1.7302e-02, 1.5947e-02, 1.3594e-02, 1.3412e-02,\n",
       "             1.2307e-02, 1.1468e-02, 9.9335e-03, 8.3273e-03, 7.3189e-03, 6.2117e-03,\n",
       "             5.7084e-03, 5.6714e-03, 5.4588e-03, 5.3091e-03, 3.2172e-03, 2.1634e-03,\n",
       "             2.1579e-03, 2.1178e-03, 2.1155e-03, 2.0016e-03, 1.9372e-03, 1.9290e-03,\n",
       "             1.9100e-03, 1.8026e-03, 1.3995e-03, 1.3400e-03, 1.1055e-03, 8.4562e-04,\n",
       "             7.4759e-04, 7.3670e-04, 7.0074e-04, 6.9210e-04, 6.6141e-04, 6.0797e-04,\n",
       "             6.0727e-04, 5.5854e-04, 5.5478e-04, 5.1828e-04, 4.1504e-04, 3.8806e-04,\n",
       "             2.4530e-04, 2.3077e-04, 2.3032e-04, 2.0248e-04, 1.9812e-04, 1.7315e-04,\n",
       "             1.6545e-04, 1.6233e-04, 1.2441e-04, 1.2261e-04, 1.1516e-04, 8.7824e-05,\n",
       "             8.3112e-05, 7.9984e-05, 7.2236e-05, 6.5948e-05, 6.1308e-05, 5.8819e-05,\n",
       "             5.8726e-05, 5.2317e-05, 5.0612e-05, 4.9505e-05, 4.3197e-05, 3.6787e-05,\n",
       "             3.6219e-05, 3.6077e-05, 3.3230e-05, 2.7638e-05, 2.7240e-05, 2.4943e-05,\n",
       "             2.4581e-05, 2.3472e-05, 2.1463e-05, 2.1039e-05, 1.6584e-05, 1.2748e-05,\n",
       "             1.2741e-05, 1.2315e-05, 1.1729e-05, 1.0693e-05, 1.0599e-05, 1.0297e-05,\n",
       "             8.6067e-06, 7.7985e-06, 7.6081e-06, 5.9411e-06, 5.8406e-06, 5.2771e-06,\n",
       "             5.0187e-06, 4.0395e-06, 3.9131e-06, 3.7764e-06, 3.4720e-06, 2.3065e-06,\n",
       "             2.2757e-06, 2.2555e-06, 2.0253e-06, 1.9835e-06, 1.8223e-06, 1.7715e-06,\n",
       "             1.4748e-06, 1.3245e-06, 1.3076e-06, 1.1640e-06, 1.1012e-06, 9.6044e-07,\n",
       "             9.5721e-07, 9.2382e-07, 8.9217e-07, 8.8118e-07, 8.1316e-07, 6.0371e-07,\n",
       "             3.9930e-07, 3.9879e-07, 3.5937e-07, 3.5000e-07, 3.4967e-07, 3.3571e-07,\n",
       "             3.2862e-07, 2.8354e-07, 2.5555e-07, 2.5216e-07, 2.4209e-07, 1.8852e-07,\n",
       "             1.6187e-07, 1.5266e-07, 1.4261e-07, 1.3041e-07, 8.7543e-08, 8.5090e-08,\n",
       "             7.8767e-08, 7.5763e-08, 7.2874e-08, 5.9204e-08, 5.6689e-08, 5.3054e-08,\n",
       "             4.2453e-08, 4.0702e-08, 4.0642e-08, 3.9574e-08, 3.8756e-08, 3.5536e-08,\n",
       "             3.4664e-08, 3.0754e-08, 2.2829e-08, 2.2482e-08, 2.0753e-08, 2.0524e-08,\n",
       "             2.0447e-08, 1.9428e-08, 1.9242e-08, 1.8917e-08, 1.7104e-08, 1.6823e-08,\n",
       "             1.6798e-08, 1.5290e-08, 1.5015e-08, 1.4911e-08, 1.4125e-08, 1.3394e-08,\n",
       "             1.2874e-08, 1.1611e-08, 1.0733e-08, 1.0578e-08, 1.0153e-08, 8.5777e-09,\n",
       "             8.1778e-09, 7.9371e-09, 5.8756e-09, 5.7634e-09, 5.7231e-09, 5.4351e-09,\n",
       "             5.3618e-09, 4.7260e-09, 4.6232e-09, 4.1607e-09, 3.3170e-09, 2.5594e-09,\n",
       "             2.4922e-09, 2.1618e-09, 2.1524e-09, 2.1424e-09, 2.0408e-09, 1.9187e-09,\n",
       "             1.7710e-09, 1.7291e-09, 1.4325e-09, 9.9919e-10, 9.8778e-10, 7.8716e-10,\n",
       "             6.8616e-10, 6.1237e-10, 5.4864e-10, 5.0847e-10, 5.0275e-10, 4.7809e-10,\n",
       "             4.5455e-10, 4.5275e-10, 4.2343e-10, 2.8229e-10, 2.3594e-10, 1.9926e-10,\n",
       "             1.8084e-10, 1.4112e-10, 1.3892e-10, 1.3887e-10, 1.1744e-10, 1.1424e-10,\n",
       "             1.1264e-10, 1.0456e-10, 9.3968e-11, 8.0036e-11, 6.6435e-11, 6.3152e-11,\n",
       "             5.4090e-11, 5.3912e-11, 4.9935e-11, 3.9537e-11, 3.6193e-11, 3.4491e-11,\n",
       "             2.7553e-11, 2.5801e-11, 2.1771e-11, 1.6165e-11, 1.3985e-11, 1.3894e-11,\n",
       "             1.2946e-11, 1.0370e-11, 7.3585e-12, 7.3211e-12, 6.9926e-12, 6.5477e-12,\n",
       "             6.4413e-12, 6.0222e-12, 3.8625e-12, 3.3143e-12, 2.5487e-12, 2.1488e-12,\n",
       "             1.7084e-12, 1.6242e-12, 1.5163e-12, 1.4308e-12, 1.2801e-12, 1.2687e-12,\n",
       "             1.2266e-12, 6.3438e-13, 6.0637e-13, 5.9832e-13, 5.5934e-13, 5.5305e-13,\n",
       "             4.9415e-13, 4.6200e-13, 3.5873e-13, 3.5762e-13, 3.4720e-13, 3.0918e-13,\n",
       "             2.1158e-13, 1.4853e-13, 5.3404e-14, 4.7319e-14, 2.6954e-14, 2.5783e-14,\n",
       "             1.9910e-14, 1.1615e-14, 8.0487e-16, 3.8938e-16])}},\n",
       "   {'fpr': np.float64(0.07829181494661921),\n",
       "    'tpr': np.float64(0.9832589285714286),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0142, 0.0142,\n",
       "             0.0142, 0.0178, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0285, 0.0285, 0.0285, 0.0285, 0.0285, 0.0285, 0.0285, 0.0320,\n",
       "             0.0356, 0.0356, 0.0356, 0.0391, 0.0391, 0.0391, 0.0391, 0.0427, 0.0463,\n",
       "             0.0498, 0.0534, 0.0534, 0.0534, 0.0569, 0.0569, 0.0569, 0.0569, 0.0605,\n",
       "             0.0605, 0.0605, 0.0641, 0.0641, 0.0641, 0.0676, 0.0676, 0.0676, 0.0676,\n",
       "             0.0676, 0.0676, 0.0712, 0.0747, 0.0783, 0.0819, 0.0819, 0.0854, 0.0890,\n",
       "             0.0890, 0.0890, 0.0925, 0.0961, 0.0961, 0.0961, 0.0961, 0.0996, 0.1032,\n",
       "             0.1068, 0.1103, 0.1139, 0.1174, 0.1210, 0.1210, 0.1246, 0.1281, 0.1281,\n",
       "             0.1317, 0.1352, 0.1388, 0.1423, 0.1459, 0.1495, 0.1530, 0.1566, 0.1601,\n",
       "             0.1637, 0.1673, 0.1708, 0.1708, 0.1744, 0.1779, 0.1815, 0.1851, 0.1886,\n",
       "             0.1922, 0.1957, 0.1993, 0.2028, 0.2064, 0.2100, 0.2100, 0.2135, 0.2135,\n",
       "             0.2135, 0.2171, 0.2206, 0.2242, 0.2278, 0.2313, 0.2349, 0.2384, 0.2420,\n",
       "             0.2456, 0.2491, 0.2527, 0.2527, 0.2562, 0.2598, 0.2633, 0.2669, 0.2705,\n",
       "             0.2740, 0.2776, 0.2811, 0.2847, 0.2883, 0.2883, 0.2918, 0.2954, 0.2989,\n",
       "             0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203, 0.3238, 0.3274, 0.3310,\n",
       "             0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630,\n",
       "             0.3665, 0.3701, 0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950,\n",
       "             0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270,\n",
       "             0.4306, 0.4342, 0.4377, 0.4413, 0.4448, 0.4484, 0.4520, 0.4555, 0.4591,\n",
       "             0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804, 0.4840, 0.4875, 0.4911,\n",
       "             0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5125, 0.5160, 0.5196, 0.5231,\n",
       "             0.5267, 0.5302, 0.5338, 0.5338, 0.5374, 0.5409, 0.5445, 0.5480, 0.5516,\n",
       "             0.5552, 0.5587, 0.5623, 0.5658, 0.5694, 0.5730, 0.5765, 0.5801, 0.5836,\n",
       "             0.5872, 0.5907, 0.5943, 0.5979, 0.6014, 0.6050, 0.6085, 0.6121, 0.6157,\n",
       "             0.6192, 0.6228, 0.6263, 0.6299, 0.6335, 0.6370, 0.6406, 0.6441, 0.6477,\n",
       "             0.6512, 0.6548, 0.6584, 0.6619, 0.6655, 0.6690, 0.6726, 0.6762, 0.6797,\n",
       "             0.6833, 0.6868, 0.6904, 0.6940, 0.6975, 0.7011, 0.7046, 0.7082, 0.7117,\n",
       "             0.7153, 0.7189, 0.7224, 0.7260, 0.7295, 0.7331, 0.7367, 0.7402, 0.7438,\n",
       "             0.7473, 0.7509, 0.7544, 0.7580, 0.7616, 0.7651, 0.7687, 0.7722, 0.7758,\n",
       "             0.7794, 0.7829, 0.7865, 0.7900, 0.7936, 0.7972, 0.8007, 0.8043, 0.8078,\n",
       "             0.8114, 0.8149, 0.8185, 0.8221, 0.8256, 0.8292, 0.8327, 0.8363, 0.8399,\n",
       "             0.8434, 0.8470, 0.8505, 0.8541, 0.8577, 0.8612, 0.8648, 0.8683, 0.8719,\n",
       "             0.8754, 0.8790, 0.8826, 0.8861, 0.8897, 0.8932, 0.8968, 0.9004, 0.9039,\n",
       "             0.9075, 0.9110, 0.9146, 0.9181, 0.9217, 0.9253, 0.9288, 0.9324, 0.9359,\n",
       "             0.9395, 0.9431, 0.9466, 0.9502, 0.9537, 0.9573, 0.9609, 0.9644, 0.9680,\n",
       "             0.9715, 0.9751, 0.9786, 0.9822, 0.9858, 0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.4141, 0.4833, 0.5223, 0.5513, 0.5647, 0.5837, 0.5926, 0.6049,\n",
       "             0.6150, 0.6239, 0.6339, 0.6417, 0.6484, 0.6529, 0.6551, 0.6585, 0.6652,\n",
       "             0.6696, 0.6708, 0.6752, 0.6775, 0.6808, 0.6842, 0.6853, 0.6864, 0.6886,\n",
       "             0.6897, 0.6908, 0.6953, 0.6987, 0.7009, 0.7020, 0.7054, 0.7065, 0.7087,\n",
       "             0.7109, 0.7121, 0.7143, 0.7154, 0.7176, 0.7199, 0.7232, 0.7254, 0.7266,\n",
       "             0.7299, 0.7310, 0.7321, 0.7333, 0.7344, 0.7355, 0.7388, 0.7400, 0.7411,\n",
       "             0.7433, 0.7455, 0.7467, 0.7478, 0.7489, 0.7511, 0.7522, 0.7533, 0.7545,\n",
       "             0.7556, 0.7567, 0.7589, 0.7600, 0.7612, 0.7623, 0.7634, 0.7645, 0.7656,\n",
       "             0.7679, 0.7690, 0.7701, 0.7712, 0.7723, 0.7734, 0.7746, 0.7757, 0.7768,\n",
       "             0.7790, 0.7801, 0.7812, 0.7824, 0.7835, 0.7857, 0.7868, 0.7879, 0.7891,\n",
       "             0.7902, 0.7913, 0.7924, 0.7935, 0.7946, 0.7980, 0.8002, 0.8025, 0.8036,\n",
       "             0.8047, 0.8058, 0.8069, 0.8080, 0.8092, 0.8103, 0.8114, 0.8125, 0.8136,\n",
       "             0.8147, 0.8158, 0.8170, 0.8181, 0.8192, 0.8203, 0.8214, 0.8237, 0.8248,\n",
       "             0.8259, 0.8270, 0.8281, 0.8304, 0.8315, 0.8326, 0.8337, 0.8348, 0.8359,\n",
       "             0.8371, 0.8382, 0.8393, 0.8404, 0.8415, 0.8426, 0.8449, 0.8460, 0.8471,\n",
       "             0.8482, 0.8493, 0.8504, 0.8516, 0.8527, 0.8527, 0.8538, 0.8549, 0.8560,\n",
       "             0.8583, 0.8594, 0.8605, 0.8616, 0.8627, 0.8638, 0.8650, 0.8661, 0.8672,\n",
       "             0.8683, 0.8694, 0.8717, 0.8728, 0.8739, 0.8750, 0.8761, 0.8772, 0.8783,\n",
       "             0.8795, 0.8806, 0.8817, 0.8828, 0.8839, 0.8850, 0.8862, 0.8873, 0.8884,\n",
       "             0.8895, 0.8906, 0.8917, 0.8929, 0.8940, 0.8951, 0.8962, 0.8973, 0.8996,\n",
       "             0.9007, 0.9018, 0.9029, 0.9040, 0.9051, 0.9062, 0.9074, 0.9085, 0.9096,\n",
       "             0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9163, 0.9174, 0.9185, 0.9196,\n",
       "             0.9208, 0.9219, 0.9230, 0.9241, 0.9252, 0.9263, 0.9275, 0.9286, 0.9297,\n",
       "             0.9308, 0.9319, 0.9330, 0.9342, 0.9353, 0.9364, 0.9364, 0.9375, 0.9386,\n",
       "             0.9397, 0.9408, 0.9420, 0.9431, 0.9442, 0.9453, 0.9464, 0.9464, 0.9475,\n",
       "             0.9487, 0.9487, 0.9487, 0.9498, 0.9509, 0.9520, 0.9520, 0.9531, 0.9542,\n",
       "             0.9554, 0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621, 0.9621,\n",
       "             0.9621, 0.9632, 0.9643, 0.9643, 0.9654, 0.9665, 0.9676, 0.9676, 0.9676,\n",
       "             0.9676, 0.9676, 0.9688, 0.9699, 0.9699, 0.9710, 0.9721, 0.9732, 0.9732,\n",
       "             0.9743, 0.9754, 0.9754, 0.9766, 0.9777, 0.9777, 0.9788, 0.9799, 0.9810,\n",
       "             0.9821, 0.9833, 0.9833, 0.9833, 0.9833, 0.9833, 0.9844, 0.9844, 0.9844,\n",
       "             0.9855, 0.9866, 0.9866, 0.9866, 0.9877, 0.9888, 0.9900, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9911, 0.9911, 0.9911, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9944, 0.9944, 0.9955,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9981e-01, 9.9979e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9974e-01, 9.9971e-01, 9.9969e-01, 9.9969e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9964e-01, 9.9963e-01, 9.9960e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9958e-01, 9.9958e-01, 9.9958e-01, 9.9957e-01, 9.9955e-01,\n",
       "             9.9953e-01, 9.9952e-01, 9.9947e-01, 9.9944e-01, 9.9944e-01, 9.9941e-01,\n",
       "             9.9937e-01, 9.9934e-01, 9.9931e-01, 9.9928e-01, 9.9928e-01, 9.9923e-01,\n",
       "             9.9917e-01, 9.9916e-01, 9.9900e-01, 9.9899e-01, 9.9897e-01, 9.9852e-01,\n",
       "             9.9843e-01, 9.9840e-01, 9.9837e-01, 9.9831e-01, 9.9830e-01, 9.9825e-01,\n",
       "             9.9816e-01, 9.9796e-01, 9.9783e-01, 9.9781e-01, 9.9720e-01, 9.9700e-01,\n",
       "             9.9683e-01, 9.9659e-01, 9.9655e-01, 9.9634e-01, 9.9632e-01, 9.9614e-01,\n",
       "             9.9335e-01, 9.9147e-01, 9.9069e-01, 9.8681e-01, 9.8467e-01, 9.8460e-01,\n",
       "             9.8459e-01, 9.7757e-01, 9.7520e-01, 9.7143e-01, 9.6636e-01, 9.6037e-01,\n",
       "             9.5441e-01, 9.5428e-01, 9.4873e-01, 9.4756e-01, 9.2870e-01, 9.2098e-01,\n",
       "             8.6452e-01, 8.5500e-01, 8.4323e-01, 8.2757e-01, 8.1286e-01, 7.9469e-01,\n",
       "             7.8378e-01, 7.7188e-01, 7.4889e-01, 7.4877e-01, 6.8998e-01, 6.2035e-01,\n",
       "             6.0685e-01, 5.8848e-01, 5.6128e-01, 5.4222e-01, 5.0248e-01, 4.8245e-01,\n",
       "             4.4884e-01, 3.8721e-01, 3.7902e-01, 3.2041e-01, 3.1218e-01, 2.9044e-01,\n",
       "             2.8536e-01, 2.8012e-01, 2.7446e-01, 2.3439e-01, 1.9659e-01, 1.7435e-01,\n",
       "             1.5222e-01, 1.3891e-01, 1.3764e-01, 1.2941e-01, 1.1917e-01, 9.2031e-02,\n",
       "             6.6481e-02, 5.6081e-02, 5.1734e-02, 4.4063e-02, 4.3980e-02, 3.6864e-02,\n",
       "             3.5539e-02, 3.4548e-02, 3.1900e-02, 3.1418e-02, 2.7080e-02, 2.5935e-02,\n",
       "             2.5753e-02, 2.3082e-02, 2.2935e-02, 1.8566e-02, 1.7941e-02, 1.6091e-02,\n",
       "             1.4651e-02, 1.2613e-02, 1.1541e-02, 1.1381e-02, 6.1783e-03, 6.1370e-03,\n",
       "             5.9530e-03, 5.4872e-03, 5.2805e-03, 5.1670e-03, 5.0007e-03, 4.9204e-03,\n",
       "             4.5091e-03, 4.0074e-03, 3.4756e-03, 3.2355e-03, 3.1580e-03, 2.9083e-03,\n",
       "             2.6258e-03, 1.8286e-03, 1.3842e-03, 1.3526e-03, 1.1634e-03, 1.0713e-03,\n",
       "             8.8174e-04, 6.4945e-04, 6.2617e-04, 5.8310e-04, 5.4971e-04, 5.3634e-04,\n",
       "             5.3504e-04, 5.3100e-04, 4.8737e-04, 4.6849e-04, 4.6791e-04, 4.6327e-04,\n",
       "             3.4941e-04, 3.3921e-04, 3.3724e-04, 3.3542e-04, 3.0613e-04, 2.1783e-04,\n",
       "             2.1454e-04, 1.9503e-04, 1.9022e-04, 1.8943e-04, 1.8491e-04, 1.7439e-04,\n",
       "             1.6259e-04, 1.5496e-04, 1.5448e-04, 1.4514e-04, 8.7399e-05, 8.7365e-05,\n",
       "             8.2339e-05, 7.9166e-05, 7.6424e-05, 6.3809e-05, 6.1403e-05, 5.2052e-05,\n",
       "             4.1597e-05, 3.7614e-05, 2.8106e-05, 2.4154e-05, 1.9949e-05, 1.9290e-05,\n",
       "             1.7429e-05, 1.6199e-05, 1.4236e-05, 1.3486e-05, 1.1091e-05, 1.0378e-05,\n",
       "             1.0091e-05, 9.5001e-06, 7.4999e-06, 7.3464e-06, 7.0484e-06, 6.9194e-06,\n",
       "             6.8357e-06, 4.7002e-06, 4.2845e-06, 4.0054e-06, 3.6653e-06, 3.3172e-06,\n",
       "             2.2832e-06, 2.2257e-06, 2.1243e-06, 1.9986e-06, 1.8449e-06, 1.7822e-06,\n",
       "             1.5809e-06, 1.1601e-06, 1.1390e-06, 1.1052e-06, 9.4334e-07, 8.9781e-07,\n",
       "             8.6089e-07, 6.7234e-07, 5.8673e-07, 5.7657e-07, 5.1524e-07, 4.9632e-07,\n",
       "             4.4427e-07, 4.3433e-07, 3.5486e-07, 3.2356e-07, 2.8006e-07, 2.3175e-07,\n",
       "             2.0937e-07, 1.9210e-07, 1.6726e-07, 1.5251e-07, 1.4234e-07, 1.2045e-07,\n",
       "             1.1032e-07, 1.0209e-07, 9.8514e-08, 9.5466e-08, 7.8512e-08, 7.4156e-08,\n",
       "             7.1818e-08, 5.6246e-08, 5.5589e-08, 5.3019e-08, 5.1702e-08, 4.3706e-08,\n",
       "             4.2830e-08, 3.8241e-08, 3.6682e-08, 3.0450e-08, 2.6859e-08, 2.1024e-08,\n",
       "             1.8384e-08, 1.7157e-08, 1.6729e-08, 1.6255e-08, 1.6137e-08, 1.6106e-08,\n",
       "             1.6065e-08, 1.5628e-08, 1.5551e-08, 1.5325e-08, 1.4761e-08, 1.3564e-08,\n",
       "             1.3556e-08, 1.2871e-08, 1.2268e-08, 1.2108e-08, 1.1748e-08, 1.1217e-08,\n",
       "             1.1193e-08, 1.0958e-08, 1.0851e-08, 8.6418e-09, 8.5567e-09, 8.4900e-09,\n",
       "             8.0302e-09, 7.1378e-09, 6.9341e-09, 5.4070e-09, 3.3752e-09, 2.8763e-09,\n",
       "             2.5767e-09, 2.4221e-09, 2.3118e-09, 2.1158e-09, 2.0332e-09, 2.0165e-09,\n",
       "             1.9183e-09, 1.7020e-09, 1.5808e-09, 1.4416e-09, 1.2836e-09, 1.2258e-09,\n",
       "             1.1802e-09, 1.1152e-09, 7.7341e-10, 7.4509e-10, 7.3410e-10, 7.1358e-10,\n",
       "             7.0534e-10, 4.4439e-10, 4.0136e-10, 3.6803e-10, 3.0678e-10, 2.7046e-10,\n",
       "             2.4914e-10, 2.4003e-10, 2.3256e-10, 2.2007e-10, 1.8650e-10, 1.8521e-10,\n",
       "             1.6289e-10, 1.5857e-10, 1.4260e-10, 1.3582e-10, 1.0826e-10, 1.0681e-10,\n",
       "             8.5769e-11, 7.4266e-11, 6.8492e-11, 4.0895e-11, 3.9116e-11, 2.8749e-11,\n",
       "             2.4662e-11, 2.4291e-11, 1.6073e-11, 1.5931e-11, 1.5857e-11, 1.3728e-11,\n",
       "             1.1934e-11, 1.0778e-11, 8.7170e-12, 7.4139e-12, 6.6662e-12, 5.4806e-12,\n",
       "             3.4489e-12, 3.3290e-12, 3.0981e-12, 2.8969e-12, 1.9446e-12, 1.3098e-12,\n",
       "             9.0781e-13, 6.9787e-13, 5.4031e-13, 4.1198e-13, 3.2969e-13, 1.9495e-13,\n",
       "             1.7565e-13, 1.6897e-13, 1.5913e-13, 1.0997e-13, 7.1927e-14, 3.7110e-14,\n",
       "             3.2308e-14, 1.8884e-14, 1.7129e-14])}},\n",
       "   {'fpr': np.float64(0.099644128113879),\n",
       "    'tpr': np.float64(0.9933035714285714),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0107, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0249, 0.0285, 0.0285, 0.0285, 0.0320, 0.0356, 0.0356, 0.0356,\n",
       "             0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356,\n",
       "             0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0391, 0.0391, 0.0391, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0427, 0.0427, 0.0427,\n",
       "             0.0427, 0.0463, 0.0463, 0.0463, 0.0463, 0.0498, 0.0498, 0.0534, 0.0534,\n",
       "             0.0569, 0.0605, 0.0605, 0.0605, 0.0605, 0.0641, 0.0676, 0.0712, 0.0747,\n",
       "             0.0783, 0.0819, 0.0854, 0.0854, 0.0890, 0.0925, 0.0961, 0.0996, 0.1032,\n",
       "             0.1068, 0.1068, 0.1103, 0.1139, 0.1139, 0.1174, 0.1210, 0.1246, 0.1281,\n",
       "             0.1317, 0.1352, 0.1352, 0.1388, 0.1423, 0.1459, 0.1495, 0.1530, 0.1566,\n",
       "             0.1601, 0.1637, 0.1673, 0.1708, 0.1744, 0.1779, 0.1815, 0.1851, 0.1886,\n",
       "             0.1922, 0.1957, 0.1993, 0.2028, 0.2028, 0.2064, 0.2100, 0.2135, 0.2171,\n",
       "             0.2206, 0.2242, 0.2278, 0.2313, 0.2349, 0.2384, 0.2420, 0.2420, 0.2420,\n",
       "             0.2456, 0.2491, 0.2527, 0.2562, 0.2598, 0.2633, 0.2669, 0.2705, 0.2740,\n",
       "             0.2776, 0.2811, 0.2847, 0.2883, 0.2918, 0.2954, 0.2989, 0.3025, 0.3060,\n",
       "             0.3096, 0.3132, 0.3167, 0.3203, 0.3238, 0.3274, 0.3310, 0.3345, 0.3381,\n",
       "             0.3416, 0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630, 0.3665, 0.3701,\n",
       "             0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950, 0.3986, 0.4021,\n",
       "             0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270, 0.4306, 0.4342,\n",
       "             0.4377, 0.4413, 0.4448, 0.4484, 0.4520, 0.4555, 0.4591, 0.4626, 0.4662,\n",
       "             0.4698, 0.4733, 0.4769, 0.4804, 0.4840, 0.4875, 0.4911, 0.4947, 0.4982,\n",
       "             0.5018, 0.5053, 0.5089, 0.5125, 0.5160, 0.5196, 0.5231, 0.5267, 0.5302,\n",
       "             0.5338, 0.5374, 0.5409, 0.5445, 0.5480, 0.5516, 0.5552, 0.5587, 0.5623,\n",
       "             0.5658, 0.5694, 0.5730, 0.5765, 0.5801, 0.5836, 0.5872, 0.5907, 0.5943,\n",
       "             0.5979, 0.6014, 0.6050, 0.6085, 0.6121, 0.6157, 0.6192, 0.6228, 0.6263,\n",
       "             0.6299, 0.6335, 0.6370, 0.6406, 0.6441, 0.6477, 0.6512, 0.6548, 0.6584,\n",
       "             0.6619, 0.6655, 0.6690, 0.6726, 0.6762, 0.6797, 0.6833, 0.6868, 0.6904,\n",
       "             0.6940, 0.6975, 0.7011, 0.7046, 0.7082, 0.7117, 0.7153, 0.7189, 0.7224,\n",
       "             0.7260, 0.7295, 0.7331, 0.7367, 0.7402, 0.7438, 0.7473, 0.7509, 0.7544,\n",
       "             0.7580, 0.7616, 0.7651, 0.7687, 0.7722, 0.7758, 0.7794, 0.7829, 0.7865,\n",
       "             0.7900, 0.7936, 0.7972, 0.8007, 0.8043, 0.8078, 0.8114, 0.8149, 0.8185,\n",
       "             0.8221, 0.8256, 0.8292, 0.8327, 0.8363, 0.8399, 0.8434, 0.8470, 0.8505,\n",
       "             0.8541, 0.8577, 0.8612, 0.8648, 0.8683, 0.8719, 0.8754, 0.8790, 0.8826,\n",
       "             0.8861, 0.8897, 0.8932, 0.8968, 0.9004, 0.9039, 0.9075, 0.9110, 0.9146,\n",
       "             0.9181, 0.9217, 0.9253, 0.9288, 0.9324, 0.9359, 0.9395, 0.9431, 0.9466,\n",
       "             0.9502, 0.9537, 0.9573, 0.9609, 0.9644, 0.9680, 0.9715, 0.9751, 0.9786,\n",
       "             0.9822, 0.9858, 0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7299, 0.7768, 0.8025, 0.8170, 0.8292, 0.8371, 0.8438, 0.8471,\n",
       "             0.8527, 0.8549, 0.8583, 0.8616, 0.8650, 0.8661, 0.8683, 0.8694, 0.8739,\n",
       "             0.8750, 0.8761, 0.8772, 0.8783, 0.8795, 0.8806, 0.8817, 0.8839, 0.8862,\n",
       "             0.8862, 0.8873, 0.8884, 0.8895, 0.8906, 0.8917, 0.8929, 0.8940, 0.8951,\n",
       "             0.8962, 0.8973, 0.8996, 0.9007, 0.9018, 0.9029, 0.9051, 0.9074, 0.9085,\n",
       "             0.9096, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9163, 0.9174, 0.9185,\n",
       "             0.9196, 0.9208, 0.9219, 0.9230, 0.9241, 0.9252, 0.9263, 0.9275, 0.9286,\n",
       "             0.9297, 0.9308, 0.9319, 0.9330, 0.9342, 0.9353, 0.9364, 0.9386, 0.9397,\n",
       "             0.9408, 0.9420, 0.9420, 0.9431, 0.9442, 0.9453, 0.9453, 0.9464, 0.9475,\n",
       "             0.9487, 0.9498, 0.9498, 0.9509, 0.9520, 0.9520, 0.9520, 0.9531, 0.9542,\n",
       "             0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621, 0.9632, 0.9643,\n",
       "             0.9654, 0.9665, 0.9676, 0.9688, 0.9699, 0.9699, 0.9710, 0.9721, 0.9732,\n",
       "             0.9743, 0.9754, 0.9766, 0.9777, 0.9788, 0.9799, 0.9799, 0.9810, 0.9821,\n",
       "             0.9833, 0.9833, 0.9844, 0.9855, 0.9866, 0.9866, 0.9877, 0.9877, 0.9888,\n",
       "             0.9888, 0.9888, 0.9900, 0.9911, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9944, 0.9944, 0.9944, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9955, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9989, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9982e-01, 9.9979e-01, 9.9977e-01,\n",
       "             9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9972e-01, 9.9970e-01, 9.9966e-01,\n",
       "             9.9966e-01, 9.9963e-01, 9.9961e-01, 9.9959e-01, 9.9958e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9949e-01, 9.9944e-01, 9.9938e-01, 9.9933e-01, 9.9911e-01,\n",
       "             9.9909e-01, 9.9901e-01, 9.9899e-01, 9.9892e-01, 9.9892e-01, 9.9885e-01,\n",
       "             9.9879e-01, 9.9873e-01, 9.9845e-01, 9.9820e-01, 9.9756e-01, 9.9744e-01,\n",
       "             9.9667e-01, 9.9567e-01, 9.9476e-01, 9.9444e-01, 9.9296e-01, 9.9202e-01,\n",
       "             9.8857e-01, 9.8722e-01, 9.8604e-01, 9.7985e-01, 9.7276e-01, 9.5373e-01,\n",
       "             9.5143e-01, 9.1923e-01, 9.0979e-01, 8.9715e-01, 8.6410e-01, 8.4439e-01,\n",
       "             8.3411e-01, 8.2958e-01, 7.9270e-01, 7.6907e-01, 7.5204e-01, 7.5062e-01,\n",
       "             6.8792e-01, 6.8426e-01, 6.0468e-01, 6.0464e-01, 5.7906e-01, 4.7775e-01,\n",
       "             4.4365e-01, 4.1016e-01, 3.9996e-01, 3.8674e-01, 3.2070e-01, 2.9235e-01,\n",
       "             2.5317e-01, 2.4550e-01, 2.2853e-01, 2.1995e-01, 2.0608e-01, 1.6625e-01,\n",
       "             1.5709e-01, 1.5161e-01, 1.3884e-01, 1.3695e-01, 1.3509e-01, 1.3055e-01,\n",
       "             1.1216e-01, 1.1164e-01, 1.1056e-01, 8.5030e-02, 7.3192e-02, 6.7197e-02,\n",
       "             6.6481e-02, 6.2310e-02, 6.1779e-02, 5.9346e-02, 5.8948e-02, 4.5715e-02,\n",
       "             4.0289e-02, 3.6578e-02, 3.0765e-02, 2.9502e-02, 2.1485e-02, 2.0812e-02,\n",
       "             1.5100e-02, 1.4571e-02, 1.4157e-02, 1.3451e-02, 1.1659e-02, 1.0919e-02,\n",
       "             8.3600e-03, 8.0971e-03, 7.8570e-03, 7.1660e-03, 6.0562e-03, 5.6479e-03,\n",
       "             5.3734e-03, 4.5346e-03, 4.1503e-03, 3.9906e-03, 3.7237e-03, 3.5517e-03,\n",
       "             3.5048e-03, 2.9383e-03, 2.9304e-03, 2.8083e-03, 2.5153e-03, 2.5033e-03,\n",
       "             2.5027e-03, 2.4648e-03, 2.2668e-03, 2.2228e-03, 2.0294e-03, 1.5551e-03,\n",
       "             1.5235e-03, 1.4714e-03, 1.4563e-03, 1.3093e-03, 1.1737e-03, 1.0255e-03,\n",
       "             9.3592e-04, 9.1356e-04, 8.1547e-04, 7.9920e-04, 7.3963e-04, 7.3619e-04,\n",
       "             7.1282e-04, 6.8635e-04, 6.8393e-04, 6.2635e-04, 5.9011e-04, 5.8607e-04,\n",
       "             5.8049e-04, 5.5098e-04, 4.9909e-04, 4.7940e-04, 4.4477e-04, 4.1826e-04,\n",
       "             3.8766e-04, 3.7350e-04, 3.4031e-04, 2.8342e-04, 2.6534e-04, 2.4171e-04,\n",
       "             2.3756e-04, 2.2166e-04, 2.1045e-04, 1.8122e-04, 1.7474e-04, 1.7208e-04,\n",
       "             1.4708e-04, 1.3240e-04, 1.1471e-04, 1.1424e-04, 9.8320e-05, 9.6339e-05,\n",
       "             8.0069e-05, 7.8818e-05, 7.7996e-05, 6.7477e-05, 6.4631e-05, 5.8832e-05,\n",
       "             5.2950e-05, 5.1906e-05, 5.0942e-05, 5.0633e-05, 4.9381e-05, 4.7492e-05,\n",
       "             4.4276e-05, 4.3320e-05, 3.7542e-05, 2.7659e-05, 2.4369e-05, 2.4297e-05,\n",
       "             2.2601e-05, 2.1915e-05, 2.1775e-05, 1.8309e-05, 1.7707e-05, 1.5813e-05,\n",
       "             1.4705e-05, 1.4705e-05, 1.4025e-05, 1.2771e-05, 1.2504e-05, 1.2404e-05,\n",
       "             1.2255e-05, 1.1685e-05, 1.0952e-05, 1.0890e-05, 9.6605e-06, 8.8195e-06,\n",
       "             8.2179e-06, 6.3390e-06, 5.3661e-06, 5.3205e-06, 5.0804e-06, 4.7831e-06,\n",
       "             4.3769e-06, 3.1220e-06, 2.7151e-06, 2.6644e-06, 2.2197e-06, 1.9329e-06,\n",
       "             1.8131e-06, 1.5685e-06, 1.5457e-06, 1.4629e-06, 1.3034e-06, 1.1171e-06,\n",
       "             1.1130e-06, 1.0598e-06, 1.0217e-06, 1.0115e-06, 8.9334e-07, 8.6634e-07,\n",
       "             8.4665e-07, 7.8121e-07, 7.6955e-07, 7.4662e-07, 7.4001e-07, 6.1791e-07,\n",
       "             5.5905e-07, 5.4017e-07, 5.0463e-07, 4.7430e-07, 4.3230e-07, 3.8817e-07,\n",
       "             3.7870e-07, 3.6764e-07, 3.2005e-07, 3.0673e-07, 2.9666e-07, 2.7470e-07,\n",
       "             2.4947e-07, 2.4128e-07, 2.3432e-07, 2.0962e-07, 1.7587e-07, 1.7398e-07,\n",
       "             1.6100e-07, 1.3960e-07, 1.3651e-07, 1.3382e-07, 1.1879e-07, 1.0962e-07,\n",
       "             1.0446e-07, 9.1028e-08, 8.9906e-08, 8.6539e-08, 8.1118e-08, 7.7244e-08,\n",
       "             7.6462e-08, 6.4917e-08, 5.5445e-08, 5.3301e-08, 3.6710e-08, 3.1910e-08,\n",
       "             2.8405e-08, 2.7480e-08, 2.7238e-08, 2.6293e-08, 2.4106e-08, 1.7218e-08,\n",
       "             1.3455e-08, 1.2757e-08, 1.2322e-08, 1.2252e-08, 1.1397e-08, 1.1349e-08,\n",
       "             8.6507e-09, 6.9985e-09, 5.1889e-09, 5.1273e-09, 4.6295e-09, 4.5754e-09,\n",
       "             4.2680e-09, 3.3343e-09, 3.2624e-09, 3.2214e-09, 2.9855e-09, 1.8133e-09,\n",
       "             1.3882e-09, 1.3603e-09, 1.1994e-09, 8.4195e-10, 7.4035e-10, 6.0859e-10,\n",
       "             4.8852e-10, 4.5045e-10, 2.2562e-10, 2.1561e-10, 1.5155e-10, 1.4167e-10,\n",
       "             1.2668e-10, 8.1325e-11, 7.1520e-11, 6.4784e-11, 5.9243e-11, 5.4520e-11,\n",
       "             2.8787e-11, 1.2123e-11, 9.5886e-12, 4.1634e-12, 9.9026e-13, 5.4309e-13])}},\n",
       "   {'fpr': np.float64(0.06761565836298933),\n",
       "    'tpr': np.float64(0.9921875),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0071, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0285, 0.0320, 0.0320, 0.0320, 0.0356, 0.0356, 0.0356, 0.0356,\n",
       "             0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0391, 0.0391,\n",
       "             0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0463, 0.0463, 0.0463, 0.0463,\n",
       "             0.0463, 0.0463, 0.0498, 0.0534, 0.0534, 0.0534, 0.0534, 0.0569, 0.0569,\n",
       "             0.0605, 0.0641, 0.0676, 0.0676, 0.0712, 0.0712, 0.0747, 0.0783, 0.0783,\n",
       "             0.0819, 0.0854, 0.0890, 0.0890, 0.0925, 0.0961, 0.0996, 0.1032, 0.1068,\n",
       "             0.1103, 0.1139, 0.1174, 0.1210, 0.1246, 0.1281, 0.1317, 0.1352, 0.1388,\n",
       "             0.1423, 0.1459, 0.1495, 0.1530, 0.1566, 0.1601, 0.1637, 0.1673, 0.1708,\n",
       "             0.1744, 0.1779, 0.1815, 0.1815, 0.1851, 0.1886, 0.1922, 0.1957, 0.1993,\n",
       "             0.2028, 0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242, 0.2278, 0.2313,\n",
       "             0.2349, 0.2384, 0.2420, 0.2456, 0.2456, 0.2491, 0.2527, 0.2562, 0.2598,\n",
       "             0.2633, 0.2669, 0.2705, 0.2740, 0.2776, 0.2811, 0.2847, 0.2883, 0.2918,\n",
       "             0.2918, 0.2954, 0.2989, 0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203,\n",
       "             0.3238, 0.3274, 0.3310, 0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523,\n",
       "             0.3559, 0.3594, 0.3630, 0.3665, 0.3701, 0.3737, 0.3772, 0.3808, 0.3843,\n",
       "             0.3879, 0.3915, 0.3950, 0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164,\n",
       "             0.4199, 0.4235, 0.4270, 0.4306, 0.4342, 0.4377, 0.4413, 0.4448, 0.4484,\n",
       "             0.4520, 0.4555, 0.4591, 0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804,\n",
       "             0.4840, 0.4875, 0.4911, 0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5125,\n",
       "             0.5160, 0.5196, 0.5231, 0.5267, 0.5302, 0.5338, 0.5374, 0.5409, 0.5445,\n",
       "             0.5480, 0.5516, 0.5552, 0.5587, 0.5623, 0.5658, 0.5694, 0.5730, 0.5765,\n",
       "             0.5801, 0.5836, 0.5872, 0.5907, 0.5943, 0.5979, 0.6014, 0.6050, 0.6085,\n",
       "             0.6121, 0.6157, 0.6192, 0.6228, 0.6263, 0.6299, 0.6335, 0.6370, 0.6406,\n",
       "             0.6441, 0.6477, 0.6512, 0.6548, 0.6584, 0.6619, 0.6655, 0.6690, 0.6726,\n",
       "             0.6762, 0.6797, 0.6833, 0.6868, 0.6904, 0.6940, 0.6975, 0.7011, 0.7046,\n",
       "             0.7082, 0.7117, 0.7153, 0.7189, 0.7224, 0.7260, 0.7295, 0.7331, 0.7367,\n",
       "             0.7402, 0.7438, 0.7473, 0.7509, 0.7544, 0.7580, 0.7616, 0.7651, 0.7687,\n",
       "             0.7722, 0.7758, 0.7794, 0.7829, 0.7865, 0.7900, 0.7936, 0.7972, 0.8007,\n",
       "             0.8043, 0.8078, 0.8114, 0.8149, 0.8185, 0.8221, 0.8256, 0.8292, 0.8327,\n",
       "             0.8363, 0.8399, 0.8434, 0.8470, 0.8505, 0.8541, 0.8577, 0.8612, 0.8648,\n",
       "             0.8683, 0.8719, 0.8754, 0.8790, 0.8826, 0.8861, 0.8897, 0.8932, 0.8968,\n",
       "             0.9004, 0.9039, 0.9075, 0.9110, 0.9146, 0.9181, 0.9217, 0.9253, 0.9288,\n",
       "             0.9324, 0.9359, 0.9395, 0.9431, 0.9466, 0.9502, 0.9537, 0.9573, 0.9609,\n",
       "             0.9644, 0.9680, 0.9715, 0.9751, 0.9786, 0.9822, 0.9858, 0.9893, 0.9929,\n",
       "             0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8393, 0.8728, 0.8862, 0.8906, 0.8962, 0.8973, 0.9007, 0.9018,\n",
       "             0.9040, 0.9062, 0.9074, 0.9085, 0.9096, 0.9107, 0.9118, 0.9141, 0.9152,\n",
       "             0.9163, 0.9174, 0.9185, 0.9196, 0.9208, 0.9219, 0.9230, 0.9241, 0.9252,\n",
       "             0.9263, 0.9275, 0.9286, 0.9297, 0.9297, 0.9308, 0.9319, 0.9330, 0.9342,\n",
       "             0.9353, 0.9364, 0.9375, 0.9386, 0.9397, 0.9408, 0.9408, 0.9420, 0.9431,\n",
       "             0.9442, 0.9453, 0.9464, 0.9475, 0.9487, 0.9498, 0.9498, 0.9509, 0.9520,\n",
       "             0.9531, 0.9542, 0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621,\n",
       "             0.9632, 0.9632, 0.9632, 0.9643, 0.9654, 0.9654, 0.9665, 0.9676, 0.9688,\n",
       "             0.9699, 0.9710, 0.9721, 0.9732, 0.9743, 0.9754, 0.9766, 0.9766, 0.9777,\n",
       "             0.9777, 0.9788, 0.9799, 0.9810, 0.9821, 0.9821, 0.9833, 0.9844, 0.9855,\n",
       "             0.9866, 0.9877, 0.9877, 0.9877, 0.9888, 0.9900, 0.9911, 0.9911, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9944, 0.9944, 0.9944, 0.9955,\n",
       "             0.9955, 0.9955, 0.9955, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9988e-01, 9.9985e-01, 9.9984e-01, 9.9981e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9976e-01, 9.9973e-01, 9.9966e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9961e-01, 9.9960e-01, 9.9948e-01, 9.9929e-01, 9.9889e-01,\n",
       "             9.9885e-01, 9.9834e-01, 9.9819e-01, 9.9759e-01, 9.9741e-01, 9.9739e-01,\n",
       "             9.9721e-01, 9.9705e-01, 9.9588e-01, 9.9584e-01, 9.9554e-01, 9.9513e-01,\n",
       "             9.9225e-01, 9.9210e-01, 9.8872e-01, 9.8803e-01, 9.8750e-01, 9.7528e-01,\n",
       "             9.7506e-01, 9.7446e-01, 9.6800e-01, 9.4797e-01, 9.4611e-01, 9.4354e-01,\n",
       "             8.9899e-01, 8.5323e-01, 8.5167e-01, 8.4835e-01, 8.2074e-01, 7.9407e-01,\n",
       "             7.8106e-01, 7.7790e-01, 7.1565e-01, 5.4377e-01, 5.2941e-01, 5.2747e-01,\n",
       "             4.3850e-01, 4.1524e-01, 4.0947e-01, 3.2049e-01, 2.3114e-01, 2.2235e-01,\n",
       "             2.1034e-01, 1.6445e-01, 1.6245e-01, 1.1131e-01, 7.4833e-02, 6.0509e-02,\n",
       "             5.5004e-02, 5.1802e-02, 5.0628e-02, 3.6660e-02, 3.0725e-02, 2.5452e-02,\n",
       "             2.5215e-02, 1.9891e-02, 1.6463e-02, 1.6101e-02, 1.5892e-02, 1.5175e-02,\n",
       "             1.4758e-02, 1.4467e-02, 1.3644e-02, 1.3564e-02, 1.2880e-02, 1.1131e-02,\n",
       "             7.5194e-03, 6.5584e-03, 6.4919e-03, 6.2146e-03, 4.8574e-03, 4.4371e-03,\n",
       "             4.4352e-03, 3.9441e-03, 3.7191e-03, 3.4653e-03, 3.3733e-03, 2.9925e-03,\n",
       "             2.9619e-03, 2.8318e-03, 2.3956e-03, 2.0231e-03, 1.9463e-03, 1.6526e-03,\n",
       "             1.3242e-03, 1.2655e-03, 1.1367e-03, 9.8847e-04, 9.0283e-04, 8.1680e-04,\n",
       "             8.0307e-04, 7.2512e-04, 6.7601e-04, 5.4176e-04, 5.3934e-04, 5.3739e-04,\n",
       "             5.1690e-04, 4.2794e-04, 3.7047e-04, 3.5008e-04, 3.0009e-04, 2.5774e-04,\n",
       "             2.2687e-04, 2.0474e-04, 2.0237e-04, 1.8650e-04, 1.8626e-04, 1.6888e-04,\n",
       "             1.2769e-04, 1.2090e-04, 9.9388e-05, 9.7242e-05, 7.4821e-05, 6.3476e-05,\n",
       "             5.1957e-05, 4.7976e-05, 4.3549e-05, 4.1425e-05, 4.0384e-05, 3.0159e-05,\n",
       "             2.9774e-05, 2.9701e-05, 2.8575e-05, 2.0429e-05, 1.8226e-05, 1.4261e-05,\n",
       "             1.2823e-05, 1.2707e-05, 1.2161e-05, 1.1264e-05, 1.0659e-05, 9.9420e-06,\n",
       "             9.4895e-06, 8.6675e-06, 8.4280e-06, 7.1343e-06, 6.9322e-06, 6.6454e-06,\n",
       "             6.5162e-06, 5.2423e-06, 5.1697e-06, 4.7835e-06, 3.8524e-06, 3.6878e-06,\n",
       "             3.5237e-06, 3.3502e-06, 3.2563e-06, 3.1516e-06, 3.1382e-06, 2.9479e-06,\n",
       "             2.9471e-06, 2.8600e-06, 2.6746e-06, 2.6603e-06, 2.3931e-06, 2.2948e-06,\n",
       "             2.0041e-06, 1.9936e-06, 1.9136e-06, 1.6292e-06, 1.5271e-06, 1.4803e-06,\n",
       "             1.4288e-06, 1.4094e-06, 1.0438e-06, 9.7109e-07, 9.1782e-07, 8.9236e-07,\n",
       "             6.0549e-07, 5.9324e-07, 4.5640e-07, 4.5183e-07, 4.2574e-07, 4.1962e-07,\n",
       "             4.1101e-07, 3.9242e-07, 3.8545e-07, 3.3410e-07, 3.1601e-07, 2.7726e-07,\n",
       "             2.3709e-07, 2.3402e-07, 2.3242e-07, 2.0674e-07, 2.0316e-07, 2.0165e-07,\n",
       "             2.0119e-07, 1.9809e-07, 1.8969e-07, 1.8790e-07, 1.7199e-07, 1.3100e-07,\n",
       "             1.2751e-07, 1.1691e-07, 1.1312e-07, 1.1102e-07, 9.1275e-08, 8.3158e-08,\n",
       "             8.2134e-08, 7.4629e-08, 7.3570e-08, 7.2013e-08, 5.6697e-08, 5.5657e-08,\n",
       "             3.9180e-08, 3.6141e-08, 3.3463e-08, 3.2793e-08, 3.2035e-08, 3.1234e-08,\n",
       "             3.0768e-08, 3.0135e-08, 2.4744e-08, 2.4255e-08, 2.3933e-08, 2.2975e-08,\n",
       "             2.2357e-08, 2.2141e-08, 2.1316e-08, 1.9431e-08, 1.9425e-08, 1.5908e-08,\n",
       "             1.5820e-08, 1.5367e-08, 1.5348e-08, 1.3131e-08, 1.1375e-08, 1.0727e-08,\n",
       "             9.8369e-09, 9.7120e-09, 8.4718e-09, 7.3659e-09, 7.0932e-09, 7.0474e-09,\n",
       "             6.7790e-09, 5.8151e-09, 5.4428e-09, 5.4244e-09, 4.9645e-09, 4.8525e-09,\n",
       "             3.1498e-09, 2.6880e-09, 2.6060e-09, 2.6040e-09, 2.5949e-09, 2.5597e-09,\n",
       "             2.5338e-09, 2.4959e-09, 2.2729e-09, 2.2151e-09, 2.0504e-09, 1.9414e-09,\n",
       "             1.6868e-09, 1.6052e-09, 1.1015e-09, 9.7773e-10, 8.6945e-10, 8.2059e-10,\n",
       "             7.3824e-10, 7.2438e-10, 5.5949e-10, 4.8434e-10, 4.6159e-10, 4.2842e-10,\n",
       "             4.1002e-10, 2.6979e-10, 2.5551e-10, 2.5050e-10, 2.4430e-10, 2.2022e-10,\n",
       "             2.1302e-10, 1.6927e-10, 1.6095e-10, 1.5002e-10, 1.4421e-10, 1.1266e-10,\n",
       "             9.7093e-11, 9.6175e-11, 7.0201e-11, 4.6668e-11, 4.5933e-11, 3.9893e-11,\n",
       "             3.4936e-11, 3.0265e-11, 2.4582e-11, 2.3553e-11, 1.7789e-11, 1.2356e-11,\n",
       "             8.1497e-12, 7.5343e-12, 6.4765e-12, 4.3964e-12, 4.0174e-12, 2.3911e-12,\n",
       "             1.5034e-12, 1.0568e-12, 6.7893e-13, 6.7487e-13, 5.8081e-13, 3.2341e-13,\n",
       "             2.1888e-13, 1.7376e-13, 1.0976e-13, 3.9428e-14, 2.8520e-16])}},\n",
       "   {'fpr': np.float64(0.3487544483985765),\n",
       "    'tpr': np.float64(0.9988839285714286),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0747, 0.0961, 0.1068, 0.1103, 0.1103, 0.1139, 0.1174, 0.1210,\n",
       "             0.1246, 0.1281, 0.1317, 0.1352, 0.1352, 0.1352, 0.1352, 0.1388, 0.1423,\n",
       "             0.1423, 0.1459, 0.1495, 0.1530, 0.1530, 0.1566, 0.1566, 0.1601, 0.1637,\n",
       "             0.1637, 0.1673, 0.1708, 0.1708, 0.1708, 0.1744, 0.1744, 0.1779, 0.1815,\n",
       "             0.1815, 0.1815, 0.1851, 0.1886, 0.1922, 0.1957, 0.1993, 0.2028, 0.2064,\n",
       "             0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242, 0.2278, 0.2313, 0.2349,\n",
       "             0.2384, 0.2420, 0.2456, 0.2491, 0.2527, 0.2527, 0.2562, 0.2598, 0.2633,\n",
       "             0.2669, 0.2705, 0.2740, 0.2776, 0.2776, 0.2811, 0.2811, 0.2847, 0.2883,\n",
       "             0.2918, 0.2954, 0.2989, 0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203,\n",
       "             0.3238, 0.3238, 0.3238, 0.3274, 0.3310, 0.3310, 0.3345, 0.3381, 0.3416,\n",
       "             0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630, 0.3665, 0.3701, 0.3701,\n",
       "             0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950, 0.3986, 0.4021,\n",
       "             0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270, 0.4306, 0.4342,\n",
       "             0.4377, 0.4413, 0.4448, 0.4484, 0.4520, 0.4555, 0.4591, 0.4626, 0.4662,\n",
       "             0.4698, 0.4733, 0.4769, 0.4804, 0.4840, 0.4875, 0.4911, 0.4947, 0.4982,\n",
       "             0.5018, 0.5053, 0.5089, 0.5125, 0.5160, 0.5196, 0.5231, 0.5267, 0.5302,\n",
       "             0.5338, 0.5374, 0.5409, 0.5445, 0.5480, 0.5516, 0.5552, 0.5587, 0.5623,\n",
       "             0.5658, 0.5694, 0.5730, 0.5765, 0.5801, 0.5836, 0.5872, 0.5907, 0.5943,\n",
       "             0.5979, 0.6014, 0.6050, 0.6085, 0.6121, 0.6157, 0.6192, 0.6228, 0.6263,\n",
       "             0.6299, 0.6335, 0.6370, 0.6406, 0.6441, 0.6477, 0.6512, 0.6548, 0.6584,\n",
       "             0.6619, 0.6655, 0.6690, 0.6726, 0.6762, 0.6797, 0.6833, 0.6868, 0.6904,\n",
       "             0.6940, 0.6975, 0.7011, 0.7046, 0.7082, 0.7117, 0.7153, 0.7189, 0.7224,\n",
       "             0.7260, 0.7295, 0.7331, 0.7367, 0.7402, 0.7438, 0.7473, 0.7509, 0.7544,\n",
       "             0.7580, 0.7616, 0.7651, 0.7687, 0.7722, 0.7758, 0.7794, 0.7829, 0.7865,\n",
       "             0.7900, 0.7936, 0.7972, 0.8007, 0.8043, 0.8078, 0.8114, 0.8149, 0.8185,\n",
       "             0.8221, 0.8256, 0.8292, 0.8327, 0.8363, 0.8399, 0.8434, 0.8470, 0.8505,\n",
       "             0.8541, 0.8577, 0.8612, 0.8648, 0.8683, 0.8719, 0.8754, 0.8790, 0.8826,\n",
       "             0.8861, 0.8897, 0.8932, 0.8968, 0.9004, 0.9039, 0.9075, 0.9110, 0.9146,\n",
       "             0.9181, 0.9217, 0.9253, 0.9288, 0.9324, 0.9359, 0.9395, 0.9431, 0.9466,\n",
       "             0.9502, 0.9537, 0.9573, 0.9609, 0.9644, 0.9680, 0.9715, 0.9751, 0.9786,\n",
       "             0.9822, 0.9858, 0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9621, 0.9676, 0.9699, 0.9699, 0.9721, 0.9754, 0.9754, 0.9766,\n",
       "             0.9777, 0.9777, 0.9777, 0.9777, 0.9788, 0.9799, 0.9810, 0.9810, 0.9810,\n",
       "             0.9821, 0.9821, 0.9821, 0.9821, 0.9833, 0.9833, 0.9844, 0.9844, 0.9844,\n",
       "             0.9855, 0.9855, 0.9855, 0.9866, 0.9877, 0.9877, 0.9888, 0.9888, 0.9888,\n",
       "             0.9900, 0.9911, 0.9911, 0.9911, 0.9911, 0.9911, 0.9911, 0.9911, 0.9911,\n",
       "             0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9933, 0.9933, 0.9933, 0.9944, 0.9944, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9967, 0.9978, 0.9978, 0.9978, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9989e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9966e-01, 9.9956e-01, 9.9944e-01, 9.9937e-01, 9.9923e-01,\n",
       "             9.9895e-01, 9.9864e-01, 9.9850e-01, 9.9834e-01, 9.9827e-01, 9.9820e-01,\n",
       "             9.9816e-01, 9.9747e-01, 9.9689e-01, 9.9674e-01, 9.9659e-01, 9.9652e-01,\n",
       "             9.9489e-01, 9.9417e-01, 9.9394e-01, 9.9147e-01, 9.8986e-01, 9.8984e-01,\n",
       "             9.8921e-01, 9.8026e-01, 9.7823e-01, 9.7207e-01, 9.6284e-01, 9.5060e-01,\n",
       "             9.4275e-01, 9.3372e-01, 9.3115e-01, 9.0355e-01, 9.0129e-01, 8.9088e-01,\n",
       "             8.8861e-01, 8.7861e-01, 8.4611e-01, 8.1317e-01, 8.0788e-01, 7.6028e-01,\n",
       "             7.4414e-01, 7.3981e-01, 7.1177e-01, 6.7983e-01, 6.4332e-01, 5.6686e-01,\n",
       "             5.2748e-01, 5.2077e-01, 4.2976e-01, 4.2691e-01, 4.1544e-01, 4.0746e-01,\n",
       "             3.6096e-01, 3.4445e-01, 3.2238e-01, 2.4552e-01, 2.1151e-01, 1.9415e-01,\n",
       "             1.4016e-01, 1.1343e-01, 8.4925e-02, 6.1973e-02, 6.0276e-02, 5.6347e-02,\n",
       "             5.3604e-02, 4.0857e-02, 3.7127e-02, 3.6289e-02, 3.5563e-02, 3.5493e-02,\n",
       "             2.9515e-02, 2.6721e-02, 2.6610e-02, 2.1457e-02, 2.1073e-02, 2.0511e-02,\n",
       "             1.9421e-02, 1.3207e-02, 1.3133e-02, 1.0398e-02, 6.0361e-03, 5.9832e-03,\n",
       "             4.4668e-03, 3.5514e-03, 3.1322e-03, 2.4984e-03, 2.1506e-03, 1.4297e-03,\n",
       "             1.3307e-03, 1.0156e-03, 9.4453e-04, 7.5755e-04, 6.7953e-04, 6.1205e-04,\n",
       "             5.2175e-04, 4.5752e-04, 4.0111e-04, 1.3920e-04, 1.3006e-04, 1.2096e-04,\n",
       "             1.1430e-04, 1.1211e-04, 9.3480e-05, 9.1914e-05, 9.1150e-05, 7.8020e-05,\n",
       "             7.7055e-05, 7.5508e-05, 7.4092e-05, 4.5456e-05, 3.8370e-05, 3.6621e-05,\n",
       "             3.5887e-05, 3.4630e-05, 3.1330e-05, 2.4830e-05, 1.6187e-05, 1.4843e-05,\n",
       "             1.2899e-05, 1.1239e-05, 1.1096e-05, 9.1164e-06, 7.4277e-06, 7.1645e-06,\n",
       "             6.9969e-06, 6.2391e-06, 5.0205e-06, 4.1712e-06, 3.8449e-06, 3.1822e-06,\n",
       "             1.9569e-06, 1.8042e-06, 1.6187e-06, 1.1986e-06, 1.0809e-06, 1.0655e-06,\n",
       "             1.0059e-06, 9.8727e-07, 6.7897e-07, 6.4817e-07, 4.8999e-07, 4.7061e-07,\n",
       "             4.6884e-07, 4.2542e-07, 4.1116e-07, 3.4793e-07, 2.7347e-07, 2.3554e-07,\n",
       "             1.8988e-07, 1.7530e-07, 1.4282e-07, 1.3699e-07, 1.0809e-07, 1.0511e-07,\n",
       "             1.0388e-07, 9.4249e-08, 8.1110e-08, 7.7922e-08, 5.3562e-08, 5.1319e-08,\n",
       "             5.0232e-08, 4.8396e-08, 3.7407e-08, 3.4793e-08, 2.6312e-08, 1.6956e-08,\n",
       "             1.3090e-08, 9.8565e-09, 9.7055e-09, 8.9802e-09, 8.5319e-09, 7.5162e-09,\n",
       "             7.2331e-09, 6.0528e-09, 5.9505e-09, 4.7186e-09, 4.4765e-09, 3.7791e-09,\n",
       "             3.4924e-09, 2.8208e-09, 2.7065e-09, 2.6235e-09, 2.5295e-09, 2.4915e-09,\n",
       "             1.7329e-09, 1.5877e-09, 1.5503e-09, 1.1955e-09, 9.9062e-10, 9.1316e-10,\n",
       "             7.0347e-10, 6.1767e-10, 4.5955e-10, 4.5214e-10, 1.3543e-10, 1.2749e-10,\n",
       "             1.1837e-10, 8.4878e-11, 7.1897e-11, 6.9795e-11, 5.1699e-11, 4.3964e-11,\n",
       "             4.2394e-11, 3.6696e-11, 2.5772e-11, 2.5189e-11, 2.3024e-11, 2.2720e-11,\n",
       "             1.1148e-11, 8.0878e-12, 4.0528e-12, 2.2524e-12, 1.1423e-12, 3.2898e-13,\n",
       "             1.7344e-13, 8.1380e-14, 2.1365e-14, 1.5519e-14, 7.7312e-15, 5.2853e-15,\n",
       "             4.0320e-15, 1.8459e-15, 9.8677e-16, 8.3382e-16, 6.5143e-16, 6.1229e-16,\n",
       "             6.1066e-16, 2.3135e-16, 1.5034e-16, 1.4943e-16, 8.6705e-17, 5.2950e-21])}}],\n",
       "  [{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.5166093928980527),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9613e-01, 9.9480e-01,  ..., 1.1939e-09, 6.3452e-10,\n",
       "             4.0871e-10])}},\n",
       "   {'fpr': np.float64(0.006578947368421052),\n",
       "    'tpr': np.float64(0.8075601374570447),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9539e-01, 9.9523e-01,  ..., 9.4897e-06, 5.1119e-06,\n",
       "             2.9281e-06])}},\n",
       "   {'fpr': np.float64(0.03618421052631579),\n",
       "    'tpr': np.float64(0.9415807560137457),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9929e-01, 9.9902e-01,  ..., 3.2747e-05, 2.6414e-05,\n",
       "             2.5067e-05])}},\n",
       "   {'fpr': np.float64(0.039473684210526314),\n",
       "    'tpr': np.float64(0.9599083619702177),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9954e-01, 9.9927e-01,  ..., 1.4751e-05, 1.4493e-05,\n",
       "             8.1029e-06])}},\n",
       "   {'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.865979381443299),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9971e-01, 9.9953e-01,  ..., 6.4662e-08, 5.4990e-08,\n",
       "             3.8530e-08])}},\n",
       "   {'fpr': np.float64(0.04276315789473684),\n",
       "    'tpr': np.float64(0.97709049255441),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9911e-01, 9.9910e-01,  ..., 3.2590e-05, 2.0359e-05,\n",
       "             1.8481e-05])}},\n",
       "   {'fpr': np.float64(0.003289473684210526),\n",
       "    'tpr': np.float64(0.9392898052691867),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.7436e-09, 1.7701e-09,\n",
       "             9.2837e-10])}},\n",
       "   {'fpr': np.float64(0.06578947368421052),\n",
       "    'tpr': np.float64(0.9873997709049256),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9997e-01, 9.9996e-01,  ..., 7.5083e-07, 6.3422e-07,\n",
       "             4.0186e-07])}},\n",
       "   {'fpr': np.float64(0.12171052631578948),\n",
       "    'tpr': np.float64(0.9965635738831615),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.1348e-06, 8.1142e-07,\n",
       "             2.3587e-07])}},\n",
       "   {'fpr': np.float64(0.006578947368421052),\n",
       "    'tpr': np.float64(0.9656357388316151),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9998e-01, 9.9998e-01,  ..., 4.4753e-08, 9.5980e-09,\n",
       "             2.2391e-09])}},\n",
       "   {'fpr': np.float64(0.046052631578947366),\n",
       "    'tpr': np.float64(0.97709049255441),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.3811e-10, 5.3819e-11,\n",
       "             2.1742e-11])}},\n",
       "   {'fpr': np.float64(0.06907894736842106),\n",
       "    'tpr': np.float64(0.9885452462772051),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099,\n",
       "             0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0197, 0.0230, 0.0263, 0.0263, 0.0263, 0.0263, 0.0296,\n",
       "             0.0329, 0.0362, 0.0395, 0.0428, 0.0428, 0.0428, 0.0461, 0.0461, 0.0461,\n",
       "             0.0493, 0.0493, 0.0526, 0.0559, 0.0592, 0.0625, 0.0658, 0.0658, 0.0658,\n",
       "             0.0691, 0.0691, 0.0724, 0.0724, 0.0724, 0.0757, 0.0789, 0.0789, 0.0789,\n",
       "             0.0822, 0.0855, 0.0888, 0.0921, 0.0954, 0.0987, 0.1020, 0.1053, 0.1086,\n",
       "             0.1086, 0.1086, 0.1118, 0.1151, 0.1184, 0.1217, 0.1250, 0.1283, 0.1316,\n",
       "             0.1316, 0.1349, 0.1382, 0.1414, 0.1447, 0.1480, 0.1513, 0.1546, 0.1546,\n",
       "             0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1776, 0.1809, 0.1842,\n",
       "             0.1875, 0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138,\n",
       "             0.2138, 0.2171, 0.2204, 0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401,\n",
       "             0.2434, 0.2467, 0.2500, 0.2533, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697,\n",
       "             0.2730, 0.2763, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993,\n",
       "             0.3026, 0.3059, 0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289,\n",
       "             0.3322, 0.3355, 0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586,\n",
       "             0.3618, 0.3651, 0.3684, 0.3717, 0.3750, 0.3783, 0.3783, 0.3816, 0.3849,\n",
       "             0.3882, 0.3914, 0.3947, 0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145,\n",
       "             0.4178, 0.4211, 0.4243, 0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441,\n",
       "             0.4474, 0.4507, 0.4539, 0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737,\n",
       "             0.4770, 0.4803, 0.4836, 0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033,\n",
       "             0.5066, 0.5099, 0.5132, 0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329,\n",
       "             0.5362, 0.5395, 0.5428, 0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625,\n",
       "             0.5658, 0.5691, 0.5724, 0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921,\n",
       "             0.5954, 0.5987, 0.6020, 0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217,\n",
       "             0.6250, 0.6283, 0.6316, 0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513,\n",
       "             0.6546, 0.6579, 0.6612, 0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809,\n",
       "             0.6842, 0.6875, 0.6908, 0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105,\n",
       "             0.7138, 0.7171, 0.7204, 0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401,\n",
       "             0.7434, 0.7467, 0.7500, 0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697,\n",
       "             0.7730, 0.7763, 0.7796, 0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993,\n",
       "             0.8026, 0.8059, 0.8092, 0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289,\n",
       "             0.8322, 0.8355, 0.8388, 0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586,\n",
       "             0.8618, 0.8651, 0.8684, 0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882,\n",
       "             0.8914, 0.8947, 0.8980, 0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178,\n",
       "             0.9211, 0.9243, 0.9276, 0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474,\n",
       "             0.9507, 0.9539, 0.9572, 0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770,\n",
       "             0.9803, 0.9836, 0.9868, 0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0069, 0.0229, 0.0367, 0.0504, 0.0641, 0.0733, 0.0848, 0.0905,\n",
       "             0.0951, 0.0974, 0.1100, 0.1134, 0.1191, 0.1271, 0.1363, 0.1409, 0.1455,\n",
       "             0.1512, 0.1569, 0.1592, 0.1638, 0.1661, 0.1707, 0.1730, 0.1821, 0.1856,\n",
       "             0.1936, 0.1982, 0.2027, 0.2050, 0.2096, 0.2131, 0.2188, 0.2199, 0.2245,\n",
       "             0.2257, 0.2279, 0.2314, 0.2325, 0.2348, 0.2383, 0.2417, 0.2440, 0.2463,\n",
       "             0.2509, 0.2532, 0.2554, 0.2612, 0.2646, 0.2658, 0.2680, 0.2692, 0.2703,\n",
       "             0.2726, 0.2761, 0.2795, 0.2818, 0.2841, 0.2864, 0.2875, 0.2887, 0.2910,\n",
       "             0.2932, 0.2944, 0.2967, 0.2978, 0.3013, 0.3024, 0.3036, 0.3058, 0.3081,\n",
       "             0.3093, 0.3104, 0.3127, 0.3139, 0.3150, 0.3173, 0.3196, 0.3219, 0.3230,\n",
       "             0.3242, 0.3253, 0.3288, 0.3310, 0.3345, 0.3368, 0.3379, 0.3402, 0.3414,\n",
       "             0.3448, 0.3459, 0.3471, 0.3482, 0.3494, 0.3517, 0.3528, 0.3551, 0.3574,\n",
       "             0.3608, 0.3620, 0.3631, 0.3654, 0.3677, 0.3688, 0.3711, 0.3723, 0.3734,\n",
       "             0.3757, 0.3780, 0.3792, 0.3803, 0.3837, 0.3860, 0.3872, 0.3883, 0.3895,\n",
       "             0.3906, 0.3918, 0.3952, 0.3963, 0.3986, 0.3998, 0.4009, 0.4021, 0.4032,\n",
       "             0.4055, 0.4066, 0.4078, 0.4112, 0.4124, 0.4135, 0.4147, 0.4158, 0.4181,\n",
       "             0.4204, 0.4227, 0.4238, 0.4261, 0.4284, 0.4296, 0.4307, 0.4318, 0.4330,\n",
       "             0.4341, 0.4353, 0.4364, 0.4387, 0.4399, 0.4410, 0.4433, 0.4456, 0.4467,\n",
       "             0.4479, 0.4490, 0.4502, 0.4513, 0.4525, 0.4536, 0.4559, 0.4570, 0.4605,\n",
       "             0.4616, 0.4639, 0.4651, 0.4662, 0.4685, 0.4696, 0.4719, 0.4731, 0.4742,\n",
       "             0.4777, 0.4788, 0.4800, 0.4811, 0.4822, 0.4834, 0.4845, 0.4857, 0.4868,\n",
       "             0.4880, 0.4891, 0.4903, 0.4914, 0.4926, 0.4937, 0.4960, 0.4971, 0.4994,\n",
       "             0.5006, 0.5017, 0.5040, 0.5052, 0.5074, 0.5097, 0.5109, 0.5120, 0.5132,\n",
       "             0.5143, 0.5166, 0.5189, 0.5200, 0.5212, 0.5223, 0.5235, 0.5246, 0.5258,\n",
       "             0.5269, 0.5281, 0.5292, 0.5304, 0.5315, 0.5326, 0.5338, 0.5349, 0.5361,\n",
       "             0.5384, 0.5395, 0.5407, 0.5418, 0.5430, 0.5441, 0.5452, 0.5464, 0.5475,\n",
       "             0.5487, 0.5498, 0.5510, 0.5521, 0.5533, 0.5544, 0.5567, 0.5578, 0.5590,\n",
       "             0.5601, 0.5613, 0.5624, 0.5636, 0.5647, 0.5659, 0.5670, 0.5682, 0.5693,\n",
       "             0.5704, 0.5716, 0.5727, 0.5739, 0.5750, 0.5762, 0.5773, 0.5785, 0.5796,\n",
       "             0.5808, 0.5819, 0.5830, 0.5842, 0.5853, 0.5865, 0.5876, 0.5888, 0.5899,\n",
       "             0.5911, 0.5922, 0.5934, 0.5945, 0.5956, 0.5968, 0.5979, 0.5991, 0.6002,\n",
       "             0.6014, 0.6025, 0.6037, 0.6048, 0.6060, 0.6071, 0.6082, 0.6094, 0.6105,\n",
       "             0.6117, 0.6128, 0.6140, 0.6151, 0.6163, 0.6174, 0.6186, 0.6197, 0.6208,\n",
       "             0.6220, 0.6243, 0.6254, 0.6266, 0.6277, 0.6289, 0.6300, 0.6312, 0.6323,\n",
       "             0.6334, 0.6346, 0.6357, 0.6369, 0.6380, 0.6392, 0.6403, 0.6415, 0.6438,\n",
       "             0.6449, 0.6460, 0.6472, 0.6483, 0.6495, 0.6506, 0.6529, 0.6541, 0.6552,\n",
       "             0.6575, 0.6586, 0.6598, 0.6609, 0.6621, 0.6632, 0.6644, 0.6667, 0.6678,\n",
       "             0.6690, 0.6701, 0.6712, 0.6724, 0.6735, 0.6747, 0.6758, 0.6770, 0.6781,\n",
       "             0.6793, 0.6804, 0.6816, 0.6827, 0.6838, 0.6850, 0.6861, 0.6873, 0.6884,\n",
       "             0.6896, 0.6907, 0.6919, 0.6930, 0.6942, 0.6953, 0.6964, 0.6976, 0.6987,\n",
       "             0.6999, 0.7010, 0.7022, 0.7033, 0.7045, 0.7056, 0.7068, 0.7079, 0.7090,\n",
       "             0.7102, 0.7113, 0.7125, 0.7136, 0.7148, 0.7159, 0.7171, 0.7182, 0.7194,\n",
       "             0.7205, 0.7216, 0.7228, 0.7239, 0.7251, 0.7262, 0.7274, 0.7285, 0.7297,\n",
       "             0.7308, 0.7320, 0.7331, 0.7342, 0.7354, 0.7365, 0.7377, 0.7388, 0.7400,\n",
       "             0.7411, 0.7423, 0.7434, 0.7446, 0.7457, 0.7468, 0.7480, 0.7491, 0.7503,\n",
       "             0.7514, 0.7526, 0.7537, 0.7549, 0.7560, 0.7572, 0.7583, 0.7595, 0.7606,\n",
       "             0.7617, 0.7629, 0.7640, 0.7652, 0.7663, 0.7675, 0.7686, 0.7698, 0.7709,\n",
       "             0.7721, 0.7732, 0.7743, 0.7755, 0.7766, 0.7778, 0.7789, 0.7801, 0.7812,\n",
       "             0.7824, 0.7835, 0.7847, 0.7858, 0.7869, 0.7881, 0.7892, 0.7904, 0.7915,\n",
       "             0.7927, 0.7938, 0.7950, 0.7961, 0.7973, 0.7984, 0.7995, 0.8007, 0.8018,\n",
       "             0.8030, 0.8041, 0.8053, 0.8076, 0.8087, 0.8099, 0.8110, 0.8121, 0.8133,\n",
       "             0.8144, 0.8156, 0.8167, 0.8179, 0.8190, 0.8202, 0.8213, 0.8225, 0.8236,\n",
       "             0.8247, 0.8259, 0.8270, 0.8282, 0.8293, 0.8305, 0.8316, 0.8328, 0.8339,\n",
       "             0.8351, 0.8362, 0.8373, 0.8385, 0.8396, 0.8408, 0.8419, 0.8431, 0.8442,\n",
       "             0.8454, 0.8465, 0.8477, 0.8488, 0.8499, 0.8511, 0.8522, 0.8534, 0.8545,\n",
       "             0.8557, 0.8568, 0.8580, 0.8591, 0.8603, 0.8614, 0.8625, 0.8637, 0.8648,\n",
       "             0.8660, 0.8671, 0.8683, 0.8694, 0.8706, 0.8717, 0.8729, 0.8740, 0.8751,\n",
       "             0.8763, 0.8774, 0.8786, 0.8797, 0.8809, 0.8820, 0.8832, 0.8843, 0.8855,\n",
       "             0.8866, 0.8877, 0.8889, 0.8900, 0.8912, 0.8923, 0.8935, 0.8946, 0.8946,\n",
       "             0.8958, 0.8969, 0.8992, 0.9003, 0.9015, 0.9026, 0.9038, 0.9049, 0.9061,\n",
       "             0.9072, 0.9084, 0.9095, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9164,\n",
       "             0.9175, 0.9187, 0.9198, 0.9210, 0.9221, 0.9233, 0.9244, 0.9255, 0.9267,\n",
       "             0.9278, 0.9290, 0.9301, 0.9313, 0.9324, 0.9336, 0.9347, 0.9359, 0.9370,\n",
       "             0.9381, 0.9393, 0.9404, 0.9416, 0.9416, 0.9427, 0.9439, 0.9450, 0.9450,\n",
       "             0.9462, 0.9473, 0.9473, 0.9485, 0.9496, 0.9507, 0.9519, 0.9530, 0.9542,\n",
       "             0.9553, 0.9565, 0.9576, 0.9588, 0.9599, 0.9611, 0.9611, 0.9622, 0.9633,\n",
       "             0.9645, 0.9656, 0.9668, 0.9679, 0.9691, 0.9702, 0.9714, 0.9725, 0.9737,\n",
       "             0.9748, 0.9759, 0.9759, 0.9759, 0.9759, 0.9771, 0.9782, 0.9794, 0.9794,\n",
       "             0.9794, 0.9794, 0.9794, 0.9794, 0.9805, 0.9817, 0.9817, 0.9828, 0.9840,\n",
       "             0.9840, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9863, 0.9874,\n",
       "             0.9874, 0.9885, 0.9885, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9970e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01,\n",
       "             9.9966e-01, 9.9966e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9963e-01,\n",
       "             9.9963e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01, 9.9962e-01, 9.9961e-01,\n",
       "             9.9961e-01, 9.9961e-01, 9.9961e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9959e-01, 9.9959e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9955e-01, 9.9954e-01, 9.9954e-01, 9.9954e-01, 9.9954e-01, 9.9953e-01,\n",
       "             9.9953e-01, 9.9952e-01, 9.9951e-01, 9.9951e-01, 9.9948e-01, 9.9948e-01,\n",
       "             9.9946e-01, 9.9946e-01, 9.9943e-01, 9.9939e-01, 9.9939e-01, 9.9938e-01,\n",
       "             9.9938e-01, 9.9937e-01, 9.9936e-01, 9.9934e-01, 9.9933e-01, 9.9933e-01,\n",
       "             9.9926e-01, 9.9926e-01, 9.9926e-01, 9.9924e-01, 9.9921e-01, 9.9920e-01,\n",
       "             9.9920e-01, 9.9919e-01, 9.9918e-01, 9.9913e-01, 9.9911e-01, 9.9910e-01,\n",
       "             9.9909e-01, 9.9908e-01, 9.9908e-01, 9.9907e-01, 9.9905e-01, 9.9903e-01,\n",
       "             9.9896e-01, 9.9894e-01, 9.9892e-01, 9.9890e-01, 9.9889e-01, 9.9885e-01,\n",
       "             9.9884e-01, 9.9883e-01, 9.9882e-01, 9.9877e-01, 9.9875e-01, 9.9873e-01,\n",
       "             9.9870e-01, 9.9862e-01, 9.9862e-01, 9.9860e-01, 9.9858e-01, 9.9856e-01,\n",
       "             9.9856e-01, 9.9849e-01, 9.9849e-01, 9.9848e-01, 9.9845e-01, 9.9842e-01,\n",
       "             9.9840e-01, 9.9835e-01, 9.9834e-01, 9.9834e-01, 9.9828e-01, 9.9827e-01,\n",
       "             9.9827e-01, 9.9825e-01, 9.9815e-01, 9.9807e-01, 9.9797e-01, 9.9789e-01,\n",
       "             9.9787e-01, 9.9785e-01, 9.9783e-01, 9.9778e-01, 9.9774e-01, 9.9749e-01,\n",
       "             9.9747e-01, 9.9745e-01, 9.9742e-01, 9.9738e-01, 9.9729e-01, 9.9725e-01,\n",
       "             9.9721e-01, 9.9719e-01, 9.9712e-01, 9.9709e-01, 9.9689e-01, 9.9683e-01,\n",
       "             9.9665e-01, 9.9651e-01, 9.9647e-01, 9.9647e-01, 9.9645e-01, 9.9634e-01,\n",
       "             9.9630e-01, 9.9622e-01, 9.9614e-01, 9.9564e-01, 9.9552e-01, 9.9530e-01,\n",
       "             9.9524e-01, 9.9521e-01, 9.9520e-01, 9.9495e-01, 9.9491e-01, 9.9487e-01,\n",
       "             9.9484e-01, 9.9396e-01, 9.9361e-01, 9.9359e-01, 9.9359e-01, 9.9333e-01,\n",
       "             9.9287e-01, 9.9232e-01, 9.9223e-01, 9.9204e-01, 9.9201e-01, 9.9147e-01,\n",
       "             9.9088e-01, 9.9073e-01, 9.9004e-01, 9.8988e-01, 9.8964e-01, 9.8938e-01,\n",
       "             9.8897e-01, 9.8877e-01, 9.8820e-01, 9.8689e-01, 9.8663e-01, 9.8657e-01,\n",
       "             9.8547e-01, 9.8507e-01, 9.8319e-01, 9.8318e-01, 9.8317e-01, 9.8243e-01,\n",
       "             9.8094e-01, 9.7775e-01, 9.7642e-01, 9.7535e-01, 9.7521e-01, 9.7327e-01,\n",
       "             9.7178e-01, 9.6797e-01, 9.6648e-01, 9.6486e-01, 9.6328e-01, 9.6283e-01,\n",
       "             9.6026e-01, 9.5978e-01, 9.5957e-01, 9.5807e-01, 9.4911e-01, 9.4264e-01,\n",
       "             9.4168e-01, 9.3170e-01, 9.2405e-01, 8.9711e-01, 8.9490e-01, 8.7687e-01,\n",
       "             8.7508e-01, 8.3980e-01, 8.2747e-01, 7.8543e-01, 7.8521e-01, 7.8188e-01,\n",
       "             7.8133e-01, 7.7481e-01, 7.7140e-01, 7.4328e-01, 7.3345e-01, 7.2804e-01,\n",
       "             7.2746e-01, 7.2559e-01, 6.9863e-01, 6.8947e-01, 6.6287e-01, 6.4442e-01,\n",
       "             6.3965e-01, 6.1435e-01, 5.7295e-01, 5.4386e-01, 5.2212e-01, 4.9323e-01,\n",
       "             4.5311e-01, 4.4312e-01, 3.8432e-01, 2.9583e-01, 2.9255e-01, 2.7722e-01,\n",
       "             2.6071e-01, 2.4245e-01, 1.9914e-01, 1.9064e-01, 1.4612e-01, 1.4573e-01,\n",
       "             1.3007e-01, 1.2331e-01, 1.1158e-01, 1.1068e-01, 7.9803e-02, 7.8859e-02,\n",
       "             7.8376e-02, 7.4642e-02, 6.8343e-02, 6.5441e-02, 6.4254e-02, 5.4179e-02,\n",
       "             5.1158e-02, 5.0219e-02, 4.5704e-02, 4.4529e-02, 4.1932e-02, 4.1320e-02,\n",
       "             3.9651e-02, 3.6182e-02, 3.2276e-02, 3.2165e-02, 3.1022e-02, 2.6524e-02,\n",
       "             2.5364e-02, 2.4172e-02, 1.9421e-02, 1.9400e-02, 1.9149e-02, 1.8994e-02,\n",
       "             1.8690e-02, 1.7845e-02, 1.6922e-02, 1.6316e-02, 1.5419e-02, 1.4806e-02,\n",
       "             1.3832e-02, 1.2909e-02, 1.2828e-02, 1.2773e-02, 1.2313e-02, 1.2241e-02,\n",
       "             9.7571e-03, 7.9543e-03, 7.8635e-03, 7.1079e-03, 6.7822e-03, 6.6166e-03,\n",
       "             6.3433e-03, 6.1058e-03, 5.7953e-03, 5.7443e-03, 5.7155e-03, 5.5428e-03,\n",
       "             5.5338e-03, 5.4749e-03, 5.3267e-03, 5.1172e-03, 4.7432e-03, 4.4948e-03,\n",
       "             4.4403e-03, 4.0044e-03, 3.8266e-03, 3.7742e-03, 3.7351e-03, 3.5498e-03,\n",
       "             3.3085e-03, 3.2899e-03, 3.2213e-03, 3.2151e-03, 3.1355e-03, 2.7991e-03,\n",
       "             2.7095e-03, 2.5321e-03, 2.4869e-03, 2.4611e-03, 2.2439e-03, 2.2157e-03,\n",
       "             2.1640e-03, 2.0976e-03, 2.0791e-03, 2.0515e-03, 2.0475e-03, 1.8702e-03,\n",
       "             1.8008e-03, 1.5186e-03, 1.5114e-03, 1.4978e-03, 1.4525e-03, 1.2441e-03,\n",
       "             1.2439e-03, 1.1461e-03, 1.1278e-03, 1.1248e-03, 1.0957e-03, 1.0442e-03,\n",
       "             1.0265e-03, 9.9555e-04, 8.3854e-04, 7.9560e-04, 6.2250e-04, 6.2226e-04,\n",
       "             5.9866e-04, 5.8874e-04, 5.7844e-04, 5.6126e-04, 5.3311e-04, 5.1989e-04,\n",
       "             4.6351e-04, 4.3084e-04, 4.2004e-04, 3.8069e-04, 3.8056e-04, 3.7269e-04,\n",
       "             3.6283e-04, 3.4353e-04, 2.8376e-04, 2.5799e-04, 2.2871e-04, 2.2786e-04,\n",
       "             2.2334e-04, 2.2260e-04, 2.1962e-04, 2.1782e-04, 2.0829e-04, 2.0642e-04,\n",
       "             2.0078e-04, 1.9875e-04, 1.9137e-04, 1.8321e-04, 1.7770e-04, 1.6899e-04,\n",
       "             1.6619e-04, 1.5862e-04, 1.4721e-04, 1.4327e-04, 1.3405e-04, 1.0692e-04,\n",
       "             1.0383e-04, 9.1328e-05, 8.6115e-05, 7.9680e-05, 7.6850e-05, 7.4613e-05,\n",
       "             7.1281e-05, 6.8412e-05, 6.6844e-05, 6.5533e-05, 6.4643e-05, 6.3582e-05,\n",
       "             6.3295e-05, 6.2300e-05, 6.2229e-05, 6.0678e-05, 5.8061e-05, 5.6323e-05,\n",
       "             5.5378e-05, 5.3648e-05, 5.3376e-05, 5.2235e-05, 5.1645e-05, 4.3321e-05,\n",
       "             4.2587e-05, 3.9726e-05, 3.8821e-05, 3.8756e-05, 3.8258e-05, 3.7332e-05,\n",
       "             3.6614e-05, 3.5466e-05, 2.9816e-05, 2.9723e-05, 2.9259e-05, 2.9114e-05,\n",
       "             2.7997e-05, 2.7399e-05, 2.6628e-05, 2.6261e-05, 2.6253e-05, 2.4901e-05,\n",
       "             2.3749e-05, 2.3020e-05, 2.0848e-05, 2.0476e-05, 2.0334e-05, 1.9316e-05,\n",
       "             1.8216e-05, 1.5373e-05, 1.5051e-05, 1.4994e-05, 1.3936e-05, 1.3032e-05,\n",
       "             1.2935e-05, 1.2639e-05, 1.1402e-05, 1.1287e-05, 1.0302e-05, 1.0218e-05,\n",
       "             8.8336e-06, 8.7398e-06, 8.7053e-06, 8.2012e-06, 8.1368e-06, 7.7428e-06,\n",
       "             7.4454e-06, 6.7392e-06, 6.4426e-06, 6.2439e-06, 5.6845e-06, 5.0643e-06,\n",
       "             4.8473e-06, 4.2525e-06, 4.2139e-06, 4.2073e-06, 3.9802e-06, 3.7324e-06,\n",
       "             3.4514e-06, 3.2606e-06, 3.1674e-06, 3.1206e-06, 3.0645e-06, 2.9240e-06,\n",
       "             2.6053e-06, 2.5455e-06, 2.4104e-06, 2.2027e-06, 2.1172e-06, 2.0984e-06,\n",
       "             1.9502e-06, 1.7999e-06, 1.5928e-06, 1.2226e-06, 1.1045e-06, 1.0522e-06,\n",
       "             1.0288e-06, 9.8380e-07, 8.4461e-07, 7.7413e-07, 7.7195e-07, 6.5044e-07,\n",
       "             6.4220e-07, 6.1785e-07, 6.1424e-07, 5.7765e-07, 5.7072e-07, 5.2319e-07,\n",
       "             4.6906e-07, 4.4940e-07, 4.4753e-07, 4.2360e-07, 3.9055e-07, 3.7973e-07,\n",
       "             3.6209e-07, 3.5923e-07, 2.1635e-07, 1.8441e-07, 1.7259e-07, 1.3369e-07,\n",
       "             1.0980e-07, 5.7502e-08, 4.5071e-08, 4.1612e-08, 4.0214e-08, 3.3063e-08,\n",
       "             3.0197e-08, 2.9940e-08, 2.1662e-08, 1.8913e-08, 1.7388e-08, 1.5393e-08,\n",
       "             1.3987e-08, 1.1156e-08, 2.9825e-09, 2.8368e-09, 2.1659e-09, 1.1309e-09,\n",
       "             4.9948e-10, 4.2765e-10, 3.4285e-10, 3.5979e-11])}},\n",
       "   {'fpr': np.float64(0.0756578947368421),\n",
       "    'tpr': np.float64(0.9885452462772051),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0230, 0.0263, 0.0296, 0.0329, 0.0329,\n",
       "             0.0362, 0.0362, 0.0395, 0.0428, 0.0428, 0.0461, 0.0493, 0.0526, 0.0559,\n",
       "             0.0559, 0.0559, 0.0559, 0.0592, 0.0625, 0.0625, 0.0625, 0.0658, 0.0691,\n",
       "             0.0724, 0.0757, 0.0789, 0.0822, 0.0855, 0.0888, 0.0888, 0.0921, 0.0954,\n",
       "             0.0987, 0.1020, 0.1053, 0.1086, 0.1086, 0.1118, 0.1151, 0.1184, 0.1184,\n",
       "             0.1217, 0.1250, 0.1250, 0.1283, 0.1316, 0.1349, 0.1382, 0.1414, 0.1414,\n",
       "             0.1414, 0.1447, 0.1480, 0.1513, 0.1546, 0.1579, 0.1612, 0.1645, 0.1678,\n",
       "             0.1678, 0.1711, 0.1711, 0.1743, 0.1776, 0.1809, 0.1842, 0.1875, 0.1908,\n",
       "             0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138, 0.2171, 0.2204,\n",
       "             0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2500,\n",
       "             0.2500, 0.2533, 0.2566, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697, 0.2730,\n",
       "             0.2763, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026,\n",
       "             0.3059, 0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322,\n",
       "             0.3355, 0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618,\n",
       "             0.3651, 0.3684, 0.3717, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914,\n",
       "             0.3947, 0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211,\n",
       "             0.4243, 0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507,\n",
       "             0.4539, 0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803,\n",
       "             0.4836, 0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099,\n",
       "             0.5132, 0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395,\n",
       "             0.5428, 0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691,\n",
       "             0.5724, 0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987,\n",
       "             0.6020, 0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283,\n",
       "             0.6316, 0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579,\n",
       "             0.6612, 0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875,\n",
       "             0.6908, 0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171,\n",
       "             0.7204, 0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467,\n",
       "             0.7500, 0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763,\n",
       "             0.7796, 0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059,\n",
       "             0.8092, 0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355,\n",
       "             0.8388, 0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651,\n",
       "             0.8684, 0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947,\n",
       "             0.8980, 0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243,\n",
       "             0.9276, 0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539,\n",
       "             0.9572, 0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836,\n",
       "             0.9868, 0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0034, 0.0092, 0.0160, 0.0206, 0.0263, 0.0298, 0.0355, 0.0435,\n",
       "             0.0481, 0.0550, 0.0584, 0.0619, 0.0722, 0.0779, 0.0825, 0.0859, 0.0905,\n",
       "             0.0916, 0.0928, 0.0997, 0.1054, 0.1111, 0.1157, 0.1168, 0.1191, 0.1203,\n",
       "             0.1226, 0.1260, 0.1271, 0.1283, 0.1294, 0.1317, 0.1352, 0.1397, 0.1432,\n",
       "             0.1501, 0.1558, 0.1569, 0.1604, 0.1615, 0.1627, 0.1638, 0.1672, 0.1695,\n",
       "             0.1707, 0.1753, 0.1775, 0.1798, 0.1821, 0.1833, 0.1856, 0.1879, 0.1901,\n",
       "             0.1936, 0.1970, 0.1993, 0.2005, 0.2016, 0.2027, 0.2062, 0.2073, 0.2085,\n",
       "             0.2108, 0.2131, 0.2142, 0.2153, 0.2176, 0.2199, 0.2222, 0.2257, 0.2279,\n",
       "             0.2302, 0.2314, 0.2325, 0.2348, 0.2360, 0.2383, 0.2394, 0.2405, 0.2417,\n",
       "             0.2428, 0.2440, 0.2451, 0.2474, 0.2486, 0.2509, 0.2532, 0.2543, 0.2554,\n",
       "             0.2566, 0.2577, 0.2612, 0.2635, 0.2658, 0.2669, 0.2680, 0.2692, 0.2703,\n",
       "             0.2715, 0.2738, 0.2772, 0.2795, 0.2806, 0.2818, 0.2829, 0.2841, 0.2864,\n",
       "             0.2887, 0.2898, 0.2910, 0.2921, 0.2932, 0.2944, 0.2967, 0.2990, 0.3001,\n",
       "             0.3013, 0.3024, 0.3058, 0.3081, 0.3093, 0.3116, 0.3127, 0.3162, 0.3173,\n",
       "             0.3184, 0.3196, 0.3207, 0.3230, 0.3242, 0.3253, 0.3265, 0.3288, 0.3310,\n",
       "             0.3322, 0.3333, 0.3345, 0.3356, 0.3368, 0.3379, 0.3391, 0.3402, 0.3414,\n",
       "             0.3425, 0.3436, 0.3471, 0.3494, 0.3517, 0.3528, 0.3562, 0.3574, 0.3585,\n",
       "             0.3608, 0.3631, 0.3643, 0.3654, 0.3666, 0.3688, 0.3700, 0.3734, 0.3757,\n",
       "             0.3769, 0.3780, 0.3792, 0.3803, 0.3814, 0.3826, 0.3837, 0.3849, 0.3860,\n",
       "             0.3872, 0.3883, 0.3906, 0.3929, 0.3952, 0.3963, 0.3975, 0.4009, 0.4021,\n",
       "             0.4032, 0.4044, 0.4055, 0.4066, 0.4078, 0.4089, 0.4101, 0.4112, 0.4124,\n",
       "             0.4147, 0.4158, 0.4181, 0.4192, 0.4215, 0.4227, 0.4238, 0.4250, 0.4261,\n",
       "             0.4273, 0.4284, 0.4296, 0.4318, 0.4330, 0.4341, 0.4353, 0.4364, 0.4376,\n",
       "             0.4387, 0.4399, 0.4410, 0.4422, 0.4433, 0.4444, 0.4456, 0.4467, 0.4490,\n",
       "             0.4502, 0.4525, 0.4536, 0.4548, 0.4559, 0.4570, 0.4582, 0.4593, 0.4605,\n",
       "             0.4616, 0.4628, 0.4639, 0.4651, 0.4662, 0.4674, 0.4696, 0.4708, 0.4719,\n",
       "             0.4742, 0.4754, 0.4777, 0.4788, 0.4800, 0.4811, 0.4822, 0.4834, 0.4845,\n",
       "             0.4857, 0.4868, 0.4880, 0.4891, 0.4903, 0.4914, 0.4926, 0.4937, 0.4948,\n",
       "             0.4960, 0.4971, 0.4983, 0.4994, 0.5006, 0.5029, 0.5040, 0.5052, 0.5063,\n",
       "             0.5074, 0.5086, 0.5097, 0.5109, 0.5120, 0.5132, 0.5143, 0.5155, 0.5166,\n",
       "             0.5178, 0.5189, 0.5200, 0.5212, 0.5223, 0.5258, 0.5269, 0.5281, 0.5292,\n",
       "             0.5304, 0.5326, 0.5338, 0.5349, 0.5361, 0.5372, 0.5395, 0.5407, 0.5418,\n",
       "             0.5430, 0.5441, 0.5464, 0.5475, 0.5487, 0.5498, 0.5510, 0.5521, 0.5544,\n",
       "             0.5556, 0.5567, 0.5578, 0.5590, 0.5601, 0.5613, 0.5624, 0.5636, 0.5647,\n",
       "             0.5659, 0.5670, 0.5682, 0.5693, 0.5704, 0.5716, 0.5727, 0.5739, 0.5750,\n",
       "             0.5762, 0.5773, 0.5785, 0.5796, 0.5808, 0.5819, 0.5830, 0.5842, 0.5853,\n",
       "             0.5865, 0.5876, 0.5888, 0.5899, 0.5911, 0.5922, 0.5934, 0.5945, 0.5956,\n",
       "             0.5968, 0.5979, 0.5991, 0.6002, 0.6014, 0.6025, 0.6037, 0.6048, 0.6060,\n",
       "             0.6071, 0.6082, 0.6094, 0.6105, 0.6117, 0.6128, 0.6140, 0.6151, 0.6163,\n",
       "             0.6174, 0.6186, 0.6197, 0.6208, 0.6220, 0.6231, 0.6243, 0.6254, 0.6266,\n",
       "             0.6277, 0.6289, 0.6300, 0.6312, 0.6323, 0.6334, 0.6346, 0.6357, 0.6369,\n",
       "             0.6380, 0.6392, 0.6403, 0.6415, 0.6426, 0.6438, 0.6449, 0.6460, 0.6483,\n",
       "             0.6495, 0.6506, 0.6518, 0.6529, 0.6541, 0.6552, 0.6564, 0.6575, 0.6586,\n",
       "             0.6598, 0.6609, 0.6621, 0.6632, 0.6644, 0.6655, 0.6667, 0.6678, 0.6690,\n",
       "             0.6701, 0.6712, 0.6724, 0.6735, 0.6747, 0.6758, 0.6770, 0.6781, 0.6793,\n",
       "             0.6804, 0.6816, 0.6827, 0.6838, 0.6850, 0.6861, 0.6873, 0.6884, 0.6896,\n",
       "             0.6907, 0.6919, 0.6930, 0.6942, 0.6953, 0.6964, 0.6976, 0.6987, 0.6999,\n",
       "             0.7022, 0.7045, 0.7056, 0.7068, 0.7079, 0.7090, 0.7102, 0.7113, 0.7125,\n",
       "             0.7136, 0.7148, 0.7159, 0.7171, 0.7182, 0.7194, 0.7205, 0.7216, 0.7228,\n",
       "             0.7239, 0.7251, 0.7262, 0.7274, 0.7285, 0.7297, 0.7308, 0.7320, 0.7331,\n",
       "             0.7342, 0.7365, 0.7377, 0.7388, 0.7400, 0.7411, 0.7411, 0.7423, 0.7434,\n",
       "             0.7446, 0.7457, 0.7468, 0.7480, 0.7491, 0.7503, 0.7514, 0.7526, 0.7537,\n",
       "             0.7549, 0.7560, 0.7572, 0.7583, 0.7595, 0.7606, 0.7617, 0.7629, 0.7640,\n",
       "             0.7652, 0.7663, 0.7675, 0.7686, 0.7698, 0.7709, 0.7721, 0.7732, 0.7743,\n",
       "             0.7755, 0.7766, 0.7778, 0.7789, 0.7801, 0.7812, 0.7824, 0.7835, 0.7847,\n",
       "             0.7858, 0.7869, 0.7881, 0.7892, 0.7904, 0.7915, 0.7927, 0.7938, 0.7950,\n",
       "             0.7961, 0.7973, 0.7984, 0.7995, 0.8007, 0.8018, 0.8030, 0.8041, 0.8053,\n",
       "             0.8064, 0.8076, 0.8087, 0.8099, 0.8110, 0.8121, 0.8133, 0.8144, 0.8156,\n",
       "             0.8167, 0.8179, 0.8190, 0.8202, 0.8213, 0.8225, 0.8236, 0.8247, 0.8259,\n",
       "             0.8270, 0.8282, 0.8293, 0.8305, 0.8316, 0.8328, 0.8339, 0.8351, 0.8362,\n",
       "             0.8373, 0.8385, 0.8396, 0.8408, 0.8419, 0.8431, 0.8442, 0.8454, 0.8465,\n",
       "             0.8477, 0.8488, 0.8499, 0.8511, 0.8522, 0.8534, 0.8545, 0.8557, 0.8568,\n",
       "             0.8580, 0.8591, 0.8603, 0.8614, 0.8625, 0.8637, 0.8648, 0.8660, 0.8671,\n",
       "             0.8683, 0.8694, 0.8706, 0.8717, 0.8729, 0.8740, 0.8751, 0.8763, 0.8774,\n",
       "             0.8786, 0.8797, 0.8809, 0.8820, 0.8832, 0.8843, 0.8855, 0.8866, 0.8877,\n",
       "             0.8889, 0.8900, 0.8912, 0.8923, 0.8935, 0.8946, 0.8958, 0.8969, 0.8981,\n",
       "             0.8992, 0.9003, 0.9015, 0.9026, 0.9038, 0.9049, 0.9061, 0.9072, 0.9084,\n",
       "             0.9095, 0.9107, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9164, 0.9175,\n",
       "             0.9187, 0.9198, 0.9210, 0.9221, 0.9233, 0.9244, 0.9255, 0.9267, 0.9278,\n",
       "             0.9290, 0.9301, 0.9313, 0.9324, 0.9336, 0.9347, 0.9359, 0.9370, 0.9381,\n",
       "             0.9393, 0.9404, 0.9416, 0.9427, 0.9439, 0.9450, 0.9462, 0.9473, 0.9485,\n",
       "             0.9496, 0.9507, 0.9507, 0.9519, 0.9519, 0.9530, 0.9542, 0.9553, 0.9565,\n",
       "             0.9576, 0.9588, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668,\n",
       "             0.9679, 0.9691, 0.9702, 0.9702, 0.9714, 0.9725, 0.9725, 0.9737, 0.9748,\n",
       "             0.9759, 0.9771, 0.9782, 0.9794, 0.9794, 0.9794, 0.9794, 0.9794, 0.9805,\n",
       "             0.9805, 0.9817, 0.9817, 0.9817, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828,\n",
       "             0.9840, 0.9851, 0.9863, 0.9863, 0.9863, 0.9874, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9897, 0.9897, 0.9897,\n",
       "             0.9897, 0.9897, 0.9897, 0.9897, 0.9908, 0.9908, 0.9908, 0.9908, 0.9920,\n",
       "             0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9943,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9966, 0.9966, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9968e-01, 9.9968e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9968e-01, 9.9967e-01, 9.9967e-01, 9.9967e-01, 9.9967e-01,\n",
       "             9.9967e-01, 9.9967e-01, 9.9967e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01,\n",
       "             9.9965e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01, 9.9961e-01, 9.9961e-01,\n",
       "             9.9958e-01, 9.9958e-01, 9.9958e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9957e-01, 9.9957e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9955e-01,\n",
       "             9.9955e-01, 9.9955e-01, 9.9954e-01, 9.9953e-01, 9.9952e-01, 9.9952e-01,\n",
       "             9.9952e-01, 9.9948e-01, 9.9948e-01, 9.9948e-01, 9.9947e-01, 9.9944e-01,\n",
       "             9.9944e-01, 9.9944e-01, 9.9944e-01, 9.9944e-01, 9.9944e-01, 9.9943e-01,\n",
       "             9.9942e-01, 9.9941e-01, 9.9941e-01, 9.9938e-01, 9.9938e-01, 9.9937e-01,\n",
       "             9.9937e-01, 9.9935e-01, 9.9935e-01, 9.9934e-01, 9.9934e-01, 9.9934e-01,\n",
       "             9.9932e-01, 9.9932e-01, 9.9930e-01, 9.9930e-01, 9.9927e-01, 9.9926e-01,\n",
       "             9.9926e-01, 9.9925e-01, 9.9924e-01, 9.9923e-01, 9.9923e-01, 9.9921e-01,\n",
       "             9.9919e-01, 9.9919e-01, 9.9917e-01, 9.9917e-01, 9.9916e-01, 9.9916e-01,\n",
       "             9.9916e-01, 9.9916e-01, 9.9915e-01, 9.9913e-01, 9.9910e-01, 9.9910e-01,\n",
       "             9.9908e-01, 9.9908e-01, 9.9906e-01, 9.9905e-01, 9.9904e-01, 9.9900e-01,\n",
       "             9.9900e-01, 9.9898e-01, 9.9898e-01, 9.9896e-01, 9.9895e-01, 9.9893e-01,\n",
       "             9.9892e-01, 9.9884e-01, 9.9881e-01, 9.9881e-01, 9.9880e-01, 9.9876e-01,\n",
       "             9.9873e-01, 9.9873e-01, 9.9872e-01, 9.9871e-01, 9.9870e-01, 9.9868e-01,\n",
       "             9.9865e-01, 9.9864e-01, 9.9862e-01, 9.9857e-01, 9.9856e-01, 9.9853e-01,\n",
       "             9.9853e-01, 9.9853e-01, 9.9850e-01, 9.9848e-01, 9.9843e-01, 9.9843e-01,\n",
       "             9.9842e-01, 9.9842e-01, 9.9838e-01, 9.9838e-01, 9.9834e-01, 9.9834e-01,\n",
       "             9.9831e-01, 9.9829e-01, 9.9824e-01, 9.9823e-01, 9.9820e-01, 9.9817e-01,\n",
       "             9.9817e-01, 9.9817e-01, 9.9816e-01, 9.9810e-01, 9.9803e-01, 9.9798e-01,\n",
       "             9.9798e-01, 9.9797e-01, 9.9793e-01, 9.9788e-01, 9.9785e-01, 9.9783e-01,\n",
       "             9.9778e-01, 9.9778e-01, 9.9778e-01, 9.9774e-01, 9.9773e-01, 9.9773e-01,\n",
       "             9.9772e-01, 9.9764e-01, 9.9745e-01, 9.9744e-01, 9.9738e-01, 9.9735e-01,\n",
       "             9.9730e-01, 9.9727e-01, 9.9679e-01, 9.9679e-01, 9.9667e-01, 9.9655e-01,\n",
       "             9.9646e-01, 9.9636e-01, 9.9633e-01, 9.9593e-01, 9.9591e-01, 9.9590e-01,\n",
       "             9.9566e-01, 9.9537e-01, 9.9518e-01, 9.9516e-01, 9.9514e-01, 9.9493e-01,\n",
       "             9.9488e-01, 9.9478e-01, 9.9443e-01, 9.9425e-01, 9.9406e-01, 9.9386e-01,\n",
       "             9.9364e-01, 9.9347e-01, 9.9340e-01, 9.9307e-01, 9.9293e-01, 9.9277e-01,\n",
       "             9.9276e-01, 9.9162e-01, 9.9141e-01, 9.9128e-01, 9.9113e-01, 9.9108e-01,\n",
       "             9.9100e-01, 9.9058e-01, 9.9054e-01, 9.8871e-01, 9.8828e-01, 9.8784e-01,\n",
       "             9.8740e-01, 9.8561e-01, 9.8354e-01, 9.8333e-01, 9.8268e-01, 9.8249e-01,\n",
       "             9.8169e-01, 9.7927e-01, 9.7628e-01, 9.7128e-01, 9.6721e-01, 9.6683e-01,\n",
       "             9.6505e-01, 9.6261e-01, 9.6075e-01, 9.5993e-01, 9.5769e-01, 9.5653e-01,\n",
       "             9.5544e-01, 9.5506e-01, 9.5449e-01, 9.5163e-01, 9.5041e-01, 9.4819e-01,\n",
       "             9.4627e-01, 9.4543e-01, 9.4502e-01, 9.4441e-01, 9.4132e-01, 9.3877e-01,\n",
       "             9.3612e-01, 9.2813e-01, 9.1363e-01, 9.0942e-01, 9.0591e-01, 8.9901e-01,\n",
       "             8.9643e-01, 8.9259e-01, 8.8322e-01, 8.8067e-01, 8.7687e-01, 8.5220e-01,\n",
       "             8.4678e-01, 8.4455e-01, 8.3252e-01, 8.2329e-01, 8.2216e-01, 8.1959e-01,\n",
       "             8.1889e-01, 8.0920e-01, 7.8950e-01, 7.8103e-01, 7.3829e-01, 7.3735e-01,\n",
       "             7.2703e-01, 7.0247e-01, 6.9002e-01, 5.9925e-01, 5.0087e-01, 4.4741e-01,\n",
       "             4.4641e-01, 4.3707e-01, 4.2658e-01, 3.8659e-01, 3.8251e-01, 3.4542e-01,\n",
       "             3.4488e-01, 3.1040e-01, 2.8407e-01, 2.1414e-01, 1.9727e-01, 1.9181e-01,\n",
       "             1.8754e-01, 1.8480e-01, 1.7794e-01, 1.7297e-01, 1.6540e-01, 1.6465e-01,\n",
       "             1.3688e-01, 1.3023e-01, 1.2266e-01, 1.2144e-01, 1.0831e-01, 1.0829e-01,\n",
       "             1.0066e-01, 1.0002e-01, 9.7849e-02, 9.1787e-02, 8.7234e-02, 8.4198e-02,\n",
       "             8.1685e-02, 7.8333e-02, 6.9576e-02, 6.7585e-02, 6.3788e-02, 6.0175e-02,\n",
       "             5.4506e-02, 5.4072e-02, 5.3733e-02, 5.3046e-02, 5.0182e-02, 5.0017e-02,\n",
       "             4.4648e-02, 4.2984e-02, 4.2268e-02, 3.8757e-02, 3.8554e-02, 3.4950e-02,\n",
       "             3.3015e-02, 3.2502e-02, 2.4192e-02, 2.3039e-02, 2.1245e-02, 1.7077e-02,\n",
       "             1.6549e-02, 1.3021e-02, 1.2969e-02, 1.2707e-02, 1.2696e-02, 1.2547e-02,\n",
       "             1.2530e-02, 1.2147e-02, 1.1164e-02, 9.3260e-03, 8.7908e-03, 8.5821e-03,\n",
       "             8.3293e-03, 8.2250e-03, 7.9162e-03, 6.7349e-03, 6.5002e-03, 6.4062e-03,\n",
       "             6.2558e-03, 5.7851e-03, 4.3222e-03, 3.9839e-03, 3.9476e-03, 3.7782e-03,\n",
       "             3.6185e-03, 3.4812e-03, 3.4742e-03, 3.4561e-03, 3.3989e-03, 3.3915e-03,\n",
       "             3.0189e-03, 2.7354e-03, 2.7329e-03, 2.7255e-03, 2.6855e-03, 2.3266e-03,\n",
       "             2.2664e-03, 2.1492e-03, 2.1126e-03, 2.0983e-03, 2.0908e-03, 2.0527e-03,\n",
       "             2.0354e-03, 1.9300e-03, 1.7097e-03, 1.5319e-03, 1.4727e-03, 1.4690e-03,\n",
       "             1.2069e-03, 1.1002e-03, 1.0028e-03, 8.2583e-04, 8.0942e-04, 8.0139e-04,\n",
       "             7.6732e-04, 7.5439e-04, 7.0459e-04, 6.3387e-04, 6.2621e-04, 6.1586e-04,\n",
       "             5.9989e-04, 5.9955e-04, 5.6116e-04, 5.5396e-04, 5.4854e-04, 5.4584e-04,\n",
       "             5.2289e-04, 5.1343e-04, 4.9699e-04, 4.5865e-04, 4.1337e-04, 4.0783e-04,\n",
       "             3.9763e-04, 3.6771e-04, 3.4736e-04, 3.4406e-04, 3.2177e-04, 2.9119e-04,\n",
       "             2.5550e-04, 2.4847e-04, 2.2670e-04, 1.9610e-04, 1.8402e-04, 1.7418e-04,\n",
       "             1.5390e-04, 1.2928e-04, 1.2743e-04, 1.2039e-04, 1.1517e-04, 1.0461e-04,\n",
       "             1.0265e-04, 9.3433e-05, 7.6179e-05, 7.5775e-05, 7.2162e-05, 6.8055e-05,\n",
       "             6.6720e-05, 6.4426e-05, 6.3914e-05, 6.2899e-05, 6.1659e-05, 5.9742e-05,\n",
       "             5.7830e-05, 5.7666e-05, 5.5740e-05, 5.4529e-05, 5.4078e-05, 5.3887e-05,\n",
       "             5.1506e-05, 5.0589e-05, 5.0369e-05, 4.9422e-05, 4.5250e-05, 4.4499e-05,\n",
       "             4.3885e-05, 4.3575e-05, 4.2608e-05, 4.0517e-05, 3.6961e-05, 3.6186e-05,\n",
       "             3.5464e-05, 3.5433e-05, 2.7854e-05, 2.7726e-05, 2.6156e-05, 2.5685e-05,\n",
       "             2.4607e-05, 2.3508e-05, 2.1863e-05, 2.1697e-05, 2.1227e-05, 2.0769e-05,\n",
       "             2.0514e-05, 1.9925e-05, 1.8772e-05, 1.8562e-05, 1.7184e-05, 1.6708e-05,\n",
       "             1.6334e-05, 1.5941e-05, 1.5543e-05, 1.3510e-05, 1.3453e-05, 1.2961e-05,\n",
       "             1.1690e-05, 1.1280e-05, 1.0545e-05, 9.4593e-06, 8.2024e-06, 8.1712e-06,\n",
       "             7.6349e-06, 7.1514e-06, 6.6396e-06, 6.2620e-06, 5.2105e-06, 5.1551e-06,\n",
       "             5.1451e-06, 5.0419e-06, 4.6538e-06, 4.3477e-06, 4.2924e-06, 4.0031e-06,\n",
       "             3.5308e-06, 3.4885e-06, 3.3502e-06, 3.2549e-06, 3.2181e-06, 2.9062e-06,\n",
       "             2.4250e-06, 2.4191e-06, 2.3987e-06, 2.3335e-06, 2.1850e-06, 2.1820e-06,\n",
       "             2.0395e-06, 1.9305e-06, 1.8508e-06, 1.7212e-06, 1.6770e-06, 1.5436e-06,\n",
       "             1.5289e-06, 1.4532e-06, 1.3192e-06, 1.0909e-06, 1.0740e-06, 9.4223e-07,\n",
       "             8.2203e-07, 8.0272e-07, 7.0777e-07, 6.7663e-07, 6.5870e-07, 5.9748e-07,\n",
       "             4.3699e-07, 4.3565e-07, 4.2319e-07, 4.1971e-07, 3.7315e-07, 3.7075e-07,\n",
       "             3.6501e-07, 3.0923e-07, 2.9375e-07, 2.7710e-07, 2.5875e-07, 2.3380e-07,\n",
       "             2.0817e-07, 1.8307e-07, 1.4645e-07, 1.2867e-07, 1.0938e-07, 1.0104e-07,\n",
       "             7.6695e-08, 5.7352e-08, 3.5388e-08, 3.4265e-08, 3.2773e-08, 3.1812e-08,\n",
       "             2.5705e-08, 2.4316e-08, 2.0855e-08, 1.8554e-08, 1.5984e-08, 1.5111e-08,\n",
       "             1.2905e-08, 8.5651e-09, 7.9152e-09, 6.1324e-09, 6.0513e-09, 5.4353e-09,\n",
       "             2.3807e-09, 1.4317e-09, 9.2194e-10, 9.2015e-10, 5.7000e-10, 4.7494e-11,\n",
       "             7.3970e-12, 3.0041e-12])}},\n",
       "   {'fpr': np.float64(0.10526315789473684),\n",
       "    'tpr': np.float64(0.9919816723940436),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0132, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0230, 0.0230, 0.0230, 0.0263, 0.0296,\n",
       "             0.0296, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0362, 0.0362, 0.0362,\n",
       "             0.0395, 0.0428, 0.0461, 0.0493, 0.0493, 0.0493, 0.0526, 0.0526, 0.0559,\n",
       "             0.0592, 0.0625, 0.0658, 0.0691, 0.0724, 0.0757, 0.0789, 0.0822, 0.0855,\n",
       "             0.0855, 0.0855, 0.0855, 0.0888, 0.0888, 0.0921, 0.0954, 0.0954, 0.0987,\n",
       "             0.1020, 0.1053, 0.1053, 0.1053, 0.1086, 0.1118, 0.1151, 0.1151, 0.1184,\n",
       "             0.1217, 0.1250, 0.1283, 0.1283, 0.1316, 0.1316, 0.1349, 0.1382, 0.1414,\n",
       "             0.1447, 0.1480, 0.1513, 0.1546, 0.1546, 0.1579, 0.1612, 0.1645, 0.1678,\n",
       "             0.1711, 0.1743, 0.1776, 0.1809, 0.1842, 0.1875, 0.1908, 0.1941, 0.1974,\n",
       "             0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138, 0.2171, 0.2204, 0.2237,\n",
       "             0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2467, 0.2500,\n",
       "             0.2533, 0.2566, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697, 0.2730, 0.2763,\n",
       "             0.2796, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026, 0.3059,\n",
       "             0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322, 0.3355,\n",
       "             0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618, 0.3651,\n",
       "             0.3684, 0.3717, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914, 0.3947,\n",
       "             0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243,\n",
       "             0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539,\n",
       "             0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836,\n",
       "             0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132,\n",
       "             0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428,\n",
       "             0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724,\n",
       "             0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020,\n",
       "             0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316,\n",
       "             0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612,\n",
       "             0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908,\n",
       "             0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204,\n",
       "             0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500,\n",
       "             0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796,\n",
       "             0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092,\n",
       "             0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388,\n",
       "             0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684,\n",
       "             0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980,\n",
       "             0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276,\n",
       "             0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572,\n",
       "             0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868,\n",
       "             0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.2497, 0.3711, 0.4101, 0.4502, 0.4777, 0.4983, 0.5155, 0.5292,\n",
       "             0.5407, 0.5487, 0.5578, 0.5682, 0.5808, 0.5899, 0.5968, 0.6002, 0.6025,\n",
       "             0.6071, 0.6140, 0.6197, 0.6231, 0.6312, 0.6403, 0.6426, 0.6460, 0.6483,\n",
       "             0.6495, 0.6529, 0.6564, 0.6598, 0.6632, 0.6701, 0.6747, 0.6781, 0.6804,\n",
       "             0.6816, 0.6827, 0.6838, 0.6873, 0.6907, 0.6942, 0.6964, 0.6987, 0.6999,\n",
       "             0.7022, 0.7068, 0.7102, 0.7113, 0.7136, 0.7148, 0.7205, 0.7216, 0.7228,\n",
       "             0.7239, 0.7251, 0.7262, 0.7274, 0.7285, 0.7297, 0.7320, 0.7365, 0.7377,\n",
       "             0.7400, 0.7411, 0.7423, 0.7434, 0.7446, 0.7457, 0.7491, 0.7514, 0.7526,\n",
       "             0.7537, 0.7560, 0.7572, 0.7583, 0.7595, 0.7606, 0.7617, 0.7629, 0.7640,\n",
       "             0.7652, 0.7663, 0.7675, 0.7686, 0.7698, 0.7709, 0.7721, 0.7743, 0.7755,\n",
       "             0.7766, 0.7789, 0.7801, 0.7812, 0.7824, 0.7835, 0.7847, 0.7881, 0.7892,\n",
       "             0.7904, 0.7915, 0.7915, 0.7927, 0.7938, 0.7961, 0.7973, 0.7984, 0.7995,\n",
       "             0.8007, 0.8018, 0.8030, 0.8041, 0.8053, 0.8064, 0.8076, 0.8087, 0.8099,\n",
       "             0.8121, 0.8133, 0.8144, 0.8156, 0.8179, 0.8190, 0.8202, 0.8213, 0.8225,\n",
       "             0.8236, 0.8247, 0.8259, 0.8270, 0.8282, 0.8305, 0.8316, 0.8328, 0.8339,\n",
       "             0.8351, 0.8362, 0.8373, 0.8396, 0.8408, 0.8419, 0.8431, 0.8442, 0.8454,\n",
       "             0.8465, 0.8477, 0.8488, 0.8499, 0.8511, 0.8522, 0.8534, 0.8557, 0.8568,\n",
       "             0.8580, 0.8591, 0.8614, 0.8625, 0.8637, 0.8648, 0.8660, 0.8671, 0.8683,\n",
       "             0.8694, 0.8706, 0.8717, 0.8729, 0.8740, 0.8751, 0.8763, 0.8774, 0.8786,\n",
       "             0.8797, 0.8809, 0.8820, 0.8832, 0.8843, 0.8855, 0.8866, 0.8877, 0.8889,\n",
       "             0.8900, 0.8912, 0.8923, 0.8935, 0.8946, 0.8958, 0.8969, 0.8981, 0.8992,\n",
       "             0.9003, 0.9015, 0.9026, 0.9038, 0.9049, 0.9061, 0.9072, 0.9084, 0.9095,\n",
       "             0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198,\n",
       "             0.9210, 0.9221, 0.9233, 0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301,\n",
       "             0.9313, 0.9324, 0.9336, 0.9347, 0.9347, 0.9359, 0.9370, 0.9381, 0.9381,\n",
       "             0.9393, 0.9404, 0.9416, 0.9427, 0.9439, 0.9450, 0.9462, 0.9473, 0.9485,\n",
       "             0.9496, 0.9507, 0.9519, 0.9530, 0.9542, 0.9553, 0.9565, 0.9565, 0.9565,\n",
       "             0.9576, 0.9588, 0.9599, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656,\n",
       "             0.9668, 0.9679, 0.9691, 0.9702, 0.9702, 0.9714, 0.9725, 0.9725, 0.9725,\n",
       "             0.9737, 0.9737, 0.9748, 0.9759, 0.9771, 0.9782, 0.9782, 0.9794, 0.9805,\n",
       "             0.9805, 0.9805, 0.9805, 0.9805, 0.9817, 0.9828, 0.9828, 0.9840, 0.9840,\n",
       "             0.9840, 0.9840, 0.9840, 0.9840, 0.9840, 0.9840, 0.9840, 0.9840, 0.9840,\n",
       "             0.9851, 0.9863, 0.9874, 0.9874, 0.9885, 0.9885, 0.9885, 0.9897, 0.9897,\n",
       "             0.9897, 0.9897, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9974e-01,\n",
       "             9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9966e-01, 9.9965e-01, 9.9962e-01,\n",
       "             9.9961e-01, 9.9961e-01, 9.9960e-01, 9.9957e-01, 9.9955e-01, 9.9955e-01,\n",
       "             9.9955e-01, 9.9952e-01, 9.9945e-01, 9.9942e-01, 9.9936e-01, 9.9935e-01,\n",
       "             9.9933e-01, 9.9930e-01, 9.9919e-01, 9.9906e-01, 9.9904e-01, 9.9896e-01,\n",
       "             9.9892e-01, 9.9889e-01, 9.9882e-01, 9.9877e-01, 9.9864e-01, 9.9862e-01,\n",
       "             9.9859e-01, 9.9851e-01, 9.9848e-01, 9.9831e-01, 9.9819e-01, 9.9814e-01,\n",
       "             9.9795e-01, 9.9791e-01, 9.9755e-01, 9.9746e-01, 9.9730e-01, 9.9719e-01,\n",
       "             9.9717e-01, 9.9707e-01, 9.9706e-01, 9.9655e-01, 9.9617e-01, 9.9599e-01,\n",
       "             9.9475e-01, 9.9459e-01, 9.9438e-01, 9.9388e-01, 9.9353e-01, 9.9346e-01,\n",
       "             9.9228e-01, 9.9142e-01, 9.9122e-01, 9.9045e-01, 9.8882e-01, 9.8600e-01,\n",
       "             9.8552e-01, 9.8285e-01, 9.8256e-01, 9.8189e-01, 9.8101e-01, 9.8017e-01,\n",
       "             9.7700e-01, 9.7368e-01, 9.6804e-01, 9.6353e-01, 9.6261e-01, 9.6092e-01,\n",
       "             9.5720e-01, 9.5233e-01, 9.4528e-01, 9.2755e-01, 9.1185e-01, 9.0641e-01,\n",
       "             9.0525e-01, 8.9906e-01, 8.3950e-01, 8.3220e-01, 8.3074e-01, 8.1675e-01,\n",
       "             8.1652e-01, 8.1080e-01, 7.9984e-01, 7.9771e-01, 7.5717e-01, 7.5503e-01,\n",
       "             7.3565e-01, 6.9259e-01, 6.8831e-01, 6.3797e-01, 6.3651e-01, 6.3087e-01,\n",
       "             5.1717e-01, 4.8424e-01, 4.5465e-01, 4.5158e-01, 4.4263e-01, 4.1548e-01,\n",
       "             3.2289e-01, 3.1626e-01, 3.0371e-01, 2.7897e-01, 2.6159e-01, 2.5678e-01,\n",
       "             1.6482e-01, 1.5742e-01, 1.2019e-01, 1.1969e-01, 1.1379e-01, 1.1202e-01,\n",
       "             9.2865e-02, 8.8185e-02, 8.6287e-02, 8.0619e-02, 7.7396e-02, 7.5477e-02,\n",
       "             7.0966e-02, 7.0065e-02, 6.7434e-02, 6.6578e-02, 6.3319e-02, 6.3145e-02,\n",
       "             5.6688e-02, 5.5543e-02, 5.3653e-02, 5.2371e-02, 5.1577e-02, 5.1318e-02,\n",
       "             5.1137e-02, 4.0743e-02, 3.8210e-02, 3.6985e-02, 3.3629e-02, 3.3186e-02,\n",
       "             3.2244e-02, 3.1828e-02, 3.0611e-02, 2.7223e-02, 2.3797e-02, 2.3328e-02,\n",
       "             2.1163e-02, 1.9164e-02, 1.7072e-02, 1.6697e-02, 1.6659e-02, 1.6522e-02,\n",
       "             1.4949e-02, 1.4488e-02, 9.0709e-03, 8.2386e-03, 8.0792e-03, 7.8448e-03,\n",
       "             6.3775e-03, 5.5260e-03, 5.4137e-03, 4.1751e-03, 4.0317e-03, 3.8583e-03,\n",
       "             3.7409e-03, 3.5942e-03, 3.3777e-03, 3.2559e-03, 3.0674e-03, 2.8009e-03,\n",
       "             2.6485e-03, 2.3649e-03, 2.1509e-03, 1.9388e-03, 1.8398e-03, 1.7598e-03,\n",
       "             1.6536e-03, 1.5923e-03, 1.5240e-03, 1.4706e-03, 1.4136e-03, 1.2905e-03,\n",
       "             1.2635e-03, 1.2501e-03, 1.2479e-03, 1.0189e-03, 9.9103e-04, 9.8113e-04,\n",
       "             7.7206e-04, 7.5995e-04, 7.4145e-04, 7.1789e-04, 6.9258e-04, 6.8750e-04,\n",
       "             6.7387e-04, 5.7518e-04, 5.6328e-04, 5.5004e-04, 5.2346e-04, 5.1098e-04,\n",
       "             5.1052e-04, 3.8693e-04, 3.3347e-04, 2.8470e-04, 2.5672e-04, 2.5597e-04,\n",
       "             2.4221e-04, 2.3132e-04, 2.2657e-04, 2.0620e-04, 1.9478e-04, 1.9254e-04,\n",
       "             1.8869e-04, 1.8088e-04, 1.3726e-04, 1.3359e-04, 1.1874e-04, 1.1857e-04,\n",
       "             1.0829e-04, 1.0563e-04, 8.5611e-05, 8.3532e-05, 8.1967e-05, 8.0567e-05,\n",
       "             7.1625e-05, 6.6461e-05, 6.3024e-05, 5.3616e-05, 5.2677e-05, 4.8883e-05,\n",
       "             4.2477e-05, 4.1771e-05, 4.1470e-05, 3.8923e-05, 3.7439e-05, 3.3015e-05,\n",
       "             3.2922e-05, 3.2206e-05, 2.8504e-05, 2.5858e-05, 2.5260e-05, 2.4763e-05,\n",
       "             2.2553e-05, 2.0970e-05, 1.7501e-05, 1.7489e-05, 1.5867e-05, 1.5182e-05,\n",
       "             1.5114e-05, 1.4411e-05, 1.4205e-05, 1.3722e-05, 1.3465e-05, 1.1323e-05,\n",
       "             1.1124e-05, 1.0391e-05, 9.8176e-06, 9.0025e-06, 8.9602e-06, 8.9176e-06,\n",
       "             8.8805e-06, 8.0085e-06, 7.9860e-06, 7.7319e-06, 6.4408e-06, 6.1480e-06,\n",
       "             5.3839e-06, 5.1910e-06, 4.3956e-06, 4.0332e-06, 3.9955e-06, 3.3405e-06,\n",
       "             3.2848e-06, 3.0437e-06, 2.8860e-06, 2.8415e-06, 2.5824e-06, 2.5091e-06,\n",
       "             2.4740e-06, 2.3874e-06, 2.3201e-06, 2.1793e-06, 2.0887e-06, 2.0623e-06,\n",
       "             1.7740e-06, 1.6775e-06, 1.6551e-06, 1.5460e-06, 1.5053e-06, 1.4424e-06,\n",
       "             1.3070e-06, 1.2564e-06, 1.2185e-06, 1.0556e-06, 8.5242e-07, 7.1337e-07,\n",
       "             6.3953e-07, 6.1347e-07, 5.9839e-07, 4.5876e-07, 4.2885e-07, 4.2113e-07,\n",
       "             4.1808e-07, 4.0314e-07, 3.7698e-07, 3.5898e-07, 3.5028e-07, 2.3492e-07,\n",
       "             2.3092e-07, 2.2953e-07, 2.2108e-07, 2.1330e-07, 2.1194e-07, 2.0788e-07,\n",
       "             1.8224e-07, 1.7703e-07, 1.7618e-07, 1.7375e-07, 1.6701e-07, 1.6320e-07,\n",
       "             1.5863e-07, 1.0521e-07, 1.0041e-07, 7.7449e-08, 6.1182e-08, 5.7356e-08,\n",
       "             5.7076e-08, 4.6657e-08, 4.2617e-08, 4.1327e-08, 3.6651e-08, 3.4400e-08,\n",
       "             3.3158e-08, 3.2577e-08, 3.0370e-08, 2.6239e-08, 2.4744e-08, 1.8493e-08,\n",
       "             1.6279e-08, 1.1791e-08, 8.9203e-09, 8.8781e-09, 7.5771e-09, 7.0958e-09,\n",
       "             6.7694e-09, 5.4109e-09, 5.0627e-09, 4.3012e-09, 3.6560e-09, 3.2895e-09,\n",
       "             2.9830e-09, 2.3060e-09, 1.7889e-09, 1.3455e-09, 1.1781e-09, 1.0263e-09,\n",
       "             8.7538e-10, 5.1188e-10, 4.7239e-10, 4.0932e-10, 3.8569e-10, 3.2266e-10,\n",
       "             1.6512e-10, 9.7684e-11, 8.3809e-11, 8.1484e-11, 7.8272e-11, 3.5224e-11,\n",
       "             2.3285e-11, 1.5775e-11, 1.1052e-11, 1.0411e-11, 6.8361e-12, 9.8251e-13,\n",
       "             9.5887e-13, 5.1288e-14, 1.7178e-14, 1.6255e-14])}},\n",
       "   {'fpr': np.float64(0.049342105263157895),\n",
       "    'tpr': np.float64(0.981672394043528),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0197, 0.0197, 0.0197, 0.0197, 0.0230, 0.0230, 0.0263, 0.0263,\n",
       "             0.0296, 0.0296, 0.0329, 0.0362, 0.0395, 0.0428, 0.0461, 0.0461, 0.0461,\n",
       "             0.0461, 0.0493, 0.0493, 0.0526, 0.0559, 0.0559, 0.0559, 0.0592, 0.0592,\n",
       "             0.0625, 0.0625, 0.0658, 0.0658, 0.0691, 0.0691, 0.0724, 0.0724, 0.0724,\n",
       "             0.0757, 0.0789, 0.0789, 0.0822, 0.0855, 0.0888, 0.0921, 0.0954, 0.0987,\n",
       "             0.1020, 0.1053, 0.1086, 0.1118, 0.1151, 0.1184, 0.1217, 0.1250, 0.1283,\n",
       "             0.1316, 0.1349, 0.1382, 0.1414, 0.1414, 0.1447, 0.1480, 0.1513, 0.1513,\n",
       "             0.1546, 0.1546, 0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1776,\n",
       "             0.1809, 0.1842, 0.1875, 0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072,\n",
       "             0.2072, 0.2105, 0.2138, 0.2171, 0.2204, 0.2237, 0.2270, 0.2303, 0.2336,\n",
       "             0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2500, 0.2533, 0.2566, 0.2599,\n",
       "             0.2599, 0.2632, 0.2664, 0.2697, 0.2697, 0.2730, 0.2763, 0.2796, 0.2829,\n",
       "             0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026, 0.3059, 0.3092, 0.3125,\n",
       "             0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322, 0.3355, 0.3388, 0.3421,\n",
       "             0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618, 0.3651, 0.3684, 0.3717,\n",
       "             0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914, 0.3947, 0.3980, 0.4013,\n",
       "             0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243, 0.4276, 0.4309,\n",
       "             0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539, 0.4572, 0.4605,\n",
       "             0.4638, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836, 0.4868, 0.4901,\n",
       "             0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132, 0.5164, 0.5197,\n",
       "             0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428, 0.5461, 0.5493,\n",
       "             0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724, 0.5757, 0.5789,\n",
       "             0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020, 0.6053, 0.6086,\n",
       "             0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316, 0.6349, 0.6382,\n",
       "             0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612, 0.6645, 0.6678,\n",
       "             0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908, 0.6941, 0.6974,\n",
       "             0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204, 0.7237, 0.7270,\n",
       "             0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500, 0.7533, 0.7566,\n",
       "             0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796, 0.7829, 0.7862,\n",
       "             0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092, 0.8125, 0.8158,\n",
       "             0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388, 0.8421, 0.8454,\n",
       "             0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684, 0.8717, 0.8750,\n",
       "             0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980, 0.9013, 0.9046,\n",
       "             0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276, 0.9309, 0.9342,\n",
       "             0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572, 0.9605, 0.9638,\n",
       "             0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868, 0.9901, 0.9934,\n",
       "             0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.1077, 0.1856, 0.2199, 0.2417, 0.2623, 0.2806, 0.3070, 0.3265,\n",
       "             0.3322, 0.3414, 0.3436, 0.3608, 0.3666, 0.3746, 0.3803, 0.3872, 0.3895,\n",
       "             0.3918, 0.3986, 0.4009, 0.4032, 0.4101, 0.4147, 0.4181, 0.4204, 0.4238,\n",
       "             0.4318, 0.4341, 0.4422, 0.4456, 0.4490, 0.4513, 0.4548, 0.4570, 0.4616,\n",
       "             0.4639, 0.4662, 0.4685, 0.4731, 0.4742, 0.4788, 0.4800, 0.4822, 0.4834,\n",
       "             0.4857, 0.4868, 0.4880, 0.4914, 0.4937, 0.4983, 0.5006, 0.5040, 0.5063,\n",
       "             0.5074, 0.5097, 0.5109, 0.5132, 0.5189, 0.5200, 0.5212, 0.5246, 0.5269,\n",
       "             0.5292, 0.5304, 0.5338, 0.5372, 0.5395, 0.5407, 0.5418, 0.5475, 0.5487,\n",
       "             0.5510, 0.5533, 0.5544, 0.5556, 0.5590, 0.5601, 0.5624, 0.5636, 0.5647,\n",
       "             0.5659, 0.5682, 0.5693, 0.5704, 0.5716, 0.5727, 0.5750, 0.5762, 0.5785,\n",
       "             0.5796, 0.5808, 0.5830, 0.5842, 0.5853, 0.5865, 0.5876, 0.5888, 0.5911,\n",
       "             0.5922, 0.5945, 0.5956, 0.5979, 0.5991, 0.6002, 0.6025, 0.6037, 0.6060,\n",
       "             0.6071, 0.6082, 0.6094, 0.6105, 0.6117, 0.6128, 0.6140, 0.6151, 0.6174,\n",
       "             0.6186, 0.6197, 0.6208, 0.6231, 0.6254, 0.6266, 0.6277, 0.6300, 0.6312,\n",
       "             0.6323, 0.6334, 0.6357, 0.6369, 0.6380, 0.6392, 0.6403, 0.6415, 0.6426,\n",
       "             0.6438, 0.6449, 0.6460, 0.6472, 0.6483, 0.6495, 0.6506, 0.6518, 0.6529,\n",
       "             0.6541, 0.6552, 0.6564, 0.6575, 0.6586, 0.6598, 0.6609, 0.6621, 0.6632,\n",
       "             0.6644, 0.6655, 0.6667, 0.6678, 0.6690, 0.6701, 0.6712, 0.6724, 0.6747,\n",
       "             0.6758, 0.6770, 0.6781, 0.6793, 0.6804, 0.6816, 0.6827, 0.6838, 0.6850,\n",
       "             0.6861, 0.6873, 0.6884, 0.6896, 0.6907, 0.6919, 0.6930, 0.6942, 0.6953,\n",
       "             0.6964, 0.6976, 0.6987, 0.6999, 0.7010, 0.7022, 0.7033, 0.7045, 0.7068,\n",
       "             0.7079, 0.7090, 0.7102, 0.7113, 0.7125, 0.7136, 0.7148, 0.7159, 0.7171,\n",
       "             0.7182, 0.7194, 0.7205, 0.7216, 0.7228, 0.7239, 0.7251, 0.7262, 0.7274,\n",
       "             0.7285, 0.7297, 0.7308, 0.7320, 0.7331, 0.7342, 0.7354, 0.7365, 0.7377,\n",
       "             0.7388, 0.7400, 0.7411, 0.7423, 0.7434, 0.7446, 0.7457, 0.7468, 0.7480,\n",
       "             0.7491, 0.7503, 0.7514, 0.7526, 0.7537, 0.7549, 0.7560, 0.7572, 0.7583,\n",
       "             0.7595, 0.7606, 0.7617, 0.7629, 0.7640, 0.7652, 0.7663, 0.7675, 0.7686,\n",
       "             0.7698, 0.7709, 0.7721, 0.7732, 0.7743, 0.7755, 0.7766, 0.7778, 0.7789,\n",
       "             0.7801, 0.7812, 0.7824, 0.7835, 0.7847, 0.7858, 0.7869, 0.7881, 0.7892,\n",
       "             0.7904, 0.7915, 0.7927, 0.7938, 0.7950, 0.7961, 0.7973, 0.7984, 0.7995,\n",
       "             0.8007, 0.8018, 0.8030, 0.8041, 0.8053, 0.8064, 0.8076, 0.8087, 0.8099,\n",
       "             0.8110, 0.8121, 0.8133, 0.8144, 0.8156, 0.8167, 0.8179, 0.8190, 0.8202,\n",
       "             0.8213, 0.8225, 0.8236, 0.8247, 0.8259, 0.8270, 0.8282, 0.8293, 0.8305,\n",
       "             0.8316, 0.8328, 0.8339, 0.8351, 0.8362, 0.8373, 0.8385, 0.8396, 0.8408,\n",
       "             0.8419, 0.8431, 0.8442, 0.8454, 0.8465, 0.8477, 0.8488, 0.8499, 0.8499,\n",
       "             0.8511, 0.8522, 0.8534, 0.8545, 0.8557, 0.8568, 0.8580, 0.8591, 0.8603,\n",
       "             0.8614, 0.8625, 0.8637, 0.8648, 0.8660, 0.8671, 0.8683, 0.8694, 0.8706,\n",
       "             0.8717, 0.8729, 0.8740, 0.8751, 0.8763, 0.8774, 0.8786, 0.8797, 0.8809,\n",
       "             0.8820, 0.8832, 0.8843, 0.8855, 0.8866, 0.8877, 0.8889, 0.8900, 0.8912,\n",
       "             0.8923, 0.8935, 0.8946, 0.8958, 0.8969, 0.8981, 0.8992, 0.9003, 0.9015,\n",
       "             0.9026, 0.9038, 0.9049, 0.9061, 0.9072, 0.9084, 0.9095, 0.9107, 0.9118,\n",
       "             0.9129, 0.9141, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198, 0.9210, 0.9221,\n",
       "             0.9233, 0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324,\n",
       "             0.9336, 0.9347, 0.9359, 0.9370, 0.9381, 0.9393, 0.9393, 0.9404, 0.9416,\n",
       "             0.9416, 0.9427, 0.9439, 0.9450, 0.9450, 0.9462, 0.9473, 0.9485, 0.9496,\n",
       "             0.9507, 0.9519, 0.9530, 0.9542, 0.9553, 0.9565, 0.9576, 0.9588, 0.9599,\n",
       "             0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668, 0.9668, 0.9679, 0.9691,\n",
       "             0.9702, 0.9702, 0.9714, 0.9725, 0.9737, 0.9737, 0.9748, 0.9748, 0.9759,\n",
       "             0.9759, 0.9771, 0.9771, 0.9771, 0.9771, 0.9771, 0.9771, 0.9782, 0.9794,\n",
       "             0.9805, 0.9805, 0.9817, 0.9817, 0.9817, 0.9828, 0.9840, 0.9840, 0.9851,\n",
       "             0.9851, 0.9863, 0.9863, 0.9874, 0.9874, 0.9885, 0.9885, 0.9897, 0.9908,\n",
       "             0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931, 0.9943,\n",
       "             0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9969e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9966e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9963e-01,\n",
       "             9.9962e-01, 9.9961e-01, 9.9960e-01, 9.9960e-01, 9.9960e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9955e-01, 9.9955e-01, 9.9953e-01, 9.9951e-01, 9.9949e-01,\n",
       "             9.9948e-01, 9.9946e-01, 9.9945e-01, 9.9944e-01, 9.9942e-01, 9.9941e-01,\n",
       "             9.9940e-01, 9.9937e-01, 9.9934e-01, 9.9933e-01, 9.9932e-01, 9.9931e-01,\n",
       "             9.9931e-01, 9.9930e-01, 9.9930e-01, 9.9929e-01, 9.9929e-01, 9.9928e-01,\n",
       "             9.9927e-01, 9.9926e-01, 9.9925e-01, 9.9925e-01, 9.9924e-01, 9.9920e-01,\n",
       "             9.9919e-01, 9.9918e-01, 9.9916e-01, 9.9912e-01, 9.9906e-01, 9.9898e-01,\n",
       "             9.9891e-01, 9.9888e-01, 9.9886e-01, 9.9879e-01, 9.9877e-01, 9.9875e-01,\n",
       "             9.9869e-01, 9.9869e-01, 9.9860e-01, 9.9859e-01, 9.9859e-01, 9.9856e-01,\n",
       "             9.9855e-01, 9.9851e-01, 9.9843e-01, 9.9834e-01, 9.9833e-01, 9.9832e-01,\n",
       "             9.9830e-01, 9.9825e-01, 9.9818e-01, 9.9814e-01, 9.9813e-01, 9.9813e-01,\n",
       "             9.9811e-01, 9.9806e-01, 9.9802e-01, 9.9774e-01, 9.9720e-01, 9.9709e-01,\n",
       "             9.9690e-01, 9.9687e-01, 9.9670e-01, 9.9662e-01, 9.9653e-01, 9.9618e-01,\n",
       "             9.9597e-01, 9.9574e-01, 9.9572e-01, 9.9563e-01, 9.9528e-01, 9.9507e-01,\n",
       "             9.9493e-01, 9.9478e-01, 9.9439e-01, 9.9363e-01, 9.9362e-01, 9.9340e-01,\n",
       "             9.9287e-01, 9.9275e-01, 9.9268e-01, 9.9206e-01, 9.9200e-01, 9.9154e-01,\n",
       "             9.9072e-01, 9.9044e-01, 9.8752e-01, 9.8705e-01, 9.8664e-01, 9.8510e-01,\n",
       "             9.8140e-01, 9.8088e-01, 9.8084e-01, 9.7988e-01, 9.7820e-01, 9.7766e-01,\n",
       "             9.7528e-01, 9.7121e-01, 9.7025e-01, 9.6982e-01, 9.6899e-01, 9.6866e-01,\n",
       "             9.6717e-01, 9.6247e-01, 9.5439e-01, 9.5298e-01, 9.5221e-01, 9.4905e-01,\n",
       "             9.4518e-01, 9.3821e-01, 9.2837e-01, 9.2810e-01, 9.0525e-01, 8.8398e-01,\n",
       "             8.8369e-01, 8.7534e-01, 8.7155e-01, 8.6867e-01, 8.6481e-01, 8.6463e-01,\n",
       "             8.5655e-01, 8.5206e-01, 8.3221e-01, 8.2422e-01, 8.0415e-01, 7.9699e-01,\n",
       "             7.6636e-01, 7.6464e-01, 7.4188e-01, 7.3786e-01, 7.1352e-01, 7.0603e-01,\n",
       "             6.6563e-01, 6.6370e-01, 6.4747e-01, 6.0317e-01, 6.0216e-01, 5.0151e-01,\n",
       "             4.6140e-01, 3.8589e-01, 3.6710e-01, 3.5735e-01, 3.2599e-01, 3.2330e-01,\n",
       "             3.1150e-01, 2.7950e-01, 2.6481e-01, 2.2524e-01, 2.1402e-01, 2.1233e-01,\n",
       "             1.9197e-01, 1.7803e-01, 1.7642e-01, 1.0791e-01, 1.0719e-01, 1.0712e-01,\n",
       "             1.0610e-01, 1.0258e-01, 8.3002e-02, 7.6448e-02, 6.9508e-02, 5.4249e-02,\n",
       "             4.5557e-02, 4.0340e-02, 3.7628e-02, 3.7401e-02, 2.8668e-02, 2.3868e-02,\n",
       "             2.3317e-02, 2.0493e-02, 1.8973e-02, 1.6644e-02, 1.1236e-02, 1.0455e-02,\n",
       "             8.6985e-03, 8.4549e-03, 7.5070e-03, 6.6889e-03, 5.9801e-03, 5.2941e-03,\n",
       "             4.8270e-03, 4.7555e-03, 4.4074e-03, 4.2275e-03, 4.2018e-03, 3.9321e-03,\n",
       "             3.8443e-03, 3.5746e-03, 3.5504e-03, 2.7064e-03, 2.3896e-03, 2.2051e-03,\n",
       "             1.9132e-03, 1.8103e-03, 1.3004e-03, 1.0812e-03, 1.0190e-03, 9.0102e-04,\n",
       "             7.9684e-04, 7.6610e-04, 6.6773e-04, 6.6279e-04, 5.9287e-04, 5.5542e-04,\n",
       "             5.1379e-04, 4.9782e-04, 4.5318e-04, 4.4945e-04, 4.2479e-04, 4.0313e-04,\n",
       "             3.0259e-04, 2.9518e-04, 2.9192e-04, 2.8600e-04, 2.6364e-04, 2.4559e-04,\n",
       "             2.1914e-04, 2.1604e-04, 2.0115e-04, 1.9420e-04, 1.6080e-04, 1.4650e-04,\n",
       "             1.3416e-04, 1.2986e-04, 9.2047e-05, 7.8536e-05, 6.4052e-05, 5.0863e-05,\n",
       "             4.7513e-05, 4.6049e-05, 3.8774e-05, 3.7749e-05, 3.6635e-05, 3.3727e-05,\n",
       "             2.7488e-05, 2.6769e-05, 2.6319e-05, 2.5770e-05, 2.2360e-05, 2.0696e-05,\n",
       "             1.9488e-05, 1.6409e-05, 1.4595e-05, 1.3269e-05, 1.3034e-05, 1.1069e-05,\n",
       "             8.6189e-06, 8.0111e-06, 7.7792e-06, 7.5838e-06, 7.3957e-06, 7.0149e-06,\n",
       "             6.7846e-06, 6.2461e-06, 6.2127e-06, 6.0534e-06, 5.9439e-06, 5.7609e-06,\n",
       "             5.4422e-06, 5.0328e-06, 5.0067e-06, 4.4186e-06, 4.4121e-06, 4.4102e-06,\n",
       "             4.2901e-06, 4.0472e-06, 3.8927e-06, 3.4570e-06, 3.4041e-06, 2.9949e-06,\n",
       "             2.9637e-06, 2.7276e-06, 2.6525e-06, 2.4433e-06, 2.1427e-06, 2.1258e-06,\n",
       "             2.1208e-06, 2.0026e-06, 1.6887e-06, 1.6251e-06, 1.5281e-06, 1.4530e-06,\n",
       "             1.2268e-06, 1.2180e-06, 1.0843e-06, 1.0841e-06, 9.4386e-07, 8.7102e-07,\n",
       "             8.5610e-07, 8.5566e-07, 8.3575e-07, 7.0523e-07, 6.7483e-07, 6.3803e-07,\n",
       "             6.2043e-07, 6.2034e-07, 5.3296e-07, 5.1005e-07, 5.0249e-07, 4.9057e-07,\n",
       "             4.1375e-07, 3.9659e-07, 3.8164e-07, 3.6717e-07, 3.2694e-07, 2.9255e-07,\n",
       "             2.8720e-07, 2.8140e-07, 2.0966e-07, 2.0957e-07, 1.9789e-07, 1.9477e-07,\n",
       "             1.8306e-07, 1.7435e-07, 1.6946e-07, 1.6678e-07, 1.6484e-07, 1.5485e-07,\n",
       "             1.4964e-07, 1.2582e-07, 1.1550e-07, 1.1383e-07, 1.0716e-07, 9.8929e-08,\n",
       "             8.2843e-08, 7.5974e-08, 7.1185e-08, 7.0672e-08, 6.9270e-08, 5.3432e-08,\n",
       "             5.1814e-08, 4.8543e-08, 4.8513e-08, 4.3989e-08, 4.2732e-08, 3.9492e-08,\n",
       "             3.7963e-08, 3.6951e-08, 3.0730e-08, 2.7589e-08, 2.6982e-08, 2.6253e-08,\n",
       "             2.4798e-08, 1.7557e-08, 1.7335e-08, 1.5875e-08, 1.5145e-08, 1.3645e-08,\n",
       "             1.3215e-08, 1.2218e-08, 1.1356e-08, 1.0182e-08, 7.3953e-09, 7.2848e-09,\n",
       "             7.2715e-09, 6.7532e-09, 6.6545e-09, 6.0561e-09, 5.9527e-09, 5.7739e-09,\n",
       "             4.8553e-09, 4.7879e-09, 2.9944e-09, 2.9934e-09, 2.7888e-09, 2.5106e-09,\n",
       "             2.3904e-09, 1.8635e-09, 1.6982e-09, 1.5383e-09, 1.5290e-09, 1.4046e-09,\n",
       "             1.1390e-09, 1.1239e-09, 1.0089e-09, 6.6397e-10, 5.2450e-10, 4.9098e-10,\n",
       "             4.2537e-10, 4.1661e-10, 3.7274e-10, 3.4807e-10, 3.2385e-10, 2.9861e-10,\n",
       "             2.9855e-10, 2.9087e-10, 2.3921e-10, 1.9887e-10, 1.9804e-10, 1.4786e-10,\n",
       "             1.4746e-10, 1.4709e-10, 1.2961e-10, 8.3088e-11, 7.7755e-11, 7.5289e-11,\n",
       "             5.8596e-11, 5.0007e-11, 4.5757e-11, 4.2918e-11, 4.1637e-11, 3.3148e-11,\n",
       "             2.9717e-11, 2.9503e-11, 2.1619e-11, 2.0539e-11, 1.8607e-11, 1.7061e-11,\n",
       "             1.4158e-11, 1.2694e-11, 8.9376e-12, 7.4927e-12, 5.7324e-12, 4.7303e-12,\n",
       "             4.3949e-12, 4.0012e-12, 3.9319e-12, 3.7803e-12, 2.6589e-12, 2.5035e-12,\n",
       "             2.1480e-12, 2.1030e-12, 1.7713e-12, 1.0771e-12, 7.6127e-13, 7.4178e-13,\n",
       "             7.0571e-13, 7.0514e-13, 3.3256e-13, 2.9313e-13, 9.8435e-14, 8.2988e-14,\n",
       "             7.4610e-14, 6.7413e-14, 6.1653e-14, 5.8183e-14, 2.8348e-14, 1.4037e-14,\n",
       "             8.9286e-16, 1.5281e-16, 1.2609e-16, 7.7686e-17, 1.8496e-17])}},\n",
       "   {'fpr': np.float64(0.03618421052631579),\n",
       "    'tpr': np.float64(0.9564719358533792),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0197, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230,\n",
       "             0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0263, 0.0263,\n",
       "             0.0263, 0.0263, 0.0263, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296,\n",
       "             0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296,\n",
       "             0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0329, 0.0329, 0.0362, 0.0362,\n",
       "             0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362,\n",
       "             0.0362, 0.0362, 0.0395, 0.0428, 0.0428, 0.0461, 0.0461, 0.0461, 0.0461,\n",
       "             0.0461, 0.0461, 0.0493, 0.0493, 0.0493, 0.0526, 0.0559, 0.0592, 0.0625,\n",
       "             0.0625, 0.0625, 0.0658, 0.0658, 0.0658, 0.0658, 0.0658, 0.0658, 0.0691,\n",
       "             0.0724, 0.0757, 0.0789, 0.0822, 0.0855, 0.0855, 0.0855, 0.0888, 0.0888,\n",
       "             0.0921, 0.0954, 0.0987, 0.1020, 0.1020, 0.1053, 0.1086, 0.1118, 0.1151,\n",
       "             0.1151, 0.1184, 0.1217, 0.1250, 0.1283, 0.1316, 0.1349, 0.1382, 0.1414,\n",
       "             0.1447, 0.1480, 0.1513, 0.1513, 0.1546, 0.1546, 0.1579, 0.1612, 0.1645,\n",
       "             0.1678, 0.1711, 0.1743, 0.1776, 0.1776, 0.1809, 0.1842, 0.1875, 0.1908,\n",
       "             0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138, 0.2171, 0.2204,\n",
       "             0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2500,\n",
       "             0.2533, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697, 0.2730, 0.2763, 0.2796,\n",
       "             0.2829, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026, 0.3059,\n",
       "             0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3257, 0.3289, 0.3322,\n",
       "             0.3355, 0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618,\n",
       "             0.3651, 0.3684, 0.3717, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914,\n",
       "             0.3947, 0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211,\n",
       "             0.4243, 0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507,\n",
       "             0.4539, 0.4572, 0.4605, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770,\n",
       "             0.4803, 0.4836, 0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066,\n",
       "             0.5099, 0.5132, 0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362,\n",
       "             0.5395, 0.5428, 0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658,\n",
       "             0.5691, 0.5724, 0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954,\n",
       "             0.5987, 0.6020, 0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250,\n",
       "             0.6283, 0.6316, 0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546,\n",
       "             0.6579, 0.6612, 0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842,\n",
       "             0.6875, 0.6908, 0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138,\n",
       "             0.7171, 0.7204, 0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434,\n",
       "             0.7467, 0.7500, 0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730,\n",
       "             0.7763, 0.7796, 0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026,\n",
       "             0.8059, 0.8092, 0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322,\n",
       "             0.8355, 0.8388, 0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618,\n",
       "             0.8651, 0.8684, 0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914,\n",
       "             0.8947, 0.8980, 0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211,\n",
       "             0.9243, 0.9276, 0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507,\n",
       "             0.9539, 0.9572, 0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803,\n",
       "             0.9836, 0.9868, 0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0309, 0.0527, 0.0836, 0.0905, 0.1008, 0.1157, 0.1260, 0.1317,\n",
       "             0.1397, 0.1466, 0.1535, 0.1615, 0.1707, 0.1741, 0.1775, 0.1833, 0.1844,\n",
       "             0.1879, 0.1947, 0.1970, 0.1993, 0.2005, 0.2039, 0.2085, 0.2119, 0.2176,\n",
       "             0.2199, 0.2211, 0.2245, 0.2257, 0.2291, 0.2348, 0.2371, 0.2383, 0.2394,\n",
       "             0.2440, 0.2463, 0.2486, 0.2543, 0.2577, 0.2612, 0.2623, 0.2646, 0.2658,\n",
       "             0.2669, 0.2692, 0.2715, 0.2738, 0.2772, 0.2795, 0.2841, 0.2864, 0.2887,\n",
       "             0.2921, 0.2955, 0.2990, 0.3013, 0.3036, 0.3058, 0.3081, 0.3093, 0.3127,\n",
       "             0.3162, 0.3173, 0.3184, 0.3196, 0.3230, 0.3253, 0.3265, 0.3276, 0.3288,\n",
       "             0.3299, 0.3310, 0.3322, 0.3333, 0.3345, 0.3368, 0.3391, 0.3414, 0.3448,\n",
       "             0.3459, 0.3471, 0.3505, 0.3517, 0.3540, 0.3562, 0.3597, 0.3608, 0.3631,\n",
       "             0.3654, 0.3666, 0.3677, 0.3688, 0.3700, 0.3711, 0.3723, 0.3746, 0.3757,\n",
       "             0.3780, 0.3814, 0.3826, 0.3849, 0.3860, 0.3883, 0.3895, 0.3906, 0.3918,\n",
       "             0.3929, 0.3952, 0.3975, 0.3986, 0.4009, 0.4021, 0.4032, 0.4044, 0.4055,\n",
       "             0.4066, 0.4078, 0.4101, 0.4112, 0.4124, 0.4158, 0.4170, 0.4181, 0.4192,\n",
       "             0.4204, 0.4215, 0.4227, 0.4238, 0.4250, 0.4261, 0.4273, 0.4284, 0.4296,\n",
       "             0.4307, 0.4318, 0.4330, 0.4353, 0.4376, 0.4387, 0.4399, 0.4410, 0.4422,\n",
       "             0.4433, 0.4444, 0.4456, 0.4467, 0.4479, 0.4490, 0.4502, 0.4525, 0.4536,\n",
       "             0.4548, 0.4559, 0.4582, 0.4593, 0.4605, 0.4616, 0.4628, 0.4639, 0.4651,\n",
       "             0.4662, 0.4674, 0.4685, 0.4696, 0.4708, 0.4719, 0.4731, 0.4742, 0.4754,\n",
       "             0.4765, 0.4777, 0.4788, 0.4811, 0.4822, 0.4834, 0.4845, 0.4857, 0.4868,\n",
       "             0.4880, 0.4903, 0.4914, 0.4926, 0.4937, 0.4948, 0.4960, 0.4971, 0.4983,\n",
       "             0.4994, 0.5006, 0.5017, 0.5029, 0.5040, 0.5052, 0.5063, 0.5074, 0.5086,\n",
       "             0.5097, 0.5109, 0.5120, 0.5132, 0.5155, 0.5166, 0.5178, 0.5200, 0.5212,\n",
       "             0.5223, 0.5235, 0.5246, 0.5258, 0.5269, 0.5281, 0.5292, 0.5315, 0.5326,\n",
       "             0.5349, 0.5372, 0.5384, 0.5395, 0.5407, 0.5418, 0.5464, 0.5475, 0.5487,\n",
       "             0.5510, 0.5521, 0.5533, 0.5544, 0.5556, 0.5567, 0.5578, 0.5590, 0.5601,\n",
       "             0.5613, 0.5636, 0.5647, 0.5659, 0.5670, 0.5682, 0.5693, 0.5716, 0.5727,\n",
       "             0.5739, 0.5750, 0.5762, 0.5773, 0.5785, 0.5796, 0.5808, 0.5819, 0.5830,\n",
       "             0.5842, 0.5853, 0.5865, 0.5876, 0.5888, 0.5899, 0.5911, 0.5934, 0.5945,\n",
       "             0.5956, 0.5968, 0.5979, 0.5991, 0.6002, 0.6014, 0.6025, 0.6037, 0.6048,\n",
       "             0.6060, 0.6071, 0.6082, 0.6094, 0.6105, 0.6117, 0.6128, 0.6140, 0.6151,\n",
       "             0.6163, 0.6174, 0.6186, 0.6197, 0.6208, 0.6220, 0.6231, 0.6243, 0.6254,\n",
       "             0.6266, 0.6277, 0.6289, 0.6300, 0.6312, 0.6323, 0.6334, 0.6346, 0.6357,\n",
       "             0.6369, 0.6380, 0.6392, 0.6403, 0.6415, 0.6426, 0.6438, 0.6449, 0.6460,\n",
       "             0.6472, 0.6483, 0.6495, 0.6506, 0.6518, 0.6529, 0.6541, 0.6552, 0.6564,\n",
       "             0.6575, 0.6586, 0.6598, 0.6609, 0.6621, 0.6632, 0.6644, 0.6655, 0.6667,\n",
       "             0.6678, 0.6690, 0.6701, 0.6712, 0.6724, 0.6735, 0.6747, 0.6758, 0.6770,\n",
       "             0.6781, 0.6793, 0.6804, 0.6816, 0.6827, 0.6838, 0.6850, 0.6861, 0.6873,\n",
       "             0.6884, 0.6896, 0.6907, 0.6919, 0.6942, 0.6953, 0.6964, 0.6976, 0.6987,\n",
       "             0.6999, 0.7010, 0.7022, 0.7033, 0.7056, 0.7068, 0.7079, 0.7090, 0.7102,\n",
       "             0.7113, 0.7125, 0.7136, 0.7148, 0.7159, 0.7171, 0.7182, 0.7194, 0.7205,\n",
       "             0.7216, 0.7228, 0.7239, 0.7251, 0.7262, 0.7274, 0.7285, 0.7297, 0.7308,\n",
       "             0.7320, 0.7331, 0.7342, 0.7354, 0.7365, 0.7377, 0.7388, 0.7400, 0.7411,\n",
       "             0.7423, 0.7446, 0.7457, 0.7468, 0.7480, 0.7491, 0.7503, 0.7514, 0.7526,\n",
       "             0.7537, 0.7549, 0.7560, 0.7572, 0.7583, 0.7595, 0.7606, 0.7617, 0.7629,\n",
       "             0.7640, 0.7652, 0.7663, 0.7675, 0.7686, 0.7698, 0.7709, 0.7721, 0.7732,\n",
       "             0.7743, 0.7766, 0.7778, 0.7789, 0.7801, 0.7812, 0.7824, 0.7835, 0.7847,\n",
       "             0.7858, 0.7869, 0.7881, 0.7892, 0.7904, 0.7915, 0.7927, 0.7938, 0.7950,\n",
       "             0.7961, 0.7973, 0.7984, 0.7995, 0.8007, 0.8018, 0.8030, 0.8041, 0.8053,\n",
       "             0.8064, 0.8076, 0.8087, 0.8099, 0.8110, 0.8121, 0.8133, 0.8144, 0.8156,\n",
       "             0.8167, 0.8179, 0.8190, 0.8202, 0.8202, 0.8213, 0.8225, 0.8236, 0.8247,\n",
       "             0.8259, 0.8270, 0.8282, 0.8293, 0.8305, 0.8316, 0.8328, 0.8328, 0.8339,\n",
       "             0.8351, 0.8362, 0.8373, 0.8385, 0.8396, 0.8408, 0.8419, 0.8431, 0.8442,\n",
       "             0.8454, 0.8465, 0.8477, 0.8488, 0.8499, 0.8511, 0.8522, 0.8534, 0.8545,\n",
       "             0.8557, 0.8568, 0.8580, 0.8591, 0.8603, 0.8614, 0.8614, 0.8625, 0.8637,\n",
       "             0.8648, 0.8660, 0.8671, 0.8683, 0.8694, 0.8706, 0.8717, 0.8729, 0.8740,\n",
       "             0.8751, 0.8763, 0.8774, 0.8786, 0.8797, 0.8809, 0.8820, 0.8832, 0.8843,\n",
       "             0.8855, 0.8866, 0.8877, 0.8889, 0.8900, 0.8912, 0.8923, 0.8935, 0.8946,\n",
       "             0.8958, 0.8969, 0.8981, 0.8992, 0.9003, 0.9015, 0.9026, 0.9038, 0.9049,\n",
       "             0.9061, 0.9061, 0.9072, 0.9072, 0.9084, 0.9095, 0.9107, 0.9118, 0.9129,\n",
       "             0.9141, 0.9152, 0.9152, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198, 0.9210,\n",
       "             0.9221, 0.9233, 0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9290, 0.9301,\n",
       "             0.9313, 0.9324, 0.9336, 0.9336, 0.9347, 0.9359, 0.9370, 0.9381, 0.9393,\n",
       "             0.9404, 0.9416, 0.9427, 0.9439, 0.9450, 0.9462, 0.9473, 0.9485, 0.9496,\n",
       "             0.9507, 0.9519, 0.9530, 0.9542, 0.9553, 0.9553, 0.9565, 0.9565, 0.9576,\n",
       "             0.9588, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668, 0.9679,\n",
       "             0.9691, 0.9702, 0.9702, 0.9702, 0.9714, 0.9714, 0.9725, 0.9737, 0.9748,\n",
       "             0.9759, 0.9771, 0.9771, 0.9782, 0.9794, 0.9794, 0.9794, 0.9794, 0.9794,\n",
       "             0.9805, 0.9817, 0.9817, 0.9828, 0.9840, 0.9851, 0.9863, 0.9874, 0.9874,\n",
       "             0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9885, 0.9897, 0.9897, 0.9908,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9970e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9968e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9967e-01, 9.9967e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9964e-01,\n",
       "             9.9964e-01, 9.9962e-01, 9.9961e-01, 9.9961e-01, 9.9960e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9958e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9956e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9954e-01, 9.9953e-01,\n",
       "             9.9952e-01, 9.9951e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01, 9.9948e-01,\n",
       "             9.9946e-01, 9.9946e-01, 9.9946e-01, 9.9945e-01, 9.9944e-01, 9.9941e-01,\n",
       "             9.9940e-01, 9.9939e-01, 9.9939e-01, 9.9939e-01, 9.9931e-01, 9.9931e-01,\n",
       "             9.9928e-01, 9.9928e-01, 9.9927e-01, 9.9927e-01, 9.9926e-01, 9.9925e-01,\n",
       "             9.9925e-01, 9.9925e-01, 9.9925e-01, 9.9924e-01, 9.9924e-01, 9.9922e-01,\n",
       "             9.9922e-01, 9.9919e-01, 9.9918e-01, 9.9918e-01, 9.9918e-01, 9.9914e-01,\n",
       "             9.9913e-01, 9.9911e-01, 9.9907e-01, 9.9901e-01, 9.9901e-01, 9.9897e-01,\n",
       "             9.9896e-01, 9.9896e-01, 9.9895e-01, 9.9893e-01, 9.9891e-01, 9.9888e-01,\n",
       "             9.9887e-01, 9.9879e-01, 9.9876e-01, 9.9871e-01, 9.9866e-01, 9.9862e-01,\n",
       "             9.9861e-01, 9.9861e-01, 9.9859e-01, 9.9857e-01, 9.9857e-01, 9.9854e-01,\n",
       "             9.9854e-01, 9.9849e-01, 9.9842e-01, 9.9841e-01, 9.9837e-01, 9.9830e-01,\n",
       "             9.9824e-01, 9.9818e-01, 9.9818e-01, 9.9815e-01, 9.9815e-01, 9.9810e-01,\n",
       "             9.9803e-01, 9.9802e-01, 9.9797e-01, 9.9793e-01, 9.9792e-01, 9.9792e-01,\n",
       "             9.9784e-01, 9.9783e-01, 9.9782e-01, 9.9776e-01, 9.9771e-01, 9.9754e-01,\n",
       "             9.9748e-01, 9.9745e-01, 9.9743e-01, 9.9737e-01, 9.9727e-01, 9.9721e-01,\n",
       "             9.9718e-01, 9.9715e-01, 9.9712e-01, 9.9708e-01, 9.9703e-01, 9.9683e-01,\n",
       "             9.9682e-01, 9.9680e-01, 9.9680e-01, 9.9677e-01, 9.9673e-01, 9.9657e-01,\n",
       "             9.9654e-01, 9.9647e-01, 9.9604e-01, 9.9602e-01, 9.9596e-01, 9.9594e-01,\n",
       "             9.9590e-01, 9.9570e-01, 9.9546e-01, 9.9534e-01, 9.9516e-01, 9.9515e-01,\n",
       "             9.9507e-01, 9.9506e-01, 9.9505e-01, 9.9503e-01, 9.9491e-01, 9.9474e-01,\n",
       "             9.9466e-01, 9.9459e-01, 9.9436e-01, 9.9429e-01, 9.9426e-01, 9.9416e-01,\n",
       "             9.9371e-01, 9.9357e-01, 9.9310e-01, 9.9299e-01, 9.9294e-01, 9.9293e-01,\n",
       "             9.9230e-01, 9.9226e-01, 9.9203e-01, 9.9146e-01, 9.9106e-01, 9.9080e-01,\n",
       "             9.9006e-01, 9.9005e-01, 9.8983e-01, 9.8919e-01, 9.8882e-01, 9.8854e-01,\n",
       "             9.8828e-01, 9.8820e-01, 9.8786e-01, 9.8766e-01, 9.8754e-01, 9.8676e-01,\n",
       "             9.8589e-01, 9.8561e-01, 9.8544e-01, 9.8335e-01, 9.8312e-01, 9.7876e-01,\n",
       "             9.7830e-01, 9.7728e-01, 9.7673e-01, 9.7665e-01, 9.7621e-01, 9.7610e-01,\n",
       "             9.7610e-01, 9.7598e-01, 9.7163e-01, 9.6974e-01, 9.6583e-01, 9.6431e-01,\n",
       "             9.6419e-01, 9.6193e-01, 9.5557e-01, 9.5480e-01, 9.5371e-01, 9.5308e-01,\n",
       "             9.4610e-01, 9.4387e-01, 9.4381e-01, 9.4007e-01, 9.3896e-01, 9.3383e-01,\n",
       "             9.3283e-01, 9.3229e-01, 9.2235e-01, 9.2164e-01, 9.1803e-01, 9.1489e-01,\n",
       "             9.1421e-01, 9.0872e-01, 9.0469e-01, 9.0322e-01, 9.0076e-01, 8.9617e-01,\n",
       "             8.8800e-01, 8.7857e-01, 8.7772e-01, 8.7355e-01, 8.7160e-01, 8.6294e-01,\n",
       "             8.6016e-01, 8.5858e-01, 8.5274e-01, 8.5089e-01, 8.4457e-01, 8.4295e-01,\n",
       "             8.3280e-01, 8.0424e-01, 7.9656e-01, 7.8992e-01, 7.8588e-01, 7.7981e-01,\n",
       "             7.7943e-01, 7.4586e-01, 7.3795e-01, 7.3361e-01, 7.2153e-01, 7.1139e-01,\n",
       "             6.9803e-01, 6.8810e-01, 6.5645e-01, 6.5161e-01, 6.2381e-01, 6.0512e-01,\n",
       "             5.3345e-01, 5.3146e-01, 4.6645e-01, 4.6333e-01, 4.5199e-01, 4.3762e-01,\n",
       "             4.2507e-01, 4.0605e-01, 3.8075e-01, 3.5866e-01, 3.5536e-01, 3.3503e-01,\n",
       "             3.2924e-01, 3.1695e-01, 2.8631e-01, 2.8271e-01, 2.3662e-01, 1.9457e-01,\n",
       "             1.7278e-01, 1.6295e-01, 1.5324e-01, 1.4338e-01, 1.2822e-01, 1.0145e-01,\n",
       "             9.7433e-02, 9.4173e-02, 8.6957e-02, 8.5922e-02, 7.8949e-02, 5.0562e-02,\n",
       "             4.6784e-02, 4.4337e-02, 4.0589e-02, 3.7886e-02, 2.7994e-02, 2.3795e-02,\n",
       "             2.2988e-02, 2.0790e-02, 1.9983e-02, 1.6375e-02, 1.6030e-02, 1.5837e-02,\n",
       "             1.5643e-02, 1.4171e-02, 1.4142e-02, 1.4126e-02, 1.0632e-02, 1.0128e-02,\n",
       "             9.0810e-03, 5.9239e-03, 5.6874e-03, 5.4128e-03, 5.2093e-03, 4.7632e-03,\n",
       "             4.4102e-03, 4.3252e-03, 3.6165e-03, 3.2196e-03, 3.1896e-03, 2.7557e-03,\n",
       "             2.1838e-03, 1.9480e-03, 1.3849e-03, 1.3549e-03, 1.3204e-03, 1.1300e-03,\n",
       "             8.6812e-04, 7.9779e-04, 7.7377e-04, 7.5459e-04, 7.3081e-04, 5.6027e-04,\n",
       "             5.0734e-04, 4.9630e-04, 4.2553e-04, 4.1818e-04, 4.1802e-04, 3.9452e-04,\n",
       "             3.7640e-04, 3.5529e-04, 3.0110e-04, 2.8152e-04, 2.7303e-04, 2.5332e-04,\n",
       "             2.4363e-04, 2.1365e-04, 2.0676e-04, 1.9696e-04, 1.9645e-04, 1.9445e-04,\n",
       "             1.5851e-04, 1.0732e-04, 9.1700e-05, 8.0627e-05, 7.8550e-05, 7.2854e-05,\n",
       "             6.7743e-05, 6.6559e-05, 6.5662e-05, 6.2714e-05, 5.7026e-05, 5.6540e-05,\n",
       "             5.4416e-05, 5.1764e-05, 5.1653e-05, 4.3822e-05, 4.3217e-05, 4.0282e-05,\n",
       "             4.0143e-05, 3.8705e-05, 3.6185e-05, 3.1860e-05, 3.0776e-05, 3.0768e-05,\n",
       "             2.6746e-05, 2.1386e-05, 2.1255e-05, 2.0725e-05, 1.5908e-05, 1.4885e-05,\n",
       "             1.1998e-05, 1.1450e-05, 8.7671e-06, 8.6473e-06, 6.9632e-06, 6.8846e-06,\n",
       "             5.7867e-06, 5.7728e-06, 5.3317e-06, 5.2324e-06, 4.6447e-06, 4.3743e-06,\n",
       "             4.2061e-06, 3.7276e-06, 3.5916e-06, 3.3356e-06, 2.8338e-06, 2.7356e-06,\n",
       "             2.0827e-06, 1.8387e-06, 1.6847e-06, 1.6846e-06, 1.5657e-06, 1.5022e-06,\n",
       "             1.3334e-06, 1.2212e-06, 1.1947e-06, 1.1558e-06, 1.0486e-06, 9.9898e-07,\n",
       "             9.4153e-07, 9.3008e-07, 8.3481e-07, 7.3618e-07, 6.4906e-07, 6.3542e-07,\n",
       "             6.0591e-07, 5.2360e-07, 5.1652e-07, 4.8154e-07, 4.0920e-07, 4.0815e-07,\n",
       "             4.0514e-07, 3.9651e-07, 3.7816e-07, 3.3580e-07, 3.2345e-07, 3.2084e-07,\n",
       "             3.0854e-07, 2.7202e-07, 2.4675e-07, 2.4296e-07, 2.1445e-07, 1.9242e-07,\n",
       "             1.8537e-07, 1.6447e-07, 1.4704e-07, 1.4120e-07, 1.2955e-07, 1.2601e-07,\n",
       "             1.2180e-07, 1.0937e-07, 1.0585e-07, 1.0079e-07, 1.0048e-07, 9.3812e-08,\n",
       "             8.5364e-08, 7.7547e-08, 5.8413e-08, 5.4521e-08, 4.6256e-08, 3.8808e-08,\n",
       "             3.6594e-08, 3.4644e-08, 3.3455e-08, 3.1882e-08, 3.1260e-08, 3.1251e-08,\n",
       "             3.0263e-08, 3.0035e-08, 2.9449e-08, 2.8257e-08, 2.5788e-08, 2.5019e-08,\n",
       "             2.2414e-08, 2.1940e-08, 2.1867e-08, 2.0251e-08, 1.9904e-08, 1.8338e-08,\n",
       "             1.7426e-08, 1.7074e-08, 1.6412e-08, 1.5933e-08, 1.4704e-08, 1.4669e-08,\n",
       "             1.4173e-08, 1.3487e-08, 1.3397e-08, 1.3062e-08, 1.2247e-08, 1.2188e-08,\n",
       "             1.1746e-08, 1.1588e-08, 8.5660e-09, 8.3193e-09, 7.8164e-09, 7.7013e-09,\n",
       "             7.2282e-09, 6.7770e-09, 6.0327e-09, 5.8922e-09, 5.7412e-09, 5.5671e-09,\n",
       "             4.5926e-09, 4.2086e-09, 4.0125e-09, 3.9902e-09, 3.9638e-09, 3.4346e-09,\n",
       "             2.6997e-09, 1.9250e-09, 1.8942e-09, 1.7919e-09, 1.5926e-09, 1.4977e-09,\n",
       "             1.4702e-09, 1.4224e-09, 1.3019e-09, 1.2616e-09, 1.2028e-09, 1.0601e-09,\n",
       "             9.7472e-10, 7.8834e-10, 6.0645e-10, 5.2016e-10, 5.0030e-10, 4.8243e-10,\n",
       "             4.8171e-10, 4.4065e-10, 3.8819e-10, 3.8728e-10, 3.4059e-10, 3.3160e-10,\n",
       "             3.1075e-10, 2.7066e-10, 2.2307e-10, 2.1464e-10, 2.0366e-10, 1.9908e-10,\n",
       "             1.8234e-10, 1.6848e-10, 1.6340e-10, 1.5712e-10, 1.5250e-10, 1.3773e-10,\n",
       "             1.2210e-10, 6.3110e-11, 5.7505e-11, 5.0799e-11, 4.5405e-11, 4.2957e-11,\n",
       "             3.8216e-11, 3.1046e-11, 2.9229e-11, 2.2810e-11, 1.9484e-11, 1.6525e-11,\n",
       "             1.5679e-11, 1.5246e-11, 1.4071e-11, 1.3862e-11, 1.3837e-11, 1.3290e-11,\n",
       "             1.2849e-11, 1.0046e-11, 8.0215e-12, 6.1473e-12, 5.3988e-12, 4.9760e-12,\n",
       "             4.5201e-12, 3.7011e-12, 3.4819e-12, 2.8136e-12, 1.4772e-12, 1.3611e-12,\n",
       "             1.1941e-12, 1.1344e-12, 9.2561e-13, 8.8814e-13, 7.4492e-13, 5.3608e-13,\n",
       "             4.8168e-13, 4.2222e-13, 3.6857e-13, 2.5131e-13, 2.3343e-13, 2.2263e-13,\n",
       "             2.0654e-13, 1.9867e-13, 1.7982e-13, 1.2747e-13, 1.1565e-13, 9.7425e-14,\n",
       "             8.6041e-14, 6.2443e-14, 2.6796e-14, 2.2198e-14, 5.9215e-15, 1.9099e-15,\n",
       "             4.6763e-16, 2.0310e-16, 7.5636e-19])}},\n",
       "   {'fpr': np.float64(0.05921052631578947),\n",
       "    'tpr': np.float64(0.983963344788087),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099,\n",
       "             0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164, 0.0197,\n",
       "             0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230,\n",
       "             0.0230, 0.0263, 0.0263, 0.0296, 0.0329, 0.0362, 0.0362, 0.0362, 0.0395,\n",
       "             0.0428, 0.0428, 0.0428, 0.0428, 0.0428, 0.0428, 0.0428, 0.0461, 0.0493,\n",
       "             0.0493, 0.0526, 0.0559, 0.0592, 0.0592, 0.0592, 0.0592, 0.0592, 0.0592,\n",
       "             0.0592, 0.0625, 0.0625, 0.0658, 0.0658, 0.0658, 0.0691, 0.0724, 0.0724,\n",
       "             0.0724, 0.0757, 0.0789, 0.0822, 0.0855, 0.0888, 0.0921, 0.0954, 0.0987,\n",
       "             0.1020, 0.1053, 0.1086, 0.1118, 0.1151, 0.1184, 0.1217, 0.1250, 0.1283,\n",
       "             0.1283, 0.1316, 0.1349, 0.1382, 0.1414, 0.1447, 0.1480, 0.1513, 0.1546,\n",
       "             0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1776, 0.1809, 0.1842,\n",
       "             0.1875, 0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138,\n",
       "             0.2171, 0.2204, 0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2401,\n",
       "             0.2434, 0.2467, 0.2467, 0.2500, 0.2533, 0.2566, 0.2599, 0.2599, 0.2632,\n",
       "             0.2664, 0.2697, 0.2730, 0.2763, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928,\n",
       "             0.2961, 0.2993, 0.3026, 0.3059, 0.3092, 0.3125, 0.3158, 0.3191, 0.3224,\n",
       "             0.3257, 0.3257, 0.3289, 0.3322, 0.3355, 0.3355, 0.3388, 0.3421, 0.3454,\n",
       "             0.3487, 0.3520, 0.3553, 0.3586, 0.3618, 0.3651, 0.3684, 0.3717, 0.3750,\n",
       "             0.3783, 0.3816, 0.3849, 0.3882, 0.3914, 0.3947, 0.3980, 0.4013, 0.4046,\n",
       "             0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243, 0.4276, 0.4309, 0.4342,\n",
       "             0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539, 0.4572, 0.4605, 0.4638,\n",
       "             0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836, 0.4868, 0.4901, 0.4934,\n",
       "             0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132, 0.5164, 0.5197, 0.5230,\n",
       "             0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428, 0.5461, 0.5493, 0.5526,\n",
       "             0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724, 0.5757, 0.5789, 0.5822,\n",
       "             0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020, 0.6053, 0.6086, 0.6118,\n",
       "             0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316, 0.6349, 0.6382, 0.6414,\n",
       "             0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612, 0.6645, 0.6678, 0.6711,\n",
       "             0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908, 0.6941, 0.6974, 0.7007,\n",
       "             0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204, 0.7237, 0.7270, 0.7303,\n",
       "             0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500, 0.7533, 0.7566, 0.7599,\n",
       "             0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796, 0.7829, 0.7862, 0.7895,\n",
       "             0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092, 0.8125, 0.8158, 0.8191,\n",
       "             0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388, 0.8421, 0.8454, 0.8487,\n",
       "             0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684, 0.8717, 0.8750, 0.8783,\n",
       "             0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980, 0.9013, 0.9046, 0.9079,\n",
       "             0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276, 0.9309, 0.9342, 0.9375,\n",
       "             0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572, 0.9605, 0.9638, 0.9671,\n",
       "             0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868, 0.9901, 0.9934, 0.9967,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.2360, 0.3242, 0.3688, 0.3998, 0.4147, 0.4307, 0.4467, 0.4605,\n",
       "             0.4742, 0.4822, 0.4948, 0.4994, 0.5052, 0.5109, 0.5166, 0.5200, 0.5246,\n",
       "             0.5315, 0.5361, 0.5395, 0.5418, 0.5441, 0.5487, 0.5521, 0.5533, 0.5601,\n",
       "             0.5624, 0.5659, 0.5670, 0.5727, 0.5739, 0.5750, 0.5762, 0.5773, 0.5796,\n",
       "             0.5842, 0.5853, 0.5865, 0.5876, 0.5888, 0.5899, 0.5945, 0.5979, 0.6037,\n",
       "             0.6060, 0.6071, 0.6082, 0.6117, 0.6140, 0.6151, 0.6174, 0.6186, 0.6208,\n",
       "             0.6220, 0.6243, 0.6254, 0.6266, 0.6277, 0.6289, 0.6300, 0.6312, 0.6323,\n",
       "             0.6346, 0.6357, 0.6380, 0.6403, 0.6415, 0.6426, 0.6438, 0.6460, 0.6472,\n",
       "             0.6495, 0.6506, 0.6518, 0.6541, 0.6552, 0.6564, 0.6575, 0.6586, 0.6598,\n",
       "             0.6609, 0.6621, 0.6632, 0.6644, 0.6655, 0.6667, 0.6678, 0.6690, 0.6701,\n",
       "             0.6712, 0.6724, 0.6735, 0.6747, 0.6758, 0.6770, 0.6793, 0.6804, 0.6816,\n",
       "             0.6827, 0.6850, 0.6861, 0.6873, 0.6884, 0.6907, 0.6919, 0.6930, 0.6964,\n",
       "             0.6976, 0.6987, 0.6999, 0.7010, 0.7022, 0.7033, 0.7045, 0.7056, 0.7068,\n",
       "             0.7079, 0.7090, 0.7102, 0.7113, 0.7125, 0.7136, 0.7148, 0.7159, 0.7171,\n",
       "             0.7182, 0.7194, 0.7205, 0.7216, 0.7228, 0.7239, 0.7251, 0.7262, 0.7274,\n",
       "             0.7285, 0.7297, 0.7308, 0.7320, 0.7331, 0.7342, 0.7354, 0.7365, 0.7377,\n",
       "             0.7388, 0.7400, 0.7411, 0.7423, 0.7434, 0.7446, 0.7457, 0.7468, 0.7480,\n",
       "             0.7491, 0.7503, 0.7514, 0.7526, 0.7537, 0.7549, 0.7560, 0.7572, 0.7583,\n",
       "             0.7595, 0.7606, 0.7617, 0.7629, 0.7640, 0.7652, 0.7663, 0.7675, 0.7686,\n",
       "             0.7698, 0.7709, 0.7721, 0.7732, 0.7743, 0.7755, 0.7766, 0.7778, 0.7789,\n",
       "             0.7801, 0.7812, 0.7824, 0.7835, 0.7847, 0.7858, 0.7858, 0.7869, 0.7881,\n",
       "             0.7892, 0.7904, 0.7915, 0.7927, 0.7938, 0.7950, 0.7961, 0.7973, 0.7984,\n",
       "             0.7995, 0.8007, 0.8018, 0.8030, 0.8041, 0.8053, 0.8064, 0.8076, 0.8087,\n",
       "             0.8099, 0.8110, 0.8121, 0.8133, 0.8144, 0.8167, 0.8179, 0.8190, 0.8202,\n",
       "             0.8213, 0.8225, 0.8236, 0.8247, 0.8259, 0.8270, 0.8282, 0.8293, 0.8305,\n",
       "             0.8316, 0.8328, 0.8339, 0.8351, 0.8362, 0.8373, 0.8385, 0.8396, 0.8408,\n",
       "             0.8419, 0.8431, 0.8442, 0.8454, 0.8465, 0.8477, 0.8488, 0.8499, 0.8511,\n",
       "             0.8522, 0.8534, 0.8545, 0.8557, 0.8568, 0.8580, 0.8591, 0.8603, 0.8614,\n",
       "             0.8625, 0.8637, 0.8648, 0.8660, 0.8671, 0.8683, 0.8694, 0.8706, 0.8717,\n",
       "             0.8729, 0.8740, 0.8751, 0.8763, 0.8774, 0.8786, 0.8797, 0.8809, 0.8820,\n",
       "             0.8832, 0.8843, 0.8855, 0.8866, 0.8877, 0.8889, 0.8900, 0.8912, 0.8923,\n",
       "             0.8935, 0.8946, 0.8958, 0.8969, 0.8981, 0.8992, 0.9003, 0.9015, 0.9026,\n",
       "             0.9038, 0.9049, 0.9061, 0.9072, 0.9072, 0.9084, 0.9095, 0.9107, 0.9118,\n",
       "             0.9129, 0.9141, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198, 0.9210, 0.9221,\n",
       "             0.9233, 0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324,\n",
       "             0.9336, 0.9347, 0.9359, 0.9370, 0.9381, 0.9393, 0.9404, 0.9416, 0.9416,\n",
       "             0.9427, 0.9439, 0.9439, 0.9450, 0.9462, 0.9473, 0.9485, 0.9496, 0.9507,\n",
       "             0.9519, 0.9530, 0.9542, 0.9553, 0.9565, 0.9565, 0.9576, 0.9588, 0.9588,\n",
       "             0.9588, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668, 0.9679,\n",
       "             0.9691, 0.9691, 0.9702, 0.9702, 0.9702, 0.9702, 0.9714, 0.9725, 0.9725,\n",
       "             0.9725, 0.9737, 0.9748, 0.9759, 0.9771, 0.9782, 0.9794, 0.9794, 0.9794,\n",
       "             0.9805, 0.9805, 0.9805, 0.9805, 0.9817, 0.9828, 0.9840, 0.9851, 0.9863,\n",
       "             0.9874, 0.9874, 0.9885, 0.9885, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954,\n",
       "             0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01,\n",
       "             9.9975e-01, 9.9973e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01,\n",
       "             9.9967e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01, 9.9961e-01, 9.9960e-01,\n",
       "             9.9960e-01, 9.9960e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01, 9.9958e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9954e-01,\n",
       "             9.9953e-01, 9.9947e-01, 9.9945e-01, 9.9944e-01, 9.9943e-01, 9.9943e-01,\n",
       "             9.9938e-01, 9.9937e-01, 9.9935e-01, 9.9933e-01, 9.9932e-01, 9.9930e-01,\n",
       "             9.9929e-01, 9.9927e-01, 9.9927e-01, 9.9920e-01, 9.9917e-01, 9.9917e-01,\n",
       "             9.9916e-01, 9.9909e-01, 9.9904e-01, 9.9896e-01, 9.9886e-01, 9.9872e-01,\n",
       "             9.9870e-01, 9.9869e-01, 9.9858e-01, 9.9850e-01, 9.9849e-01, 9.9844e-01,\n",
       "             9.9843e-01, 9.9842e-01, 9.9836e-01, 9.9828e-01, 9.9828e-01, 9.9828e-01,\n",
       "             9.9807e-01, 9.9791e-01, 9.9785e-01, 9.9780e-01, 9.9752e-01, 9.9748e-01,\n",
       "             9.9737e-01, 9.9725e-01, 9.9687e-01, 9.9684e-01, 9.9621e-01, 9.9607e-01,\n",
       "             9.9578e-01, 9.9572e-01, 9.9556e-01, 9.9550e-01, 9.9499e-01, 9.9470e-01,\n",
       "             9.9463e-01, 9.9450e-01, 9.9412e-01, 9.9403e-01, 9.9325e-01, 9.9271e-01,\n",
       "             9.9187e-01, 9.9171e-01, 9.9046e-01, 9.8971e-01, 9.8921e-01, 9.8668e-01,\n",
       "             9.8597e-01, 9.8315e-01, 9.8125e-01, 9.8025e-01, 9.7916e-01, 9.7827e-01,\n",
       "             9.7665e-01, 9.7531e-01, 9.7489e-01, 9.7112e-01, 9.7070e-01, 9.6940e-01,\n",
       "             9.6566e-01, 9.6247e-01, 9.6227e-01, 9.6084e-01, 9.5897e-01, 9.5707e-01,\n",
       "             9.4645e-01, 9.4608e-01, 9.3581e-01, 9.3006e-01, 9.2517e-01, 9.1069e-01,\n",
       "             9.0388e-01, 8.9452e-01, 8.9276e-01, 8.8220e-01, 8.7619e-01, 8.6546e-01,\n",
       "             8.6052e-01, 8.3821e-01, 8.3480e-01, 8.1681e-01, 7.9201e-01, 7.3492e-01,\n",
       "             7.1196e-01, 6.5833e-01, 6.5746e-01, 6.5190e-01, 6.1924e-01, 5.9635e-01,\n",
       "             5.3256e-01, 5.0160e-01, 5.0084e-01, 5.0003e-01, 4.9893e-01, 4.9005e-01,\n",
       "             4.7971e-01, 4.7782e-01, 4.6317e-01, 4.5157e-01, 3.5226e-01, 2.5127e-01,\n",
       "             2.4789e-01, 2.3625e-01, 1.7506e-01, 1.6516e-01, 1.6182e-01, 1.2735e-01,\n",
       "             1.1565e-01, 1.1514e-01, 1.1144e-01, 8.8570e-02, 7.9579e-02, 7.4363e-02,\n",
       "             6.4933e-02, 6.2796e-02, 5.9519e-02, 5.6016e-02, 5.0828e-02, 4.5854e-02,\n",
       "             4.2999e-02, 4.2122e-02, 4.1954e-02, 3.9727e-02, 3.8677e-02, 3.7560e-02,\n",
       "             3.6349e-02, 2.9827e-02, 2.2701e-02, 2.1597e-02, 2.1433e-02, 2.0716e-02,\n",
       "             1.8205e-02, 1.7681e-02, 1.4652e-02, 1.4383e-02, 1.3926e-02, 1.3689e-02,\n",
       "             1.3380e-02, 1.2492e-02, 1.2055e-02, 9.5773e-03, 8.6885e-03, 7.3943e-03,\n",
       "             6.8251e-03, 5.9153e-03, 5.8718e-03, 4.7451e-03, 4.5839e-03, 4.4682e-03,\n",
       "             4.2955e-03, 3.7166e-03, 2.9871e-03, 2.8405e-03, 2.6581e-03, 2.2919e-03,\n",
       "             2.2261e-03, 1.9477e-03, 1.6877e-03, 1.6641e-03, 1.3879e-03, 1.0746e-03,\n",
       "             1.0581e-03, 1.0082e-03, 9.6571e-04, 9.6040e-04, 7.6030e-04, 7.5373e-04,\n",
       "             7.3473e-04, 7.2279e-04, 6.7790e-04, 6.0765e-04, 5.0008e-04, 4.9608e-04,\n",
       "             3.2592e-04, 3.0470e-04, 2.9243e-04, 2.5713e-04, 2.3429e-04, 2.2750e-04,\n",
       "             1.9729e-04, 1.9297e-04, 1.7808e-04, 1.6145e-04, 1.4855e-04, 1.3590e-04,\n",
       "             1.2585e-04, 1.0611e-04, 1.0187e-04, 6.2303e-05, 5.6543e-05, 5.5891e-05,\n",
       "             5.4893e-05, 4.7838e-05, 4.7387e-05, 4.1708e-05, 3.9505e-05, 3.7716e-05,\n",
       "             3.7668e-05, 3.7280e-05, 3.6860e-05, 3.5279e-05, 3.3799e-05, 3.1570e-05,\n",
       "             3.1468e-05, 2.5427e-05, 2.5389e-05, 2.3811e-05, 2.0394e-05, 1.9678e-05,\n",
       "             1.8837e-05, 1.5251e-05, 1.5183e-05, 1.2800e-05, 1.2405e-05, 1.2171e-05,\n",
       "             1.0602e-05, 1.0317e-05, 9.9718e-06, 9.9672e-06, 9.6241e-06, 9.2730e-06,\n",
       "             8.6842e-06, 8.3247e-06, 8.2703e-06, 8.0856e-06, 6.7341e-06, 6.6518e-06,\n",
       "             6.6345e-06, 6.5716e-06, 6.3593e-06, 6.2648e-06, 5.9867e-06, 5.8363e-06,\n",
       "             5.4465e-06, 4.9838e-06, 4.9042e-06, 4.6100e-06, 4.5266e-06, 3.5231e-06,\n",
       "             3.1941e-06, 3.0782e-06, 2.9079e-06, 2.7403e-06, 2.5611e-06, 2.5136e-06,\n",
       "             2.5091e-06, 2.2554e-06, 2.2174e-06, 1.8663e-06, 1.8152e-06, 1.8047e-06,\n",
       "             1.7207e-06, 1.6995e-06, 1.5385e-06, 1.4259e-06, 1.3301e-06, 1.2852e-06,\n",
       "             1.1326e-06, 9.6685e-07, 8.6824e-07, 7.2698e-07, 6.8295e-07, 6.2947e-07,\n",
       "             5.6224e-07, 5.0684e-07, 4.9416e-07, 4.9299e-07, 4.8941e-07, 4.8762e-07,\n",
       "             3.9055e-07, 3.3427e-07, 3.2868e-07, 3.1100e-07, 2.6795e-07, 2.5452e-07,\n",
       "             2.2294e-07, 2.2153e-07, 1.9314e-07, 1.6427e-07, 1.5416e-07, 1.4673e-07,\n",
       "             1.4209e-07, 1.3085e-07, 1.2305e-07, 1.1030e-07, 1.1019e-07, 9.9458e-08,\n",
       "             8.6880e-08, 7.7733e-08, 7.7374e-08, 5.6347e-08, 5.6337e-08, 4.8381e-08,\n",
       "             4.6587e-08, 4.4023e-08, 4.0838e-08, 4.0355e-08, 3.9290e-08, 3.8665e-08,\n",
       "             3.8310e-08, 3.1931e-08, 2.2106e-08, 2.1744e-08, 2.1678e-08, 2.1004e-08,\n",
       "             1.8506e-08, 1.5613e-08, 1.3810e-08, 1.0046e-08, 9.4018e-09, 9.0396e-09,\n",
       "             8.0778e-09, 7.9612e-09, 5.9543e-09, 4.8740e-09, 4.5500e-09, 4.1817e-09,\n",
       "             3.5493e-09, 3.3467e-09, 3.3405e-09, 2.7731e-09, 2.4205e-09, 2.3752e-09,\n",
       "             2.3087e-09, 1.7363e-09, 1.1839e-09, 1.1556e-09, 1.1051e-09, 1.0298e-09,\n",
       "             7.4833e-10, 6.9390e-10, 5.3526e-10, 4.3625e-10, 4.0805e-10, 3.1073e-10,\n",
       "             3.0146e-10, 2.8699e-10, 2.8692e-10, 2.0208e-10, 1.5592e-10, 1.4076e-10,\n",
       "             1.4059e-10, 1.1696e-10, 1.1466e-10, 1.0570e-10, 1.0009e-10, 8.8120e-11,\n",
       "             6.5045e-11, 5.8678e-11, 5.4898e-11, 5.3409e-11, 4.8837e-11, 4.0286e-11,\n",
       "             3.8155e-11, 2.9495e-11, 2.3820e-11, 2.0933e-11, 1.8918e-11, 1.7547e-11,\n",
       "             1.6541e-11, 9.3959e-12, 8.2467e-12, 3.2920e-12, 2.6369e-12, 2.5194e-12,\n",
       "             2.3893e-12, 2.3136e-12, 2.0601e-12, 1.5210e-12, 7.2334e-13, 6.9275e-13,\n",
       "             6.4818e-13, 5.0680e-13, 4.6295e-13, 4.3799e-13, 4.3565e-13, 2.8098e-13,\n",
       "             2.3741e-13, 2.3487e-13, 1.9022e-13, 7.5560e-14, 3.9188e-14, 1.1966e-14,\n",
       "             1.0215e-14, 7.2041e-15, 2.9108e-15, 2.8382e-15, 9.9976e-16, 4.2557e-16,\n",
       "             3.2816e-16, 8.0560e-17, 8.5218e-18, 3.1177e-18])}},\n",
       "   {'fpr': np.float64(0.17763157894736842),\n",
       "    'tpr': np.float64(0.995418098510882),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0033, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0197,\n",
       "             0.0197, 0.0197, 0.0230, 0.0230, 0.0263, 0.0296, 0.0296, 0.0296, 0.0296,\n",
       "             0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296,\n",
       "             0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0329,\n",
       "             0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329,\n",
       "             0.0329, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362,\n",
       "             0.0362, 0.0362, 0.0362, 0.0395, 0.0395, 0.0395, 0.0395, 0.0395, 0.0428,\n",
       "             0.0428, 0.0461, 0.0461, 0.0461, 0.0461, 0.0493, 0.0526, 0.0526, 0.0559,\n",
       "             0.0592, 0.0592, 0.0625, 0.0625, 0.0658, 0.0658, 0.0658, 0.0658, 0.0691,\n",
       "             0.0691, 0.0691, 0.0691, 0.0724, 0.0757, 0.0789, 0.0822, 0.0855, 0.0888,\n",
       "             0.0888, 0.0921, 0.0921, 0.0954, 0.0954, 0.0954, 0.0987, 0.1020, 0.1053,\n",
       "             0.1053, 0.1053, 0.1086, 0.1118, 0.1151, 0.1151, 0.1184, 0.1217, 0.1250,\n",
       "             0.1283, 0.1316, 0.1349, 0.1349, 0.1382, 0.1414, 0.1447, 0.1480, 0.1513,\n",
       "             0.1546, 0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1743, 0.1776,\n",
       "             0.1809, 0.1842, 0.1875, 0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072,\n",
       "             0.2105, 0.2138, 0.2171, 0.2204, 0.2237, 0.2270, 0.2270, 0.2303, 0.2336,\n",
       "             0.2368, 0.2401, 0.2434, 0.2467, 0.2500, 0.2533, 0.2566, 0.2599, 0.2632,\n",
       "             0.2664, 0.2697, 0.2730, 0.2763, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928,\n",
       "             0.2961, 0.2993, 0.3026, 0.3059, 0.3092, 0.3092, 0.3125, 0.3158, 0.3191,\n",
       "             0.3224, 0.3257, 0.3289, 0.3322, 0.3355, 0.3388, 0.3421, 0.3454, 0.3487,\n",
       "             0.3520, 0.3553, 0.3586, 0.3618, 0.3651, 0.3684, 0.3717, 0.3750, 0.3783,\n",
       "             0.3816, 0.3849, 0.3882, 0.3914, 0.3947, 0.3947, 0.3980, 0.4013, 0.4046,\n",
       "             0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243, 0.4276, 0.4309, 0.4342,\n",
       "             0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539, 0.4572, 0.4605, 0.4638,\n",
       "             0.4671, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836, 0.4868, 0.4901,\n",
       "             0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132, 0.5164, 0.5197,\n",
       "             0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428, 0.5461, 0.5493,\n",
       "             0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724, 0.5757, 0.5789,\n",
       "             0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020, 0.6053, 0.6086,\n",
       "             0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316, 0.6349, 0.6382,\n",
       "             0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612, 0.6645, 0.6678,\n",
       "             0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908, 0.6941, 0.6974,\n",
       "             0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204, 0.7237, 0.7270,\n",
       "             0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500, 0.7533, 0.7566,\n",
       "             0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796, 0.7829, 0.7862,\n",
       "             0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092, 0.8125, 0.8158,\n",
       "             0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388, 0.8421, 0.8454,\n",
       "             0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684, 0.8717, 0.8750,\n",
       "             0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980, 0.9013, 0.9046,\n",
       "             0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276, 0.9309, 0.9342,\n",
       "             0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572, 0.9605, 0.9638,\n",
       "             0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868, 0.9901, 0.9934,\n",
       "             0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.6976, 0.7709, 0.7984, 0.8190, 0.8351, 0.8419, 0.8545, 0.8580,\n",
       "             0.8648, 0.8694, 0.8751, 0.8774, 0.8809, 0.8820, 0.8843, 0.8866, 0.8877,\n",
       "             0.8889, 0.8900, 0.8923, 0.8935, 0.8946, 0.8958, 0.8969, 0.8981, 0.8992,\n",
       "             0.9015, 0.9038, 0.9049, 0.9061, 0.9072, 0.9084, 0.9095, 0.9118, 0.9118,\n",
       "             0.9129, 0.9152, 0.9164, 0.9175, 0.9187, 0.9187, 0.9198, 0.9221, 0.9233,\n",
       "             0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324, 0.9336,\n",
       "             0.9347, 0.9359, 0.9370, 0.9381, 0.9393, 0.9404, 0.9416, 0.9427, 0.9427,\n",
       "             0.9439, 0.9450, 0.9462, 0.9473, 0.9485, 0.9496, 0.9507, 0.9519, 0.9530,\n",
       "             0.9542, 0.9542, 0.9553, 0.9565, 0.9576, 0.9588, 0.9599, 0.9611, 0.9622,\n",
       "             0.9633, 0.9645, 0.9656, 0.9656, 0.9668, 0.9679, 0.9691, 0.9702, 0.9702,\n",
       "             0.9714, 0.9714, 0.9725, 0.9737, 0.9748, 0.9748, 0.9748, 0.9759, 0.9759,\n",
       "             0.9759, 0.9771, 0.9771, 0.9782, 0.9782, 0.9794, 0.9805, 0.9817, 0.9817,\n",
       "             0.9828, 0.9840, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851,\n",
       "             0.9863, 0.9863, 0.9874, 0.9874, 0.9885, 0.9897, 0.9897, 0.9897, 0.9897,\n",
       "             0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9982e-01, 9.9977e-01, 9.9976e-01, 9.9964e-01, 9.9961e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9957e-01, 9.9954e-01, 9.9948e-01, 9.9947e-01, 9.9944e-01,\n",
       "             9.9934e-01, 9.9932e-01, 9.9917e-01, 9.9909e-01, 9.9909e-01, 9.9896e-01,\n",
       "             9.9894e-01, 9.9893e-01, 9.9885e-01, 9.9880e-01, 9.9864e-01, 9.9844e-01,\n",
       "             9.9826e-01, 9.9817e-01, 9.9798e-01, 9.9765e-01, 9.9721e-01, 9.9691e-01,\n",
       "             9.9578e-01, 9.9569e-01, 9.9409e-01, 9.9367e-01, 9.9283e-01, 9.9182e-01,\n",
       "             9.9160e-01, 9.8883e-01, 9.8622e-01, 9.8500e-01, 9.8168e-01, 9.7575e-01,\n",
       "             9.7484e-01, 9.7146e-01, 9.7013e-01, 9.6977e-01, 9.6651e-01, 9.6627e-01,\n",
       "             9.5161e-01, 9.3841e-01, 9.3549e-01, 9.2404e-01, 8.8920e-01, 8.7282e-01,\n",
       "             8.6762e-01, 8.4946e-01, 8.1768e-01, 7.9154e-01, 6.7849e-01, 6.7836e-01,\n",
       "             6.7544e-01, 6.6965e-01, 6.3907e-01, 6.3793e-01, 6.3178e-01, 6.0931e-01,\n",
       "             5.6452e-01, 5.2369e-01, 5.1456e-01, 3.6444e-01, 2.7300e-01, 2.2936e-01,\n",
       "             2.1551e-01, 1.9631e-01, 1.6401e-01, 1.6240e-01, 1.5754e-01, 1.5373e-01,\n",
       "             1.4891e-01, 1.4305e-01, 1.3626e-01, 1.2417e-01, 1.1138e-01, 1.0466e-01,\n",
       "             1.0132e-01, 8.4525e-02, 7.7086e-02, 7.3245e-02, 7.0051e-02, 5.5668e-02,\n",
       "             4.3810e-02, 4.0063e-02, 4.0026e-02, 3.5353e-02, 2.9336e-02, 2.8931e-02,\n",
       "             2.7630e-02, 2.6995e-02, 2.6353e-02, 2.1657e-02, 2.1613e-02, 2.0801e-02,\n",
       "             1.8661e-02, 1.4554e-02, 1.2250e-02, 1.1615e-02, 1.0772e-02, 1.0470e-02,\n",
       "             9.1324e-03, 8.6019e-03, 7.4008e-03, 5.9201e-03, 5.5595e-03, 5.4560e-03,\n",
       "             5.1302e-03, 3.8553e-03, 3.3107e-03, 3.0854e-03, 2.9916e-03, 2.3762e-03,\n",
       "             2.1386e-03, 2.0350e-03, 1.9745e-03, 1.9638e-03, 1.7847e-03, 1.2395e-03,\n",
       "             1.1996e-03, 9.1967e-04, 9.0755e-04, 8.5835e-04, 8.5585e-04, 7.2024e-04,\n",
       "             6.6341e-04, 6.2517e-04, 6.2282e-04, 5.9627e-04, 5.1199e-04, 4.9121e-04,\n",
       "             4.8410e-04, 4.2461e-04, 4.1513e-04, 4.1136e-04, 4.0817e-04, 4.0380e-04,\n",
       "             3.6554e-04, 2.8119e-04, 2.5048e-04, 2.4316e-04, 1.9898e-04, 1.7218e-04,\n",
       "             1.4106e-04, 1.3682e-04, 1.2167e-04, 1.1666e-04, 1.1217e-04, 8.5247e-05,\n",
       "             7.5743e-05, 6.7267e-05, 5.6394e-05, 5.3400e-05, 4.3567e-05, 3.8956e-05,\n",
       "             3.8225e-05, 3.8156e-05, 3.6494e-05, 3.6192e-05, 3.3967e-05, 3.1056e-05,\n",
       "             2.9470e-05, 2.3413e-05, 2.2875e-05, 2.2448e-05, 2.1889e-05, 2.0838e-05,\n",
       "             2.0727e-05, 1.9019e-05, 1.7213e-05, 1.6679e-05, 1.4648e-05, 1.4597e-05,\n",
       "             1.3537e-05, 1.3464e-05, 1.1701e-05, 1.1236e-05, 1.0813e-05, 1.0502e-05,\n",
       "             9.0898e-06, 8.8345e-06, 7.1619e-06, 5.7696e-06, 5.6213e-06, 5.1020e-06,\n",
       "             4.4070e-06, 4.1961e-06, 4.1686e-06, 3.5947e-06, 3.4242e-06, 2.9045e-06,\n",
       "             2.8752e-06, 2.7840e-06, 2.7233e-06, 2.6858e-06, 2.5552e-06, 2.2109e-06,\n",
       "             2.1371e-06, 2.0015e-06, 1.7796e-06, 1.7335e-06, 1.5868e-06, 1.5083e-06,\n",
       "             1.4604e-06, 1.3229e-06, 1.1445e-06, 1.1409e-06, 1.0957e-06, 1.0242e-06,\n",
       "             8.0613e-07, 7.1527e-07, 7.0778e-07, 6.1398e-07, 5.7409e-07, 5.6036e-07,\n",
       "             4.4428e-07, 3.1682e-07, 2.7270e-07, 1.3086e-07, 1.1890e-07, 9.0668e-08,\n",
       "             8.3845e-08, 8.3323e-08, 7.6464e-08, 7.3348e-08, 6.6068e-08, 6.4276e-08,\n",
       "             5.2150e-08, 5.0825e-08, 4.7216e-08, 4.6207e-08, 4.5815e-08, 4.4705e-08,\n",
       "             3.9971e-08, 3.8692e-08, 2.7682e-08, 2.2107e-08, 1.9312e-08, 1.8963e-08,\n",
       "             1.7567e-08, 1.4953e-08, 1.4002e-08, 8.7553e-09, 8.0542e-09, 7.6910e-09,\n",
       "             6.8148e-09, 3.4485e-09, 3.3473e-09, 2.8710e-09, 2.5653e-09, 2.2644e-09,\n",
       "             2.0469e-09, 1.8982e-09, 1.7965e-09, 1.2992e-09, 1.1736e-09, 9.0156e-10,\n",
       "             8.5375e-10, 7.7809e-10, 7.6770e-10, 5.5510e-10, 5.0530e-10, 5.0210e-10,\n",
       "             3.9234e-10, 3.6347e-10, 3.4144e-10, 2.7172e-10, 2.5942e-10, 2.5756e-10,\n",
       "             2.3960e-10, 1.3474e-10, 1.2373e-10, 1.0025e-10, 6.1434e-11, 5.4665e-11,\n",
       "             4.8749e-11, 4.3882e-11, 4.1858e-11, 3.9763e-11, 3.8400e-11, 3.1821e-11,\n",
       "             2.7596e-11, 2.4727e-11, 2.1759e-11, 2.1354e-11, 1.5169e-11, 5.3018e-12,\n",
       "             4.8904e-12, 3.6403e-12, 3.5952e-12, 1.9553e-12, 1.6336e-12, 1.4189e-12,\n",
       "             1.0418e-12, 7.4736e-13, 5.7341e-13, 4.0922e-13, 3.4555e-13, 3.0602e-13,\n",
       "             1.6929e-13, 1.4972e-13, 1.0869e-13, 1.0820e-13, 1.0252e-13, 3.1188e-14,\n",
       "             3.0694e-14, 2.7041e-14, 1.0342e-14, 9.1505e-15, 1.9945e-15, 5.2114e-16,\n",
       "             2.8243e-16, 9.2959e-17, 4.4538e-17, 2.3259e-17, 3.1942e-21])}},\n",
       "   {'fpr': np.float64(0.13157894736842105),\n",
       "    'tpr': np.float64(0.993127147766323),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0099, 0.0099, 0.0099, 0.0132, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0230, 0.0230, 0.0230, 0.0263, 0.0263, 0.0263, 0.0296, 0.0329,\n",
       "             0.0329, 0.0362, 0.0362, 0.0362, 0.0395, 0.0395, 0.0395, 0.0428, 0.0428,\n",
       "             0.0428, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461,\n",
       "             0.0461, 0.0461, 0.0461, 0.0461, 0.0493, 0.0526, 0.0526, 0.0526, 0.0559,\n",
       "             0.0559, 0.0559, 0.0592, 0.0625, 0.0625, 0.0625, 0.0658, 0.0658, 0.0658,\n",
       "             0.0691, 0.0691, 0.0724, 0.0757, 0.0757, 0.0757, 0.0757, 0.0789, 0.0789,\n",
       "             0.0822, 0.0822, 0.0822, 0.0855, 0.0888, 0.0888, 0.0888, 0.0888, 0.0921,\n",
       "             0.0954, 0.0987, 0.1020, 0.1020, 0.1053, 0.1086, 0.1118, 0.1151, 0.1184,\n",
       "             0.1217, 0.1250, 0.1283, 0.1316, 0.1349, 0.1382, 0.1414, 0.1447, 0.1480,\n",
       "             0.1513, 0.1546, 0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1776,\n",
       "             0.1809, 0.1809, 0.1842, 0.1875, 0.1908, 0.1941, 0.1941, 0.1974, 0.2007,\n",
       "             0.2039, 0.2072, 0.2105, 0.2138, 0.2171, 0.2204, 0.2237, 0.2237, 0.2270,\n",
       "             0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2500, 0.2533, 0.2566,\n",
       "             0.2599, 0.2632, 0.2632, 0.2664, 0.2664, 0.2697, 0.2730, 0.2763, 0.2796,\n",
       "             0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2961, 0.2993, 0.3026, 0.3059,\n",
       "             0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322, 0.3355,\n",
       "             0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618, 0.3651,\n",
       "             0.3684, 0.3717, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914, 0.3947,\n",
       "             0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243,\n",
       "             0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539,\n",
       "             0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836,\n",
       "             0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132,\n",
       "             0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428,\n",
       "             0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724,\n",
       "             0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020,\n",
       "             0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316,\n",
       "             0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612,\n",
       "             0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908,\n",
       "             0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204,\n",
       "             0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500,\n",
       "             0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796,\n",
       "             0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092,\n",
       "             0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388,\n",
       "             0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684,\n",
       "             0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980,\n",
       "             0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276,\n",
       "             0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572,\n",
       "             0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868,\n",
       "             0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7950, 0.8282, 0.8373, 0.8499, 0.8580, 0.8648, 0.8683, 0.8729,\n",
       "             0.8763, 0.8797, 0.8832, 0.8843, 0.8855, 0.8866, 0.8900, 0.8923, 0.8935,\n",
       "             0.8958, 0.8981, 0.8992, 0.9015, 0.9061, 0.9084, 0.9095, 0.9118, 0.9129,\n",
       "             0.9141, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198, 0.9210, 0.9221, 0.9233,\n",
       "             0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324, 0.9336,\n",
       "             0.9347, 0.9359, 0.9370, 0.9381, 0.9393, 0.9404, 0.9416, 0.9427, 0.9439,\n",
       "             0.9450, 0.9450, 0.9462, 0.9473, 0.9473, 0.9485, 0.9496, 0.9507, 0.9507,\n",
       "             0.9519, 0.9519, 0.9530, 0.9542, 0.9542, 0.9553, 0.9565, 0.9565, 0.9576,\n",
       "             0.9588, 0.9588, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668,\n",
       "             0.9679, 0.9691, 0.9702, 0.9714, 0.9714, 0.9714, 0.9725, 0.9737, 0.9737,\n",
       "             0.9748, 0.9759, 0.9759, 0.9759, 0.9771, 0.9782, 0.9782, 0.9794, 0.9805,\n",
       "             0.9805, 0.9817, 0.9817, 0.9817, 0.9828, 0.9840, 0.9851, 0.9851, 0.9863,\n",
       "             0.9863, 0.9874, 0.9885, 0.9885, 0.9885, 0.9897, 0.9908, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9991e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9986e-01, 9.9982e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9975e-01, 9.9974e-01, 9.9967e-01, 9.9966e-01, 9.9964e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9944e-01, 9.9919e-01, 9.9916e-01, 9.9902e-01, 9.9874e-01,\n",
       "             9.9852e-01, 9.9831e-01, 9.9830e-01, 9.9781e-01, 9.9735e-01, 9.9702e-01,\n",
       "             9.9694e-01, 9.9676e-01, 9.9634e-01, 9.9609e-01, 9.9406e-01, 9.9389e-01,\n",
       "             9.9335e-01, 9.9309e-01, 9.9234e-01, 9.9131e-01, 9.9098e-01, 9.8872e-01,\n",
       "             9.8518e-01, 9.8264e-01, 9.7291e-01, 9.5423e-01, 9.5342e-01, 9.5060e-01,\n",
       "             9.4909e-01, 9.4882e-01, 9.4556e-01, 9.4061e-01, 9.3727e-01, 9.1842e-01,\n",
       "             9.0650e-01, 8.8941e-01, 8.7347e-01, 8.6123e-01, 8.6108e-01, 8.5113e-01,\n",
       "             7.7457e-01, 7.3035e-01, 6.6470e-01, 6.6446e-01, 6.2278e-01, 5.7795e-01,\n",
       "             5.6275e-01, 5.5854e-01, 5.2411e-01, 5.0130e-01, 4.5169e-01, 4.3158e-01,\n",
       "             3.6414e-01, 2.1954e-01, 1.9185e-01, 1.4197e-01, 1.3954e-01, 1.3597e-01,\n",
       "             9.6032e-02, 8.4016e-02, 5.2165e-02, 4.8580e-02, 4.5914e-02, 2.8372e-02,\n",
       "             2.5319e-02, 1.8770e-02, 1.8338e-02, 1.7950e-02, 1.4605e-02, 1.4549e-02,\n",
       "             1.3584e-02, 1.1691e-02, 1.0098e-02, 9.7631e-03, 8.8883e-03, 7.8653e-03,\n",
       "             7.0442e-03, 5.2857e-03, 4.5150e-03, 3.6618e-03, 2.6304e-03, 2.2645e-03,\n",
       "             1.8807e-03, 1.4082e-03, 9.3250e-04, 9.0116e-04, 8.2317e-04, 7.7757e-04,\n",
       "             5.1538e-04, 4.1842e-04, 3.8197e-04, 3.5731e-04, 2.5923e-04, 2.5755e-04,\n",
       "             1.2145e-04, 1.1065e-04, 1.0940e-04, 8.7445e-05, 7.9692e-05, 7.0703e-05,\n",
       "             4.8156e-05, 3.9831e-05, 3.7507e-05, 3.7225e-05, 2.3282e-05, 2.2304e-05,\n",
       "             2.1540e-05, 2.1371e-05, 2.0706e-05, 1.9575e-05, 1.8770e-05, 1.6750e-05,\n",
       "             1.5349e-05, 1.3825e-05, 1.0113e-05, 9.2036e-06, 8.5331e-06, 7.7266e-06,\n",
       "             7.1806e-06, 5.8582e-06, 5.4885e-06, 5.1387e-06, 4.8250e-06, 4.0289e-06,\n",
       "             3.1569e-06, 3.1511e-06, 2.8240e-06, 2.6724e-06, 2.5671e-06, 2.1737e-06,\n",
       "             1.7429e-06, 9.9936e-07, 8.1196e-07, 8.0434e-07, 6.4418e-07, 5.8661e-07,\n",
       "             5.7933e-07, 5.7566e-07, 5.4391e-07, 5.3248e-07, 5.0547e-07, 4.6375e-07,\n",
       "             4.2435e-07, 3.6169e-07, 3.3805e-07, 3.3455e-07, 3.3277e-07, 3.0262e-07,\n",
       "             2.8188e-07, 2.3830e-07, 2.0680e-07, 1.9049e-07, 1.6427e-07, 1.2594e-07,\n",
       "             1.1840e-07, 8.0713e-08, 7.8793e-08, 7.8782e-08, 5.7112e-08, 5.3930e-08,\n",
       "             3.6286e-08, 3.4501e-08, 3.2654e-08, 2.5635e-08, 2.2616e-08, 2.0641e-08,\n",
       "             1.9872e-08, 1.8969e-08, 1.8116e-08, 1.6682e-08, 1.6558e-08, 1.5859e-08,\n",
       "             1.5312e-08, 1.1135e-08, 1.1021e-08, 1.0730e-08, 8.9364e-09, 8.4541e-09,\n",
       "             7.8123e-09, 7.3091e-09, 7.1136e-09, 6.2477e-09, 6.1086e-09, 5.9087e-09,\n",
       "             5.8897e-09, 5.5949e-09, 5.0585e-09, 4.7929e-09, 4.4455e-09, 3.9121e-09,\n",
       "             3.3342e-09, 3.2197e-09, 2.8984e-09, 2.8250e-09, 2.7219e-09, 1.7912e-09,\n",
       "             1.5696e-09, 1.3134e-09, 1.1925e-09, 1.1780e-09, 1.0732e-09, 9.8221e-10,\n",
       "             9.6175e-10, 8.2748e-10, 7.9363e-10, 6.0986e-10, 5.9861e-10, 5.7320e-10,\n",
       "             4.1987e-10, 3.6584e-10, 2.9200e-10, 2.5496e-10, 2.4972e-10, 2.2540e-10,\n",
       "             1.5285e-10, 1.3122e-10, 1.2483e-10, 1.1313e-10, 9.6272e-11, 7.7510e-11,\n",
       "             7.3713e-11, 6.4173e-11, 5.8209e-11, 5.7305e-11, 5.0626e-11, 4.4844e-11,\n",
       "             4.2383e-11, 4.0488e-11, 4.0139e-11, 3.4055e-11, 2.7292e-11, 2.5550e-11,\n",
       "             2.4468e-11, 2.2829e-11, 1.5903e-11, 1.3712e-11, 1.2224e-11, 1.1257e-11,\n",
       "             9.6460e-12, 7.7197e-12, 5.6371e-12, 4.5658e-12, 3.2727e-12, 3.2019e-12,\n",
       "             2.7752e-12, 2.5617e-12, 1.7159e-12, 1.5845e-12, 1.5565e-12, 1.4828e-12,\n",
       "             1.1418e-12, 1.0255e-12, 9.2246e-13, 8.3785e-13, 7.7438e-13, 7.1671e-13,\n",
       "             6.5876e-13, 6.5564e-13, 5.7458e-13, 5.6083e-13, 4.9392e-13, 4.4118e-13,\n",
       "             4.1608e-13, 3.8826e-13, 3.4182e-13, 3.2735e-13, 2.9248e-13, 2.4837e-13,\n",
       "             2.0433e-13, 1.8641e-13, 1.4622e-13, 7.7741e-14, 7.3269e-14, 7.2671e-14,\n",
       "             7.1698e-14, 6.9794e-14, 6.6237e-14, 6.5352e-14, 4.4759e-14, 3.9862e-14,\n",
       "             2.4474e-14, 2.0791e-14, 1.7938e-14, 1.5844e-14, 6.2983e-15, 5.6475e-15,\n",
       "             3.6196e-15, 3.3536e-15, 2.6120e-15, 2.1364e-15, 1.3300e-15, 1.2471e-15,\n",
       "             1.2438e-15, 1.2341e-15, 1.0267e-15, 1.0108e-15, 8.4275e-16, 5.8087e-16,\n",
       "             5.4593e-16, 1.5754e-16, 1.4745e-16, 9.9001e-17, 5.6980e-17, 5.5530e-17,\n",
       "             2.6460e-17, 1.6027e-17, 1.5665e-17, 9.0270e-18, 4.0013e-18, 1.6302e-18,\n",
       "             1.1097e-18, 9.5623e-19, 4.6432e-19, 2.4010e-19, 1.1908e-19, 3.6979e-21,\n",
       "             1.3850e-22, 1.4794e-23, 1.2177e-23, 3.2834e-24])}},\n",
       "   {'fpr': np.float64(0.05592105263157895),\n",
       "    'tpr': np.float64(0.9782359679266895),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0230, 0.0230, 0.0230, 0.0263, 0.0263, 0.0263, 0.0263, 0.0296,\n",
       "             0.0329, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0395, 0.0428,\n",
       "             0.0428, 0.0461, 0.0493, 0.0526, 0.0559, 0.0559, 0.0592, 0.0592, 0.0592,\n",
       "             0.0625, 0.0625, 0.0658, 0.0658, 0.0658, 0.0658, 0.0691, 0.0724, 0.0757,\n",
       "             0.0789, 0.0789, 0.0789, 0.0789, 0.0789, 0.0822, 0.0855, 0.0855, 0.0888,\n",
       "             0.0921, 0.0954, 0.0954, 0.0987, 0.1020, 0.1020, 0.1053, 0.1053, 0.1086,\n",
       "             0.1118, 0.1151, 0.1184, 0.1217, 0.1250, 0.1283, 0.1316, 0.1349, 0.1382,\n",
       "             0.1382, 0.1414, 0.1414, 0.1447, 0.1480, 0.1513, 0.1546, 0.1579, 0.1612,\n",
       "             0.1645, 0.1678, 0.1711, 0.1743, 0.1776, 0.1809, 0.1809, 0.1842, 0.1875,\n",
       "             0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138, 0.2171,\n",
       "             0.2204, 0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467,\n",
       "             0.2500, 0.2533, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697, 0.2730, 0.2763,\n",
       "             0.2796, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026,\n",
       "             0.3059, 0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322,\n",
       "             0.3355, 0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618,\n",
       "             0.3651, 0.3684, 0.3717, 0.3750, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882,\n",
       "             0.3914, 0.3947, 0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178,\n",
       "             0.4211, 0.4243, 0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474,\n",
       "             0.4507, 0.4539, 0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770,\n",
       "             0.4803, 0.4836, 0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066,\n",
       "             0.5099, 0.5132, 0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362,\n",
       "             0.5395, 0.5428, 0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658,\n",
       "             0.5691, 0.5724, 0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954,\n",
       "             0.5987, 0.6020, 0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250,\n",
       "             0.6283, 0.6316, 0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546,\n",
       "             0.6579, 0.6612, 0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842,\n",
       "             0.6875, 0.6908, 0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138,\n",
       "             0.7171, 0.7204, 0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434,\n",
       "             0.7467, 0.7500, 0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730,\n",
       "             0.7763, 0.7796, 0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026,\n",
       "             0.8059, 0.8092, 0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322,\n",
       "             0.8355, 0.8388, 0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618,\n",
       "             0.8651, 0.8684, 0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914,\n",
       "             0.8947, 0.8980, 0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211,\n",
       "             0.9243, 0.9276, 0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507,\n",
       "             0.9539, 0.9572, 0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803,\n",
       "             0.9836, 0.9868, 0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7801, 0.8087, 0.8236, 0.8259, 0.8316, 0.8362, 0.8419, 0.8465,\n",
       "             0.8499, 0.8511, 0.8522, 0.8534, 0.8557, 0.8580, 0.8603, 0.8603, 0.8614,\n",
       "             0.8625, 0.8637, 0.8648, 0.8660, 0.8683, 0.8706, 0.8717, 0.8729, 0.8740,\n",
       "             0.8751, 0.8763, 0.8786, 0.8797, 0.8809, 0.8820, 0.8820, 0.8832, 0.8843,\n",
       "             0.8855, 0.8866, 0.8877, 0.8889, 0.8912, 0.8923, 0.8935, 0.8946, 0.8958,\n",
       "             0.8969, 0.8981, 0.8992, 0.9003, 0.9015, 0.9026, 0.9038, 0.9049, 0.9049,\n",
       "             0.9061, 0.9072, 0.9084, 0.9095, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152,\n",
       "             0.9164, 0.9175, 0.9187, 0.9198, 0.9210, 0.9221, 0.9233, 0.9244, 0.9255,\n",
       "             0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324, 0.9336, 0.9347, 0.9359,\n",
       "             0.9370, 0.9381, 0.9393, 0.9404, 0.9416, 0.9427, 0.9439, 0.9450, 0.9462,\n",
       "             0.9473, 0.9485, 0.9496, 0.9507, 0.9507, 0.9519, 0.9530, 0.9542, 0.9553,\n",
       "             0.9565, 0.9576, 0.9588, 0.9588, 0.9599, 0.9599, 0.9611, 0.9622, 0.9633,\n",
       "             0.9645, 0.9645, 0.9656, 0.9668, 0.9668, 0.9679, 0.9691, 0.9702, 0.9702,\n",
       "             0.9702, 0.9702, 0.9714, 0.9725, 0.9737, 0.9748, 0.9759, 0.9759, 0.9759,\n",
       "             0.9771, 0.9771, 0.9771, 0.9771, 0.9771, 0.9782, 0.9782, 0.9794, 0.9805,\n",
       "             0.9805, 0.9817, 0.9817, 0.9828, 0.9840, 0.9851, 0.9851, 0.9851, 0.9851,\n",
       "             0.9851, 0.9863, 0.9874, 0.9885, 0.9897, 0.9897, 0.9897, 0.9908, 0.9908,\n",
       "             0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9979e-01, 9.9978e-01, 9.9976e-01, 9.9976e-01, 9.9974e-01,\n",
       "             9.9965e-01, 9.9964e-01, 9.9963e-01, 9.9963e-01, 9.9963e-01, 9.9962e-01,\n",
       "             9.9955e-01, 9.9948e-01, 9.9938e-01, 9.9936e-01, 9.9931e-01, 9.9918e-01,\n",
       "             9.9909e-01, 9.9865e-01, 9.9833e-01, 9.9775e-01, 9.9763e-01, 9.9697e-01,\n",
       "             9.9688e-01, 9.9652e-01, 9.9639e-01, 9.9530e-01, 9.9518e-01, 9.9497e-01,\n",
       "             9.9479e-01, 9.9463e-01, 9.9459e-01, 9.9410e-01, 9.9409e-01, 9.9329e-01,\n",
       "             9.9284e-01, 9.9264e-01, 9.9261e-01, 9.9172e-01, 9.9024e-01, 9.8904e-01,\n",
       "             9.8416e-01, 9.8398e-01, 9.8391e-01, 9.7604e-01, 9.7567e-01, 9.7433e-01,\n",
       "             9.7411e-01, 9.7227e-01, 9.7196e-01, 9.6967e-01, 9.6310e-01, 9.5781e-01,\n",
       "             9.5628e-01, 9.5200e-01, 9.4686e-01, 9.4650e-01, 9.4070e-01, 9.2112e-01,\n",
       "             9.1821e-01, 8.9795e-01, 8.9766e-01, 8.7184e-01, 8.6806e-01, 8.6769e-01,\n",
       "             8.6021e-01, 7.6720e-01, 7.0475e-01, 6.8686e-01, 6.2231e-01, 5.7857e-01,\n",
       "             3.9709e-01, 3.2676e-01, 3.0241e-01, 2.9333e-01, 2.0963e-01, 1.5054e-01,\n",
       "             1.4476e-01, 1.4173e-01, 1.3058e-01, 8.5378e-02, 7.9462e-02, 3.8723e-02,\n",
       "             3.6934e-02, 2.8715e-02, 2.8198e-02, 1.7384e-02, 1.7369e-02, 8.9205e-03,\n",
       "             7.0605e-03, 3.9990e-03, 2.6726e-03, 1.7290e-03, 1.4921e-03, 9.4835e-04,\n",
       "             8.6367e-04, 7.5790e-04, 5.3087e-04, 5.1943e-04, 4.2687e-04, 3.4470e-04,\n",
       "             1.7904e-04, 1.6877e-04, 1.4132e-04, 1.1914e-04, 9.8637e-05, 8.9513e-05,\n",
       "             8.6545e-05, 6.9600e-05, 6.2527e-05, 6.0310e-05, 2.6988e-05, 2.2882e-05,\n",
       "             2.2272e-05, 1.9570e-05, 1.6078e-05, 1.4407e-05, 1.3017e-05, 7.1417e-06,\n",
       "             4.6072e-06, 2.1109e-06, 1.0926e-06, 9.6170e-07, 9.3919e-07, 7.8920e-07,\n",
       "             4.7735e-07, 4.6905e-07, 4.1466e-07, 3.4436e-07, 3.4264e-07, 3.1507e-07,\n",
       "             2.9750e-07, 2.9715e-07, 2.8353e-07, 2.7946e-07, 1.8027e-07, 1.7115e-07,\n",
       "             1.3285e-07, 1.0440e-07, 8.6431e-08, 7.7255e-08, 5.2340e-08, 4.0141e-08,\n",
       "             2.9723e-08, 2.1754e-08, 1.7415e-08, 1.6314e-08, 1.5192e-08, 1.0983e-08,\n",
       "             6.8954e-09, 5.8400e-09, 5.7724e-09, 3.8409e-09, 3.4806e-09, 3.4059e-09,\n",
       "             2.9491e-09, 2.7990e-09, 2.7227e-09, 2.3597e-09, 2.3477e-09, 2.2509e-09,\n",
       "             1.7261e-09, 1.3116e-09, 1.2724e-09, 1.2621e-09, 1.0407e-09, 9.8080e-10,\n",
       "             6.0515e-10, 5.3904e-10, 5.3397e-10, 5.1410e-10, 5.0718e-10, 4.3403e-10,\n",
       "             4.2179e-10, 4.0485e-10, 3.9125e-10, 3.5307e-10, 3.4842e-10, 3.1009e-10,\n",
       "             2.9597e-10, 2.6320e-10, 2.2231e-10, 1.2995e-10, 7.0321e-11, 6.2436e-11,\n",
       "             6.2319e-11, 6.0762e-11, 6.0435e-11, 5.5361e-11, 5.3550e-11, 4.6332e-11,\n",
       "             3.7187e-11, 2.7731e-11, 2.5191e-11, 2.4558e-11, 2.4493e-11, 2.4479e-11,\n",
       "             2.2311e-11, 1.5546e-11, 1.5147e-11, 1.2765e-11, 1.0986e-11, 1.0282e-11,\n",
       "             1.0236e-11, 1.0169e-11, 7.8837e-12, 6.9329e-12, 6.2287e-12, 6.0182e-12,\n",
       "             5.2228e-12, 4.6890e-12, 3.7130e-12, 2.6920e-12, 2.0165e-12, 1.2999e-12,\n",
       "             1.1915e-12, 8.6548e-13, 7.8341e-13, 7.4957e-13, 7.3163e-13, 6.5100e-13,\n",
       "             5.4638e-13, 3.2502e-13, 2.9503e-13, 2.5495e-13, 1.7923e-13, 1.5603e-13,\n",
       "             1.2555e-13, 1.0282e-13, 5.0474e-14, 4.6205e-14, 2.5839e-14, 1.9891e-14,\n",
       "             1.9184e-14, 1.7409e-14, 1.5447e-14, 1.5329e-14, 1.2466e-14, 1.2024e-14,\n",
       "             1.0160e-14, 1.0101e-14, 8.6405e-15, 8.5331e-15, 4.4710e-15, 4.4622e-15,\n",
       "             4.2292e-15, 4.2190e-15, 3.8142e-15, 3.7275e-15, 3.4074e-15, 3.0326e-15,\n",
       "             2.7170e-15, 2.3550e-15, 2.2612e-15, 1.8260e-15, 1.2404e-15, 1.0730e-15,\n",
       "             9.1413e-16, 9.0923e-16, 8.6207e-16, 8.3795e-16, 7.6821e-16, 7.3214e-16,\n",
       "             7.2846e-16, 6.7984e-16, 5.9535e-16, 5.9358e-16, 4.8837e-16, 4.4510e-16,\n",
       "             3.6368e-16, 3.5848e-16, 3.0656e-16, 2.7301e-16, 2.4867e-16, 2.4135e-16,\n",
       "             1.8515e-16, 1.8011e-16, 1.5361e-16, 1.2985e-16, 1.0588e-16, 9.4896e-17,\n",
       "             7.7639e-17, 7.1235e-17, 6.4968e-17, 5.8188e-17, 4.9143e-17, 4.7618e-17,\n",
       "             4.2758e-17, 3.9464e-17, 3.4049e-17, 3.0811e-17, 2.2151e-17, 1.9834e-17,\n",
       "             1.8522e-17, 1.6347e-17, 1.1987e-17, 1.1379e-17, 1.0964e-17, 1.0082e-17,\n",
       "             9.3393e-18, 7.3540e-18, 5.4252e-18, 4.6941e-18, 4.5822e-18, 4.0521e-18,\n",
       "             3.6058e-18, 3.5278e-18, 2.1278e-18, 1.9591e-18, 1.6842e-18, 1.4964e-18,\n",
       "             1.3369e-18, 1.1036e-18, 9.2863e-19, 9.0231e-19, 8.6086e-19, 8.3988e-19,\n",
       "             7.3995e-19, 6.8766e-19, 5.6539e-19, 4.4663e-19, 4.0895e-19, 4.0132e-19,\n",
       "             3.9078e-19, 3.8575e-19, 2.9082e-19, 2.6984e-19, 2.6691e-19, 1.9478e-19,\n",
       "             1.6975e-19, 1.5867e-19, 1.2293e-19, 7.3191e-20, 6.2268e-20, 5.5952e-20,\n",
       "             5.1220e-20, 4.0869e-20, 3.6580e-20, 9.6597e-21, 5.3104e-21, 2.1943e-21,\n",
       "             1.8005e-21, 1.7627e-21, 1.7251e-21, 1.4454e-21, 1.1475e-21, 8.5215e-22,\n",
       "             7.4256e-22, 7.2293e-22, 6.8830e-22, 6.5427e-22, 3.5722e-22, 2.8469e-22,\n",
       "             2.7477e-22, 2.5860e-22, 1.2840e-22, 1.1152e-22, 8.1854e-23, 3.7000e-23,\n",
       "             1.5634e-23, 6.0398e-24, 5.9221e-24, 5.4105e-24, 5.1001e-24, 4.3057e-24,\n",
       "             3.4070e-24, 2.8045e-24, 9.2160e-25, 7.9068e-25, 1.5910e-25, 1.5539e-25,\n",
       "             3.3493e-26, 1.5164e-26, 4.1168e-28, 8.8454e-30, 4.8627e-31, 2.1256e-32])}}]],\n",
       " 'roc_results': {'fpr': tensor([0.0000, 0.0043, 0.0043, 0.0043, 0.0043, 0.0085, 0.0085, 0.0085, 0.0085,\n",
       "          0.0085, 0.0085, 0.0085, 0.0085, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
       "          0.0128, 0.0128, 0.0128, 0.0171, 0.0171, 0.0171, 0.0171, 0.0214, 0.0214,\n",
       "          0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.0299, 0.0299, 0.0299,\n",
       "          0.0299, 0.0299, 0.0299, 0.0299, 0.0299, 0.0299, 0.0299, 0.0342, 0.0342,\n",
       "          0.0342, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0427, 0.0427, 0.0427,\n",
       "          0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0470,\n",
       "          0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0513, 0.0513,\n",
       "          0.0513, 0.0513, 0.0513, 0.0513, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556,\n",
       "          0.0556, 0.0598, 0.0598, 0.0598, 0.0598, 0.0641, 0.0641, 0.0641, 0.0641,\n",
       "          0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641,\n",
       "          0.0641, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684,\n",
       "          0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0726, 0.0726, 0.0726, 0.0726,\n",
       "          0.0726, 0.0726, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0812,\n",
       "          0.0812, 0.0812, 0.0812, 0.0855, 0.0940, 0.0940, 0.0940, 0.0940, 0.0940,\n",
       "          0.0983, 0.0983, 0.1026, 0.1026, 0.1026, 0.1026, 0.1026, 0.1026, 0.1026,\n",
       "          0.1026, 0.1068, 0.1068, 0.1068, 0.1068, 0.1154, 0.1154, 0.1154, 0.1154,\n",
       "          0.1154, 0.1154, 0.1154, 0.1154, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197,\n",
       "          0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1239, 0.1239, 0.1239,\n",
       "          0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282,\n",
       "          0.1325, 0.1368, 0.1368, 0.1368, 0.1368, 0.1368, 0.1410, 0.1410, 0.1410,\n",
       "          0.1453, 0.1496, 0.1496, 0.1496, 0.1496, 0.1538, 0.1538, 0.1581, 0.1624,\n",
       "          0.1624, 0.1667, 0.1667, 0.1709, 0.1752, 0.1795, 0.1838, 0.1838, 0.1838,\n",
       "          0.1880, 0.1880, 0.1923, 0.1923, 0.1923, 0.1966, 0.2009, 0.2009, 0.2051,\n",
       "          0.2051, 0.2051, 0.2051, 0.2051, 0.2051, 0.2094, 0.2094, 0.2094, 0.2179,\n",
       "          0.2179, 0.2179, 0.2179, 0.2222, 0.2265, 0.2308, 0.2350, 0.2393, 0.2393,\n",
       "          0.2436, 0.2479, 0.2479, 0.2521, 0.2521, 0.2521, 0.2521, 0.2564, 0.2607,\n",
       "          0.2650, 0.2735, 0.2778, 0.2778, 0.2778, 0.2821, 0.2863, 0.2906, 0.2949,\n",
       "          0.2991, 0.3034, 0.3034, 0.3077, 0.3077, 0.3077, 0.3120, 0.3162, 0.3162,\n",
       "          0.3205, 0.3248, 0.3291, 0.3333, 0.3376, 0.3419, 0.3462, 0.3504, 0.3547,\n",
       "          0.3590, 0.3632, 0.3675, 0.3718, 0.3761, 0.3803, 0.3846, 0.3846, 0.3889,\n",
       "          0.3932, 0.3932, 0.3974, 0.3974, 0.3974, 0.4017, 0.4060, 0.4103, 0.4145,\n",
       "          0.4188, 0.4231, 0.4274, 0.4316, 0.4359, 0.4359, 0.4359, 0.4402, 0.4402,\n",
       "          0.4444, 0.4487, 0.4530, 0.4573, 0.4573, 0.4615, 0.4658, 0.4701, 0.4744,\n",
       "          0.4744, 0.4786, 0.4829, 0.4872, 0.4957, 0.5000, 0.5000, 0.5043, 0.5043,\n",
       "          0.5085, 0.5128, 0.5171, 0.5214, 0.5256, 0.5299, 0.5299, 0.5342, 0.5385,\n",
       "          0.5427, 0.5470, 0.5513, 0.5556, 0.5598, 0.5641, 0.5684, 0.5726, 0.5769,\n",
       "          0.5812, 0.5855, 0.5897, 0.5897, 0.5940, 0.5983, 0.6026, 0.6068, 0.6111,\n",
       "          0.6154, 0.6197, 0.6239, 0.6282, 0.6325, 0.6368, 0.6410, 0.6453, 0.6496,\n",
       "          0.6496, 0.6538, 0.6581, 0.6624, 0.6667, 0.6709, 0.6752, 0.6838, 0.6880,\n",
       "          0.6923, 0.6966, 0.7009, 0.7051, 0.7094, 0.7137, 0.7222, 0.7265, 0.7308,\n",
       "          0.7350, 0.7393, 0.7436, 0.7479, 0.7521, 0.7564, 0.7607, 0.7650, 0.7692,\n",
       "          0.7735, 0.7778, 0.7821, 0.7863, 0.7949, 0.7991, 0.8034, 0.8120, 0.8162,\n",
       "          0.8205, 0.8248, 0.8291, 0.8333, 0.8376, 0.8419, 0.8462, 0.8504, 0.8547,\n",
       "          0.8590, 0.8632, 0.8675, 0.8718, 0.8761, 0.8803, 0.8846, 0.8889, 0.8932,\n",
       "          0.8974, 0.9017, 0.9060, 0.9103, 0.9145, 0.9188, 0.9231, 0.9274, 0.9316,\n",
       "          0.9359, 0.9402, 0.9444, 0.9487, 0.9530, 0.9573, 0.9615, 0.9658, 0.9701,\n",
       "          0.9744, 0.9786, 0.9829, 0.9872, 0.9915, 0.9957, 1.0000]),\n",
       "  'tpr': tensor([0.0000, 0.0000, 0.0026, 0.0077, 0.0179, 0.0282, 0.0462, 0.0615, 0.0846,\n",
       "          0.1051, 0.1282, 0.1436, 0.1615, 0.1846, 0.2051, 0.2103, 0.2205, 0.2282,\n",
       "          0.2462, 0.2538, 0.2692, 0.2744, 0.2821, 0.2897, 0.2974, 0.3026, 0.3154,\n",
       "          0.3205, 0.3282, 0.3385, 0.3410, 0.3462, 0.3538, 0.3564, 0.3667, 0.3769,\n",
       "          0.3795, 0.3821, 0.3923, 0.4000, 0.4051, 0.4128, 0.4179, 0.4308, 0.4333,\n",
       "          0.4359, 0.4410, 0.4487, 0.4564, 0.4590, 0.4692, 0.4744, 0.4821, 0.4872,\n",
       "          0.4897, 0.4949, 0.4974, 0.5026, 0.5051, 0.5077, 0.5154, 0.5231, 0.5282,\n",
       "          0.5333, 0.5359, 0.5385, 0.5436, 0.5462, 0.5487, 0.5538, 0.5564, 0.5590,\n",
       "          0.5615, 0.5692, 0.5718, 0.5744, 0.5795, 0.5821, 0.5846, 0.5872, 0.5897,\n",
       "          0.5949, 0.5949, 0.5974, 0.6000, 0.6026, 0.6026, 0.6051, 0.6077, 0.6103,\n",
       "          0.6128, 0.6154, 0.6179, 0.6205, 0.6231, 0.6282, 0.6308, 0.6359, 0.6385,\n",
       "          0.6410, 0.6436, 0.6462, 0.6487, 0.6513, 0.6538, 0.6564, 0.6615, 0.6641,\n",
       "          0.6692, 0.6718, 0.6744, 0.6769, 0.6795, 0.6795, 0.6821, 0.6846, 0.6872,\n",
       "          0.6923, 0.6949, 0.6949, 0.7000, 0.7051, 0.7077, 0.7103, 0.7128, 0.7154,\n",
       "          0.7179, 0.7205, 0.7231, 0.7256, 0.7282, 0.7308, 0.7333, 0.7359, 0.7385,\n",
       "          0.7436, 0.7462, 0.7462, 0.7487, 0.7513, 0.7538, 0.7590, 0.7615, 0.7641,\n",
       "          0.7667, 0.7667, 0.7692, 0.7718, 0.7744, 0.7744, 0.7769, 0.7795, 0.7821,\n",
       "          0.7872, 0.7897, 0.7949, 0.7974, 0.7974, 0.8000, 0.8026, 0.8051, 0.8077,\n",
       "          0.8103, 0.8128, 0.8154, 0.8179, 0.8205, 0.8231, 0.8231, 0.8256, 0.8282,\n",
       "          0.8282, 0.8308, 0.8333, 0.8359, 0.8385, 0.8410, 0.8436, 0.8462, 0.8487,\n",
       "          0.8487, 0.8487, 0.8513, 0.8538, 0.8564, 0.8590, 0.8590, 0.8615, 0.8667,\n",
       "          0.8667, 0.8667, 0.8692, 0.8718, 0.8744, 0.8744, 0.8769, 0.8769, 0.8769,\n",
       "          0.8821, 0.8821, 0.8872, 0.8872, 0.8872, 0.8872, 0.8872, 0.8897, 0.8923,\n",
       "          0.8923, 0.8949, 0.8949, 0.8974, 0.9000, 0.9000, 0.9000, 0.9026, 0.9026,\n",
       "          0.9051, 0.9077, 0.9103, 0.9128, 0.9154, 0.9154, 0.9179, 0.9205, 0.9205,\n",
       "          0.9231, 0.9256, 0.9282, 0.9282, 0.9282, 0.9282, 0.9282, 0.9282, 0.9308,\n",
       "          0.9308, 0.9308, 0.9333, 0.9333, 0.9359, 0.9385, 0.9436, 0.9436, 0.9436,\n",
       "          0.9436, 0.9436, 0.9436, 0.9462, 0.9487, 0.9487, 0.9487, 0.9487, 0.9487,\n",
       "          0.9487, 0.9487, 0.9513, 0.9513, 0.9538, 0.9564, 0.9564, 0.9564, 0.9590,\n",
       "          0.9590, 0.9590, 0.9590, 0.9590, 0.9590, 0.9590, 0.9590, 0.9615, 0.9615,\n",
       "          0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9641, 0.9641,\n",
       "          0.9641, 0.9667, 0.9667, 0.9692, 0.9718, 0.9718, 0.9718, 0.9718, 0.9718,\n",
       "          0.9718, 0.9718, 0.9718, 0.9718, 0.9718, 0.9744, 0.9769, 0.9769, 0.9795,\n",
       "          0.9795, 0.9795, 0.9795, 0.9795, 0.9821, 0.9821, 0.9821, 0.9846, 0.9846,\n",
       "          0.9872, 0.9872, 0.9872, 0.9872, 0.9872, 0.9872, 0.9897, 0.9897, 0.9923,\n",
       "          0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9949, 0.9949, 0.9949,\n",
       "          0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949,\n",
       "          0.9949, 0.9949, 0.9949, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974,\n",
       "          0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "  'thresholds': tensor([1.0000, 1.0000, 0.9995, 0.9990, 0.9985, 0.9980, 0.9976, 0.9971, 0.9966,\n",
       "          0.9961, 0.9956, 0.9951, 0.9946, 0.9941, 0.9937, 0.9932, 0.9927, 0.9922,\n",
       "          0.9917, 0.9912, 0.9907, 0.9902, 0.9897, 0.9893, 0.9888, 0.9883, 0.9878,\n",
       "          0.9873, 0.9868, 0.9863, 0.9858, 0.9854, 0.9849, 0.9844, 0.9839, 0.9829,\n",
       "          0.9824, 0.9819, 0.9814, 0.9810, 0.9805, 0.9800, 0.9795, 0.9790, 0.9785,\n",
       "          0.9775, 0.9771, 0.9766, 0.9761, 0.9756, 0.9751, 0.9746, 0.9741, 0.9731,\n",
       "          0.9727, 0.9722, 0.9717, 0.9712, 0.9702, 0.9697, 0.9692, 0.9688, 0.9683,\n",
       "          0.9678, 0.9663, 0.9653, 0.9639, 0.9619, 0.9614, 0.9609, 0.9595, 0.9585,\n",
       "          0.9580, 0.9570, 0.9565, 0.9556, 0.9551, 0.9546, 0.9531, 0.9521, 0.9507,\n",
       "          0.9497, 0.9492, 0.9478, 0.9463, 0.9458, 0.9448, 0.9438, 0.9424, 0.9419,\n",
       "          0.9399, 0.9395, 0.9370, 0.9365, 0.9360, 0.9326, 0.9316, 0.9292, 0.9277,\n",
       "          0.9272, 0.9248, 0.9243, 0.9233, 0.9229, 0.9219, 0.9199, 0.9194, 0.9160,\n",
       "          0.9146, 0.9116, 0.9092, 0.9048, 0.9028, 0.8989, 0.8965, 0.8940, 0.8931,\n",
       "          0.8901, 0.8877, 0.8872, 0.8862, 0.8848, 0.8818, 0.8813, 0.8809, 0.8804,\n",
       "          0.8784, 0.8760, 0.8745, 0.8730, 0.8726, 0.8711, 0.8696, 0.8691, 0.8667,\n",
       "          0.8662, 0.8628, 0.8618, 0.8579, 0.8521, 0.8516, 0.8511, 0.8506, 0.8501,\n",
       "          0.8486, 0.8442, 0.8423, 0.8398, 0.8394, 0.8345, 0.8325, 0.8276, 0.8262,\n",
       "          0.8247, 0.8237, 0.8208, 0.8198, 0.8159, 0.8154, 0.8120, 0.8110, 0.8081,\n",
       "          0.8066, 0.8022, 0.7988, 0.7983, 0.7959, 0.7939, 0.7925, 0.7915, 0.7900,\n",
       "          0.7886, 0.7856, 0.7817, 0.7803, 0.7720, 0.7695, 0.7690, 0.7661, 0.7642,\n",
       "          0.7627, 0.7598, 0.7588, 0.7573, 0.7559, 0.7510, 0.7441, 0.7417, 0.7383,\n",
       "          0.7368, 0.7363, 0.7344, 0.7271, 0.7241, 0.7231, 0.7217, 0.7173, 0.7158,\n",
       "          0.7095, 0.7051, 0.7046, 0.7031, 0.7026, 0.6982, 0.6953, 0.6943, 0.6826,\n",
       "          0.6738, 0.6719, 0.6636, 0.6631, 0.6621, 0.6582, 0.6572, 0.6558, 0.6523,\n",
       "          0.6509, 0.6494, 0.6470, 0.6392, 0.6387, 0.6353, 0.6333, 0.6274, 0.6270,\n",
       "          0.6216, 0.6191, 0.6187, 0.6084, 0.6079, 0.6074, 0.5981, 0.5889, 0.5854,\n",
       "          0.5825, 0.5742, 0.5737, 0.5630, 0.5625, 0.5610, 0.5591, 0.5356, 0.5342,\n",
       "          0.5332, 0.5278, 0.5220, 0.5171, 0.5098, 0.5068, 0.5039, 0.5010, 0.4976,\n",
       "          0.4890, 0.4822, 0.4746, 0.4709, 0.4543, 0.4487, 0.4429, 0.4397, 0.4353,\n",
       "          0.4351, 0.4304, 0.4258, 0.4143, 0.3970, 0.3955, 0.3906, 0.3887, 0.3855,\n",
       "          0.3816, 0.3772, 0.3696, 0.3662, 0.3616, 0.3606, 0.3555, 0.3538, 0.3481,\n",
       "          0.3462, 0.3384, 0.3374, 0.3372, 0.3357, 0.3342, 0.3269, 0.3262, 0.3252,\n",
       "          0.3235, 0.3228, 0.3210, 0.3191, 0.3164, 0.3000, 0.2842, 0.2834, 0.2830,\n",
       "          0.2712, 0.2668, 0.2661, 0.2651, 0.2637, 0.2607, 0.2603, 0.2487, 0.2424,\n",
       "          0.2343, 0.2339, 0.2311, 0.2307, 0.2203, 0.2200, 0.2152, 0.2148, 0.2129,\n",
       "          0.2069, 0.2006, 0.2004, 0.1965, 0.1901, 0.1853, 0.1844, 0.1772, 0.1587,\n",
       "          0.1564, 0.1481, 0.1451, 0.1400, 0.1392, 0.1355, 0.1328, 0.1322, 0.1317,\n",
       "          0.1310, 0.1300, 0.1295, 0.1255, 0.1246, 0.1242, 0.1180, 0.1172, 0.1160,\n",
       "          0.1142, 0.1122, 0.1108, 0.1099, 0.1097, 0.1089, 0.1084, 0.1072, 0.1067,\n",
       "          0.1047, 0.1034, 0.1021, 0.1014, 0.1000, 0.0997, 0.0953, 0.0939, 0.0937,\n",
       "          0.0903, 0.0896, 0.0891, 0.0885, 0.0865, 0.0847, 0.0830, 0.0812, 0.0797,\n",
       "          0.0789, 0.0770, 0.0768, 0.0765, 0.0754, 0.0753, 0.0740, 0.0729, 0.0725,\n",
       "          0.0720, 0.0712, 0.0690, 0.0679, 0.0636, 0.0621, 0.0608, 0.0604, 0.0588,\n",
       "          0.0582, 0.0574, 0.0568, 0.0561, 0.0555, 0.0530, 0.0503, 0.0500, 0.0474,\n",
       "          0.0473, 0.0453, 0.0448, 0.0433, 0.0429, 0.0426, 0.0423, 0.0414, 0.0399,\n",
       "          0.0394, 0.0388, 0.0380, 0.0365, 0.0350, 0.0348, 0.0336, 0.0317, 0.0296,\n",
       "          0.0295, 0.0273, 0.0263, 0.0261, 0.0245, 0.0241, 0.0240, 0.0237, 0.0228,\n",
       "          0.0208, 0.0207, 0.0206, 0.0199, 0.0193, 0.0169, 0.0133],\n",
       "         dtype=torch.float16),\n",
       "  'name': 'Original NN PneumoniaMNIST',\n",
       "  'auc': tensor(0.9215, device='cuda:0'),\n",
       "  'model': LitSimpleCNN(\n",
       "    (model): SimpleCNN(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "        (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): ReLU()\n",
       "        (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "      (relu_fc): ReLU()\n",
       "      (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (criterion): BCEWithLogitsLoss()\n",
       "    (train_accuracy): BinaryAccuracy()\n",
       "    (val_accuracy): BinaryAccuracy()\n",
       "    (val_auc): BinaryAUROC()\n",
       "    (test_accuracy): BinaryAccuracy()\n",
       "    (test_auc): BinaryAUROC()\n",
       "    (test_precision): BinaryPrecision()\n",
       "    (test_recall): BinaryRecall()\n",
       "    (test_f1): BinaryF1Score()\n",
       "  )},\n",
       " 'test_loader': <torch.utils.data.dataloader.DataLoader at 0x76c683faa840>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predict_ensemble_and_evaluate import save_to_pickle, load_from_pickle\n",
    "from predict_ensemble_and_evaluate import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate import plot_roc_comparison\n",
    "from predict_ensemble_and_evaluate import make_curve_monotonic\n",
    "\n",
    "# Save the data\n",
    "save_to_pickle(list_folds_best_models, list_folds_weighted_clfs, results_original_roc, test_loader, filename='pickle/medMNIST_weighted.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d29cb",
   "metadata": {},
   "source": [
    "# Load pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "974bbf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from predict_ensemble_and_evaluate import save_to_pickle, load_from_pickle\n",
    "from predict_ensemble_and_evaluate import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate import plot_roc_comparison\n",
    "from predict_ensemble_and_evaluate import make_curve_monotonic\n",
    "\n",
    "# Load the data\n",
    "list_folds_best_models, list_folds_weighted_clfs, results_original_roc, test_loader = load_from_pickle(filename='pickle/medMNIST_weighted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2787fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "results_lists.extend(list_folds_best_models)\n",
    "results_lists.extend(list_folds_weighted_clfs)\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    names=[\"Fold 1\", \"Fold 2\", \"Fold 3\", \"Fold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\"],\n",
    "    results_original_roc=results_original_roc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a49e4",
   "metadata": {},
   "source": [
    "### Ensemble prediction using voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77ee047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Ensemble ROC Curve by iterating through FPR values ---\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.0000\n",
      "Soft Voting -> Achieved [TPR: 0.1256, FPR: 0.0000]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.0204\n",
      "Soft Voting -> Achieved [TPR: 0.1974, FPR: 0.0171]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.0408\n",
      "Soft Voting -> Achieved [TPR: 0.5513, FPR: 0.0385]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.0612\n",
      "Soft Voting -> Achieved [TPR: 0.7205, FPR: 0.0598]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.0816\n",
      "Soft Voting -> Achieved [TPR: 0.7872, FPR: 0.0812]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.1020\n",
      "Soft Voting -> Achieved [TPR: 0.7872, FPR: 0.0983]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.1224\n",
      "Soft Voting -> Achieved [TPR: 0.9000, FPR: 0.1197]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.1429\n",
      "Soft Voting -> Achieved [TPR: 0.9436, FPR: 0.1410]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.1633\n",
      "Soft Voting -> Achieved [TPR: 0.9385, FPR: 0.1581]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.1837\n",
      "Soft Voting -> Achieved [TPR: 0.9538, FPR: 0.1795]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.2041\n",
      "Soft Voting -> Achieved [TPR: 0.9615, FPR: 0.2009]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.2245\n",
      "Soft Voting -> Achieved [TPR: 0.9718, FPR: 0.2222]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.2449\n",
      "Soft Voting -> Achieved [TPR: 0.9795, FPR: 0.2436]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.2653\n",
      "Soft Voting -> Achieved [TPR: 0.9821, FPR: 0.2564]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.2857\n",
      "Soft Voting -> Achieved [TPR: 0.9846, FPR: 0.2778]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.3061\n",
      "Soft Voting -> Achieved [TPR: 0.9846, FPR: 0.2778]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.3265\n",
      "Soft Voting -> Achieved [TPR: 0.9846, FPR: 0.2778]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.3469\n",
      "Soft Voting -> Achieved [TPR: 0.9846, FPR: 0.2778]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.3673\n",
      "Soft Voting -> Achieved [TPR: 0.9846, FPR: 0.2778]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.3878\n",
      "Soft Voting -> Achieved [TPR: 0.9846, FPR: 0.2778]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.4082\n",
      "Soft Voting -> Achieved [TPR: 0.9872, FPR: 0.3974]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.4286\n",
      "Soft Voting -> Achieved [TPR: 0.9872, FPR: 0.3974]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.4490\n",
      "Soft Voting -> Achieved [TPR: 0.9897, FPR: 0.4402]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.4694\n",
      "Soft Voting -> Achieved [TPR: 0.9923, FPR: 0.4615]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.4898\n",
      "Soft Voting -> Achieved [TPR: 0.9949, FPR: 0.4829]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.5102\n",
      "Soft Voting -> Achieved [TPR: 0.9974, FPR: 0.4915]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.5306\n",
      "Soft Voting -> Achieved [TPR: 0.9974, FPR: 0.4915]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.5510\n",
      "Soft Voting -> Achieved [TPR: 0.9974, FPR: 0.4915]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.5714\n",
      "Soft Voting -> Achieved [TPR: 1.0000, FPR: 0.5556]\n",
      "Hard Voting -> Resulted in [TPR: 0.9949, FPR: 0.5641]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.5918\n",
      "Soft Voting -> Achieved [TPR: 1.0000, FPR: 0.5556]\n",
      "Hard Voting -> Resulted in [TPR: 0.9949, FPR: 0.5641]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.6122\n",
      "Soft Voting -> Achieved [TPR: 1.0000, FPR: 0.5256]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.6327\n",
      "Soft Voting -> Achieved [TPR: 1.0000, FPR: 0.5043]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "\n",
      "--- Ensemble Results ---\n",
      "Target FPR: 0.6531\n",
      "Soft Voting -> Achieved [TPR: 1.0000, FPR: 0.5043]\n",
      "Hard Voting -> Resulted in [TPR: 0.0000, FPR: 0.0000]\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "Extracting full dataset from DataLoader for scikit-learn compatibility...\n",
      "-> Extracted 624 samples.\n",
      "\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Error: No model probabilities were generated. Cannot calculate metrics.\n",
      "\n",
      "--- Filtering curves to be monotonic ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Loop through all the FPR values to generate the ensemble ROC curve ---\n",
    "\n",
    "ensemble_results_soft = []\n",
    "ensemble_results_hard = []\n",
    "\n",
    "print(\"\\n--- Generating Ensemble ROC Curve by iterating through FPR values ---\")\n",
    "# We iterate from a low to high target_fpr to trace the curve\n",
    "for target_fpr in np.linspace(0.0, 1.0, 50): \n",
    "    # 1. Assign the function's output to a single variable first.\n",
    "    result_tuple = predict_ensemble_and_evaluate(\n",
    "        list_folds_best_models=list_folds_best_models,\n",
    "        test_loader=test_loader,\n",
    "        target_fpr=target_fpr\n",
    "    )\n",
    "    \n",
    "    if result_tuple is not None:\n",
    "        \n",
    "        for voting_method, metrics in result_tuple.items():\n",
    "            # Create a dictionary for each point and append it to the list\n",
    "            if voting_method == 'soft_voting':\n",
    "                point_dict = {'fpr': metrics['fpr'], 'tpr': metrics['tpr']}\n",
    "                ensemble_results_soft.append(point_dict)\n",
    "            else:\n",
    "                point_dict = {'fpr': metrics['fpr'], 'tpr': metrics['tpr']}\n",
    "                ensemble_results_hard.append(point_dict)\n",
    "            \n",
    "        \n",
    "# Ensure the curve starts at (0, 0)\n",
    "    if not ensemble_results_soft or ensemble_results_soft[0]['fpr'] > 0.0:\n",
    "        ensemble_results_soft.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if ensemble_results_soft[-1]['fpr'] < 1.0 or ensemble_results_soft[-1]['tpr'] < 1.0:\n",
    "        ensemble_results_soft.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "    if not ensemble_results_hard or ensemble_results_hard[0]['fpr'] > 0.0:\n",
    "        ensemble_results_hard.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if ensemble_results_hard[-1]['fpr'] < 1.0 or ensemble_results_hard[-1]['tpr'] < 1.0:\n",
    "        ensemble_results_hard.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "    # --- NEW: Post-process the lists to make them monotonic ---\n",
    "print(\"\\n--- Filtering curves to be monotonic ---\")\n",
    "ensemble_results_soft = make_curve_monotonic(ensemble_results_soft)\n",
    "ensemble_results_hard = make_curve_monotonic(ensemble_results_hard)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49ac634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plotting all ROC curves for comparison ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "results_lists.extend(list_folds_best_models)\n",
    "results_lists.extend(list_folds_weighted_clfs)\n",
    "results_lists.append(ensemble_results_soft)\n",
    "results_lists.append(ensemble_results_hard)\n",
    "\n",
    "\n",
    "# --- Plot the final comparison ---\n",
    "print(\"\\n--- Plotting all ROC curves for comparison ---\")\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    names=[\"Fold 1\", \"Fold 2\", \"Fold 3\", \"Fold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\", \"Ensemble_voting_soft\", \"Ensemble_voting_hard\"],\n",
    "    results_original_roc=results_original_roc\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLlabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
