{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4da4f6",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0556d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_ensemble_and_evaluate import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate import plot_roc_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d840881",
   "metadata": {},
   "source": [
    "## Display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c24af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for displaying ROC curves of multiple classifiers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def plot_roc_connected_points(results_list, results_original_roc, name=\"All Classifiers\"):\n",
    "    \"\"\"\n",
    "    Creates a plot showing the performance of all classifiers as points,\n",
    "    and connects them with a line, sorted by FPR.\n",
    "\n",
    "    Args:\n",
    "        results_list: A list of dictionaries, where each dict contains 'fpr', 'tpr'.\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # Convert results to a pandas DataFrame for easy sorting\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Sort the models by their False Positive Rate to create a left-to-right line\n",
    "    df_sorted = df.sort_values(by='fpr').reset_index(drop=True)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Plot all the individual model points as a scatter plot\n",
    "    # This helps see the actual performance of each model\n",
    "    plt.scatter(df_sorted['fpr'], df_sorted['tpr'], c='black', marker='o', \n",
    "                alpha=0.6, s=100, zorder=3) # zorder=3 puts dots on top\n",
    "\n",
    "    # Plot the line connecting the points\n",
    "    plt.plot(df_sorted['fpr'], df_sorted['tpr'], color='red', lw=1.5,\n",
    "             alpha=0.8, label=f'{name} (AUC = {auc(df_sorted[\"fpr\"], df_sorted[\"tpr\"]):.2f})')\n",
    "    \n",
    "    #plot the original ROC curve\n",
    "    plt.plot(results_original_roc[\"fpr\"], results_original_roc[\"tpr\"], color='blue', \n",
    "             label=f'{results_original_roc[\"name\"]} (AUC = {results_original_roc[\"auc\"]:.2f})')\n",
    "\n",
    "    # Plot the \"no-skill\" line for reference\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Performance Path of All Trained Classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23b1ea",
   "metadata": {},
   "source": [
    "## Best ROC envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e350c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def get_best_roc_envelope(results_list, results_original_roc=None):\n",
    "    \"\"\"\n",
    "    Constructs the upper envelope of multiple ROC curves to find the best\n",
    "    possible operating points from a collection of classifiers.\n",
    "\n",
    "    This function is guaranteed to produce a list of points that starts at\n",
    "    (FPR=0, TPR=0) and ends at (FPR=1, TPR=1), making it suitable for direct\n",
    "    plotting and AUC calculation.\n",
    "\n",
    "    Args:\n",
    "        results_list (list): A list of dictionaries. Each dictionary must\n",
    "                             contain a 'model' name and the full ROC results\n",
    "                             under the 'full_roc' key.\n",
    "        results_original_roc (dict, optional): A dictionary containing the\n",
    "                                               ROC results of a baseline model,\n",
    "                                               which will also be included in\n",
    "                                               the comparison. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of dictionaries representing the optimal ROC\n",
    "              envelope. Each dictionary includes 'model', 'fpr', 'tpr',\n",
    "              and 'threshold'.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "\n",
    "    # Step 1: Gather all ROC points from all provided models\n",
    "    all_results = results_list[:]\n",
    "    if results_original_roc:\n",
    "        all_results.append({\n",
    "            'model': 'original_baseline',\n",
    "            'full_roc': results_original_roc\n",
    "        })\n",
    "\n",
    "    for result in all_results:\n",
    "        model_name = result.get('model', 'Unknown Model')\n",
    "        roc = result.get('full_roc')\n",
    "        if not roc: continue\n",
    "\n",
    "        fprs = roc.get('fpr', [])\n",
    "        tprs = roc.get('tpr', [])\n",
    "        thresholds = roc.get('thresholds', [])\n",
    "\n",
    "        for i in range(len(fprs)):\n",
    "            threshold = thresholds[i] if i < len(thresholds) else thresholds[-1]\n",
    "            all_points.append({\n",
    "                'model': model_name, 'fpr': fprs[i],\n",
    "                'tpr': tprs[i], 'threshold': threshold\n",
    "            })\n",
    "\n",
    "    # Handle case with no data by returning a default diagonal line\n",
    "    if not all_points:\n",
    "        return [\n",
    "            {'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf},\n",
    "            {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}\n",
    "        ]\n",
    "\n",
    "    # Step 2: Sort all points by FPR (asc), then by TPR (desc)\n",
    "    all_points.sort(key=lambda p: (p['fpr'], -p['tpr']))\n",
    "\n",
    "    # Step 3: Build the core ROC envelope\n",
    "    best_roc_envelope = []\n",
    "    max_tpr_so_far = -1.0\n",
    "    for point in all_points:\n",
    "        # Add a point only if it has a strictly higher TPR\n",
    "        if point['tpr'] > max_tpr_so_far:\n",
    "            # To prevent adding multiple points at the same FPR, check if the last point\n",
    "            # has the same FPR and replace it if so (since this one has a higher TPR)\n",
    "            if best_roc_envelope and best_roc_envelope[-1]['fpr'] == point['fpr']:\n",
    "                best_roc_envelope[-1] = point\n",
    "            else:\n",
    "                best_roc_envelope.append(point)\n",
    "            max_tpr_so_far = point['tpr']\n",
    "    \n",
    "    # Step 4: Manually add extremities if they are missing\n",
    "    \n",
    "    # Ensure the curve starts at (0, 0)\n",
    "    if not best_roc_envelope or best_roc_envelope[0]['fpr'] > 0.0:\n",
    "        best_roc_envelope.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if best_roc_envelope[-1]['fpr'] < 1.0 or best_roc_envelope[-1]['tpr'] < 1.0:\n",
    "        best_roc_envelope.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "    return best_roc_envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b781e9",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c3deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(dataset):\n",
    "    if dataset == \"data1\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [0, 2], [2, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "    elif dataset == \"data2\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [1, 2], [1, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    elif dataset == \"pneumoniaMNIST\":\n",
    "        import medmnist\n",
    "        print(medmnist.__version__)\n",
    "        # read in the PneumoniaMNIST dataset\n",
    "        from medmnist import PneumoniaMNIST\n",
    "        train_data = PneumoniaMNIST(split='train', download=True, size=224)\n",
    "        test_data = PneumoniaMNIST(split='test', download=True, size=224)\n",
    "        val_data = PneumoniaMNIST(split='val', download=True, size=224)\n",
    "        # print the dataset info\n",
    "        print(f\"Number of training samples: {len(train_data)}\")\n",
    "        print(f\"Number of test samples: {len(test_data)}\")\n",
    "        print(f\"Number of validation samples: {len(val_data)}\")\n",
    "\n",
    "    return train_data, test_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfe355",
   "metadata": {},
   "source": [
    "## Calculate Statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af73be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_fpr_tpr(clf_model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the False Positive Rate (FPR) and True Positive Rate (TPR) at a given threshold.\n",
    "\n",
    "    Args:\n",
    "        X_test: The test features.\n",
    "        y_test: The true test labels (0 or 1).\n",
    "        threshold: The probability threshold.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the FPR and TPR. Returns None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_prob = clf_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  #Avoid division by zero\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0 #Avoid division by zero\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        misclassification_rate = 1 - accuracy\n",
    "\n",
    "        return {\"fpr\": fpr, \"tpr\": tpr, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy, \"misclassification_rate\": misclassification_rate}\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error calculating FPR and TPR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59034e",
   "metadata": {},
   "source": [
    "## PneumoniaMNIST: NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74133e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a060ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 1\n",
    "NUM_CHANNELS = 1\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 4\n",
    "K_FOLDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d23d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, image_height, image_width):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        #Convoluional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 224x224 -> 112x112\n",
    "        \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 112x112 -> 56x56\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 56x56 -> 28x28\n",
    "        \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 28x28 -> 14x14\n",
    "        )\n",
    "\n",
    "        # --- DYNAMIC FLATTENED SIZE CALCULATION ---\n",
    "        # Create a dummy tensor with the specified input dimensions\n",
    "        dummy_input = torch.randn(1, in_channels, image_height, image_width)\n",
    "        # Pass it through the feature extractor to see the output shape\n",
    "        dummy_output = self.features(dummy_input)\n",
    "        # The number of elements in the output tensor is our flattened size\n",
    "        self.flattened_size = dummy_output.numel()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        # Output layer: num_classes=1 for binary classification (outputting logits)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a702675",
   "metadata": {},
   "source": [
    "### Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac92eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitSimpleCNN(pl.LightningModule):\n",
    "    def __init__(self, in_channels, num_classes, learning_rate, image_height, image_width, training_mode='full_network'):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define the model\n",
    "        self.model = SimpleCNN(\n",
    "            in_channels=self.hparams.in_channels, \n",
    "            num_classes=self.hparams.num_classes,\n",
    "            image_height=self.hparams.image_height,\n",
    "            image_width=self.hparams.image_width\n",
    "        )\n",
    "        \n",
    "        # Define loss function\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Define metrics\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.val_auc = torchmetrics.AUROC(task=\"binary\")\n",
    "\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.test_auc = torchmetrics.AUROC(task=\"binary\")\n",
    "        self.test_precision = torchmetrics.Precision(task=\"binary\")\n",
    "        self.test_recall = torchmetrics.Recall(task=\"binary\")\n",
    "        self.test_f1 = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "        # This list will store outputs from each test step\n",
    "        self.last_test_results = {}\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def freeze_feature_extractor(self):\n",
    "        print(\"Freezing feature extractor layers...\")\n",
    "        for param in self.model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.fc1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        # For BCEWithLogitsLoss, labels must be float\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = self(inputs) # Forward pass\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # We need to handle which part of the network we are training\n",
    "        if self.hparams.training_mode == 'full_network':\n",
    "            self.log('train_loss_full', loss)\n",
    "        elif self.hparams.training_mode == 'classifier_only':\n",
    "            self.log('train_loss_classifier', loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.train_accuracy(outputs, labels.int())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.val_accuracy(outputs, labels.int())\n",
    "        self.val_auc(outputs, labels.int())\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('val_acc', self.val_accuracy, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('val_auc', self.val_auc, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels.float())\n",
    "        \n",
    "        # Append predictions and labels to our list\n",
    "        self.test_step_outputs.append({\"preds\": outputs.detach(), \"labels\": labels.detach()})\n",
    "        \n",
    "        # Log the loss for this batch\n",
    "        self.log('test_loss', loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Concatenate all predictions and labels from the list we built\n",
    "        all_preds = torch.cat([x['preds'] for x in self.test_step_outputs])\n",
    "        all_labels = torch.cat([x['labels'] for x in self.test_step_outputs])\n",
    "\n",
    "        # Calculate final metrics over the entire test set\n",
    "        test_acc = self.test_accuracy(all_preds, all_labels.int())\n",
    "        test_auc_val = self.test_auc(all_preds, all_labels.int())\n",
    "        test_prec = self.test_precision(all_preds, all_labels.int())\n",
    "        test_rec = self.test_recall(all_preds, all_labels.int())\n",
    "        test_f1_val = self.test_f1(all_preds, all_labels.int())\n",
    "        test_cm_val = torchmetrics.functional.confusion_matrix(all_preds, all_labels.int(), task=\"binary\")\n",
    "\n",
    "        # Log the final metrics\n",
    "        self.log(\"test_acc_epoch\", test_acc)\n",
    "        self.log(\"test_auc_epoch\", test_auc_val)\n",
    "\n",
    "        print(f\"\\n--- Final Test Metrics ---\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"AUC: {test_auc_val:.4f}\")\n",
    "        print(f\"Precision: {test_prec:.4f}\")\n",
    "        print(f\"Recall: {test_rec:.4f}\")\n",
    "        print(f\"F1-Score: {test_f1_val:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{test_cm_val}\")\n",
    "        print(\"--------------------------\\n\")\n",
    "\n",
    "        # Calculate data for the ROC Curve\n",
    "        fpr, tpr, thresholds = torchmetrics.functional.roc(\n",
    "            torch.sigmoid(all_preds),\n",
    "            all_labels.int(),\n",
    "            task=\"binary\"\n",
    "        )\n",
    "\n",
    "        # Store the results to be retrieved later in the main script\n",
    "        self.last_test_results = {\n",
    "            \"fpr\": fpr.cpu(),\n",
    "            \"tpr\": tpr.cpu(),\n",
    "            \"auc\": test_auc_val,\n",
    "            \"f1\": test_f1_val,\n",
    "            \"precision\": test_prec,\n",
    "            \"recall\": test_rec,\n",
    "            \"cm\": test_cm_val,\n",
    "            \"thresholds\": thresholds.cpu(),\n",
    "        }\n",
    "        # Free up memory\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ea652",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d785e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214M/214M [03:46<00:00, 945kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 4708\n",
      "Number of test samples: 624\n",
      "Number of validation samples: 524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "train_dataset, test_dataset, val_dataset = generate_data(\"pneumoniaMNIST\")\n",
    "\n",
    "# Ensure you have your train_loader and val_loader defined here\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset.transform = data_transforms\n",
    "test_dataset.transform = data_transforms\n",
    "val_dataset.transform = data_transforms\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#new_train_loader = data.DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) to be done inside the sample_ratio loop\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# initialize fold\n",
    "kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "971945fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training for 1 epochs on train_loader ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 74/74 [00:42<00:00,  1.74it/s, v_num=0, train_loss_step=0.088, train_acc_step=1.000, val_loss=0.203, val_acc=0.926, val_auc=0.976, train_loss_epoch=0.277, train_acc_epoch=0.877]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 74/74 [00:43<00:00,  1.68it/s, v_num=0, train_loss_step=0.088, train_acc_step=1.000, val_loss=0.203, val_acc=0.926, val_auc=0.976, train_loss_epoch=0.277, train_acc_epoch=0.877]\n",
      "Best model from Phase 1 saved to: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93.ckpt\n",
      "\n",
      "--- Testing model from Phase 1 checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.80it/s]\n",
      "--- Final Test Metrics ---\n",
      "Accuracy: 0.8590\n",
      "AUC: 0.9213\n",
      "Precision: 0.8447\n",
      "Recall: 0.9487\n",
      "F1-Score: 0.8937\n",
      "Confusion Matrix:\n",
      "tensor([[166,  68],\n",
      "        [ 20, 370]], device='cuda:0')\n",
      "--------------------------\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.47it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.8589743375778198\n",
      "     test_auc_epoch          0.921329140663147\n",
      "        test_loss           0.3651410937309265\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import numpy as np\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "# 1. Instantiate the LightningModule\n",
    "model = LitSimpleCNN(\n",
    "    in_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    training_mode='full_network',  # Change to 'classifier_only' if you want to train only the classifier\n",
    "    image_height=IMAGE_SIZE,\n",
    "    image_width=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# Optional: Print model summary\n",
    "# You need to move the model to a device first for torchsummary to work\n",
    "# summary(model.to('cuda'), (NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE))\n",
    "# model.to('cpu') # Move it back if needed\n",
    "\n",
    "checkpoint_callback_full_model = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='simple-cnn-full-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "train_trainer_1 = pl.Trainer(\n",
    "    accelerator='gpu',                 # Use GPUs\n",
    "    devices=1,                         # Number of GPUs to use\n",
    "    #strategy='ddp_notebook',                    # DistributedDataParallel strategy (best for multi-GPU)\n",
    "    max_epochs=NUM_EPOCHS,              # Total number of epochs\n",
    "    callbacks=[checkpoint_callback_full_model, progress_bar],\n",
    "    logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"simple_cnn-full\")\n",
    ")\n",
    "\n",
    "test_trainer_1 = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,  # Use only one GPU\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"simple_cnn_test\"),\n",
    "    # Add the callback here so the Trainer can find the 'best' path\n",
    "    callbacks=[checkpoint_callback_full_model],\n",
    "    precision='16-mixed'  # Use mixed precision for testing\n",
    ")\n",
    "\n",
    "\n",
    "# --- Phase 1 Training ---\n",
    "print(f\"--- Starting Training for {NUM_EPOCHS} epochs on train_loader ---\")\n",
    "train_trainer_1.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# CRITICAL: Save the path to the best model from this phase\n",
    "path_after_phase1 = checkpoint_callback_full_model.best_model_path\n",
    "print(f\"Best model from Phase 1 saved to: {path_after_phase1}\")\n",
    "\n",
    "# --- Test 1: Evaluate the model from after Phase 1 ---\n",
    "print(f\"\\n--- Testing model from Phase 1 checkpoint: {path_after_phase1} ---\")\n",
    "model.current_test_title = \"ROC Curve after training on DataLoader 1\"\n",
    "model = LitSimpleCNN.load_from_checkpoint(path_after_phase1)  # Load the model from Phase 1\n",
    "model.eval().to('cuda:0')  # Ensure the model is in evaluation mode and on GPU\n",
    "test_trainer_1.test(model, dataloaders=test_loader, ckpt_path=path_after_phase1)\n",
    "results_phase1 = model.last_test_results.copy()\n",
    "results_original_roc = {\"fpr\": results_phase1[\"fpr\"], \"tpr\": results_phase1[\"tpr\"], \"thresholds\": results_phase1[\"thresholds\"], \"name\": \"Original NN PneumoniaMNIST\", \"auc\": results_phase1[\"auc\"], \"model\": model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c2d0a",
   "metadata": {},
   "source": [
    "### Weighted ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226e70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSIFIERS = 10\n",
    "w = np.linspace(0.001, 0.999, NUM_CLASSIFIERS, endpoint=True)\n",
    "pos_weights = w/(1-w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d1bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fold 1/4 ---\n",
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=0, train_loss_step=0.00256, train_acc_step=0.182, val_loss=0.00296, val_acc=0.274, val_auc=0.960, train_loss_epoch=0.00805, train_acc_epoch=0.253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.20it/s, v_num=0, train_loss_step=0.00256, train_acc_step=0.182, val_loss=0.00296, val_acc=0.274, val_auc=0.960, train_loss_epoch=0.00805, train_acc_epoch=0.253]\n",
      "--- Stage 1 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.27.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.27.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=0, train_loss_step=0.0523, train_acc_step=0.818, val_loss=0.0568, val_acc=0.939, val_auc=0.984, train_loss_epoch=0.0816, train_acc_epoch=0.839]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.20it/s, v_num=0, train_loss_step=0.0523, train_acc_step=0.818, val_loss=0.0568, val_acc=0.939, val_auc=0.984, train_loss_epoch=0.0816, train_acc_epoch=0.839]\n",
      "--- Stage 2 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.94.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.94.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=0, train_loss_step=0.292, train_acc_step=0.818, val_loss=0.0617, val_acc=0.948, val_auc=0.990, train_loss_epoch=0.0693, train_acc_epoch=0.938]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.20it/s, v_num=0, train_loss_step=0.292, train_acc_step=0.818, val_loss=0.0617, val_acc=0.948, val_auc=0.990, train_loss_epoch=0.0693, train_acc_epoch=0.938]\n",
      "--- Stage 3 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.95.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.95.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=0, train_loss_step=0.285, train_acc_step=0.818, val_loss=0.0792, val_acc=0.963, val_auc=0.990, train_loss_epoch=0.0756, train_acc_epoch=0.958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.20it/s, v_num=0, train_loss_step=0.285, train_acc_step=0.818, val_loss=0.0792, val_acc=0.963, val_auc=0.990, train_loss_epoch=0.0756, train_acc_epoch=0.958]\n",
      "--- Stage 4 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=0, train_loss_step=0.0563, train_acc_step=1.000, val_loss=0.081, val_acc=0.970, val_auc=0.992, train_loss_epoch=0.0775, train_acc_epoch=0.967]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.19it/s, v_num=0, train_loss_step=0.0563, train_acc_step=1.000, val_loss=0.081, val_acc=0.970, val_auc=0.992, train_loss_epoch=0.0775, train_acc_epoch=0.967]\n",
      "--- Stage 5 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.97.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.97.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=0, train_loss_step=0.0232, train_acc_step=1.000, val_loss=0.097, val_acc=0.969, val_auc=0.994, train_loss_epoch=0.0758, train_acc_epoch=0.974]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.20it/s, v_num=0, train_loss_step=0.0232, train_acc_step=1.000, val_loss=0.097, val_acc=0.969, val_auc=0.994, train_loss_epoch=0.0758, train_acc_epoch=0.974]\n",
      "--- Stage 6 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.24it/s, v_num=0, train_loss_step=0.00757, train_acc_step=1.000, val_loss=0.136, val_acc=0.967, val_auc=0.992, train_loss_epoch=0.0832, train_acc_epoch=0.974]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.19it/s, v_num=0, train_loss_step=0.00757, train_acc_step=1.000, val_loss=0.136, val_acc=0.967, val_auc=0.992, train_loss_epoch=0.0832, train_acc_epoch=0.974]\n",
      "--- Stage 7 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.97.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.97.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=0, train_loss_step=0.00112, train_acc_step=1.000, val_loss=0.147, val_acc=0.969, val_auc=0.994, train_loss_epoch=0.114, train_acc_epoch=0.974]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.19it/s, v_num=0, train_loss_step=0.00112, train_acc_step=1.000, val_loss=0.147, val_acc=0.969, val_auc=0.994, train_loss_epoch=0.114, train_acc_epoch=0.974]\n",
      "--- Stage 8 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=0, train_loss_step=0.137, train_acc_step=1.000, val_loss=0.291, val_acc=0.935, val_auc=0.985, train_loss_epoch=0.146, train_acc_epoch=0.971]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:47<00:00,  1.19it/s, v_num=0, train_loss_step=0.137, train_acc_step=1.000, val_loss=0.291, val_acc=0.935, val_auc=0.985, train_loss_epoch=0.146, train_acc_epoch=0.971]\n",
      "--- Stage 9 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.93.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.93.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.24it/s, v_num=0, train_loss_step=0.482, train_acc_step=0.909, val_loss=11.50, val_acc=0.930, val_auc=0.959, train_loss_epoch=1.160, train_acc_epoch=0.933]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:47<00:00,  1.18it/s, v_num=0, train_loss_step=0.482, train_acc_step=0.909, val_loss=11.50, val_acc=0.930, val_auc=0.959, train_loss_epoch=1.160, train_acc_epoch=0.933]\n",
      "--- Stage 10 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.93.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.93.ckpt to continue...\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.27.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with model 1: 100%|██████████| 10/10 [00:01<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.0000, FPR=0.0000, F1-Score=0.0000\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.94.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 2: 100%|██████████| 10/10 [00:01<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9410, FPR=0.2863, F1-Score=0.8908\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.95.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 3: 100%|██████████| 10/10 [00:01<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9692, FPR=0.2607, F1-Score=0.9119\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.96.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 4: 100%|██████████| 10/10 [00:01<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9846, FPR=0.3846, F1-Score=0.8889\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.97.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 5: 100%|██████████| 10/10 [00:01<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9949, FPR=0.4145, F1-Score=0.8869\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.97.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 6: 100%|██████████| 10/10 [00:01<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9846, FPR=0.3675, F1-Score=0.8930\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.97.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 7: 100%|██████████| 10/10 [00:01<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9949, FPR=0.5171, F1-Score=0.8632\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.97.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 8: 100%|██████████| 10/10 [00:01<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9949, FPR=0.5726, F1-Score=0.8509\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.93.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 9: 100%|██████████| 10/10 [00:01<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=1.0000, FPR=0.6709, F1-Score=0.8324\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.93.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 10: 100%|██████████| 10/10 [00:01<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=1.0000, FPR=0.6838, F1-Score=0.8298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fold 2/4 ---\n",
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.24it/s, v_num=1, train_loss_step=0.000986, train_acc_step=0.909, val_loss=0.000588, val_acc=0.892, val_auc=0.997, train_loss_epoch=0.0168, train_acc_epoch=0.707]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:47<00:00,  1.17it/s, v_num=1, train_loss_step=0.000986, train_acc_step=0.909, val_loss=0.000588, val_acc=0.892, val_auc=0.997, train_loss_epoch=0.0168, train_acc_epoch=0.707]\n",
      "--- Stage 1 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.89.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.89.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.24it/s, v_num=1, train_loss_step=0.000879, train_acc_step=1.000, val_loss=0.0236, val_acc=0.945, val_auc=0.998, train_loss_epoch=0.0324, train_acc_epoch=0.949]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:47<00:00,  1.18it/s, v_num=1, train_loss_step=0.000879, train_acc_step=1.000, val_loss=0.0236, val_acc=0.945, val_auc=0.998, train_loss_epoch=0.0324, train_acc_epoch=0.949]\n",
      "--- Stage 2 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.94-v1.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.94-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=1, train_loss_step=0.00422, train_acc_step=1.000, val_loss=0.0298, val_acc=0.964, val_auc=0.999, train_loss_epoch=0.0388, train_acc_epoch=0.974]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.21it/s, v_num=1, train_loss_step=0.00422, train_acc_step=1.000, val_loss=0.0298, val_acc=0.964, val_auc=0.999, train_loss_epoch=0.0388, train_acc_epoch=0.974]\n",
      "--- Stage 3 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.96.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.96.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.24it/s, v_num=1, train_loss_step=0.0326, train_acc_step=0.909, val_loss=0.0238, val_acc=0.986, val_auc=0.999, train_loss_epoch=0.034, train_acc_epoch=0.981]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.21it/s, v_num=1, train_loss_step=0.0326, train_acc_step=0.909, val_loss=0.0238, val_acc=0.986, val_auc=0.999, train_loss_epoch=0.034, train_acc_epoch=0.981]\n",
      "--- Stage 4 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.99.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.99.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.24it/s, v_num=1, train_loss_step=0.0974, train_acc_step=0.909, val_loss=0.0468, val_acc=0.983, val_auc=0.999, train_loss_epoch=0.0307, train_acc_epoch=0.989]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.21it/s, v_num=1, train_loss_step=0.0974, train_acc_step=0.909, val_loss=0.0468, val_acc=0.983, val_auc=0.999, train_loss_epoch=0.0307, train_acc_epoch=0.989]\n",
      "--- Stage 5 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.98.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.98.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=1, train_loss_step=0.00163, train_acc_step=1.000, val_loss=0.0727, val_acc=0.978, val_auc=0.999, train_loss_epoch=0.0352, train_acc_epoch=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.22it/s, v_num=1, train_loss_step=0.00163, train_acc_step=1.000, val_loss=0.0727, val_acc=0.978, val_auc=0.999, train_loss_epoch=0.0352, train_acc_epoch=0.988]\n",
      "--- Stage 6 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.98.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.98.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.24it/s, v_num=1, train_loss_step=0.000354, train_acc_step=1.000, val_loss=0.158, val_acc=0.958, val_auc=0.994, train_loss_epoch=0.0423, train_acc_epoch=0.987]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.21it/s, v_num=1, train_loss_step=0.000354, train_acc_step=1.000, val_loss=0.158, val_acc=0.958, val_auc=0.994, train_loss_epoch=0.0423, train_acc_epoch=0.987]\n",
      "--- Stage 7 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.24it/s, v_num=1, train_loss_step=2.16e-5, train_acc_step=1.000, val_loss=0.145, val_acc=0.981, val_auc=0.994, train_loss_epoch=0.0363, train_acc_epoch=0.989]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.21it/s, v_num=1, train_loss_step=2.16e-5, train_acc_step=1.000, val_loss=0.145, val_acc=0.981, val_auc=0.994, train_loss_epoch=0.0363, train_acc_epoch=0.989]\n",
      "--- Stage 8 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.98.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.98.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.24it/s, v_num=1, train_loss_step=0.00604, train_acc_step=1.000, val_loss=0.177, val_acc=0.983, val_auc=0.996, train_loss_epoch=0.0669, train_acc_epoch=0.984]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.20it/s, v_num=1, train_loss_step=0.00604, train_acc_step=1.000, val_loss=0.177, val_acc=0.983, val_auc=0.996, train_loss_epoch=0.0669, train_acc_epoch=0.984]\n",
      "--- Stage 9 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.98.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.98.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=1, train_loss_step=0.215, train_acc_step=1.000, val_loss=48.60, val_acc=0.963, val_auc=0.992, train_loss_epoch=3.110, train_acc_epoch=0.966]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.21it/s, v_num=1, train_loss_step=0.215, train_acc_step=1.000, val_loss=48.60, val_acc=0.963, val_auc=0.992, train_loss_epoch=3.110, train_acc_epoch=0.966]\n",
      "--- Stage 10 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.96.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.96.ckpt to continue...\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.89.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with model 1: 100%|██████████| 10/10 [00:01<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.8769, FPR=0.1325, F1-Score=0.8965\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.94-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 2: 100%|██████████| 10/10 [00:01<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9436, FPR=0.1325, F1-Score=0.9328\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.96.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 3: 100%|██████████| 10/10 [00:01<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9667, FPR=0.2179, F1-Score=0.9218\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=0.99.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 4: 100%|██████████| 10/10 [00:01<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9795, FPR=0.3162, F1-Score=0.9031\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=0.98.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 5: 100%|██████████| 10/10 [00:02<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9949, FPR=0.5256, F1-Score=0.8613\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.98.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 6: 100%|██████████| 10/10 [00:01<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9795, FPR=0.2521, F1-Score=0.9194\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.96.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 7: 100%|██████████| 10/10 [00:01<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=1.0000, FPR=0.6239, F1-Score=0.8423\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.98.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 8: 100%|██████████| 10/10 [00:01<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9872, FPR=0.4231, F1-Score=0.8810\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.98.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 9: 100%|██████████| 10/10 [00:01<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9923, FPR=0.4530, F1-Score=0.8766\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.96.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 10: 100%|██████████| 10/10 [00:01<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9974, FPR=0.3803, F1-Score=0.8963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fold 3/4 ---\n",
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=2, train_loss_step=0.00359, train_acc_step=0.727, val_loss=0.000808, val_acc=0.890, val_auc=0.994, train_loss_epoch=0.008, train_acc_epoch=0.808]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.21it/s, v_num=2, train_loss_step=0.00359, train_acc_step=0.727, val_loss=0.000808, val_acc=0.890, val_auc=0.994, train_loss_epoch=0.008, train_acc_epoch=0.808]\n",
      "--- Stage 1 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.89-v1.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.89-v1.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=2, train_loss_step=0.0145, train_acc_step=0.909, val_loss=0.00651, val_acc=0.979, val_auc=1.000, train_loss_epoch=0.052, train_acc_epoch=0.965]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.22it/s, v_num=2, train_loss_step=0.0145, train_acc_step=0.909, val_loss=0.00651, val_acc=0.979, val_auc=1.000, train_loss_epoch=0.052, train_acc_epoch=0.965]\n",
      "--- Stage 2 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.98.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.98.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=2, train_loss_step=0.000204, train_acc_step=1.000, val_loss=0.00627, val_acc=0.994, val_auc=1.000, train_loss_epoch=0.0182, train_acc_epoch=0.984]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.22it/s, v_num=2, train_loss_step=0.000204, train_acc_step=1.000, val_loss=0.00627, val_acc=0.994, val_auc=1.000, train_loss_epoch=0.0182, train_acc_epoch=0.984]\n",
      "--- Stage 3 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.99.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.99.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=2, train_loss_step=0.00124, train_acc_step=1.000, val_loss=0.0063, val_acc=0.998, val_auc=1.000, train_loss_epoch=0.0186, train_acc_epoch=0.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.22it/s, v_num=2, train_loss_step=0.00124, train_acc_step=1.000, val_loss=0.0063, val_acc=0.998, val_auc=1.000, train_loss_epoch=0.0186, train_acc_epoch=0.990]\n",
      "--- Stage 4 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=1.00.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=1.00.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=2, train_loss_step=0.0604, train_acc_step=0.909, val_loss=0.00788, val_acc=0.997, val_auc=1.000, train_loss_epoch=0.00926, train_acc_epoch=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.22it/s, v_num=2, train_loss_step=0.0604, train_acc_step=0.909, val_loss=0.00788, val_acc=0.997, val_auc=1.000, train_loss_epoch=0.00926, train_acc_epoch=0.995]\n",
      "--- Stage 5 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=1.00.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=1.00.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=2, train_loss_step=0.000351, train_acc_step=1.000, val_loss=0.026, val_acc=0.994, val_auc=1.000, train_loss_epoch=0.0145, train_acc_epoch=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.21it/s, v_num=2, train_loss_step=0.000351, train_acc_step=1.000, val_loss=0.026, val_acc=0.994, val_auc=1.000, train_loss_epoch=0.0145, train_acc_epoch=0.995]\n",
      "--- Stage 6 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.99.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.99.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=2, train_loss_step=2.99e-5, train_acc_step=1.000, val_loss=0.019, val_acc=0.994, val_auc=1.000, train_loss_epoch=0.0267, train_acc_epoch=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.22it/s, v_num=2, train_loss_step=2.99e-5, train_acc_step=1.000, val_loss=0.019, val_acc=0.994, val_auc=1.000, train_loss_epoch=0.0267, train_acc_epoch=0.994]\n",
      "--- Stage 7 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.99.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.99.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.25it/s, v_num=2, train_loss_step=0.000119, train_acc_step=1.000, val_loss=0.0228, val_acc=0.994, val_auc=1.000, train_loss_epoch=0.0414, train_acc_epoch=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.22it/s, v_num=2, train_loss_step=0.000119, train_acc_step=1.000, val_loss=0.0228, val_acc=0.994, val_auc=1.000, train_loss_epoch=0.0414, train_acc_epoch=0.994]\n",
      "--- Stage 8 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.99.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_8/best-model-epoch=00-val_acc=0.99.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.26it/s, v_num=2, train_loss_step=0.452, train_acc_step=0.818, val_loss=0.450, val_acc=0.952, val_auc=0.948, train_loss_epoch=0.0445, train_acc_epoch=0.993]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:46<00:00,  1.22it/s, v_num=2, train_loss_step=0.452, train_acc_step=0.818, val_loss=0.450, val_acc=0.952, val_auc=0.948, train_loss_epoch=0.0445, train_acc_epoch=0.993]\n",
      "--- Stage 9 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.95.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_9/best-model-epoch=00-val_acc=0.95.ckpt to continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:44<00:00,  1.26it/s, v_num=2, train_loss_step=7.69e-14, train_acc_step=1.000, val_loss=0.635, val_acc=0.945, val_auc=0.943, train_loss_epoch=14.70, train_acc_epoch=0.966]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 56/56 [00:45<00:00,  1.22it/s, v_num=2, train_loss_step=7.69e-14, train_acc_step=1.000, val_loss=0.635, val_acc=0.945, val_auc=0.943, train_loss_epoch=14.70, train_acc_epoch=0.966]\n",
      "--- Stage 10 complete. Best model path: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.94.ckpt ---\n",
      "Loading best weights from /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_10/best-model-epoch=00-val_acc=0.94.ckpt to continue...\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_1/best-model-epoch=00-val_acc=0.89-v1.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with model 1: 100%|██████████| 10/10 [00:01<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.8692, FPR=0.1111, F1-Score=0.8980\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_2/best-model-epoch=00-val_acc=0.98.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 2: 100%|██████████| 10/10 [00:01<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9718, FPR=0.2137, F1-Score=0.9255\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_3/best-model-epoch=00-val_acc=0.99.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 3: 100%|██████████| 10/10 [00:01<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9897, FPR=0.3974, F1-Score=0.8884\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_4/best-model-epoch=00-val_acc=1.00.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 4: 100%|██████████| 10/10 [00:01<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9897, FPR=0.3675, F1-Score=0.8956\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_5/best-model-epoch=00-val_acc=1.00.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 5: 100%|██████████| 10/10 [00:01<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9923, FPR=0.3932, F1-Score=0.8907\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_6/best-model-epoch=00-val_acc=0.99.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 6: 100%|██████████| 10/10 [00:01<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at threshold 0.5: TPR=0.9846, FPR=0.4017, F1-Score=0.8848\n",
      "\n",
      "--- Testing model from checkpoint: /home/kartik/Documents/Stage ENSAI/RoC_Analysis/checkpoints/stage_7/best-model-epoch=00-val_acc=0.99.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['criterion.pos_weight']\n",
      "Predicting with model 7:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 3.94 GiB of which 152.94 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.12 GiB is allocated by PyTorch, and 611.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Get model output (raw logits) for the batch\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mpytorch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Append batch results to lists (move back to CPU to prevent GPU memory buildup)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m all_logits\u001b[38;5;241m.\u001b[39mappend(logits\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mLitSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[8], line 38\u001b[0m, in \u001b[0;36mSimpleCNN.extract_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLlabs/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 3.94 GiB of which 152.94 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.12 GiB is allocated by PyTorch, and 611.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import numpy as np\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "\n",
    "\n",
    "# 1. Instantiate the LightningModule\n",
    "model = LitSimpleCNN(\n",
    "    in_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    training_mode='full_network',  # Change to 'classifier_only' if you want to train only the classifier\n",
    "    image_height=IMAGE_SIZE,\n",
    "    image_width=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "\n",
    "#store the weighted classifiers of all the folds\n",
    "list_folds_weighted_clfs = []  # List to store classifiers from all folds\n",
    "#store the best classfiers of all the folds\n",
    "list_folds_best_models = []  # List to store best models from all folds\n",
    "\n",
    "# Dictionary to store the history of each fold\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# 3. K-fold cross-validation loop\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "    list_weighted_clfs = [] # List to store the models trained at each stage\n",
    "    best_clfs = [] # List to store the best models from each stage\n",
    "    best_model_paths = [] # List to store the path of the best model from each stage\n",
    "    print(f\"--- Starting Fold {fold + 1}/{K_FOLDS} ---\")\n",
    "\n",
    "    # 4. Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "                      train_dataset,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      sampler=train_subsampler,\n",
    "                      num_workers=NUM_WORKERS)\n",
    "    fold_loader = data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sampler=val_subsampler,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "\n",
    "    for i, pos_weight in enumerate(pos_weights):\n",
    "        model.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_acc',\n",
    "            dirpath=f'checkpoints/stage_{i+1}/',\n",
    "            filename=f'best-model-{{epoch:02d}}-{{val_acc:.2f}}',\n",
    "            save_top_k=1,\n",
    "            mode='max',\n",
    "        )\n",
    "        \n",
    "        # 3. Instantiate a NEW Trainer for this specific stage\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator='gpu',\n",
    "            devices=1,\n",
    "            #strategy='ddp_notebook',\n",
    "            max_epochs=NUM_EPOCHS, # Number of epochs for THIS stage\n",
    "            callbacks=[checkpoint_callback, progress_bar],\n",
    "            logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=f\"stage_{i+1}_ratio_{pos_weight}\"),\n",
    "            precision='16-mixed'\n",
    "            )\n",
    "        \n",
    "        # 4. Train the model. It will start with weights from the previous stage.\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=fold_loader)\n",
    "        \n",
    "        # 5. Get the path to the best model from THIS stage and store it\n",
    "        if checkpoint_callback.best_model_path:\n",
    "            best_path_this_stage = checkpoint_callback.best_model_path\n",
    "            print(f\"--- Stage {i+1} complete. Best model path: {best_path_this_stage} ---\")\n",
    "            best_model_paths.append(best_path_this_stage)\n",
    "            \n",
    "            # 6. CRITICAL: Load the best weights back into the model object\n",
    "            # This ensures we carry forward the BEST model to the next stage, not the last.\n",
    "            print(f\"Loading best weights from {best_path_this_stage} to continue...\")\n",
    "            model = LitSimpleCNN.load_from_checkpoint(best_path_this_stage, strict=False)  # Use strict=False to ignore missing keys if any\n",
    "        else:\n",
    "            print(f\"--- Stage {i+1} complete. No checkpoint was saved. ---\")\n",
    "            # If no checkpoint was saved, the model object will just have the weights from the last epoch.\n",
    "\n",
    "    # 7. Test the model after each stage\n",
    "    # Loop through each saved model checkpoint\n",
    "    for i, checkpoint_path in enumerate(best_model_paths):\n",
    "        print(f\"\\n--- Testing model from checkpoint: {checkpoint_path} ---\")\n",
    "\n",
    "        # 1. Load the PyTorch model from the checkpoint\n",
    "        pytorch_model = LitSimpleCNN.load_from_checkpoint(checkpoint_path, strict=False)\n",
    "        pytorch_model.eval()  # Set model to evaluation mode\n",
    "        pytorch_model.to('cuda:0') # Move model to GPU\n",
    "\n",
    "        # --- Generate Predictions for the ENTIRE test set ---\n",
    "        # We will collect the raw model outputs (logits) and true labels\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Wrap the loop in torch.no_grad() for efficiency\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=f\"Predicting with model {i+1}\"):\n",
    "                inputs, labels = batch\n",
    "                \n",
    "                # Move data to the GPU\n",
    "                inputs = inputs.to('cuda:0')\n",
    "                \n",
    "                # Get model output (raw logits) for the batch\n",
    "                logits = pytorch_model(inputs)\n",
    "                \n",
    "                # Append batch results to lists (move back to CPU to prevent GPU memory buildup)\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        # Concatenate all batch results into single tensors\n",
    "        # These now contain the predictions and labels for the full test set\n",
    "        full_dataset_logits = torch.cat(all_logits)\n",
    "        full_dataset_labels = torch.cat(all_labels).int() # Ensure labels are integers\n",
    "\n",
    "        # --- Now, Calculate ALL Metrics using the generated predictions ---\n",
    "\n",
    "        # 2. Calculate the full ROC curve data\n",
    "        # Use the raw logits, torchmetrics will handle applying all thresholds\n",
    "        array_of_all_fprs, array_of_all_tprs, threshold_vals = torchmetrics.functional.roc(\n",
    "            preds=full_dataset_logits,\n",
    "            target=full_dataset_labels,\n",
    "            task=\"binary\"\n",
    "        )\n",
    "\n",
    "        # 3. Calculate confusion matrix based on a hard threshold (e.g., 0.5)\n",
    "        # Note: The model outputs logits, so the threshold 0.5 for probabilities corresponds to 0.0 for logits.\n",
    "        hard_preds = (full_dataset_logits > 0.0).int().numpy()\n",
    "        tn, fp, fn, tp = confusion_matrix(full_dataset_labels.numpy(), hard_preds).ravel()\n",
    "\n",
    "        # 4. Calculate metrics from the confusion matrix\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # This is also Recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "        \n",
    "        print(f\"Results at threshold 0.5: TPR={tpr:.4f}, FPR={fpr:.4f}, F1-Score={f1:.4f}\")\n",
    "        \n",
    "        # 5. Store the comprehensive results for this model\n",
    "        list_weighted_clfs.append({\n",
    "            \"fpr\": fpr,\n",
    "            \"tpr\": tpr,\n",
    "            \"model\": pytorch_model, # Optional: store the model object itself\n",
    "            \"threshold\": 0.5,\n",
    "            \"full_roc\": {\n",
    "                \"fpr\": array_of_all_fprs,\n",
    "                \"tpr\": array_of_all_tprs,\n",
    "                \"thresholds\": threshold_vals\n",
    "            }\n",
    "        })\n",
    "        best_clfs = get_best_roc_envelope(list_weighted_clfs, results_original_roc)\n",
    "    list_folds_weighted_clfs.append(list_weighted_clfs)\n",
    "    list_folds_best_models.append(best_clfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2787fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "results_lists.extend(list_folds_best_models)\n",
    "results_lists.extend(list_folds_weighted_clfs)\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    names=[\"Fold 1\", \"Fold 2\", \"Fold 3\", \"Fold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\"],\n",
    "    results_original_roc=results_original_roc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a49e4",
   "metadata": {},
   "source": [
    "### Ensemble prediction using voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ee047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Loop through all the FPR values to generate the ensemble ROC curve ---\n",
    "\n",
    "ensemble_results_soft = []\n",
    "ensemble_results_hard = []\n",
    "\n",
    "print(\"\\n--- Generating Ensemble ROC Curve by iterating through FPR values ---\")\n",
    "# We iterate from a low to high target_fpr to trace the curve\n",
    "for target_fpr in np.linspace(0.0, 1.0, 50): \n",
    "    # 1. Assign the function's output to a single variable first.\n",
    "    result_tuple = predict_ensemble_and_evaluate(\n",
    "        list_folds_best_models=list_folds_best_models,\n",
    "        test_loader=test_loader,\n",
    "        target_fpr=target_fpr\n",
    "    )\n",
    "    \n",
    "    if result_tuple is not None:\n",
    "        \n",
    "        for voting_method, metrics in result_tuple.items():\n",
    "            # Create a dictionary for each point and append it to the list\n",
    "            if voting_method == 'soft_voting':\n",
    "                point_dict = {'fpr': metrics['fpr'], 'tpr': metrics['tpr']}\n",
    "                ensemble_results_soft.append(point_dict)\n",
    "            else:\n",
    "                point_dict = {'fpr': metrics['fpr'], 'tpr': metrics['tpr']}\n",
    "                ensemble_results_hard.append(point_dict)\n",
    "            \n",
    "        \n",
    "# Ensure the curve starts at (0, 0)\n",
    "    if not ensemble_results_soft or ensemble_results_soft[0]['fpr'] > 0.0:\n",
    "        ensemble_results_soft.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if ensemble_results_soft[-1]['fpr'] < 1.0 or ensemble_results_soft[-1]['tpr'] < 1.0:\n",
    "        ensemble_results_soft.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "    if not ensemble_results_hard or ensemble_results_hard[0]['fpr'] > 0.0:\n",
    "        ensemble_results_hard.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if ensemble_results_hard[-1]['fpr'] < 1.0 or ensemble_results_hard[-1]['tpr'] < 1.0:\n",
    "        ensemble_results_hard.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "results_lists.extend(list_folds_best_models)\n",
    "results_lists.extend(list_folds_weighted_clfs)\n",
    "results_lists.append(ensemble_results_soft)\n",
    "results_lists.append(ensemble_results_hard)\n",
    "\n",
    "\n",
    "# --- Plot the final comparison ---\n",
    "print(\"\\n--- Plotting all ROC curves for comparison ---\")\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    names=[\"Fold 1\", \"Fold 2\", \"Fold 3\", \"Fold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\", \"Ensemble_voting_soft\", \"Ensemble_voting_hard\"],\n",
    "    results_original_roc=results_original_roc\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLlabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
