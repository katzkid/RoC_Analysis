{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4da4f6",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0556d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "from predict_ensemble_and_evaluate_paper import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate_paper import plot_roc_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d840881",
   "metadata": {},
   "source": [
    "## Display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c24af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for displaying ROC curves of multiple classifiers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def plot_roc_connected_points(results_list, results_original_roc, name=\"All Classifiers\"):\n",
    "    \"\"\"\n",
    "    Creates a plot showing the performance of all classifiers as points,\n",
    "    and connects them with a line, sorted by FPR.\n",
    "\n",
    "    Args:\n",
    "        results_list: A list of dictionaries, where each dict contains 'fpr', 'tpr'.\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # Convert results to a pandas DataFrame for easy sorting\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Sort the models by their False Positive Rate to create a left-to-right line\n",
    "    df_sorted = df.sort_values(by='fpr').reset_index(drop=True)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Plot all the individual model points as a scatter plot\n",
    "    # This helps see the actual performance of each model\n",
    "    plt.scatter(df_sorted['fpr'], df_sorted['tpr'], c='black', marker='o', \n",
    "                alpha=0.6, s=100, zorder=3) # zorder=3 puts dots on top\n",
    "\n",
    "    # Plot the line connecting the points\n",
    "    plt.plot(df_sorted['fpr'], df_sorted['tpr'], color='red', lw=1.5,\n",
    "             alpha=0.8, label=f'{name} (AUC = {auc(df_sorted[\"fpr\"], df_sorted[\"tpr\"]):.2f})')\n",
    "    \n",
    "    #plot the original ROC curve\n",
    "    plt.plot(results_original_roc[\"fpr\"], results_original_roc[\"tpr\"], color='blue', \n",
    "             label=f'{results_original_roc[\"name\"]} (AUC = {results_original_roc[\"auc\"]:.2f})')\n",
    "\n",
    "    # Plot the \"no-skill\" line for reference\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Performance Path of All Trained Classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23b1ea",
   "metadata": {},
   "source": [
    "## Best ROC envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e350c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def get_best_roc_envelope(results_list, results_original_roc=None):\n",
    "    \"\"\"\n",
    "    Constructs the upper envelope of multiple ROC curves to find the best\n",
    "    possible operating points from a collection of classifiers.\n",
    "\n",
    "    This function is guaranteed to produce a list of points that starts at\n",
    "    (FPR=0, TPR=0) and ends at (FPR=1, TPR=1), making it suitable for direct\n",
    "    plotting and AUC calculation.\n",
    "\n",
    "    Args:\n",
    "        results_list (list): A list of dictionaries. Each dictionary must\n",
    "                             contain a 'model' name and the full ROC results\n",
    "                             under the 'full_roc' key.\n",
    "        results_original_roc (dict, optional): A dictionary containing the\n",
    "                                               ROC results of a baseline model,\n",
    "                                               which will also be included in\n",
    "                                               the comparison. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of dictionaries representing the optimal ROC\n",
    "              envelope. Each dictionary includes 'model', 'fpr', 'tpr',\n",
    "              and 'threshold'.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "\n",
    "    # Step 1: Gather all ROC points from all provided models\n",
    "    all_results = results_list[:]\n",
    "    if results_original_roc:\n",
    "        all_results.append({\n",
    "            'model': 'original_baseline',\n",
    "            'full_roc': results_original_roc\n",
    "        })\n",
    "\n",
    "    for result in all_results:\n",
    "        model_name = result.get('model', 'Unknown Model')\n",
    "        roc = result.get('full_roc')\n",
    "        if not roc: continue\n",
    "\n",
    "        fprs = roc.get('fpr', [])\n",
    "        tprs = roc.get('tpr', [])\n",
    "        thresholds = roc.get('thresholds', [])\n",
    "\n",
    "        for i in range(len(fprs)):\n",
    "            threshold = thresholds[i] if i < len(thresholds) else thresholds[-1]\n",
    "            all_points.append({\n",
    "                'model': model_name, 'fpr': fprs[i],\n",
    "                'tpr': tprs[i], 'threshold': threshold\n",
    "            })\n",
    "\n",
    "    # Handle case with no data by returning a default diagonal line\n",
    "    if not all_points:\n",
    "        return [\n",
    "            {'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf},\n",
    "            {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}\n",
    "        ]\n",
    "\n",
    "    # Step 2: Sort all points by FPR (asc), then by TPR (desc)\n",
    "    all_points.sort(key=lambda p: (p['fpr'], -p['tpr']))\n",
    "\n",
    "    # Step 3: Build the core ROC envelope\n",
    "    best_roc_envelope = []\n",
    "    max_tpr_so_far = -1.0\n",
    "    for point in all_points:\n",
    "        # Add a point only if it has a strictly higher TPR\n",
    "        if point['tpr'] > max_tpr_so_far:\n",
    "            # To prevent adding multiple points at the same FPR, check if the last point\n",
    "            # has the same FPR and replace it if so (since this one has a higher TPR)\n",
    "            if best_roc_envelope and best_roc_envelope[-1]['fpr'] == point['fpr']:\n",
    "                best_roc_envelope[-1] = point\n",
    "            else:\n",
    "                best_roc_envelope.append(point)\n",
    "            max_tpr_so_far = point['tpr']\n",
    "    \n",
    "    # Step 4: Manually add extremities if they are missing\n",
    "    \n",
    "    # Ensure the curve starts at (0, 0)\n",
    "    if not best_roc_envelope or best_roc_envelope[0]['fpr'] > 0.0:\n",
    "        best_roc_envelope.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if best_roc_envelope[-1]['fpr'] < 1.0 or best_roc_envelope[-1]['tpr'] < 1.0:\n",
    "        best_roc_envelope.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "    return best_roc_envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b781e9",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c3deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(dataset):\n",
    "    if dataset == \"data1\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [0, 2], [2, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "    elif dataset == \"data2\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [1, 2], [1, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    elif dataset == \"pneumoniaMNIST\":\n",
    "        import medmnist\n",
    "        print(medmnist.__version__)\n",
    "        # read in the PneumoniaMNIST dataset\n",
    "        from medmnist import PneumoniaMNIST\n",
    "        train_data = PneumoniaMNIST(split='train', download=True, size=224)\n",
    "        test_data = PneumoniaMNIST(split='test', download=True, size=224)\n",
    "        val_data = PneumoniaMNIST(split='val', download=True, size=224)\n",
    "        # print the dataset info\n",
    "        print(f\"Number of training samples: {len(train_data)}\")\n",
    "        print(f\"Number of test samples: {len(test_data)}\")\n",
    "        print(f\"Number of validation samples: {len(val_data)}\")\n",
    "\n",
    "    return train_data, test_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfe355",
   "metadata": {},
   "source": [
    "## Calculate Statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af73be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_fpr_tpr(clf_model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the False Positive Rate (FPR) and True Positive Rate (TPR) at a given threshold.\n",
    "\n",
    "    Args:\n",
    "        X_test: The test features.\n",
    "        y_test: The true test labels (0 or 1).\n",
    "        threshold: The probability threshold.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the FPR and TPR. Returns None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_prob = clf_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  #Avoid division by zero\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0 #Avoid division by zero\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        misclassification_rate = 1 - accuracy\n",
    "\n",
    "        return {\"fpr\": fpr, \"tpr\": tpr, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy, \"misclassification_rate\": misclassification_rate}\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error calculating FPR and TPR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59034e",
   "metadata": {},
   "source": [
    "## PneumoniaMNIST: NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74133e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a060ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 1\n",
    "NUM_CHANNELS = 1\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 4\n",
    "K_FOLDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d23d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, image_height, image_width):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        #Convoluional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 224x224 -> 112x112\n",
    "        \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 112x112 -> 56x56\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 56x56 -> 28x28\n",
    "        \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 28x28 -> 14x14\n",
    "        )\n",
    "\n",
    "        # --- DYNAMIC FLATTENED SIZE CALCULATION ---\n",
    "        # Create a dummy tensor with the specified input dimensions\n",
    "        dummy_input = torch.randn(1, in_channels, image_height, image_width)\n",
    "        # Pass it through the feature extractor to see the output shape\n",
    "        dummy_output = self.features(dummy_input)\n",
    "        # The number of elements in the output tensor is our flattened size\n",
    "        self.flattened_size = dummy_output.numel()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        # Output layer: num_classes=1 for binary classification (outputting logits)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a702675",
   "metadata": {},
   "source": [
    "### Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac92eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitSimpleCNN(pl.LightningModule):\n",
    "    def __init__(self, in_channels, num_classes, learning_rate, image_height, image_width, training_mode='full_network'):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define the model\n",
    "        self.model = SimpleCNN(\n",
    "            in_channels=self.hparams.in_channels, \n",
    "            num_classes=self.hparams.num_classes,\n",
    "            image_height=self.hparams.image_height,\n",
    "            image_width=self.hparams.image_width\n",
    "        )\n",
    "        \n",
    "        # Define loss function\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Define metrics\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.val_auc = torchmetrics.AUROC(task=\"binary\")\n",
    "\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.test_auc = torchmetrics.AUROC(task=\"binary\")\n",
    "        self.test_precision = torchmetrics.Precision(task=\"binary\")\n",
    "        self.test_recall = torchmetrics.Recall(task=\"binary\")\n",
    "        self.test_f1 = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "        # This list will store outputs from each test step\n",
    "        self.last_test_results = {}\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def freeze_feature_extractor(self):\n",
    "        print(\"Freezing feature extractor layers...\")\n",
    "        for param in self.model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.fc1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        # For BCEWithLogitsLoss, labels must be float\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = self(inputs) # Forward pass\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # We need to handle which part of the network we are training\n",
    "        if self.hparams.training_mode == 'full_network':\n",
    "            self.log('train_loss_full', loss)\n",
    "        elif self.hparams.training_mode == 'classifier_only':\n",
    "            self.log('train_loss_classifier', loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.train_accuracy(outputs, labels.int())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.val_accuracy(outputs, labels.int())\n",
    "        self.val_auc(outputs, labels.int())\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('val_acc', self.val_accuracy, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('val_auc', self.val_auc, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels.float())\n",
    "        \n",
    "        # Append predictions and labels to our list\n",
    "        self.test_step_outputs.append({\"preds\": outputs.detach(), \"labels\": labels.detach()})\n",
    "        \n",
    "        # Log the loss for this batch\n",
    "        self.log('test_loss', loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Concatenate all predictions and labels from the list we built\n",
    "        all_preds = torch.cat([x['preds'] for x in self.test_step_outputs])\n",
    "        all_labels = torch.cat([x['labels'] for x in self.test_step_outputs])\n",
    "\n",
    "        # Calculate final metrics over the entire test set\n",
    "        test_acc = self.test_accuracy(all_preds, all_labels.int())\n",
    "        test_auc_val = self.test_auc(all_preds, all_labels.int())\n",
    "        test_prec = self.test_precision(all_preds, all_labels.int())\n",
    "        test_rec = self.test_recall(all_preds, all_labels.int())\n",
    "        test_f1_val = self.test_f1(all_preds, all_labels.int())\n",
    "        test_cm_val = torchmetrics.functional.confusion_matrix(all_preds, all_labels.int(), task=\"binary\")\n",
    "\n",
    "        # Log the final metrics\n",
    "        self.log(\"test_acc_epoch\", test_acc)\n",
    "        self.log(\"test_auc_epoch\", test_auc_val)\n",
    "\n",
    "        print(f\"\\n--- Final Test Metrics ---\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"AUC: {test_auc_val:.4f}\")\n",
    "        print(f\"Precision: {test_prec:.4f}\")\n",
    "        print(f\"Recall: {test_rec:.4f}\")\n",
    "        print(f\"F1-Score: {test_f1_val:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{test_cm_val}\")\n",
    "        print(\"--------------------------\\n\")\n",
    "\n",
    "        # Calculate data for the ROC Curve\n",
    "        fpr, tpr, thresholds = torchmetrics.functional.roc(\n",
    "            torch.sigmoid(all_preds),\n",
    "            all_labels.int(),\n",
    "            task=\"binary\"\n",
    "        )\n",
    "\n",
    "        # Store the results to be retrieved later in the main script\n",
    "        self.last_test_results = {\n",
    "            \"fpr\": fpr.cpu(),\n",
    "            \"tpr\": tpr.cpu(),\n",
    "            \"auc\": test_auc_val,\n",
    "            \"f1\": test_f1_val,\n",
    "            \"precision\": test_prec,\n",
    "            \"recall\": test_rec,\n",
    "            \"cm\": test_cm_val,\n",
    "            \"thresholds\": thresholds.cpu(),\n",
    "        }\n",
    "        # Free up memory\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ea652",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d785e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n",
      "Number of training samples: 4708\n",
      "Number of test samples: 624\n",
      "Number of validation samples: 524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "train_dataset, test_dataset, val_dataset = generate_data(\"pneumoniaMNIST\")\n",
    "\n",
    "# Ensure you have your train_loader and val_loader defined here\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset.transform = data_transforms\n",
    "test_dataset.transform = data_transforms\n",
    "val_dataset.transform = data_transforms\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#new_train_loader = data.DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) to be done inside the sample_ratio loop\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# initialize fold\n",
    "kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "971945fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training for 1 epochs on train_loader ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 74/74 [00:06<00:00, 12.27it/s, v_num=8, train_loss_step=0.0863, train_acc_step=1.000, val_loss=0.199, val_acc=0.929, val_auc=0.977, train_loss_epoch=0.277, train_acc_epoch=0.879]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 74/74 [00:07<00:00, 10.30it/s, v_num=8, train_loss_step=0.0863, train_acc_step=1.000, val_loss=0.199, val_acc=0.929, val_auc=0.977, train_loss_epoch=0.277, train_acc_epoch=0.879]\n",
      "Best model from Phase 1 saved to: /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93-v6.ckpt\n",
      "\n",
      "--- Testing model from Phase 1 checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93-v6.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93-v6.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93-v6.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 41.13it/s]\n",
      "--- Final Test Metrics ---\n",
      "Accuracy: 0.8558\n",
      "AUC: 0.9218\n",
      "Precision: 0.8409\n",
      "Recall: 0.9487\n",
      "F1-Score: 0.8916\n",
      "Confusion Matrix:\n",
      "tensor([[164,  70],\n",
      "        [ 20, 370]], device='cuda:0')\n",
      "--------------------------\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 35.91it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.8557692170143127\n",
      "     test_auc_epoch         0.9218223094940186\n",
      "        test_loss           0.36687397956848145\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "--- Manually Calculating Metrics on Test Set ---\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import numpy as np\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "# 1. Instantiate the LightningModule\n",
    "model = LitSimpleCNN(\n",
    "    in_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    training_mode='full_network',  # Change to 'classifier_only' if you want to train only the classifier\n",
    "    image_height=IMAGE_SIZE,\n",
    "    image_width=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# Optional: Print model summary\n",
    "# You need to move the model to a device first for torchsummary to work\n",
    "# summary(model.to('cuda'), (NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE))\n",
    "# model.to('cpu') # Move it back if needed\n",
    "\n",
    "checkpoint_callback_full_model = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='simple-cnn-full-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "train_trainer_1 = pl.Trainer(\n",
    "    accelerator='gpu',                 # Use GPUs\n",
    "    devices=1,                         # Number of GPUs to use\n",
    "    #strategy='ddp_notebook',                    # DistributedDataParallel strategy (best for multi-GPU)\n",
    "    max_epochs=NUM_EPOCHS,              # Total number of epochs\n",
    "    callbacks=[checkpoint_callback_full_model, progress_bar],\n",
    "    logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"simple_cnn-full\")\n",
    ")\n",
    "\n",
    "test_trainer_1 = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,  # Use only one GPU\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"simple_cnn_test\"),\n",
    "    # Add the callback here so the Trainer can find the 'best' path\n",
    "    callbacks=[checkpoint_callback_full_model],\n",
    "    precision='16-mixed'  # Use mixed precision for testing\n",
    ")\n",
    "\n",
    "\n",
    "# --- Phase 1 Training ---\n",
    "print(f\"--- Starting Training for {NUM_EPOCHS} epochs on train_loader ---\")\n",
    "train_trainer_1.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# CRITICAL: Save the path to the best model from this phase\n",
    "path_after_phase1 = checkpoint_callback_full_model.best_model_path\n",
    "print(f\"Best model from Phase 1 saved to: {path_after_phase1}\")\n",
    "\n",
    "# --- Test 1: Evaluate the model from after Phase 1 ---\n",
    "print(f\"\\n--- Testing model from Phase 1 checkpoint: {path_after_phase1} ---\")\n",
    "model.current_test_title = \"ROC Curve after training on DataLoader 1\"\n",
    "model = LitSimpleCNN.load_from_checkpoint(path_after_phase1)  # Load the model from Phase 1\n",
    "model.eval().to('cuda:1')  # Ensure the model is in evaluation mode and on GPU\n",
    "test_trainer_1.test(model, dataloaders=test_loader, ckpt_path=path_after_phase1)\n",
    "results_phase1 = model.last_test_results.copy()\n",
    "results_original_roc = {\"fpr\": results_phase1[\"fpr\"], \"tpr\": results_phase1[\"tpr\"], \"thresholds\": results_phase1[\"thresholds\"], \"name\": \"Original NN PneumoniaMNIST\", \"auc\": results_phase1[\"auc\"], \"model\": model}\n",
    "\n",
    "# Metrics\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "final_predictions = [] # This will store binary predictions (0s or 1s)\n",
    "true_labels = []\n",
    "\n",
    "print(\"\\n--- Manually Calculating Metrics on Test Set ---\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move input data to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # 1. Get the raw model output (logits) and convert to probabilities\n",
    "        outputs = torch.sigmoid(model(inputs))\n",
    "\n",
    "        # 2. Convert probabilities to binary class predictions (0 or 1) using a 0.5 threshold\n",
    "        preds = (outputs > 0.5).int()\n",
    "\n",
    "        final_predictions.extend(preds.cpu().numpy().flatten())\n",
    "        true_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "# Ensure they are numpy arrays for sklearn\n",
    "true_labels = np.array(true_labels)\n",
    "final_predictions = np.array(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b9bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Confusion Matrix ---\n",
      "Misclassification Risk original model: {'risk': np.float64(0.14423076923076925), 'tpr': np.float64(0.9487179487179487), 'fpr': np.float64(0.29914529914529914), 'threshold': 0.625, 'f1': np.float64(0.891566265060241)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, calculate metrics using the correct binary predictions\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "tn, fp, fn, tp = confusion_matrix(true_labels, final_predictions).ravel()\n",
    "\n",
    "#print the misclassification risk of the original model\n",
    "total_positive = 0\n",
    "total_samples = 0\n",
    "for _, labels in test_loader:\n",
    "    total_positive += labels.sum().item()\n",
    "    total_samples += labels.size(0)\n",
    "prior_proba = total_positive / total_samples\n",
    "tpr_orig = tp/(tp + fn) if(tp + fn) > 0 else 0.0\n",
    "fpr_orig = fp/(fp + tn) if(fp + tn) > 0 else 0.0\n",
    "risk = (prior_proba * (1 - tpr_orig)) + ((1 - prior_proba) * fpr_orig)\n",
    "f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n",
    "misclassification_risk_orig = {\n",
    "    \"risk\": risk,\n",
    "    \"tpr\": tpr_orig,\n",
    "    \"fpr\": fpr_orig,\n",
    "    \"threshold\": prior_proba,\n",
    "    \"f1\": f1\n",
    "}\n",
    "print(f\"Misclassification Risk original model: {misclassification_risk_orig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849b63a",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate Weighted ROC curve (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226e70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSIFIERS = 20\n",
    "w = np.linspace(0.001, 0.999, NUM_CLASSIFIERS, endpoint=True)\n",
    "pos_weights = w/(1-w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515305ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import autocast\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#new_train_loader = data.DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) to be done inside the sample_ratio loop\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "list_weighted_clfs = []\n",
    "\n",
    "for i, pos_weight in enumerate(pos_weights):\n",
    "    model.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_acc',\n",
    "        dirpath=f'checkpoints/stage_{i+1}/',\n",
    "        filename=f'best-model-{{epoch:02d}}-{{val_acc:.2f}}',\n",
    "        save_top_k=1,\n",
    "        mode='max',\n",
    "    )\n",
    "    \n",
    "    # 3. Instantiate a NEW Trainer for this specific stage\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        #strategy='ddp_notebook',\n",
    "        max_epochs=NUM_EPOCHS, # Number of epochs for THIS stage\n",
    "        callbacks=[checkpoint_callback, progress_bar],\n",
    "        logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=f\"stage_{i+1}_ratio_{pos_weight}\"),\n",
    "        precision='16-mixed'\n",
    "        )\n",
    "    \n",
    "    # 4. Train the model. It will start with weights from the previous stage.\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    \n",
    "    # 5. Get the path to the best model from THIS stage and store it\n",
    "    if checkpoint_callback.best_model_path:\n",
    "        best_path_this_stage = checkpoint_callback.best_model_path\n",
    "        print(f\"--- Stage {i+1} complete. Best model path: {best_path_this_stage} ---\")\n",
    "        best_model_paths.append(best_path_this_stage)\n",
    "        \n",
    "        # 6. CRITICAL: Load the best weights back into the model object\n",
    "        # This ensures we carry forward the BEST model to the next stage, not the last.\n",
    "        print(f\"Loading best weights from {best_path_this_stage} to continue...\")\n",
    "        model = LitSimpleCNN.load_from_checkpoint(best_path_this_stage, strict=False)  # Use strict=False to ignore missing keys if any\n",
    "    else:\n",
    "        print(f\"--- Stage {i+1} complete. No checkpoint was saved. ---\")\n",
    "        # If no checkpoint was saved, the model object will just have the weights from the last epoch.\n",
    "\n",
    "# 7. Test the model after each stage\n",
    "# Loop through each saved model checkpoint\n",
    "for i, checkpoint_path in enumerate(best_model_paths):\n",
    "    print(f\"\\n--- Testing model from checkpoint: {checkpoint_path} ---\")\n",
    "\n",
    "    # 1. Load the PyTorch model from the checkpoint\n",
    "    pytorch_model = LitSimpleCNN.load_from_checkpoint(checkpoint_path, strict=False)\n",
    "    pytorch_model.eval()  # Set model to evaluation mode\n",
    "    pytorch_model.to('cuda:1') # Move model to GPU\n",
    "\n",
    "    # --- Generate Predictions for the ENTIRE test set ---\n",
    "    # We will collect the raw model outputs (logits) and true labels\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Wrap the loop in torch.no_grad() for efficiency\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Predicting with model {i+1}\"):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Move inputs to GPU\n",
    "            inputs = inputs.to(device, non_blocking=True) # non_blocking=True helps speed up transfer\n",
    "            \n",
    "            # Use Mixed Precision to save memory\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                logits = pytorch_model(inputs)\n",
    "                \n",
    "            # Move back to CPU immediately to keep GPU memory free\n",
    "            # .detach() ensures no hidden gradients are tracked\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_labels.append(labels.detach().cpu())\n",
    "\n",
    "    # Concatenate all batch results into single tensors\n",
    "    # These now contain the predictions and labels for the full test set\n",
    "    full_dataset_logits = torch.cat(all_logits)\n",
    "    full_dataset_labels = torch.cat(all_labels).int() # Ensure labels are integers\n",
    "\n",
    "    # --- Now, Calculate ALL Metrics using the generated predictions ---\n",
    "\n",
    "    # 2. Calculate the full ROC curve data\n",
    "    # Use the raw logits, torchmetrics will handle applying all thresholds\n",
    "    array_of_all_fprs, array_of_all_tprs, threshold_vals = torchmetrics.functional.roc(\n",
    "        preds=full_dataset_logits,\n",
    "        target=full_dataset_labels,\n",
    "        task=\"binary\"\n",
    "    )\n",
    "\n",
    "    # 3. Calculate confusion matrix based on a hard threshold (e.g., 0.5)\n",
    "    # Note: The model outputs logits, so the threshold 0.5 for probabilities corresponds to 0.0 for logits.\n",
    "    hard_preds = (full_dataset_logits > 0.0).int().numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(full_dataset_labels.numpy(), hard_preds).ravel()\n",
    "\n",
    "    # 4. Calculate metrics from the confusion matrix\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # This is also Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "    \n",
    "    print(f\"Results at threshold 0.5: TPR={tpr:.4f}, FPR={fpr:.4f}, F1-Score={f1:.4f}\")\n",
    "    \n",
    "    # 5. Store the comprehensive results for this model\n",
    "    list_weighted_clfs.append({\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"threshold\": 0.5,\n",
    "        \"full_roc\": {\n",
    "            \"fpr\": array_of_all_fprs,\n",
    "            \"tpr\": array_of_all_tprs,\n",
    "            \"thresholds\": threshold_vals\n",
    "        }\n",
    "    })\n",
    "list_full_weighted_clfs = list_weighted_clfs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d529b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save to pickle\n",
    "from predict_ensemble_and_evaluate_paper import save_to_pickle_full_weighted_roc\n",
    "save_to_pickle_full_weighted_roc(list_full_weighted_clfs, filename='pickle/medMNIST_weighted_full_weighted_roc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7976e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained ROC curve points loaded from pickle/medMNIST_weighted_full_weighted_roc.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load from pickle\n",
    "from predict_ensemble_and_evaluate_paper import load_from_pickle_full_weighted_roc\n",
    "list_full_weighted_clfs = load_from_pickle_full_weighted_roc(filename='pickle/medMNIST_weighted_full_weighted_roc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c2d0a",
   "metadata": {},
   "source": [
    "### Weighted ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "\n",
    "#store the weighted classifiers of all the folds\n",
    "list_folds_weighted_clfs = []  # List to store classifiers from all folds\n",
    "#store the best classfiers of all the folds\n",
    "list_folds_best_models = []  # List to store best models from all folds\n",
    "\n",
    "# --- Bootstrapping Setup ---\n",
    "n_samples = len(train_dataset)\n",
    "original_indices = np.arange(n_samples)\n",
    "n_iterations = 4\n",
    "# Dictionary to store the history of each fold\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# 3. K-fold cross-validation loop\n",
    "for ii in range(n_iterations):\n",
    "    list_weighted_clfs = [] # List to store the models trained at each stage\n",
    "    best_clfs = [] # List to store the best models from each stage\n",
    "    best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "    train_ids = resample(original_indices, replace=True, n_samples=n_samples, random_state=ii)\n",
    "    val_ids = np.setdiff1d(original_indices, np.unique(train_ids))\n",
    "    # 1. Instantiate the LightningModule\n",
    "    model = LitSimpleCNN(\n",
    "        in_channels=NUM_CHANNELS,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        training_mode='full_network',  # Change to 'classifier_only' if you want to train only the classifier\n",
    "        image_height=IMAGE_SIZE,\n",
    "        image_width=IMAGE_SIZE\n",
    "    )\n",
    "    print(f\"--- Starting Iteration {ii + 1}/{n_iterations} ---\")\n",
    "\n",
    "    # 4. Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = Subset(train_dataset, train_ids)\n",
    "    val_subsampler = Subset(train_dataset, val_ids)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "                      train_subsampler,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      shuffle=True,\n",
    "                      num_workers=NUM_WORKERS)\n",
    "    fold_loader = data.DataLoader(\n",
    "                    val_subsampler,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "\n",
    "    for i, pos_weight in enumerate(pos_weights):\n",
    "        model.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_acc',\n",
    "            dirpath=f'checkpoints/stage_{i+1}/',\n",
    "            filename=f'best-model-{{epoch:02d}}-{{val_acc:.2f}}',\n",
    "            save_top_k=1,\n",
    "            mode='max',\n",
    "        )\n",
    "        \n",
    "        # 3. Instantiate a NEW Trainer for this specific stage\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator='gpu',\n",
    "            devices=1,\n",
    "            #strategy='ddp_notebook',\n",
    "            max_epochs=NUM_EPOCHS, # Number of epochs for THIS stage\n",
    "            callbacks=[checkpoint_callback, progress_bar],\n",
    "            logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=f\"stage_{i+1}_ratio_{pos_weight}\"),\n",
    "            precision='16-mixed'\n",
    "            )\n",
    "        \n",
    "        # 4. Train the model. It will start with weights from the previous stage.\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "        \n",
    "        # 5. Get the path to the best model from THIS stage and store it\n",
    "        if checkpoint_callback.best_model_path:\n",
    "            best_path_this_stage = checkpoint_callback.best_model_path\n",
    "            print(f\"--- Stage {i+1} complete. Best model path: {best_path_this_stage} ---\")\n",
    "            best_model_paths.append(best_path_this_stage)\n",
    "            \n",
    "            # 6. CRITICAL: Load the best weights back into the model object\n",
    "            # This ensures we carry forward the BEST model to the next stage, not the last.\n",
    "            print(f\"Loading best weights from {best_path_this_stage} to continue...\")\n",
    "            model = LitSimpleCNN.load_from_checkpoint(best_path_this_stage, strict=False)  # Use strict=False to ignore missing keys if any\n",
    "        else:\n",
    "            print(f\"--- Stage {i+1} complete. No checkpoint was saved. ---\")\n",
    "            # If no checkpoint was saved, the model object will just have the weights from the last epoch.\n",
    "\n",
    "    # 7. Test the model after each stage\n",
    "    # Loop through each saved model checkpoint\n",
    "    for i, checkpoint_path in enumerate(best_model_paths):\n",
    "        print(f\"\\n--- Testing model from checkpoint: {checkpoint_path} ---\")\n",
    "\n",
    "        # 1. Load the PyTorch model from the checkpoint\n",
    "        pytorch_model = LitSimpleCNN.load_from_checkpoint(checkpoint_path, strict=False)\n",
    "        pytorch_model.eval()  # Set model to evaluation mode\n",
    "        pytorch_model.to('cuda:1') # Move model to GPU\n",
    "\n",
    "        # --- Generate Predictions for the ENTIRE test set ---\n",
    "        # We will collect the raw model outputs (logits) and true labels\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Wrap the loop in torch.no_grad() for efficiency\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(fold_loader, desc=f\"Predicting with model {i+1}\"):\n",
    "                inputs, labels = batch\n",
    "                \n",
    "                # Move data to the GPU\n",
    "                inputs = inputs.to('cuda:1')\n",
    "                \n",
    "                # Get model output (raw logits) for the batch\n",
    "                logits = pytorch_model(inputs)\n",
    "                \n",
    "                # Append batch results to lists (move back to CPU to prevent GPU memory buildup)\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        # Concatenate all batch results into single tensors\n",
    "        # These now contain the predictions and labels for the full test set\n",
    "        full_dataset_logits = torch.cat(all_logits)\n",
    "        full_dataset_labels = torch.cat(all_labels).int() # Ensure labels are integers\n",
    "\n",
    "        # --- Now, Calculate ALL Metrics using the generated predictions ---\n",
    "\n",
    "        # 2. Calculate the full ROC curve data\n",
    "        # Use the raw logits, torchmetrics will handle applying all thresholds\n",
    "        array_of_all_fprs, array_of_all_tprs, threshold_vals = torchmetrics.functional.roc(\n",
    "            preds=full_dataset_logits,\n",
    "            target=full_dataset_labels,\n",
    "            task=\"binary\"\n",
    "        )\n",
    "\n",
    "        # 3. Calculate confusion matrix based on a hard threshold (e.g., 0.5)\n",
    "        # Note: The model outputs logits, so the threshold 0.5 for probabilities corresponds to 0.0 for logits.\n",
    "        hard_preds = (full_dataset_logits > 0.0).int().numpy()\n",
    "        tn, fp, fn, tp = confusion_matrix(full_dataset_labels.numpy(), hard_preds).ravel()\n",
    "\n",
    "        # 4. Calculate metrics from the confusion matrix\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # This is also Recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "        \n",
    "        print(f\"Results at threshold 0.5: TPR={tpr:.4f}, FPR={fpr:.4f}, F1-Score={f1:.4f}\")\n",
    "        \n",
    "        # 5. Store the comprehensive results for this model\n",
    "        list_weighted_clfs.append({\n",
    "            \"fpr\": fpr,\n",
    "            \"tpr\": tpr,\n",
    "            \"model\": pytorch_model, # Optional: store the model object itself\n",
    "            \"threshold\": 0.5,\n",
    "            \"full_roc\": {\n",
    "                \"fpr\": array_of_all_fprs,\n",
    "                \"tpr\": array_of_all_tprs,\n",
    "                \"thresholds\": threshold_vals\n",
    "            }\n",
    "        })\n",
    "    best_clfs = get_best_roc_envelope(list_weighted_clfs, results_original_roc)\n",
    "    list_folds_weighted_clfs.append(list_weighted_clfs)\n",
    "    list_folds_best_models.append(best_clfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f49a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_ensemble_and_evaluate_paper import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate_paper import plot_roc_comparison\n",
    "from predict_ensemble_and_evaluate_paper import make_curve_monotonic\n",
    "from predict_ensemble_and_evaluate_paper import save_to_pickle, load_from_pickle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168ae7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to pickle/medMNIST_weighted_bootstrap.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_models': [[{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.8625),\n",
       "    'threshold': tensor(0.9148)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0022),\n",
       "    'tpr': tensor(0.9097),\n",
       "    'threshold': tensor(0.9863)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0044),\n",
       "    'tpr': tensor(0.9158),\n",
       "    'threshold': tensor(0.9828)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0066),\n",
       "    'tpr': tensor(0.9212),\n",
       "    'threshold': tensor(0.9801)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0088),\n",
       "    'tpr': tensor(0.9514),\n",
       "    'threshold': tensor(0.9843)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0110),\n",
       "    'tpr': tensor(0.9552),\n",
       "    'threshold': tensor(0.9801)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0154),\n",
       "    'tpr': tensor(0.9575),\n",
       "    'threshold': tensor(0.9708)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0176),\n",
       "    'tpr': tensor(0.9583),\n",
       "    'threshold': tensor(0.9660)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0198),\n",
       "    'tpr': tensor(0.9591),\n",
       "    'threshold': tensor(0.9606)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0220),\n",
       "    'tpr': tensor(0.9614),\n",
       "    'threshold': tensor(0.9502)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0242),\n",
       "    'tpr': tensor(0.9629),\n",
       "    'threshold': tensor(0.9431)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0264),\n",
       "    'tpr': tensor(0.9668),\n",
       "    'threshold': tensor(0.9436)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0308),\n",
       "    'tpr': tensor(0.9699),\n",
       "    'threshold': tensor(0.9990)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0330),\n",
       "    'tpr': tensor(0.9722),\n",
       "    'threshold': tensor(0.8988)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0352),\n",
       "    'tpr': tensor(0.9745),\n",
       "    'threshold': tensor(0.3237)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0374),\n",
       "    'tpr': tensor(0.9753),\n",
       "    'threshold': tensor(0.8112)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0396),\n",
       "    'tpr': tensor(0.9784),\n",
       "    'threshold': tensor(0.8533)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0441),\n",
       "    'tpr': tensor(0.9807),\n",
       "    'threshold': tensor(0.8337)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0463),\n",
       "    'tpr': tensor(0.9815),\n",
       "    'threshold': tensor(0.8128)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0485),\n",
       "    'tpr': tensor(0.9822),\n",
       "    'threshold': tensor(0.8042)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0529),\n",
       "    'tpr': tensor(0.9846),\n",
       "    'threshold': tensor(0.3163)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0573),\n",
       "    'tpr': tensor(0.9869),\n",
       "    'threshold': tensor(0.9723)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0617),\n",
       "    'tpr': tensor(0.9876),\n",
       "    'threshold': tensor(0.9644)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0661),\n",
       "    'tpr': tensor(0.9884),\n",
       "    'threshold': tensor(0.9414)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0705),\n",
       "    'tpr': tensor(0.9900),\n",
       "    'threshold': tensor(0.9181)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0749),\n",
       "    'tpr': tensor(0.9907),\n",
       "    'threshold': tensor(0.8951)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0859),\n",
       "    'tpr': tensor(0.9915),\n",
       "    'threshold': tensor(0.4066)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0969),\n",
       "    'tpr': tensor(0.9923),\n",
       "    'threshold': tensor(0.0648)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1013),\n",
       "    'tpr': tensor(0.9931),\n",
       "    'threshold': tensor(0.0586)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1057),\n",
       "    'tpr': tensor(0.9938),\n",
       "    'threshold': tensor(0.0539)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1256),\n",
       "    'tpr': tensor(0.9946),\n",
       "    'threshold': tensor(0.0013)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1454),\n",
       "    'tpr': tensor(0.9954),\n",
       "    'threshold': tensor(0.0008)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1476),\n",
       "    'tpr': tensor(0.9961),\n",
       "    'threshold': tensor(0.0006)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1498),\n",
       "    'tpr': tensor(0.9969),\n",
       "    'threshold': tensor(0.0006)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1718),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(3.7104e-05)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1938),\n",
       "    'tpr': tensor(0.9985),\n",
       "    'threshold': tensor(0.0001)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2313),\n",
       "    'tpr': tensor(0.9992),\n",
       "    'threshold': tensor(3.1617e-05)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.3040),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0030)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.8579),\n",
       "    'threshold': tensor(0.9961)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0022),\n",
       "    'tpr': tensor(0.8857),\n",
       "    'threshold': tensor(0.9989)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0043),\n",
       "    'tpr': tensor(0.9012),\n",
       "    'threshold': tensor(0.9900)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0065),\n",
       "    'tpr': tensor(0.9282),\n",
       "    'threshold': tensor(0.9660)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0087),\n",
       "    'tpr': tensor(0.9514),\n",
       "    'threshold': tensor(0.9899)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0109),\n",
       "    'tpr': tensor(0.9537),\n",
       "    'threshold': tensor(0.9894)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0130),\n",
       "    'tpr': tensor(0.9568),\n",
       "    'threshold': tensor(0.9862)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0152),\n",
       "    'tpr': tensor(0.9660),\n",
       "    'threshold': tensor(0.7385)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0174),\n",
       "    'tpr': tensor(0.9683),\n",
       "    'threshold': tensor(0.7051)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0196),\n",
       "    'tpr': tensor(0.9699),\n",
       "    'threshold': tensor(0.6475)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0217),\n",
       "    'tpr': tensor(0.9707),\n",
       "    'threshold': tensor(0.5661)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0261),\n",
       "    'tpr': tensor(0.9730),\n",
       "    'threshold': tensor(0.4892)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0283),\n",
       "    'tpr': tensor(0.9737),\n",
       "    'threshold': tensor(0.4554)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0304),\n",
       "    'tpr': tensor(0.9768),\n",
       "    'threshold': tensor(0.3286)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0326),\n",
       "    'tpr': tensor(0.9776),\n",
       "    'threshold': tensor(0.1231)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0348),\n",
       "    'tpr': tensor(0.9792),\n",
       "    'threshold': tensor(0.3027)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0370),\n",
       "    'tpr': tensor(0.9799),\n",
       "    'threshold': tensor(0.0411)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0391),\n",
       "    'tpr': tensor(0.9807),\n",
       "    'threshold': tensor(0.0338)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0435),\n",
       "    'tpr': tensor(0.9830),\n",
       "    'threshold': tensor(0.9584)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0457),\n",
       "    'tpr': tensor(0.9838),\n",
       "    'threshold': tensor(0.9511)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0500),\n",
       "    'tpr': tensor(0.9853),\n",
       "    'threshold': tensor(0.0038)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0587),\n",
       "    'tpr': tensor(0.9869),\n",
       "    'threshold': tensor(0.1077)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0717),\n",
       "    'tpr': tensor(0.9876),\n",
       "    'threshold': tensor(0.2519)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0761),\n",
       "    'tpr': tensor(0.9884),\n",
       "    'threshold': tensor(0.0687)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0804),\n",
       "    'tpr': tensor(0.9907),\n",
       "    'threshold': tensor(0.0768)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0870),\n",
       "    'tpr': tensor(0.9915),\n",
       "    'threshold': tensor(0.0569)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0891),\n",
       "    'tpr': tensor(0.9923),\n",
       "    'threshold': tensor(0.0537)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0935),\n",
       "    'tpr': tensor(0.9931),\n",
       "    'threshold': tensor(0.0496)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1000),\n",
       "    'tpr': tensor(0.9938),\n",
       "    'threshold': tensor(0.0411)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1065),\n",
       "    'tpr': tensor(0.9954),\n",
       "    'threshold': tensor(0.0294)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1217),\n",
       "    'tpr': tensor(0.9961),\n",
       "    'threshold': tensor(0.0182)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1348),\n",
       "    'tpr': tensor(0.9969),\n",
       "    'threshold': tensor(2.9711e-08)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1630),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(0.0780)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1674),\n",
       "    'tpr': tensor(0.9985),\n",
       "    'threshold': tensor(0.0049)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2152),\n",
       "    'tpr': tensor(0.9992),\n",
       "    'threshold': tensor(0.0003)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2891),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0006)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.6303),\n",
       "    'threshold': tensor(0.9999)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0023),\n",
       "    'tpr': tensor(0.9008),\n",
       "    'threshold': tensor(0.9997)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0047),\n",
       "    'tpr': tensor(0.9193),\n",
       "    'threshold': tensor(0.9987)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0070),\n",
       "    'tpr': tensor(0.9370),\n",
       "    'threshold': tensor(0.9890)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0094),\n",
       "    'tpr': tensor(0.9454),\n",
       "    'threshold': tensor(0.9715)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0117),\n",
       "    'tpr': tensor(0.9462),\n",
       "    'threshold': tensor(0.8660)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0141),\n",
       "    'tpr': tensor(0.9477),\n",
       "    'threshold': tensor(0.9644)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0164),\n",
       "    'tpr': tensor(0.9562),\n",
       "    'threshold': tensor(0.8078)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0188),\n",
       "    'tpr': tensor(0.9600),\n",
       "    'threshold': tensor(0.8894)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0211),\n",
       "    'tpr': tensor(0.9693),\n",
       "    'threshold': tensor(0.7808)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0282),\n",
       "    'tpr': tensor(0.9700),\n",
       "    'threshold': tensor(0.7207)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0305),\n",
       "    'tpr': tensor(0.9723),\n",
       "    'threshold': tensor(0.0983)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0376),\n",
       "    'tpr': tensor(0.9777),\n",
       "    'threshold': tensor(0.3725)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0399),\n",
       "    'tpr': tensor(0.9785),\n",
       "    'threshold': tensor(0.3498)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0423),\n",
       "    'tpr': tensor(0.9823),\n",
       "    'threshold': tensor(0.2365)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0446),\n",
       "    'tpr': tensor(0.9831),\n",
       "    'threshold': tensor(0.2187)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0493),\n",
       "    'tpr': tensor(0.9839),\n",
       "    'threshold': tensor(0.1845)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0516),\n",
       "    'tpr': tensor(0.9854),\n",
       "    'threshold': tensor(0.1746)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0610),\n",
       "    'tpr': tensor(0.9869),\n",
       "    'threshold': tensor(0.2383)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0704),\n",
       "    'tpr': tensor(0.9877),\n",
       "    'threshold': tensor(0.2240)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0728),\n",
       "    'tpr': tensor(0.9885),\n",
       "    'threshold': tensor(0.2076)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0775),\n",
       "    'tpr': tensor(0.9892),\n",
       "    'threshold': tensor(0.1919)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0962),\n",
       "    'tpr': tensor(0.9908),\n",
       "    'threshold': tensor(0.0008)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0986),\n",
       "    'tpr': tensor(0.9923),\n",
       "    'threshold': tensor(0.0005)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1103),\n",
       "    'tpr': tensor(0.9931),\n",
       "    'threshold': tensor(0.0392)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1127),\n",
       "    'tpr': tensor(0.9939),\n",
       "    'threshold': tensor(0.0284)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1221),\n",
       "    'tpr': tensor(0.9946),\n",
       "    'threshold': tensor(0.0173)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1549),\n",
       "    'tpr': tensor(0.9962),\n",
       "    'threshold': tensor(1.2739e-06)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1620),\n",
       "    'tpr': tensor(0.9969),\n",
       "    'threshold': tensor(7.3577e-07)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1972),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(7.8052e-08)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2277),\n",
       "    'tpr': tensor(0.9985),\n",
       "    'threshold': tensor(1.1705e-08)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.3826),\n",
       "    'tpr': tensor(0.9992),\n",
       "    'threshold': tensor(4.6101e-08)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.6080),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(5.8776e-10)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.7687),\n",
       "    'threshold': tensor(1.0000)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0022),\n",
       "    'tpr': tensor(0.9252),\n",
       "    'threshold': tensor(0.9854)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0044),\n",
       "    'tpr': tensor(0.9522),\n",
       "    'threshold': tensor(0.8861)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0066),\n",
       "    'tpr': tensor(0.9537),\n",
       "    'threshold': tensor(0.9464)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0088),\n",
       "    'tpr': tensor(0.9622),\n",
       "    'threshold': tensor(0.9938)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0109),\n",
       "    'tpr': tensor(0.9638),\n",
       "    'threshold': tensor(0.9981)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0153),\n",
       "    'tpr': tensor(0.9684),\n",
       "    'threshold': tensor(0.9971)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0175),\n",
       "    'tpr': tensor(0.9699),\n",
       "    'threshold': tensor(0.9113)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0197),\n",
       "    'tpr': tensor(0.9753),\n",
       "    'threshold': tensor(0.9528)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0263),\n",
       "    'tpr': tensor(0.9776),\n",
       "    'threshold': tensor(0.9867)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0284),\n",
       "    'tpr': tensor(0.9792),\n",
       "    'threshold': tensor(0.9790)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0306),\n",
       "    'tpr': tensor(0.9815),\n",
       "    'threshold': tensor(0.9608)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0350),\n",
       "    'tpr': tensor(0.9823),\n",
       "    'threshold': tensor(0.9474)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0394),\n",
       "    'tpr': tensor(0.9846),\n",
       "    'threshold': tensor(0.9223)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0438),\n",
       "    'tpr': tensor(0.9869),\n",
       "    'threshold': tensor(0.4832)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0460),\n",
       "    'tpr': tensor(0.9877),\n",
       "    'threshold': tensor(0.0136)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0481),\n",
       "    'tpr': tensor(0.9900),\n",
       "    'threshold': tensor(0.3559)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0525),\n",
       "    'tpr': tensor(0.9915),\n",
       "    'threshold': tensor(0.2501)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0635),\n",
       "    'tpr': tensor(0.9923),\n",
       "    'threshold': tensor(0.4363)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0744),\n",
       "    'tpr': tensor(0.9931),\n",
       "    'threshold': tensor(0.0026)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0766),\n",
       "    'tpr': tensor(0.9938),\n",
       "    'threshold': tensor(0.0021)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0810),\n",
       "    'tpr': tensor(0.9946),\n",
       "    'threshold': tensor(0.2266)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0832),\n",
       "    'tpr': tensor(0.9954),\n",
       "    'threshold': tensor(0.1583)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0919),\n",
       "    'tpr': tensor(0.9961),\n",
       "    'threshold': tensor(0.0007)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1007),\n",
       "    'tpr': tensor(0.9969),\n",
       "    'threshold': tensor(0.0067)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1138),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(0.0032)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1313),\n",
       "    'tpr': tensor(0.9985),\n",
       "    'threshold': tensor(0.0019)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1663),\n",
       "    'tpr': tensor(0.9992),\n",
       "    'threshold': tensor(0.0006)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2888),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(2.5846e-05)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}]],\n",
       " 'weighted_clfs': [[{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0015444015444015444),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 5.9024e-01, 5.1283e-01,  ..., 8.3816e-14, 2.6584e-14,\n",
       "             1.0172e-14])}},\n",
       "   {'fpr': np.float64(0.024229074889867842),\n",
       "    'tpr': np.float64(0.8918918918918919),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.8771e-01, 9.8573e-01,  ..., 2.1769e-05, 5.9115e-06,\n",
       "             2.5191e-06])}},\n",
       "   {'fpr': np.float64(0.06387665198237885),\n",
       "    'tpr': np.float64(0.9606177606177606),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.8722e-01, 9.8720e-01,  ..., 5.2886e-05, 1.4266e-05,\n",
       "             7.7266e-06])}},\n",
       "   {'fpr': np.float64(0.1211453744493392),\n",
       "    'tpr': np.float64(0.9884169884169884),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9989e-01, 9.9989e-01,  ..., 4.2359e-05, 6.1554e-06,\n",
       "             3.2460e-06])}},\n",
       "   {'fpr': np.float64(0.03303964757709251),\n",
       "    'tpr': np.float64(0.9498069498069498),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9968e-01, 9.9952e-01,  ..., 1.0236e-07, 4.7114e-08,\n",
       "             3.6473e-08])}},\n",
       "   {'fpr': np.float64(0.0947136563876652),\n",
       "    'tpr': np.float64(0.9853281853281853),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 4.2796e-08, 4.0345e-08,\n",
       "             1.6694e-08])}},\n",
       "   {'fpr': np.float64(0.0881057268722467),\n",
       "    'tpr': np.float64(0.9861003861003861),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0023, 0.0039,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.3504e-10, 4.3168e-11,\n",
       "             8.7231e-12])}},\n",
       "   {'fpr': np.float64(0.05066079295154185),\n",
       "    'tpr': np.float64(0.9698841698841699),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0015, 0.0039,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.1076e-09, 4.8480e-10,\n",
       "             1.2037e-10])}},\n",
       "   {'fpr': np.float64(0.048458149779735685),\n",
       "    'tpr': np.float64(0.9722007722007722),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0031, 0.0093,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 6.3989e-10, 4.2095e-11,\n",
       "             1.0522e-11])}},\n",
       "   {'fpr': np.float64(0.03524229074889868),\n",
       "    'tpr': np.float64(0.9698841698841699),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0077, 0.0224,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 4.9072e-16, 1.2854e-16,\n",
       "             1.2292e-17])}},\n",
       "   {'fpr': np.float64(0.07488986784140969),\n",
       "    'tpr': np.float64(0.9806949806949807),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0069, 0.0216,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.4507e-12, 3.4234e-13,\n",
       "             4.0614e-15])}},\n",
       "   {'fpr': np.float64(0.05947136563876652),\n",
       "    'tpr': np.float64(0.9814671814671815),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
       "             0.0110, 0.0132, 0.0154, 0.0154, 0.0154, 0.0154, 0.0176, 0.0176, 0.0198,\n",
       "             0.0198, 0.0220, 0.0220, 0.0220, 0.0220, 0.0242, 0.0242, 0.0242, 0.0264,\n",
       "             0.0264, 0.0264, 0.0264, 0.0286, 0.0308, 0.0308, 0.0308, 0.0330, 0.0352,\n",
       "             0.0352, 0.0374, 0.0374, 0.0396, 0.0396, 0.0419, 0.0441, 0.0463, 0.0463,\n",
       "             0.0463, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485,\n",
       "             0.0485, 0.0507, 0.0507, 0.0507, 0.0529, 0.0551, 0.0551, 0.0551, 0.0573,\n",
       "             0.0573, 0.0573, 0.0595, 0.0617, 0.0617, 0.0639, 0.0661, 0.0683, 0.0683,\n",
       "             0.0705, 0.0705, 0.0705, 0.0727, 0.0727, 0.0727, 0.0749, 0.0749, 0.0771,\n",
       "             0.0793, 0.0815, 0.0815, 0.0815, 0.0837, 0.0837, 0.0859, 0.0859, 0.0881,\n",
       "             0.0903, 0.0925, 0.0925, 0.0947, 0.0969, 0.0991, 0.1013, 0.1013, 0.1035,\n",
       "             0.1057, 0.1079, 0.1101, 0.1123, 0.1145, 0.1167, 0.1189, 0.1211, 0.1233,\n",
       "             0.1256, 0.1256, 0.1256, 0.1278, 0.1278, 0.1300, 0.1322, 0.1344, 0.1366,\n",
       "             0.1388, 0.1410, 0.1432, 0.1454, 0.1454, 0.1476, 0.1498, 0.1520, 0.1542,\n",
       "             0.1564, 0.1586, 0.1608, 0.1630, 0.1652, 0.1674, 0.1696, 0.1718, 0.1740,\n",
       "             0.1762, 0.1784, 0.1806, 0.1828, 0.1850, 0.1872, 0.1894, 0.1916, 0.1938,\n",
       "             0.1960, 0.1982, 0.2004, 0.2026, 0.2048, 0.2048, 0.2070, 0.2093, 0.2115,\n",
       "             0.2137, 0.2159, 0.2181, 0.2203, 0.2225, 0.2225, 0.2247, 0.2269, 0.2291,\n",
       "             0.2313, 0.2335, 0.2357, 0.2379, 0.2401, 0.2423, 0.2445, 0.2467, 0.2489,\n",
       "             0.2511, 0.2511, 0.2511, 0.2533, 0.2555, 0.2577, 0.2599, 0.2621, 0.2643,\n",
       "             0.2665, 0.2687, 0.2709, 0.2731, 0.2753, 0.2775, 0.2797, 0.2819, 0.2841,\n",
       "             0.2863, 0.2885, 0.2885, 0.2907, 0.2930, 0.2952, 0.2974, 0.2996, 0.3018,\n",
       "             0.3040, 0.3062, 0.3084, 0.3106, 0.3106, 0.3128, 0.3150, 0.3172, 0.3194,\n",
       "             0.3216, 0.3238, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370, 0.3392,\n",
       "             0.3414, 0.3436, 0.3458, 0.3480, 0.3502, 0.3524, 0.3546, 0.3568, 0.3590,\n",
       "             0.3612, 0.3634, 0.3656, 0.3678, 0.3700, 0.3722, 0.3744, 0.3767, 0.3789,\n",
       "             0.3811, 0.3833, 0.3855, 0.3877, 0.3899, 0.3921, 0.3943, 0.3965, 0.3987,\n",
       "             0.4009, 0.4031, 0.4053, 0.4075, 0.4097, 0.4119, 0.4141, 0.4163, 0.4185,\n",
       "             0.4207, 0.4229, 0.4251, 0.4273, 0.4295, 0.4317, 0.4339, 0.4361, 0.4383,\n",
       "             0.4405, 0.4427, 0.4449, 0.4471, 0.4493, 0.4515, 0.4537, 0.4559, 0.4581,\n",
       "             0.4604, 0.4626, 0.4648, 0.4670, 0.4692, 0.4714, 0.4736, 0.4758, 0.4780,\n",
       "             0.4802, 0.4824, 0.4846, 0.4868, 0.4890, 0.4912, 0.4934, 0.4956, 0.4978,\n",
       "             0.5000, 0.5022, 0.5044, 0.5066, 0.5088, 0.5110, 0.5132, 0.5154, 0.5176,\n",
       "             0.5198, 0.5220, 0.5242, 0.5264, 0.5286, 0.5308, 0.5330, 0.5352, 0.5374,\n",
       "             0.5396, 0.5419, 0.5441, 0.5463, 0.5485, 0.5507, 0.5507, 0.5529, 0.5551,\n",
       "             0.5573, 0.5595, 0.5617, 0.5639, 0.5661, 0.5683, 0.5705, 0.5727, 0.5749,\n",
       "             0.5771, 0.5793, 0.5815, 0.5837, 0.5859, 0.5881, 0.5903, 0.5925, 0.5947,\n",
       "             0.5969, 0.5991, 0.6013, 0.6035, 0.6057, 0.6079, 0.6101, 0.6123, 0.6145,\n",
       "             0.6167, 0.6189, 0.6211, 0.6233, 0.6256, 0.6278, 0.6300, 0.6322, 0.6344,\n",
       "             0.6366, 0.6388, 0.6410, 0.6432, 0.6454, 0.6476, 0.6498, 0.6520, 0.6542,\n",
       "             0.6564, 0.6586, 0.6608, 0.6630, 0.6652, 0.6674, 0.6696, 0.6718, 0.6740,\n",
       "             0.6762, 0.6784, 0.6806, 0.6828, 0.6850, 0.6872, 0.6894, 0.6916, 0.6938,\n",
       "             0.6960, 0.6982, 0.7004, 0.7026, 0.7048, 0.7070, 0.7093, 0.7115, 0.7137,\n",
       "             0.7159, 0.7181, 0.7203, 0.7225, 0.7247, 0.7269, 0.7291, 0.7313, 0.7335,\n",
       "             0.7357, 0.7379, 0.7401, 0.7423, 0.7445, 0.7467, 0.7489, 0.7511, 0.7533,\n",
       "             0.7555, 0.7577, 0.7599, 0.7621, 0.7643, 0.7665, 0.7687, 0.7709, 0.7731,\n",
       "             0.7753, 0.7775, 0.7797, 0.7819, 0.7841, 0.7863, 0.7885, 0.7907, 0.7930,\n",
       "             0.7952, 0.7974, 0.7996, 0.8018, 0.8040, 0.8062, 0.8084, 0.8106, 0.8128,\n",
       "             0.8150, 0.8172, 0.8194, 0.8216, 0.8238, 0.8260, 0.8282, 0.8304, 0.8326,\n",
       "             0.8348, 0.8370, 0.8392, 0.8414, 0.8436, 0.8458, 0.8480, 0.8502, 0.8524,\n",
       "             0.8546, 0.8568, 0.8590, 0.8612, 0.8634, 0.8656, 0.8678, 0.8700, 0.8722,\n",
       "             0.8744, 0.8767, 0.8789, 0.8811, 0.8833, 0.8855, 0.8877, 0.8899, 0.8921,\n",
       "             0.8943, 0.8965, 0.8987, 0.9009, 0.9031, 0.9053, 0.9075, 0.9097, 0.9119,\n",
       "             0.9141, 0.9163, 0.9185, 0.9207, 0.9229, 0.9251, 0.9273, 0.9295, 0.9317,\n",
       "             0.9339, 0.9361, 0.9383, 0.9405, 0.9427, 0.9449, 0.9471, 0.9493, 0.9515,\n",
       "             0.9537, 0.9559, 0.9581, 0.9604, 0.9626, 0.9648, 0.9670, 0.9692, 0.9714,\n",
       "             0.9736, 0.9758, 0.9780, 0.9802, 0.9824, 0.9846, 0.9868, 0.9890, 0.9912,\n",
       "             0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.3637, 0.4533, 0.4958, 0.5212, 0.5382, 0.5521, 0.5629, 0.5737,\n",
       "             0.5815, 0.5900, 0.5954, 0.6015, 0.6108, 0.6162, 0.6208, 0.6247, 0.6270,\n",
       "             0.6324, 0.6363, 0.6386, 0.6440, 0.6463, 0.6486, 0.6525, 0.6541, 0.6595,\n",
       "             0.6625, 0.6633, 0.6656, 0.6672, 0.6710, 0.6718, 0.6749, 0.6780, 0.6811,\n",
       "             0.6819, 0.6826, 0.6857, 0.6873, 0.6888, 0.6903, 0.6911, 0.6934, 0.6942,\n",
       "             0.6950, 0.6988, 0.6996, 0.7012, 0.7019, 0.7050, 0.7066, 0.7081, 0.7089,\n",
       "             0.7104, 0.7112, 0.7120, 0.7135, 0.7143, 0.7151, 0.7166, 0.7174, 0.7181,\n",
       "             0.7189, 0.7197, 0.7220, 0.7236, 0.7251, 0.7274, 0.7290, 0.7297, 0.7305,\n",
       "             0.7313, 0.7320, 0.7328, 0.7359, 0.7375, 0.7382, 0.7390, 0.7398, 0.7405,\n",
       "             0.7413, 0.7421, 0.7436, 0.7444, 0.7452, 0.7459, 0.7475, 0.7498, 0.7506,\n",
       "             0.7514, 0.7521, 0.7529, 0.7537, 0.7552, 0.7552, 0.7568, 0.7575, 0.7583,\n",
       "             0.7591, 0.7598, 0.7606, 0.7614, 0.7622, 0.7637, 0.7645, 0.7653, 0.7668,\n",
       "             0.7676, 0.7683, 0.7691, 0.7699, 0.7714, 0.7722, 0.7730, 0.7737, 0.7745,\n",
       "             0.7753, 0.7761, 0.7776, 0.7784, 0.7792, 0.7799, 0.7807, 0.7815, 0.7822,\n",
       "             0.7830, 0.7838, 0.7846, 0.7853, 0.7861, 0.7869, 0.7876, 0.7884, 0.7892,\n",
       "             0.7900, 0.7907, 0.7915, 0.7923, 0.7931, 0.7938, 0.7946, 0.7954, 0.7961,\n",
       "             0.7969, 0.7977, 0.7985, 0.7992, 0.8000, 0.8008, 0.8015, 0.8023, 0.8031,\n",
       "             0.8039, 0.8046, 0.8054, 0.8062, 0.8069, 0.8077, 0.8085, 0.8093, 0.8100,\n",
       "             0.8108, 0.8116, 0.8124, 0.8131, 0.8139, 0.8147, 0.8154, 0.8162, 0.8170,\n",
       "             0.8178, 0.8185, 0.8193, 0.8201, 0.8208, 0.8216, 0.8224, 0.8232, 0.8239,\n",
       "             0.8247, 0.8255, 0.8263, 0.8270, 0.8278, 0.8286, 0.8293, 0.8301, 0.8309,\n",
       "             0.8317, 0.8324, 0.8332, 0.8340, 0.8347, 0.8355, 0.8363, 0.8371, 0.8378,\n",
       "             0.8386, 0.8394, 0.8402, 0.8409, 0.8417, 0.8425, 0.8432, 0.8440, 0.8448,\n",
       "             0.8456, 0.8463, 0.8471, 0.8486, 0.8494, 0.8502, 0.8510, 0.8517, 0.8525,\n",
       "             0.8533, 0.8541, 0.8548, 0.8556, 0.8564, 0.8571, 0.8579, 0.8587, 0.8595,\n",
       "             0.8602, 0.8610, 0.8610, 0.8618, 0.8625, 0.8633, 0.8641, 0.8649, 0.8656,\n",
       "             0.8664, 0.8664, 0.8672, 0.8680, 0.8687, 0.8695, 0.8703, 0.8710, 0.8718,\n",
       "             0.8726, 0.8734, 0.8741, 0.8749, 0.8757, 0.8764, 0.8772, 0.8780, 0.8788,\n",
       "             0.8795, 0.8803, 0.8811, 0.8819, 0.8826, 0.8834, 0.8842, 0.8849, 0.8857,\n",
       "             0.8865, 0.8873, 0.8880, 0.8888, 0.8896, 0.8903, 0.8911, 0.8919, 0.8927,\n",
       "             0.8934, 0.8942, 0.8950, 0.8958, 0.8965, 0.8973, 0.8981, 0.8988, 0.8996,\n",
       "             0.9004, 0.9012, 0.9019, 0.9027, 0.9035, 0.9042, 0.9050, 0.9058, 0.9066,\n",
       "             0.9073, 0.9081, 0.9089, 0.9097, 0.9104, 0.9112, 0.9120, 0.9127, 0.9135,\n",
       "             0.9143, 0.9151, 0.9158, 0.9166, 0.9174, 0.9181, 0.9189, 0.9197, 0.9205,\n",
       "             0.9212, 0.9220, 0.9228, 0.9236, 0.9243, 0.9251, 0.9259, 0.9266, 0.9274,\n",
       "             0.9282, 0.9290, 0.9297, 0.9305, 0.9313, 0.9320, 0.9328, 0.9336, 0.9344,\n",
       "             0.9351, 0.9359, 0.9367, 0.9375, 0.9382, 0.9390, 0.9398, 0.9405, 0.9413,\n",
       "             0.9421, 0.9429, 0.9436, 0.9444, 0.9452, 0.9459, 0.9467, 0.9475, 0.9483,\n",
       "             0.9490, 0.9498, 0.9506, 0.9514, 0.9514, 0.9521, 0.9529, 0.9537, 0.9544,\n",
       "             0.9552, 0.9552, 0.9552, 0.9560, 0.9568, 0.9575, 0.9575, 0.9583, 0.9583,\n",
       "             0.9591, 0.9591, 0.9598, 0.9606, 0.9614, 0.9614, 0.9622, 0.9629, 0.9629,\n",
       "             0.9637, 0.9645, 0.9653, 0.9653, 0.9653, 0.9660, 0.9668, 0.9668, 0.9668,\n",
       "             0.9676, 0.9676, 0.9683, 0.9683, 0.9691, 0.9691, 0.9691, 0.9691, 0.9699,\n",
       "             0.9707, 0.9707, 0.9714, 0.9722, 0.9730, 0.9737, 0.9745, 0.9753, 0.9761,\n",
       "             0.9768, 0.9768, 0.9776, 0.9784, 0.9784, 0.9784, 0.9792, 0.9799, 0.9799,\n",
       "             0.9807, 0.9815, 0.9815, 0.9815, 0.9822, 0.9822, 0.9822, 0.9822, 0.9830,\n",
       "             0.9830, 0.9838, 0.9846, 0.9846, 0.9853, 0.9861, 0.9861, 0.9869, 0.9869,\n",
       "             0.9869, 0.9869, 0.9876, 0.9884, 0.9884, 0.9892, 0.9892, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9907, 0.9907, 0.9907, 0.9907, 0.9907, 0.9915, 0.9915,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9915, 0.9923, 0.9931, 0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9938, 0.9938, 0.9938, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9967e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01, 9.9965e-01,\n",
       "             9.9963e-01, 9.9962e-01, 9.9961e-01, 9.9961e-01, 9.9957e-01, 9.9956e-01,\n",
       "             9.9952e-01, 9.9951e-01, 9.9949e-01, 9.9948e-01, 9.9947e-01, 9.9947e-01,\n",
       "             9.9947e-01, 9.9946e-01, 9.9945e-01, 9.9945e-01, 9.9944e-01, 9.9943e-01,\n",
       "             9.9942e-01, 9.9941e-01, 9.9939e-01, 9.9937e-01, 9.9935e-01, 9.9935e-01,\n",
       "             9.9932e-01, 9.9931e-01, 9.9929e-01, 9.9927e-01, 9.9922e-01, 9.9922e-01,\n",
       "             9.9918e-01, 9.9918e-01, 9.9914e-01, 9.9904e-01, 9.9901e-01, 9.9900e-01,\n",
       "             9.9897e-01, 9.9897e-01, 9.9897e-01, 9.9881e-01, 9.9878e-01, 9.9874e-01,\n",
       "             9.9868e-01, 9.9866e-01, 9.9865e-01, 9.9860e-01, 9.9860e-01, 9.9841e-01,\n",
       "             9.9839e-01, 9.9835e-01, 9.9831e-01, 9.9830e-01, 9.9825e-01, 9.9810e-01,\n",
       "             9.9808e-01, 9.9805e-01, 9.9801e-01, 9.9792e-01, 9.9791e-01, 9.9779e-01,\n",
       "             9.9766e-01, 9.9761e-01, 9.9759e-01, 9.9751e-01, 9.9750e-01, 9.9736e-01,\n",
       "             9.9729e-01, 9.9726e-01, 9.9700e-01, 9.9681e-01, 9.9672e-01, 9.9653e-01,\n",
       "             9.9639e-01, 9.9619e-01, 9.9588e-01, 9.9560e-01, 9.9521e-01, 9.9519e-01,\n",
       "             9.9517e-01, 9.9498e-01, 9.9494e-01, 9.9483e-01, 9.9458e-01, 9.9397e-01,\n",
       "             9.9393e-01, 9.9368e-01, 9.9356e-01, 9.9318e-01, 9.9296e-01, 9.9065e-01,\n",
       "             9.8984e-01, 9.8901e-01, 9.8808e-01, 9.8428e-01, 9.8369e-01, 9.8312e-01,\n",
       "             9.8277e-01, 9.8260e-01, 9.8173e-01, 9.8010e-01, 9.7678e-01, 9.7426e-01,\n",
       "             9.7389e-01, 9.7260e-01, 9.7084e-01, 9.6782e-01, 9.6601e-01, 9.6183e-01,\n",
       "             9.6064e-01, 9.5816e-01, 9.5647e-01, 9.5188e-01, 9.5020e-01, 9.4874e-01,\n",
       "             9.4454e-01, 9.4306e-01, 9.3961e-01, 9.3646e-01, 9.3614e-01, 9.3447e-01,\n",
       "             9.3149e-01, 9.2870e-01, 9.2103e-01, 9.0485e-01, 9.0380e-01, 8.8525e-01,\n",
       "             8.8433e-01, 8.7446e-01, 8.6384e-01, 8.6142e-01, 8.5581e-01, 8.4334e-01,\n",
       "             8.3349e-01, 8.3111e-01, 8.2978e-01, 8.1941e-01, 7.9901e-01, 7.7679e-01,\n",
       "             7.7668e-01, 7.6134e-01, 7.4852e-01, 7.3697e-01, 6.9466e-01, 6.7456e-01,\n",
       "             6.4520e-01, 6.4327e-01, 5.9313e-01, 5.8832e-01, 5.7927e-01, 5.6709e-01,\n",
       "             5.6593e-01, 5.5220e-01, 5.3089e-01, 5.2602e-01, 5.1966e-01, 5.1778e-01,\n",
       "             4.8608e-01, 4.7024e-01, 3.9890e-01, 3.5556e-01, 3.4476e-01, 2.8694e-01,\n",
       "             2.7811e-01, 2.6182e-01, 2.5790e-01, 2.5758e-01, 2.5574e-01, 2.5110e-01,\n",
       "             2.3185e-01, 1.8770e-01, 1.8229e-01, 1.2454e-01, 1.1727e-01, 1.1006e-01,\n",
       "             1.0708e-01, 9.6857e-02, 9.1014e-02, 8.1318e-02, 6.9818e-02, 6.2088e-02,\n",
       "             4.9199e-02, 4.8948e-02, 4.6101e-02, 4.2769e-02, 4.0884e-02, 3.4355e-02,\n",
       "             3.2755e-02, 3.2223e-02, 2.9965e-02, 2.5281e-02, 2.3740e-02, 2.2258e-02,\n",
       "             2.1708e-02, 2.1420e-02, 2.0596e-02, 1.2484e-02, 1.1406e-02, 1.1303e-02,\n",
       "             9.8372e-03, 9.0067e-03, 8.9711e-03, 8.7387e-03, 7.8949e-03, 7.8692e-03,\n",
       "             7.7317e-03, 6.4892e-03, 6.2025e-03, 5.2132e-03, 4.8164e-03, 4.6108e-03,\n",
       "             4.3749e-03, 3.7453e-03, 2.7845e-03, 2.7366e-03, 2.4469e-03, 2.1972e-03,\n",
       "             1.9583e-03, 1.6407e-03, 1.5903e-03, 1.5868e-03, 1.5828e-03, 1.5380e-03,\n",
       "             1.4914e-03, 1.2060e-03, 1.0828e-03, 1.0717e-03, 1.0089e-03, 7.6889e-04,\n",
       "             7.1631e-04, 6.9028e-04, 5.8981e-04, 5.7792e-04, 5.3528e-04, 4.3441e-04,\n",
       "             3.5649e-04, 3.5390e-04, 3.1376e-04, 2.8067e-04, 2.7442e-04, 2.7410e-04,\n",
       "             2.6483e-04, 2.6143e-04, 2.5925e-04, 2.5529e-04, 2.4851e-04, 2.3861e-04,\n",
       "             2.1900e-04, 1.9152e-04, 1.9054e-04, 1.8907e-04, 1.7919e-04, 1.7794e-04,\n",
       "             1.6439e-04, 1.4210e-04, 1.4026e-04, 1.3909e-04, 1.3593e-04, 1.1803e-04,\n",
       "             1.1615e-04, 1.1401e-04, 1.0432e-04, 1.0364e-04, 9.9496e-05, 9.8356e-05,\n",
       "             8.9802e-05, 8.8737e-05, 8.5623e-05, 8.2926e-05, 7.8381e-05, 7.7304e-05,\n",
       "             7.7262e-05, 7.5912e-05, 7.5180e-05, 7.4133e-05, 6.2287e-05, 6.1133e-05,\n",
       "             4.6002e-05, 4.5698e-05, 4.4918e-05, 4.0077e-05, 3.9275e-05, 3.6012e-05,\n",
       "             3.4434e-05, 3.4327e-05, 3.3084e-05, 3.2096e-05, 3.1947e-05, 3.1926e-05,\n",
       "             3.1468e-05, 3.1040e-05, 2.7082e-05, 2.3842e-05, 2.2828e-05, 1.9500e-05,\n",
       "             1.9382e-05, 1.7099e-05, 1.6400e-05, 1.3798e-05, 1.3091e-05, 1.2656e-05,\n",
       "             1.1622e-05, 1.1592e-05, 1.1218e-05, 1.0981e-05, 9.9231e-06, 8.7469e-06,\n",
       "             8.5989e-06, 8.1605e-06, 7.7585e-06, 7.7223e-06, 7.7075e-06, 7.6457e-06,\n",
       "             7.0806e-06, 6.2225e-06, 6.1602e-06, 5.7820e-06, 5.6132e-06, 5.5507e-06,\n",
       "             5.2058e-06, 5.0281e-06, 4.8243e-06, 4.7263e-06, 4.4940e-06, 4.2256e-06,\n",
       "             3.7543e-06, 3.7517e-06, 3.5045e-06, 3.4954e-06, 3.3094e-06, 3.2926e-06,\n",
       "             3.1279e-06, 2.9781e-06, 2.9080e-06, 2.9067e-06, 2.8162e-06, 2.7088e-06,\n",
       "             2.6309e-06, 2.5013e-06, 2.3384e-06, 2.3076e-06, 2.1942e-06, 2.1657e-06,\n",
       "             1.9805e-06, 1.7301e-06, 1.7205e-06, 1.7115e-06, 1.6374e-06, 1.5730e-06,\n",
       "             1.4797e-06, 1.3342e-06, 1.2565e-06, 1.2340e-06, 1.1886e-06, 1.1841e-06,\n",
       "             1.1667e-06, 1.1343e-06, 1.1086e-06, 9.9101e-07, 8.6378e-07, 8.5649e-07,\n",
       "             8.0112e-07, 7.8067e-07, 7.7996e-07, 7.7744e-07, 7.3620e-07, 7.1731e-07,\n",
       "             7.1648e-07, 7.0271e-07, 6.3939e-07, 6.3548e-07, 6.0589e-07, 5.8148e-07,\n",
       "             5.6292e-07, 5.2607e-07, 5.1121e-07, 4.8751e-07, 4.7952e-07, 4.4607e-07,\n",
       "             4.3932e-07, 4.2737e-07, 4.1971e-07, 4.0898e-07, 4.0018e-07, 3.9400e-07,\n",
       "             3.9097e-07, 3.6842e-07, 3.6604e-07, 3.2054e-07, 3.1323e-07, 3.1297e-07,\n",
       "             2.8286e-07, 2.7711e-07, 2.7338e-07, 2.7237e-07, 2.6475e-07, 2.6195e-07,\n",
       "             2.5078e-07, 2.2113e-07, 2.1806e-07, 2.0016e-07, 1.9559e-07, 1.9489e-07,\n",
       "             1.9327e-07, 1.8896e-07, 1.8896e-07, 1.8796e-07, 1.7401e-07, 1.7357e-07,\n",
       "             1.7264e-07, 1.7012e-07, 1.6720e-07, 1.4981e-07, 1.4823e-07, 1.4285e-07,\n",
       "             1.4019e-07, 1.3821e-07, 1.3090e-07, 1.1496e-07, 1.1063e-07, 1.0808e-07,\n",
       "             1.0712e-07, 1.0230e-07, 9.9900e-08, 9.8779e-08, 9.7851e-08, 9.1831e-08,\n",
       "             9.1031e-08, 8.6071e-08, 8.4198e-08, 8.3896e-08, 7.7442e-08, 7.0580e-08,\n",
       "             6.8244e-08, 5.8282e-08, 5.7976e-08, 5.2468e-08, 5.1997e-08, 4.7325e-08,\n",
       "             4.6969e-08, 4.0532e-08, 4.0248e-08, 4.0203e-08, 3.9327e-08, 3.8544e-08,\n",
       "             3.7035e-08, 3.6289e-08, 3.2961e-08, 3.2451e-08, 3.0818e-08, 2.9322e-08,\n",
       "             2.7176e-08, 2.6985e-08, 2.6862e-08, 2.4554e-08, 2.1965e-08, 2.0877e-08,\n",
       "             2.0551e-08, 2.0374e-08, 1.9107e-08, 1.9086e-08, 1.8308e-08, 1.6822e-08,\n",
       "             1.5664e-08, 1.5364e-08, 1.3903e-08, 1.3450e-08, 1.1673e-08, 1.0710e-08,\n",
       "             1.0662e-08, 1.0552e-08, 1.0333e-08, 1.0271e-08, 1.0117e-08, 1.0080e-08,\n",
       "             9.7102e-09, 9.6534e-09, 9.5151e-09, 9.4659e-09, 8.5842e-09, 8.5258e-09,\n",
       "             8.2438e-09, 7.9301e-09, 7.4604e-09, 6.6576e-09, 5.6198e-09, 5.2292e-09,\n",
       "             5.0690e-09, 4.8749e-09, 4.7401e-09, 4.5248e-09, 3.9994e-09, 3.3109e-09,\n",
       "             3.1769e-09, 3.1304e-09, 3.0463e-09, 2.8668e-09, 2.8365e-09, 2.6215e-09,\n",
       "             2.4806e-09, 1.9940e-09, 1.9790e-09, 1.9591e-09, 1.9347e-09, 1.9345e-09,\n",
       "             1.6969e-09, 1.6810e-09, 1.5991e-09, 1.5959e-09, 1.4569e-09, 1.4086e-09,\n",
       "             1.3675e-09, 1.3634e-09, 1.2714e-09, 1.2482e-09, 1.2085e-09, 1.1576e-09,\n",
       "             1.0421e-09, 9.7988e-10, 8.7114e-10, 7.7938e-10, 7.7770e-10, 7.3289e-10,\n",
       "             7.0883e-10, 6.6212e-10, 6.5981e-10, 5.9050e-10, 5.6972e-10, 5.6453e-10,\n",
       "             5.6260e-10, 5.4564e-10, 5.3651e-10, 5.1065e-10, 4.8693e-10, 4.5220e-10,\n",
       "             3.9503e-10, 3.9038e-10, 2.7036e-10, 2.5454e-10, 2.4420e-10, 2.2263e-10,\n",
       "             2.1220e-10, 2.0369e-10, 2.0321e-10, 1.9200e-10, 1.8866e-10, 1.6051e-10,\n",
       "             1.5919e-10, 1.5794e-10, 1.3001e-10, 1.1455e-10, 1.1267e-10, 1.1029e-10,\n",
       "             1.0557e-10, 5.9618e-11, 5.8028e-11, 5.3944e-11, 4.3215e-11, 4.2209e-11,\n",
       "             3.1389e-11, 3.1164e-11, 2.6390e-11, 2.4870e-11, 2.3936e-11, 1.9352e-11,\n",
       "             1.8856e-11, 1.6753e-11, 1.5077e-11, 1.4752e-11, 1.1608e-11, 1.1351e-11,\n",
       "             1.0927e-11, 1.0905e-11, 9.3676e-12, 9.2451e-12, 8.8024e-12, 8.4665e-12,\n",
       "             8.3124e-12, 4.3086e-12, 4.1089e-12, 3.8015e-12, 3.7813e-12, 3.7216e-12,\n",
       "             3.2244e-12, 2.7648e-12, 2.5251e-12, 2.5035e-12, 1.8018e-12, 1.0070e-12,\n",
       "             9.7155e-13, 7.9370e-13, 7.5427e-13, 7.2912e-13, 4.6881e-13, 4.4968e-13,\n",
       "             3.2391e-13, 2.7710e-13, 1.8248e-13, 1.0244e-13, 5.1779e-14, 2.8097e-14,\n",
       "             2.7654e-14, 2.6327e-14, 1.7900e-14, 1.4268e-14, 1.0493e-14, 1.5789e-16,\n",
       "             4.5236e-18])}},\n",
       "   {'fpr': np.float64(0.07268722466960352),\n",
       "    'tpr': np.float64(0.9837837837837838),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0110, 0.0110,\n",
       "             0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
       "             0.0110, 0.0110, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0154, 0.0154, 0.0154, 0.0154,\n",
       "             0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
       "             0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
       "             0.0154, 0.0176, 0.0176, 0.0176, 0.0176, 0.0198, 0.0198, 0.0198, 0.0198,\n",
       "             0.0198, 0.0198, 0.0198, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220,\n",
       "             0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220,\n",
       "             0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220,\n",
       "             0.0220, 0.0242, 0.0264, 0.0264, 0.0264, 0.0286, 0.0286, 0.0286, 0.0286,\n",
       "             0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0308, 0.0330,\n",
       "             0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330,\n",
       "             0.0330, 0.0352, 0.0374, 0.0396, 0.0419, 0.0441, 0.0441, 0.0441, 0.0441,\n",
       "             0.0463, 0.0485, 0.0485, 0.0485, 0.0507, 0.0529, 0.0529, 0.0529, 0.0551,\n",
       "             0.0551, 0.0573, 0.0573, 0.0573, 0.0573, 0.0595, 0.0595, 0.0595, 0.0617,\n",
       "             0.0617, 0.0639, 0.0639, 0.0639, 0.0639, 0.0639, 0.0639, 0.0639, 0.0661,\n",
       "             0.0683, 0.0705, 0.0705, 0.0727, 0.0749, 0.0749, 0.0749, 0.0771, 0.0793,\n",
       "             0.0793, 0.0793, 0.0793, 0.0815, 0.0815, 0.0815, 0.0837, 0.0837, 0.0859,\n",
       "             0.0881, 0.0903, 0.0925, 0.0925, 0.0947, 0.0947, 0.0969, 0.0969, 0.0991,\n",
       "             0.1013, 0.1013, 0.1035, 0.1057, 0.1079, 0.1101, 0.1123, 0.1145, 0.1167,\n",
       "             0.1189, 0.1211, 0.1233, 0.1256, 0.1278, 0.1300, 0.1322, 0.1344, 0.1366,\n",
       "             0.1388, 0.1410, 0.1432, 0.1454, 0.1454, 0.1454, 0.1476, 0.1498, 0.1498,\n",
       "             0.1498, 0.1520, 0.1542, 0.1564, 0.1586, 0.1608, 0.1630, 0.1652, 0.1674,\n",
       "             0.1696, 0.1718, 0.1740, 0.1762, 0.1784, 0.1806, 0.1828, 0.1850, 0.1872,\n",
       "             0.1894, 0.1894, 0.1916, 0.1938, 0.1960, 0.1982, 0.2004, 0.2026, 0.2048,\n",
       "             0.2070, 0.2093, 0.2115, 0.2137, 0.2159, 0.2181, 0.2203, 0.2225, 0.2247,\n",
       "             0.2269, 0.2291, 0.2313, 0.2335, 0.2357, 0.2379, 0.2401, 0.2423, 0.2445,\n",
       "             0.2467, 0.2489, 0.2511, 0.2533, 0.2533, 0.2555, 0.2577, 0.2599, 0.2621,\n",
       "             0.2643, 0.2665, 0.2687, 0.2709, 0.2731, 0.2753, 0.2775, 0.2797, 0.2819,\n",
       "             0.2841, 0.2863, 0.2863, 0.2885, 0.2907, 0.2930, 0.2952, 0.2974, 0.2996,\n",
       "             0.2996, 0.3018, 0.3040, 0.3062, 0.3084, 0.3106, 0.3128, 0.3150, 0.3172,\n",
       "             0.3194, 0.3216, 0.3238, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370,\n",
       "             0.3392, 0.3414, 0.3436, 0.3458, 0.3480, 0.3502, 0.3524, 0.3546, 0.3568,\n",
       "             0.3590, 0.3612, 0.3634, 0.3656, 0.3678, 0.3700, 0.3722, 0.3744, 0.3767,\n",
       "             0.3789, 0.3811, 0.3833, 0.3855, 0.3877, 0.3899, 0.3921, 0.3943, 0.3965,\n",
       "             0.3987, 0.4009, 0.4031, 0.4053, 0.4075, 0.4097, 0.4119, 0.4141, 0.4163,\n",
       "             0.4185, 0.4207, 0.4229, 0.4251, 0.4273, 0.4295, 0.4317, 0.4339, 0.4361,\n",
       "             0.4383, 0.4405, 0.4427, 0.4449, 0.4471, 0.4493, 0.4515, 0.4537, 0.4559,\n",
       "             0.4581, 0.4604, 0.4626, 0.4648, 0.4670, 0.4692, 0.4714, 0.4736, 0.4758,\n",
       "             0.4780, 0.4802, 0.4824, 0.4846, 0.4868, 0.4890, 0.4912, 0.4934, 0.4956,\n",
       "             0.4978, 0.5000, 0.5022, 0.5044, 0.5066, 0.5088, 0.5088, 0.5110, 0.5132,\n",
       "             0.5154, 0.5176, 0.5198, 0.5220, 0.5242, 0.5264, 0.5286, 0.5308, 0.5330,\n",
       "             0.5352, 0.5374, 0.5396, 0.5419, 0.5441, 0.5463, 0.5485, 0.5507, 0.5529,\n",
       "             0.5551, 0.5573, 0.5595, 0.5617, 0.5639, 0.5661, 0.5683, 0.5705, 0.5727,\n",
       "             0.5749, 0.5771, 0.5793, 0.5815, 0.5837, 0.5859, 0.5881, 0.5903, 0.5925,\n",
       "             0.5947, 0.5969, 0.5991, 0.6013, 0.6035, 0.6057, 0.6079, 0.6101, 0.6123,\n",
       "             0.6145, 0.6167, 0.6189, 0.6211, 0.6233, 0.6256, 0.6278, 0.6300, 0.6322,\n",
       "             0.6344, 0.6366, 0.6388, 0.6410, 0.6432, 0.6454, 0.6476, 0.6498, 0.6520,\n",
       "             0.6542, 0.6564, 0.6586, 0.6608, 0.6630, 0.6652, 0.6674, 0.6696, 0.6718,\n",
       "             0.6740, 0.6762, 0.6784, 0.6806, 0.6828, 0.6850, 0.6872, 0.6894, 0.6916,\n",
       "             0.6938, 0.6960, 0.6982, 0.7004, 0.7026, 0.7048, 0.7070, 0.7093, 0.7115,\n",
       "             0.7137, 0.7159, 0.7181, 0.7203, 0.7225, 0.7247, 0.7269, 0.7291, 0.7313,\n",
       "             0.7335, 0.7357, 0.7379, 0.7401, 0.7423, 0.7445, 0.7467, 0.7489, 0.7511,\n",
       "             0.7533, 0.7555, 0.7577, 0.7599, 0.7621, 0.7643, 0.7665, 0.7687, 0.7709,\n",
       "             0.7731, 0.7753, 0.7775, 0.7797, 0.7819, 0.7841, 0.7863, 0.7885, 0.7907,\n",
       "             0.7930, 0.7952, 0.7974, 0.7996, 0.8018, 0.8040, 0.8062, 0.8084, 0.8106,\n",
       "             0.8128, 0.8150, 0.8172, 0.8194, 0.8216, 0.8238, 0.8260, 0.8282, 0.8304,\n",
       "             0.8326, 0.8348, 0.8370, 0.8392, 0.8414, 0.8436, 0.8458, 0.8480, 0.8502,\n",
       "             0.8524, 0.8546, 0.8568, 0.8590, 0.8612, 0.8634, 0.8656, 0.8678, 0.8700,\n",
       "             0.8722, 0.8744, 0.8767, 0.8789, 0.8811, 0.8833, 0.8855, 0.8877, 0.8899,\n",
       "             0.8921, 0.8943, 0.8965, 0.8987, 0.9009, 0.9031, 0.9053, 0.9075, 0.9097,\n",
       "             0.9119, 0.9141, 0.9163, 0.9185, 0.9207, 0.9229, 0.9251, 0.9273, 0.9295,\n",
       "             0.9317, 0.9339, 0.9361, 0.9383, 0.9405, 0.9427, 0.9449, 0.9471, 0.9493,\n",
       "             0.9515, 0.9537, 0.9559, 0.9581, 0.9604, 0.9626, 0.9648, 0.9670, 0.9692,\n",
       "             0.9714, 0.9736, 0.9758, 0.9780, 0.9802, 0.9824, 0.9846, 0.9868, 0.9890,\n",
       "             0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.3946, 0.4695, 0.5058, 0.5313, 0.5506, 0.5668, 0.5853, 0.5938,\n",
       "             0.6046, 0.6131, 0.6193, 0.6317, 0.6324, 0.6355, 0.6432, 0.6517, 0.6541,\n",
       "             0.6579, 0.6610, 0.6649, 0.6687, 0.6734, 0.6795, 0.6819, 0.6834, 0.6873,\n",
       "             0.6911, 0.6927, 0.6958, 0.6973, 0.6996, 0.7019, 0.7042, 0.7050, 0.7081,\n",
       "             0.7104, 0.7135, 0.7143, 0.7158, 0.7181, 0.7205, 0.7236, 0.7259, 0.7274,\n",
       "             0.7290, 0.7297, 0.7320, 0.7344, 0.7359, 0.7367, 0.7375, 0.7398, 0.7421,\n",
       "             0.7429, 0.7444, 0.7459, 0.7475, 0.7483, 0.7498, 0.7521, 0.7537, 0.7544,\n",
       "             0.7552, 0.7560, 0.7568, 0.7583, 0.7591, 0.7598, 0.7614, 0.7629, 0.7645,\n",
       "             0.7660, 0.7668, 0.7683, 0.7699, 0.7722, 0.7745, 0.7753, 0.7768, 0.7776,\n",
       "             0.7784, 0.7792, 0.7799, 0.7815, 0.7822, 0.7830, 0.7838, 0.7846, 0.7869,\n",
       "             0.7876, 0.7892, 0.7900, 0.7907, 0.7915, 0.7923, 0.7938, 0.7946, 0.7954,\n",
       "             0.7961, 0.7969, 0.7977, 0.7985, 0.8000, 0.8008, 0.8015, 0.8023, 0.8031,\n",
       "             0.8039, 0.8046, 0.8054, 0.8069, 0.8077, 0.8085, 0.8093, 0.8100, 0.8108,\n",
       "             0.8116, 0.8124, 0.8131, 0.8139, 0.8147, 0.8154, 0.8162, 0.8170, 0.8178,\n",
       "             0.8185, 0.8201, 0.8208, 0.8216, 0.8224, 0.8232, 0.8239, 0.8247, 0.8255,\n",
       "             0.8263, 0.8270, 0.8278, 0.8286, 0.8293, 0.8309, 0.8317, 0.8324, 0.8332,\n",
       "             0.8347, 0.8355, 0.8363, 0.8371, 0.8378, 0.8386, 0.8394, 0.8402, 0.8409,\n",
       "             0.8417, 0.8425, 0.8432, 0.8440, 0.8448, 0.8456, 0.8463, 0.8471, 0.8479,\n",
       "             0.8486, 0.8494, 0.8502, 0.8510, 0.8517, 0.8525, 0.8533, 0.8533, 0.8548,\n",
       "             0.8564, 0.8571, 0.8579, 0.8587, 0.8595, 0.8602, 0.8610, 0.8618, 0.8625,\n",
       "             0.8633, 0.8641, 0.8641, 0.8649, 0.8656, 0.8664, 0.8672, 0.8680, 0.8687,\n",
       "             0.8695, 0.8703, 0.8710, 0.8718, 0.8726, 0.8734, 0.8741, 0.8749, 0.8757,\n",
       "             0.8764, 0.8772, 0.8780, 0.8788, 0.8795, 0.8803, 0.8811, 0.8819, 0.8826,\n",
       "             0.8834, 0.8842, 0.8849, 0.8857, 0.8865, 0.8873, 0.8880, 0.8888, 0.8896,\n",
       "             0.8903, 0.8911, 0.8919, 0.8927, 0.8934, 0.8942, 0.8950, 0.8958, 0.8965,\n",
       "             0.8973, 0.8981, 0.8988, 0.8996, 0.9004, 0.9012, 0.9019, 0.9027, 0.9035,\n",
       "             0.9042, 0.9050, 0.9058, 0.9066, 0.9073, 0.9073, 0.9081, 0.9089, 0.9097,\n",
       "             0.9104, 0.9112, 0.9120, 0.9127, 0.9135, 0.9143, 0.9151, 0.9158, 0.9166,\n",
       "             0.9174, 0.9181, 0.9189, 0.9197, 0.9205, 0.9212, 0.9220, 0.9228, 0.9236,\n",
       "             0.9243, 0.9243, 0.9251, 0.9259, 0.9266, 0.9266, 0.9274, 0.9282, 0.9290,\n",
       "             0.9297, 0.9305, 0.9313, 0.9313, 0.9320, 0.9328, 0.9336, 0.9344, 0.9351,\n",
       "             0.9359, 0.9367, 0.9375, 0.9382, 0.9390, 0.9398, 0.9405, 0.9413, 0.9421,\n",
       "             0.9429, 0.9436, 0.9444, 0.9452, 0.9459, 0.9467, 0.9475, 0.9483, 0.9490,\n",
       "             0.9498, 0.9498, 0.9498, 0.9506, 0.9521, 0.9521, 0.9529, 0.9537, 0.9544,\n",
       "             0.9552, 0.9560, 0.9568, 0.9575, 0.9583, 0.9591, 0.9598, 0.9598, 0.9598,\n",
       "             0.9606, 0.9614, 0.9622, 0.9629, 0.9637, 0.9645, 0.9653, 0.9660, 0.9668,\n",
       "             0.9676, 0.9676, 0.9676, 0.9676, 0.9676, 0.9676, 0.9683, 0.9691, 0.9699,\n",
       "             0.9699, 0.9699, 0.9707, 0.9714, 0.9714, 0.9714, 0.9722, 0.9730, 0.9730,\n",
       "             0.9737, 0.9737, 0.9745, 0.9753, 0.9761, 0.9761, 0.9768, 0.9776, 0.9776,\n",
       "             0.9784, 0.9784, 0.9792, 0.9799, 0.9807, 0.9815, 0.9822, 0.9830, 0.9830,\n",
       "             0.9830, 0.9830, 0.9838, 0.9838, 0.9838, 0.9846, 0.9853, 0.9853, 0.9853,\n",
       "             0.9861, 0.9869, 0.9876, 0.9876, 0.9884, 0.9892, 0.9892, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9907, 0.9907, 0.9915, 0.9915, 0.9923, 0.9923,\n",
       "             0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9938, 0.9946, 0.9946, 0.9946, 0.9954,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9971e-01, 9.9970e-01,\n",
       "             9.9968e-01, 9.9965e-01, 9.9964e-01, 9.9962e-01, 9.9962e-01, 9.9962e-01,\n",
       "             9.9960e-01, 9.9960e-01, 9.9957e-01, 9.9957e-01, 9.9955e-01, 9.9953e-01,\n",
       "             9.9952e-01, 9.9951e-01, 9.9950e-01, 9.9950e-01, 9.9949e-01, 9.9949e-01,\n",
       "             9.9945e-01, 9.9941e-01, 9.9941e-01, 9.9939e-01, 9.9939e-01, 9.9939e-01,\n",
       "             9.9938e-01, 9.9936e-01, 9.9935e-01, 9.9930e-01, 9.9926e-01, 9.9921e-01,\n",
       "             9.9916e-01, 9.9909e-01, 9.9907e-01, 9.9901e-01, 9.9898e-01, 9.9895e-01,\n",
       "             9.9884e-01, 9.9883e-01, 9.9881e-01, 9.9874e-01, 9.9873e-01, 9.9866e-01,\n",
       "             9.9864e-01, 9.9864e-01, 9.9859e-01, 9.9859e-01, 9.9844e-01, 9.9824e-01,\n",
       "             9.9798e-01, 9.9797e-01, 9.9792e-01, 9.9785e-01, 9.9778e-01, 9.9777e-01,\n",
       "             9.9776e-01, 9.9758e-01, 9.9751e-01, 9.9732e-01, 9.9728e-01, 9.9724e-01,\n",
       "             9.9712e-01, 9.9706e-01, 9.9683e-01, 9.9677e-01, 9.9671e-01, 9.9661e-01,\n",
       "             9.9651e-01, 9.9639e-01, 9.9622e-01, 9.9599e-01, 9.9525e-01, 9.9507e-01,\n",
       "             9.9499e-01, 9.9424e-01, 9.9415e-01, 9.9338e-01, 9.9330e-01, 9.9306e-01,\n",
       "             9.9273e-01, 9.9271e-01, 9.9262e-01, 9.9141e-01, 9.9089e-01, 9.8714e-01,\n",
       "             9.8706e-01, 9.8654e-01, 9.8595e-01, 9.8542e-01, 9.8465e-01, 9.8295e-01,\n",
       "             9.8152e-01, 9.7798e-01, 9.7569e-01, 9.7549e-01, 9.7263e-01, 9.7119e-01,\n",
       "             9.6982e-01, 9.6919e-01, 9.6809e-01, 9.6006e-01, 9.5492e-01, 9.4625e-01,\n",
       "             9.4470e-01, 9.4351e-01, 9.4299e-01, 9.4003e-01, 9.3259e-01, 9.3226e-01,\n",
       "             9.2937e-01, 9.2553e-01, 9.2154e-01, 9.2061e-01, 9.1619e-01, 8.9499e-01,\n",
       "             8.9387e-01, 8.9006e-01, 8.8992e-01, 8.6939e-01, 8.6854e-01, 8.6176e-01,\n",
       "             8.2695e-01, 8.0177e-01, 7.5527e-01, 7.2968e-01, 7.0691e-01, 6.9444e-01,\n",
       "             6.8411e-01, 6.5419e-01, 6.4888e-01, 6.3114e-01, 6.2796e-01, 6.2553e-01,\n",
       "             5.9320e-01, 5.7110e-01, 5.6107e-01, 5.0129e-01, 4.7097e-01, 4.6729e-01,\n",
       "             4.5444e-01, 4.3365e-01, 4.3007e-01, 3.9214e-01, 3.4939e-01, 3.3581e-01,\n",
       "             3.3510e-01, 3.2636e-01, 3.2336e-01, 3.0708e-01, 2.7548e-01, 2.5645e-01,\n",
       "             2.3468e-01, 2.2514e-01, 2.0942e-01, 1.9018e-01, 1.2974e-01, 1.1388e-01,\n",
       "             1.0038e-01, 9.9444e-02, 8.6531e-02, 8.5235e-02, 8.0909e-02, 7.7672e-02,\n",
       "             7.3939e-02, 7.3068e-02, 5.8916e-02, 5.1333e-02, 5.1000e-02, 4.2838e-02,\n",
       "             4.2750e-02, 3.6512e-02, 3.3392e-02, 3.0999e-02, 3.0094e-02, 2.7121e-02,\n",
       "             2.0072e-02, 1.8646e-02, 1.7574e-02, 1.6674e-02, 1.6295e-02, 1.5132e-02,\n",
       "             1.2677e-02, 1.1691e-02, 1.1448e-02, 1.1330e-02, 1.0241e-02, 8.6679e-03,\n",
       "             7.7469e-03, 6.6124e-03, 6.3822e-03, 5.9413e-03, 4.2898e-03, 3.9538e-03,\n",
       "             3.9347e-03, 3.5947e-03, 3.1436e-03, 3.1320e-03, 2.9914e-03, 2.7977e-03,\n",
       "             2.2609e-03, 2.0281e-03, 2.0107e-03, 1.8617e-03, 1.8135e-03, 1.5407e-03,\n",
       "             1.3376e-03, 1.2187e-03, 1.1178e-03, 1.0349e-03, 9.9839e-04, 9.8328e-04,\n",
       "             9.6770e-04, 8.1168e-04, 7.9341e-04, 6.5311e-04, 6.0338e-04, 5.8488e-04,\n",
       "             5.6750e-04, 5.4774e-04, 5.4389e-04, 5.2512e-04, 5.0878e-04, 4.6612e-04,\n",
       "             4.1592e-04, 4.1454e-04, 3.9389e-04, 3.7107e-04, 3.3281e-04, 3.1445e-04,\n",
       "             2.9550e-04, 2.8839e-04, 2.5485e-04, 2.5059e-04, 2.3843e-04, 2.3604e-04,\n",
       "             2.0481e-04, 2.0149e-04, 1.8995e-04, 1.6768e-04, 1.4906e-04, 1.3720e-04,\n",
       "             1.3382e-04, 1.3272e-04, 1.3229e-04, 1.0904e-04, 8.3482e-05, 7.9169e-05,\n",
       "             7.8629e-05, 7.3829e-05, 7.0764e-05, 7.0530e-05, 6.9127e-05, 6.8712e-05,\n",
       "             6.8652e-05, 6.6372e-05, 6.1561e-05, 6.1115e-05, 5.0842e-05, 4.9946e-05,\n",
       "             3.9138e-05, 3.8648e-05, 3.5027e-05, 3.0246e-05, 2.9622e-05, 2.7743e-05,\n",
       "             2.5241e-05, 2.4235e-05, 2.3579e-05, 2.3105e-05, 2.2807e-05, 2.1414e-05,\n",
       "             2.0272e-05, 2.0119e-05, 1.6865e-05, 1.6630e-05, 1.6055e-05, 1.5272e-05,\n",
       "             1.4711e-05, 1.4231e-05, 1.3987e-05, 1.3941e-05, 1.1755e-05, 1.1344e-05,\n",
       "             1.1131e-05, 1.0511e-05, 1.0255e-05, 9.9607e-06, 9.1035e-06, 9.0921e-06,\n",
       "             8.5377e-06, 7.8195e-06, 7.7132e-06, 6.9951e-06, 6.3832e-06, 6.0154e-06,\n",
       "             5.8659e-06, 5.1333e-06, 4.5416e-06, 4.3223e-06, 4.1190e-06, 4.0500e-06,\n",
       "             3.4324e-06, 3.4051e-06, 3.3215e-06, 3.2921e-06, 3.1538e-06, 3.0134e-06,\n",
       "             2.8777e-06, 2.7518e-06, 2.6960e-06, 2.5501e-06, 2.4811e-06, 2.2924e-06,\n",
       "             2.1234e-06, 1.8458e-06, 1.8260e-06, 1.6929e-06, 1.6246e-06, 1.5063e-06,\n",
       "             1.4490e-06, 1.3630e-06, 1.3126e-06, 1.2821e-06, 1.2343e-06, 1.2331e-06,\n",
       "             1.1903e-06, 1.1767e-06, 1.1661e-06, 1.1482e-06, 1.0230e-06, 1.0089e-06,\n",
       "             9.9326e-07, 9.4330e-07, 9.0401e-07, 8.6497e-07, 8.2981e-07, 7.8939e-07,\n",
       "             7.7278e-07, 7.5475e-07, 7.0765e-07, 6.6899e-07, 6.1516e-07, 5.9465e-07,\n",
       "             5.9274e-07, 5.6137e-07, 5.3682e-07, 5.3264e-07, 5.1708e-07, 4.9812e-07,\n",
       "             4.8487e-07, 4.6300e-07, 4.5566e-07, 4.4440e-07, 4.2469e-07, 4.1697e-07,\n",
       "             4.1070e-07, 3.9313e-07, 3.9128e-07, 3.7421e-07, 3.5588e-07, 3.2729e-07,\n",
       "             2.9502e-07, 2.7084e-07, 2.6973e-07, 2.6937e-07, 2.5196e-07, 2.0743e-07,\n",
       "             1.9923e-07, 1.9675e-07, 1.9030e-07, 1.8933e-07, 1.8664e-07, 1.8351e-07,\n",
       "             1.7199e-07, 1.6807e-07, 1.6364e-07, 1.5495e-07, 1.4759e-07, 1.4172e-07,\n",
       "             1.3956e-07, 1.3948e-07, 1.3907e-07, 1.2612e-07, 1.1662e-07, 1.0939e-07,\n",
       "             1.0613e-07, 1.0575e-07, 9.8086e-08, 9.6126e-08, 7.6262e-08, 7.5717e-08,\n",
       "             6.7068e-08, 6.6725e-08, 5.8858e-08, 5.5180e-08, 5.4096e-08, 5.0313e-08,\n",
       "             4.9543e-08, 4.8767e-08, 4.7300e-08, 4.6633e-08, 4.4085e-08, 4.3951e-08,\n",
       "             4.1104e-08, 3.8079e-08, 3.6250e-08, 3.5015e-08, 3.4672e-08, 3.0190e-08,\n",
       "             2.8683e-08, 2.8472e-08, 2.7900e-08, 2.7722e-08, 2.7712e-08, 2.5447e-08,\n",
       "             2.5164e-08, 2.2819e-08, 2.0595e-08, 2.0506e-08, 2.0206e-08, 2.0041e-08,\n",
       "             1.9447e-08, 1.9326e-08, 1.8106e-08, 1.7600e-08, 1.7413e-08, 1.6864e-08,\n",
       "             1.3924e-08, 1.3865e-08, 1.1894e-08, 1.1883e-08, 1.1730e-08, 1.1652e-08,\n",
       "             1.1240e-08, 1.0814e-08, 1.0332e-08, 9.6675e-09, 9.6174e-09, 9.1623e-09,\n",
       "             8.9024e-09, 8.0645e-09, 7.9668e-09, 7.7695e-09, 6.6051e-09, 6.5958e-09,\n",
       "             6.4809e-09, 6.4802e-09, 6.4121e-09, 6.2654e-09, 5.9582e-09, 4.6973e-09,\n",
       "             4.6031e-09, 3.9106e-09, 3.8499e-09, 3.4203e-09, 3.2951e-09, 3.1972e-09,\n",
       "             2.9321e-09, 2.8282e-09, 2.6979e-09, 2.6877e-09, 2.5337e-09, 2.3959e-09,\n",
       "             1.8296e-09, 1.7941e-09, 1.7205e-09, 1.7135e-09, 1.4938e-09, 1.4552e-09,\n",
       "             1.4302e-09, 1.3875e-09, 1.2805e-09, 1.2702e-09, 1.1402e-09, 1.0375e-09,\n",
       "             9.0935e-10, 8.9179e-10, 8.7056e-10, 8.2452e-10, 8.0046e-10, 7.3211e-10,\n",
       "             7.2663e-10, 7.2269e-10, 6.2201e-10, 5.8163e-10, 4.9300e-10, 4.8611e-10,\n",
       "             4.7683e-10, 4.6435e-10, 4.3192e-10, 4.2401e-10, 4.2143e-10, 3.9967e-10,\n",
       "             3.7693e-10, 3.5357e-10, 3.4200e-10, 2.9587e-10, 2.6717e-10, 2.6605e-10,\n",
       "             2.3824e-10, 2.2030e-10, 2.1031e-10, 1.9181e-10, 1.7903e-10, 1.7844e-10,\n",
       "             1.4529e-10, 1.2751e-10, 1.1909e-10, 1.1299e-10, 8.9577e-11, 8.9015e-11,\n",
       "             8.7409e-11, 6.8994e-11, 6.7382e-11, 5.8781e-11, 4.8483e-11, 4.0484e-11,\n",
       "             3.7252e-11, 3.5133e-11, 3.4399e-11, 3.2861e-11, 3.2518e-11, 3.0758e-11,\n",
       "             2.7091e-11, 2.4306e-11, 1.8704e-11, 1.5017e-11, 1.3911e-11, 1.3770e-11,\n",
       "             1.3549e-11, 1.0721e-11, 6.9312e-12, 6.3870e-12, 6.1346e-12, 6.1227e-12,\n",
       "             5.8403e-12, 3.4421e-12, 2.9566e-12, 2.6684e-12, 2.6168e-12, 1.8638e-12,\n",
       "             1.7273e-12, 1.6714e-12, 1.6384e-12, 1.2763e-12, 1.2189e-12, 1.1449e-12,\n",
       "             1.1104e-12, 1.0181e-12, 8.4479e-13, 6.7187e-13, 6.2934e-13, 5.5096e-13,\n",
       "             5.3335e-13, 5.2483e-13, 4.6694e-13, 4.2503e-13, 4.1402e-13, 3.9823e-13,\n",
       "             3.4476e-13, 2.7363e-13, 2.6664e-13, 2.4550e-13, 1.9515e-13, 1.6715e-13,\n",
       "             1.2086e-13, 1.1165e-13, 1.1056e-13, 1.0969e-13, 6.5613e-14, 5.5372e-14,\n",
       "             4.8683e-14, 3.6486e-14, 3.4836e-14, 1.8392e-14, 2.7076e-15, 1.5931e-16,\n",
       "             9.7049e-17, 1.9512e-18])}},\n",
       "   {'fpr': np.float64(0.12334801762114538),\n",
       "    'tpr': np.float64(0.9907335907335907),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0044, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0110, 0.0110,\n",
       "             0.0132, 0.0154, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176,\n",
       "             0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176,\n",
       "             0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176,\n",
       "             0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176,\n",
       "             0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0198, 0.0198, 0.0220, 0.0220,\n",
       "             0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0264,\n",
       "             0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0286,\n",
       "             0.0286, 0.0286, 0.0286, 0.0286, 0.0308, 0.0330, 0.0330, 0.0330, 0.0330,\n",
       "             0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0352, 0.0352, 0.0352, 0.0352,\n",
       "             0.0374, 0.0374, 0.0374, 0.0374, 0.0396, 0.0396, 0.0396, 0.0396, 0.0396,\n",
       "             0.0396, 0.0396, 0.0396, 0.0419, 0.0419, 0.0441, 0.0463, 0.0485, 0.0507,\n",
       "             0.0507, 0.0507, 0.0507, 0.0529, 0.0529, 0.0529, 0.0529, 0.0529, 0.0551,\n",
       "             0.0551, 0.0573, 0.0573, 0.0595, 0.0617, 0.0639, 0.0639, 0.0661, 0.0661,\n",
       "             0.0683, 0.0683, 0.0683, 0.0705, 0.0727, 0.0727, 0.0749, 0.0771, 0.0771,\n",
       "             0.0793, 0.0793, 0.0815, 0.0837, 0.0859, 0.0859, 0.0881, 0.0903, 0.0925,\n",
       "             0.0947, 0.0969, 0.0969, 0.0991, 0.0991, 0.0991, 0.1013, 0.1013, 0.1035,\n",
       "             0.1057, 0.1079, 0.1101, 0.1101, 0.1123, 0.1145, 0.1167, 0.1167, 0.1189,\n",
       "             0.1211, 0.1211, 0.1233, 0.1256, 0.1256, 0.1278, 0.1300, 0.1322, 0.1344,\n",
       "             0.1344, 0.1366, 0.1388, 0.1410, 0.1432, 0.1454, 0.1476, 0.1498, 0.1520,\n",
       "             0.1542, 0.1564, 0.1586, 0.1608, 0.1608, 0.1630, 0.1630, 0.1652, 0.1674,\n",
       "             0.1696, 0.1718, 0.1740, 0.1740, 0.1762, 0.1784, 0.1806, 0.1828, 0.1850,\n",
       "             0.1872, 0.1872, 0.1894, 0.1916, 0.1938, 0.1938, 0.1960, 0.1982, 0.2004,\n",
       "             0.2026, 0.2048, 0.2070, 0.2093, 0.2093, 0.2115, 0.2137, 0.2159, 0.2181,\n",
       "             0.2203, 0.2225, 0.2247, 0.2269, 0.2291, 0.2313, 0.2335, 0.2357, 0.2379,\n",
       "             0.2401, 0.2423, 0.2445, 0.2467, 0.2489, 0.2511, 0.2533, 0.2555, 0.2577,\n",
       "             0.2599, 0.2621, 0.2643, 0.2665, 0.2687, 0.2709, 0.2731, 0.2753, 0.2775,\n",
       "             0.2797, 0.2819, 0.2841, 0.2863, 0.2885, 0.2907, 0.2930, 0.2952, 0.2974,\n",
       "             0.2996, 0.3018, 0.3040, 0.3062, 0.3084, 0.3106, 0.3128, 0.3150, 0.3172,\n",
       "             0.3194, 0.3216, 0.3238, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370,\n",
       "             0.3392, 0.3414, 0.3436, 0.3458, 0.3480, 0.3502, 0.3524, 0.3546, 0.3568,\n",
       "             0.3590, 0.3612, 0.3634, 0.3656, 0.3678, 0.3700, 0.3722, 0.3744, 0.3767,\n",
       "             0.3789, 0.3811, 0.3833, 0.3855, 0.3877, 0.3899, 0.3921, 0.3921, 0.3943,\n",
       "             0.3965, 0.3987, 0.4009, 0.4031, 0.4053, 0.4075, 0.4097, 0.4119, 0.4141,\n",
       "             0.4163, 0.4185, 0.4207, 0.4229, 0.4251, 0.4273, 0.4295, 0.4295, 0.4317,\n",
       "             0.4339, 0.4339, 0.4361, 0.4383, 0.4405, 0.4427, 0.4449, 0.4471, 0.4493,\n",
       "             0.4515, 0.4537, 0.4559, 0.4581, 0.4604, 0.4626, 0.4648, 0.4670, 0.4692,\n",
       "             0.4714, 0.4736, 0.4758, 0.4780, 0.4802, 0.4824, 0.4846, 0.4868, 0.4890,\n",
       "             0.4912, 0.4934, 0.4956, 0.4978, 0.5000, 0.5022, 0.5044, 0.5066, 0.5088,\n",
       "             0.5110, 0.5132, 0.5154, 0.5176, 0.5198, 0.5220, 0.5242, 0.5264, 0.5286,\n",
       "             0.5308, 0.5330, 0.5352, 0.5374, 0.5396, 0.5419, 0.5441, 0.5463, 0.5485,\n",
       "             0.5507, 0.5529, 0.5551, 0.5573, 0.5595, 0.5617, 0.5639, 0.5661, 0.5683,\n",
       "             0.5705, 0.5727, 0.5749, 0.5771, 0.5793, 0.5815, 0.5837, 0.5859, 0.5881,\n",
       "             0.5903, 0.5925, 0.5947, 0.5969, 0.5991, 0.6013, 0.6035, 0.6057, 0.6079,\n",
       "             0.6101, 0.6123, 0.6145, 0.6167, 0.6189, 0.6211, 0.6233, 0.6256, 0.6278,\n",
       "             0.6300, 0.6322, 0.6344, 0.6366, 0.6388, 0.6410, 0.6432, 0.6454, 0.6476,\n",
       "             0.6476, 0.6498, 0.6520, 0.6542, 0.6564, 0.6586, 0.6608, 0.6630, 0.6652,\n",
       "             0.6674, 0.6696, 0.6718, 0.6740, 0.6762, 0.6784, 0.6806, 0.6828, 0.6850,\n",
       "             0.6872, 0.6894, 0.6916, 0.6938, 0.6960, 0.6982, 0.7004, 0.7026, 0.7048,\n",
       "             0.7070, 0.7093, 0.7115, 0.7137, 0.7159, 0.7181, 0.7203, 0.7225, 0.7247,\n",
       "             0.7269, 0.7291, 0.7313, 0.7335, 0.7357, 0.7379, 0.7401, 0.7423, 0.7445,\n",
       "             0.7467, 0.7489, 0.7511, 0.7533, 0.7555, 0.7577, 0.7599, 0.7621, 0.7643,\n",
       "             0.7665, 0.7687, 0.7709, 0.7731, 0.7753, 0.7775, 0.7797, 0.7819, 0.7841,\n",
       "             0.7863, 0.7885, 0.7907, 0.7930, 0.7952, 0.7974, 0.7996, 0.8018, 0.8040,\n",
       "             0.8062, 0.8084, 0.8106, 0.8128, 0.8150, 0.8172, 0.8194, 0.8216, 0.8238,\n",
       "             0.8260, 0.8282, 0.8304, 0.8326, 0.8348, 0.8370, 0.8392, 0.8414, 0.8436,\n",
       "             0.8458, 0.8480, 0.8502, 0.8524, 0.8546, 0.8568, 0.8590, 0.8612, 0.8634,\n",
       "             0.8656, 0.8678, 0.8700, 0.8722, 0.8744, 0.8767, 0.8789, 0.8811, 0.8833,\n",
       "             0.8855, 0.8877, 0.8899, 0.8921, 0.8943, 0.8965, 0.8987, 0.9009, 0.9031,\n",
       "             0.9053, 0.9075, 0.9097, 0.9119, 0.9141, 0.9163, 0.9185, 0.9207, 0.9229,\n",
       "             0.9251, 0.9273, 0.9295, 0.9317, 0.9339, 0.9361, 0.9383, 0.9405, 0.9427,\n",
       "             0.9449, 0.9471, 0.9493, 0.9515, 0.9537, 0.9559, 0.9581, 0.9604, 0.9626,\n",
       "             0.9648, 0.9670, 0.9692, 0.9714, 0.9736, 0.9758, 0.9780, 0.9802, 0.9824,\n",
       "             0.9846, 0.9868, 0.9890, 0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7305, 0.7900, 0.8131, 0.8263, 0.8386, 0.8448, 0.8494, 0.8548,\n",
       "             0.8556, 0.8579, 0.8633, 0.8680, 0.8726, 0.8764, 0.8788, 0.8795, 0.8826,\n",
       "             0.8857, 0.8873, 0.8880, 0.8896, 0.8903, 0.8911, 0.8927, 0.8934, 0.8942,\n",
       "             0.8942, 0.8958, 0.8973, 0.8981, 0.8988, 0.8996, 0.9019, 0.9019, 0.9027,\n",
       "             0.9035, 0.9035, 0.9050, 0.9058, 0.9066, 0.9089, 0.9097, 0.9104, 0.9112,\n",
       "             0.9120, 0.9135, 0.9143, 0.9151, 0.9158, 0.9166, 0.9174, 0.9181, 0.9189,\n",
       "             0.9197, 0.9212, 0.9228, 0.9236, 0.9243, 0.9251, 0.9259, 0.9266, 0.9274,\n",
       "             0.9282, 0.9290, 0.9297, 0.9305, 0.9313, 0.9320, 0.9328, 0.9336, 0.9344,\n",
       "             0.9351, 0.9359, 0.9367, 0.9375, 0.9382, 0.9382, 0.9390, 0.9390, 0.9398,\n",
       "             0.9398, 0.9405, 0.9413, 0.9421, 0.9429, 0.9436, 0.9444, 0.9452, 0.9452,\n",
       "             0.9459, 0.9467, 0.9475, 0.9483, 0.9490, 0.9498, 0.9506, 0.9514, 0.9514,\n",
       "             0.9521, 0.9529, 0.9537, 0.9544, 0.9544, 0.9544, 0.9552, 0.9560, 0.9568,\n",
       "             0.9575, 0.9583, 0.9591, 0.9598, 0.9606, 0.9606, 0.9614, 0.9622, 0.9629,\n",
       "             0.9629, 0.9637, 0.9653, 0.9660, 0.9660, 0.9668, 0.9676, 0.9683, 0.9691,\n",
       "             0.9699, 0.9707, 0.9714, 0.9714, 0.9722, 0.9722, 0.9722, 0.9722, 0.9722,\n",
       "             0.9730, 0.9737, 0.9745, 0.9745, 0.9753, 0.9761, 0.9768, 0.9776, 0.9776,\n",
       "             0.9784, 0.9784, 0.9792, 0.9792, 0.9792, 0.9792, 0.9799, 0.9799, 0.9807,\n",
       "             0.9807, 0.9815, 0.9822, 0.9822, 0.9822, 0.9830, 0.9830, 0.9830, 0.9838,\n",
       "             0.9838, 0.9846, 0.9846, 0.9846, 0.9846, 0.9853, 0.9853, 0.9853, 0.9853,\n",
       "             0.9853, 0.9853, 0.9861, 0.9861, 0.9869, 0.9876, 0.9876, 0.9884, 0.9884,\n",
       "             0.9884, 0.9884, 0.9884, 0.9892, 0.9892, 0.9892, 0.9892, 0.9900, 0.9900,\n",
       "             0.9900, 0.9907, 0.9907, 0.9907, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9938, 0.9938, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985,\n",
       "             0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9983e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9976e-01, 9.9975e-01, 9.9971e-01, 9.9971e-01,\n",
       "             9.9967e-01, 9.9964e-01, 9.9963e-01, 9.9959e-01, 9.9954e-01, 9.9953e-01,\n",
       "             9.9950e-01, 9.9949e-01, 9.9947e-01, 9.9944e-01, 9.9942e-01, 9.9939e-01,\n",
       "             9.9926e-01, 9.9915e-01, 9.9914e-01, 9.9902e-01, 9.9899e-01, 9.9898e-01,\n",
       "             9.9875e-01, 9.9875e-01, 9.9873e-01, 9.9836e-01, 9.9820e-01, 9.9806e-01,\n",
       "             9.9798e-01, 9.9795e-01, 9.9789e-01, 9.9780e-01, 9.9727e-01, 9.9706e-01,\n",
       "             9.9705e-01, 9.9654e-01, 9.9653e-01, 9.9595e-01, 9.9544e-01, 9.9524e-01,\n",
       "             9.9498e-01, 9.9374e-01, 9.9193e-01, 9.9093e-01, 9.9091e-01, 9.9076e-01,\n",
       "             9.9020e-01, 9.8904e-01, 9.8880e-01, 9.8298e-01, 9.7842e-01, 9.6316e-01,\n",
       "             9.6315e-01, 9.5967e-01, 9.5855e-01, 9.5537e-01, 9.5395e-01, 9.4054e-01,\n",
       "             9.3697e-01, 9.3597e-01, 9.2966e-01, 9.2272e-01, 8.8203e-01, 8.5967e-01,\n",
       "             8.5672e-01, 8.3960e-01, 8.2871e-01, 8.0941e-01, 8.0520e-01, 7.9998e-01,\n",
       "             7.8041e-01, 7.6506e-01, 7.6366e-01, 7.3140e-01, 7.0930e-01, 7.0688e-01,\n",
       "             6.5979e-01, 6.3481e-01, 6.2984e-01, 5.6267e-01, 5.2807e-01, 5.1947e-01,\n",
       "             4.8319e-01, 4.7502e-01, 4.7168e-01, 4.3308e-01, 4.1141e-01, 3.9433e-01,\n",
       "             3.7244e-01, 3.6461e-01, 3.4573e-01, 3.1762e-01, 2.8872e-01, 2.8250e-01,\n",
       "             2.8092e-01, 2.7330e-01, 2.4400e-01, 2.4394e-01, 2.1313e-01, 1.9723e-01,\n",
       "             1.9037e-01, 1.7470e-01, 1.6183e-01, 1.5699e-01, 1.3777e-01, 1.3470e-01,\n",
       "             1.2223e-01, 9.2937e-02, 7.8032e-02, 7.3408e-02, 6.9642e-02, 6.9413e-02,\n",
       "             6.7431e-02, 6.1598e-02, 5.5723e-02, 5.5197e-02, 5.0220e-02, 4.3122e-02,\n",
       "             4.0190e-02, 3.1679e-02, 3.1192e-02, 3.0790e-02, 2.9506e-02, 2.8610e-02,\n",
       "             2.5964e-02, 2.5343e-02, 2.2211e-02, 1.7810e-02, 1.6574e-02, 1.4998e-02,\n",
       "             1.2278e-02, 1.1775e-02, 1.1103e-02, 1.1070e-02, 1.0380e-02, 9.9382e-03,\n",
       "             8.2440e-03, 6.1077e-03, 4.1780e-03, 4.1015e-03, 4.0129e-03, 3.6254e-03,\n",
       "             2.8901e-03, 2.7500e-03, 2.4362e-03, 2.2962e-03, 2.1519e-03, 1.9096e-03,\n",
       "             1.8049e-03, 1.5317e-03, 1.5289e-03, 1.4078e-03, 1.3426e-03, 1.2109e-03,\n",
       "             1.1865e-03, 1.1763e-03, 1.1629e-03, 9.4606e-04, 8.9877e-04, 5.7349e-04,\n",
       "             5.0765e-04, 4.6651e-04, 4.0344e-04, 3.8744e-04, 3.8467e-04, 3.0826e-04,\n",
       "             3.0165e-04, 2.8404e-04, 2.4214e-04, 2.3360e-04, 2.3027e-04, 2.2883e-04,\n",
       "             1.9798e-04, 1.9177e-04, 1.7333e-04, 1.6728e-04, 1.6072e-04, 1.5372e-04,\n",
       "             1.4701e-04, 1.2759e-04, 1.0613e-04, 1.0100e-04, 9.1938e-05, 7.0700e-05,\n",
       "             6.5675e-05, 6.0964e-05, 5.8667e-05, 5.7775e-05, 5.2936e-05, 5.2798e-05,\n",
       "             5.2100e-05, 5.1543e-05, 4.6441e-05, 4.4952e-05, 4.3192e-05, 4.2232e-05,\n",
       "             4.0654e-05, 3.5640e-05, 3.0666e-05, 3.0110e-05, 2.6633e-05, 2.6382e-05,\n",
       "             2.5552e-05, 2.3452e-05, 2.3309e-05, 1.9956e-05, 1.9155e-05, 1.9012e-05,\n",
       "             1.8149e-05, 1.7586e-05, 1.6458e-05, 1.3689e-05, 1.2309e-05, 1.2105e-05,\n",
       "             1.2011e-05, 1.1565e-05, 1.1493e-05, 1.1281e-05, 1.1195e-05, 1.0997e-05,\n",
       "             1.0665e-05, 9.9532e-06, 8.5010e-06, 6.9225e-06, 6.9206e-06, 6.1936e-06,\n",
       "             6.1839e-06, 5.6783e-06, 5.5204e-06, 5.4434e-06, 5.1879e-06, 5.1787e-06,\n",
       "             4.9213e-06, 4.6921e-06, 4.5614e-06, 4.0667e-06, 3.8922e-06, 3.6205e-06,\n",
       "             3.1093e-06, 2.9923e-06, 2.9672e-06, 2.8600e-06, 2.8505e-06, 2.4868e-06,\n",
       "             2.0991e-06, 1.8104e-06, 1.5759e-06, 1.4594e-06, 1.4332e-06, 1.3714e-06,\n",
       "             1.0425e-06, 1.0333e-06, 1.0113e-06, 9.9756e-07, 9.9437e-07, 9.3449e-07,\n",
       "             9.1912e-07, 9.1539e-07, 9.1208e-07, 8.6987e-07, 8.5022e-07, 8.4983e-07,\n",
       "             7.9402e-07, 7.8390e-07, 7.7706e-07, 7.6745e-07, 6.9960e-07, 6.2337e-07,\n",
       "             5.0108e-07, 4.9709e-07, 4.7400e-07, 4.5786e-07, 3.9385e-07, 2.9994e-07,\n",
       "             2.9402e-07, 2.8175e-07, 2.7300e-07, 2.2053e-07, 2.1695e-07, 2.0620e-07,\n",
       "             2.0182e-07, 1.8289e-07, 1.7193e-07, 1.7152e-07, 1.6944e-07, 1.6233e-07,\n",
       "             1.5760e-07, 1.3683e-07, 1.2127e-07, 1.1420e-07, 1.1002e-07, 1.0131e-07,\n",
       "             9.6212e-08, 9.5517e-08, 8.9668e-08, 8.5793e-08, 8.2398e-08, 7.7856e-08,\n",
       "             6.8139e-08, 6.4010e-08, 6.2550e-08, 5.9968e-08, 5.1607e-08, 4.7329e-08,\n",
       "             4.5239e-08, 3.9523e-08, 3.9247e-08, 3.6708e-08, 3.5725e-08, 3.5565e-08,\n",
       "             3.2778e-08, 3.2310e-08, 2.7670e-08, 2.3368e-08, 1.9158e-08, 1.8778e-08,\n",
       "             1.8556e-08, 1.8306e-08, 1.6756e-08, 1.6513e-08, 1.5486e-08, 1.5335e-08,\n",
       "             1.4271e-08, 1.1894e-08, 1.1276e-08, 1.1139e-08, 9.6340e-09, 9.2188e-09,\n",
       "             9.1882e-09, 8.8198e-09, 8.6459e-09, 8.5765e-09, 8.0431e-09, 6.9162e-09,\n",
       "             5.9358e-09, 5.8057e-09, 5.6876e-09, 5.3132e-09, 5.3132e-09, 4.1847e-09,\n",
       "             3.9550e-09, 3.6517e-09, 2.5133e-09, 2.4530e-09, 2.0937e-09, 2.0561e-09,\n",
       "             1.9647e-09, 1.9234e-09, 1.8999e-09, 1.8732e-09, 1.5658e-09, 1.3991e-09,\n",
       "             1.2795e-09, 1.2594e-09, 1.2195e-09, 1.0472e-09, 9.4867e-10, 9.3004e-10,\n",
       "             8.3262e-10, 8.1453e-10, 7.8223e-10, 6.9535e-10, 6.7401e-10, 6.2762e-10,\n",
       "             5.9517e-10, 5.0046e-10, 4.9447e-10, 4.6873e-10, 4.3316e-10, 3.6420e-10,\n",
       "             3.4006e-10, 2.7728e-10, 2.5339e-10, 2.4515e-10, 2.2993e-10, 2.1687e-10,\n",
       "             2.0114e-10, 1.8510e-10, 1.6368e-10, 1.5218e-10, 1.4671e-10, 1.4121e-10,\n",
       "             1.3595e-10, 1.3270e-10, 1.1489e-10, 1.1276e-10, 9.6178e-11, 9.4277e-11,\n",
       "             8.2840e-11, 6.7507e-11, 6.4437e-11, 5.2548e-11, 4.9968e-11, 4.8287e-11,\n",
       "             4.7755e-11, 4.7103e-11, 4.3499e-11, 4.0390e-11, 3.8425e-11, 3.7331e-11,\n",
       "             3.6700e-11, 3.2816e-11, 3.2459e-11, 2.9428e-11, 2.7980e-11, 2.7382e-11,\n",
       "             2.5089e-11, 2.3692e-11, 1.6167e-11, 1.2809e-11, 1.2647e-11, 1.2247e-11,\n",
       "             1.1195e-11, 1.0509e-11, 1.0033e-11, 9.9976e-12, 9.0211e-12, 8.8930e-12,\n",
       "             8.3412e-12, 7.3135e-12, 5.4838e-12, 5.3048e-12, 3.5820e-12, 3.3899e-12,\n",
       "             3.3174e-12, 3.1721e-12, 1.9691e-12, 1.9257e-12, 1.6259e-12, 1.5891e-12,\n",
       "             1.4933e-12, 1.4188e-12, 1.1772e-12, 1.1545e-12, 1.0778e-12, 9.9219e-13,\n",
       "             8.8529e-13, 8.7203e-13, 7.3528e-13, 6.8176e-13, 6.5479e-13, 6.4059e-13,\n",
       "             5.9482e-13, 5.3896e-13, 5.1621e-13, 4.8732e-13, 4.7622e-13, 3.9600e-13,\n",
       "             2.7824e-13, 2.3901e-13, 2.2389e-13, 2.1929e-13, 1.9322e-13, 1.4685e-13,\n",
       "             1.0307e-13, 1.0109e-13, 5.9053e-14, 5.8279e-14, 5.1124e-14, 5.0076e-14,\n",
       "             4.4694e-14, 4.1734e-14, 4.0200e-14, 3.8102e-14, 2.6279e-14, 1.2449e-14,\n",
       "             7.6036e-15, 7.2780e-15, 6.0616e-15, 4.5192e-15, 2.3787e-15, 1.5536e-15,\n",
       "             1.4016e-15, 7.4096e-16, 4.6454e-16, 4.6411e-16, 4.0017e-16, 3.2572e-16,\n",
       "             1.0918e-16, 6.2553e-17, 3.7628e-17, 2.5906e-17, 1.8692e-17, 1.7842e-17,\n",
       "             1.6895e-17, 1.1618e-17, 8.7735e-18, 6.8934e-18, 4.6702e-18, 1.2189e-18,\n",
       "             1.2776e-19, 8.1287e-23])}},\n",
       "   {'fpr': np.float64(0.046255506607929514),\n",
       "    'tpr': np.float64(0.9791505791505791),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
       "             0.0110, 0.0110, 0.0110, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
       "             0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
       "             0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0198, 0.0198, 0.0198, 0.0198,\n",
       "             0.0198, 0.0198, 0.0220, 0.0220, 0.0220, 0.0242, 0.0242, 0.0242, 0.0242,\n",
       "             0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242,\n",
       "             0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0264, 0.0264,\n",
       "             0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0286, 0.0308,\n",
       "             0.0308, 0.0308, 0.0308, 0.0308, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330,\n",
       "             0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0352, 0.0352, 0.0374,\n",
       "             0.0374, 0.0374, 0.0374, 0.0396, 0.0419, 0.0419, 0.0419, 0.0441, 0.0441,\n",
       "             0.0441, 0.0441, 0.0463, 0.0463, 0.0485, 0.0507, 0.0507, 0.0507, 0.0529,\n",
       "             0.0529, 0.0529, 0.0529, 0.0529, 0.0551, 0.0573, 0.0595, 0.0595, 0.0617,\n",
       "             0.0639, 0.0639, 0.0661, 0.0683, 0.0705, 0.0727, 0.0749, 0.0771, 0.0793,\n",
       "             0.0815, 0.0837, 0.0837, 0.0837, 0.0837, 0.0859, 0.0859, 0.0881, 0.0903,\n",
       "             0.0925, 0.0947, 0.0969, 0.0991, 0.0991, 0.1013, 0.1035, 0.1057, 0.1079,\n",
       "             0.1101, 0.1123, 0.1145, 0.1167, 0.1167, 0.1189, 0.1211, 0.1211, 0.1233,\n",
       "             0.1256, 0.1278, 0.1300, 0.1322, 0.1344, 0.1366, 0.1388, 0.1410, 0.1432,\n",
       "             0.1454, 0.1454, 0.1476, 0.1498, 0.1520, 0.1542, 0.1564, 0.1564, 0.1586,\n",
       "             0.1608, 0.1608, 0.1608, 0.1630, 0.1652, 0.1652, 0.1674, 0.1696, 0.1696,\n",
       "             0.1696, 0.1718, 0.1718, 0.1740, 0.1762, 0.1784, 0.1806, 0.1828, 0.1850,\n",
       "             0.1872, 0.1894, 0.1916, 0.1938, 0.1960, 0.1982, 0.2004, 0.2026, 0.2048,\n",
       "             0.2070, 0.2093, 0.2115, 0.2137, 0.2159, 0.2181, 0.2203, 0.2225, 0.2247,\n",
       "             0.2269, 0.2291, 0.2313, 0.2335, 0.2357, 0.2379, 0.2379, 0.2401, 0.2423,\n",
       "             0.2445, 0.2467, 0.2489, 0.2511, 0.2533, 0.2555, 0.2577, 0.2599, 0.2621,\n",
       "             0.2643, 0.2665, 0.2687, 0.2709, 0.2731, 0.2753, 0.2775, 0.2797, 0.2819,\n",
       "             0.2841, 0.2863, 0.2885, 0.2907, 0.2930, 0.2952, 0.2974, 0.2996, 0.3018,\n",
       "             0.3040, 0.3062, 0.3084, 0.3106, 0.3128, 0.3150, 0.3172, 0.3194, 0.3216,\n",
       "             0.3238, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370, 0.3392, 0.3414,\n",
       "             0.3436, 0.3458, 0.3480, 0.3502, 0.3524, 0.3546, 0.3568, 0.3590, 0.3612,\n",
       "             0.3634, 0.3656, 0.3678, 0.3700, 0.3700, 0.3722, 0.3744, 0.3767, 0.3789,\n",
       "             0.3811, 0.3833, 0.3855, 0.3877, 0.3899, 0.3921, 0.3943, 0.3965, 0.3987,\n",
       "             0.4009, 0.4031, 0.4053, 0.4075, 0.4097, 0.4119, 0.4141, 0.4163, 0.4185,\n",
       "             0.4207, 0.4229, 0.4251, 0.4273, 0.4295, 0.4317, 0.4339, 0.4361, 0.4383,\n",
       "             0.4405, 0.4427, 0.4449, 0.4471, 0.4493, 0.4515, 0.4537, 0.4559, 0.4581,\n",
       "             0.4604, 0.4626, 0.4648, 0.4670, 0.4692, 0.4714, 0.4736, 0.4758, 0.4780,\n",
       "             0.4802, 0.4824, 0.4846, 0.4868, 0.4890, 0.4912, 0.4934, 0.4956, 0.4978,\n",
       "             0.5000, 0.5022, 0.5044, 0.5066, 0.5088, 0.5110, 0.5132, 0.5154, 0.5176,\n",
       "             0.5198, 0.5220, 0.5242, 0.5264, 0.5286, 0.5308, 0.5330, 0.5352, 0.5374,\n",
       "             0.5396, 0.5419, 0.5441, 0.5463, 0.5485, 0.5507, 0.5529, 0.5551, 0.5573,\n",
       "             0.5595, 0.5617, 0.5639, 0.5661, 0.5683, 0.5705, 0.5727, 0.5749, 0.5771,\n",
       "             0.5793, 0.5815, 0.5837, 0.5859, 0.5881, 0.5903, 0.5925, 0.5947, 0.5969,\n",
       "             0.5991, 0.6013, 0.6035, 0.6057, 0.6079, 0.6101, 0.6123, 0.6145, 0.6167,\n",
       "             0.6189, 0.6211, 0.6233, 0.6256, 0.6278, 0.6300, 0.6322, 0.6344, 0.6366,\n",
       "             0.6388, 0.6410, 0.6432, 0.6454, 0.6476, 0.6498, 0.6520, 0.6542, 0.6564,\n",
       "             0.6586, 0.6608, 0.6630, 0.6652, 0.6674, 0.6696, 0.6718, 0.6740, 0.6762,\n",
       "             0.6784, 0.6806, 0.6828, 0.6850, 0.6872, 0.6894, 0.6916, 0.6938, 0.6960,\n",
       "             0.6982, 0.7004, 0.7026, 0.7048, 0.7070, 0.7093, 0.7115, 0.7137, 0.7159,\n",
       "             0.7181, 0.7203, 0.7225, 0.7247, 0.7269, 0.7291, 0.7313, 0.7335, 0.7357,\n",
       "             0.7379, 0.7401, 0.7423, 0.7445, 0.7467, 0.7489, 0.7511, 0.7533, 0.7555,\n",
       "             0.7577, 0.7599, 0.7621, 0.7643, 0.7665, 0.7687, 0.7709, 0.7731, 0.7753,\n",
       "             0.7775, 0.7797, 0.7819, 0.7841, 0.7863, 0.7885, 0.7907, 0.7930, 0.7952,\n",
       "             0.7952, 0.7974, 0.7996, 0.8018, 0.8040, 0.8062, 0.8084, 0.8106, 0.8128,\n",
       "             0.8150, 0.8172, 0.8194, 0.8216, 0.8238, 0.8260, 0.8282, 0.8304, 0.8326,\n",
       "             0.8348, 0.8370, 0.8392, 0.8414, 0.8436, 0.8458, 0.8480, 0.8502, 0.8524,\n",
       "             0.8546, 0.8568, 0.8590, 0.8612, 0.8634, 0.8656, 0.8678, 0.8700, 0.8722,\n",
       "             0.8744, 0.8767, 0.8789, 0.8811, 0.8833, 0.8855, 0.8877, 0.8899, 0.8921,\n",
       "             0.8943, 0.8965, 0.8987, 0.9009, 0.9031, 0.9053, 0.9075, 0.9097, 0.9119,\n",
       "             0.9141, 0.9163, 0.9185, 0.9207, 0.9229, 0.9251, 0.9273, 0.9295, 0.9317,\n",
       "             0.9339, 0.9361, 0.9383, 0.9405, 0.9427, 0.9449, 0.9471, 0.9493, 0.9515,\n",
       "             0.9537, 0.9559, 0.9581, 0.9604, 0.9626, 0.9648, 0.9670, 0.9692, 0.9714,\n",
       "             0.9736, 0.9758, 0.9780, 0.9802, 0.9824, 0.9846, 0.9868, 0.9890, 0.9912,\n",
       "             0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.5761, 0.6425, 0.6687, 0.6780, 0.6865, 0.6996, 0.7081, 0.7174,\n",
       "             0.7228, 0.7266, 0.7328, 0.7359, 0.7405, 0.7452, 0.7475, 0.7521, 0.7575,\n",
       "             0.7598, 0.7622, 0.7637, 0.7645, 0.7653, 0.7660, 0.7676, 0.7714, 0.7737,\n",
       "             0.7761, 0.7776, 0.7784, 0.7815, 0.7822, 0.7838, 0.7846, 0.7876, 0.7900,\n",
       "             0.7907, 0.7915, 0.7938, 0.7946, 0.7954, 0.7969, 0.7985, 0.7992, 0.8008,\n",
       "             0.8015, 0.8031, 0.8039, 0.8054, 0.8062, 0.8085, 0.8108, 0.8116, 0.8124,\n",
       "             0.8131, 0.8139, 0.8147, 0.8162, 0.8170, 0.8185, 0.8193, 0.8193, 0.8201,\n",
       "             0.8208, 0.8216, 0.8224, 0.8232, 0.8255, 0.8263, 0.8270, 0.8286, 0.8293,\n",
       "             0.8301, 0.8309, 0.8317, 0.8332, 0.8340, 0.8347, 0.8355, 0.8363, 0.8371,\n",
       "             0.8378, 0.8386, 0.8394, 0.8402, 0.8409, 0.8417, 0.8425, 0.8432, 0.8448,\n",
       "             0.8456, 0.8463, 0.8471, 0.8479, 0.8494, 0.8510, 0.8517, 0.8533, 0.8541,\n",
       "             0.8556, 0.8564, 0.8571, 0.8579, 0.8587, 0.8595, 0.8618, 0.8625, 0.8633,\n",
       "             0.8641, 0.8649, 0.8656, 0.8664, 0.8672, 0.8680, 0.8687, 0.8695, 0.8703,\n",
       "             0.8710, 0.8718, 0.8726, 0.8734, 0.8749, 0.8757, 0.8764, 0.8780, 0.8788,\n",
       "             0.8795, 0.8803, 0.8811, 0.8819, 0.8826, 0.8834, 0.8842, 0.8849, 0.8857,\n",
       "             0.8865, 0.8873, 0.8880, 0.8888, 0.8896, 0.8903, 0.8911, 0.8919, 0.8927,\n",
       "             0.8934, 0.8942, 0.8950, 0.8958, 0.8965, 0.8973, 0.8981, 0.8988, 0.8996,\n",
       "             0.9004, 0.9012, 0.9019, 0.9027, 0.9035, 0.9035, 0.9042, 0.9050, 0.9058,\n",
       "             0.9066, 0.9073, 0.9081, 0.9089, 0.9089, 0.9097, 0.9104, 0.9112, 0.9120,\n",
       "             0.9127, 0.9135, 0.9143, 0.9143, 0.9151, 0.9158, 0.9166, 0.9174, 0.9181,\n",
       "             0.9189, 0.9197, 0.9205, 0.9212, 0.9212, 0.9220, 0.9228, 0.9236, 0.9243,\n",
       "             0.9251, 0.9259, 0.9266, 0.9274, 0.9282, 0.9290, 0.9297, 0.9305, 0.9320,\n",
       "             0.9320, 0.9328, 0.9336, 0.9344, 0.9351, 0.9351, 0.9359, 0.9367, 0.9375,\n",
       "             0.9382, 0.9390, 0.9390, 0.9398, 0.9405, 0.9405, 0.9413, 0.9421, 0.9429,\n",
       "             0.9436, 0.9444, 0.9452, 0.9459, 0.9467, 0.9475, 0.9483, 0.9490, 0.9498,\n",
       "             0.9506, 0.9514, 0.9521, 0.9529, 0.9537, 0.9544, 0.9552, 0.9552, 0.9560,\n",
       "             0.9568, 0.9575, 0.9583, 0.9591, 0.9598, 0.9606, 0.9614, 0.9614, 0.9614,\n",
       "             0.9622, 0.9629, 0.9637, 0.9645, 0.9645, 0.9653, 0.9660, 0.9668, 0.9676,\n",
       "             0.9683, 0.9691, 0.9699, 0.9707, 0.9714, 0.9722, 0.9722, 0.9730, 0.9730,\n",
       "             0.9737, 0.9745, 0.9753, 0.9753, 0.9753, 0.9761, 0.9768, 0.9768, 0.9776,\n",
       "             0.9784, 0.9792, 0.9792, 0.9799, 0.9799, 0.9799, 0.9807, 0.9815, 0.9815,\n",
       "             0.9822, 0.9830, 0.9838, 0.9846, 0.9846, 0.9846, 0.9846, 0.9853, 0.9853,\n",
       "             0.9853, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861,\n",
       "             0.9861, 0.9861, 0.9869, 0.9876, 0.9884, 0.9884, 0.9892, 0.9892, 0.9892,\n",
       "             0.9892, 0.9892, 0.9892, 0.9892, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9907, 0.9907, 0.9907, 0.9915, 0.9915,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931,\n",
       "             0.9931, 0.9938, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9961,\n",
       "             0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9972e-01, 9.9970e-01, 9.9969e-01, 9.9968e-01, 9.9968e-01, 9.9967e-01,\n",
       "             9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9964e-01, 9.9963e-01, 9.9961e-01,\n",
       "             9.9960e-01, 9.9959e-01, 9.9959e-01, 9.9958e-01, 9.9958e-01, 9.9957e-01,\n",
       "             9.9957e-01, 9.9956e-01, 9.9956e-01, 9.9953e-01, 9.9952e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9948e-01, 9.9947e-01, 9.9943e-01, 9.9939e-01, 9.9936e-01,\n",
       "             9.9934e-01, 9.9933e-01, 9.9924e-01, 9.9923e-01, 9.9923e-01, 9.9916e-01,\n",
       "             9.9895e-01, 9.9887e-01, 9.9885e-01, 9.9880e-01, 9.9863e-01, 9.9843e-01,\n",
       "             9.9841e-01, 9.9832e-01, 9.9816e-01, 9.9797e-01, 9.9795e-01, 9.9790e-01,\n",
       "             9.9778e-01, 9.9762e-01, 9.9755e-01, 9.9748e-01, 9.9723e-01, 9.9718e-01,\n",
       "             9.9718e-01, 9.9710e-01, 9.9701e-01, 9.9667e-01, 9.9653e-01, 9.9643e-01,\n",
       "             9.9585e-01, 9.9541e-01, 9.9499e-01, 9.9475e-01, 9.9470e-01, 9.9468e-01,\n",
       "             9.9371e-01, 9.9325e-01, 9.9081e-01, 9.8998e-01, 9.8961e-01, 9.8853e-01,\n",
       "             9.8762e-01, 9.8601e-01, 9.8160e-01, 9.8137e-01, 9.7693e-01, 9.7485e-01,\n",
       "             9.7320e-01, 9.7251e-01, 9.5437e-01, 9.4474e-01, 9.3819e-01, 9.3452e-01,\n",
       "             9.3281e-01, 9.2619e-01, 9.2190e-01, 9.2118e-01, 9.0948e-01, 8.9884e-01,\n",
       "             8.9763e-01, 8.8447e-01, 8.8002e-01, 8.7278e-01, 8.2909e-01, 8.1117e-01,\n",
       "             7.4710e-01, 7.0857e-01, 6.8761e-01, 6.3137e-01, 6.1883e-01, 6.1426e-01,\n",
       "             5.6034e-01, 5.5564e-01, 5.5268e-01, 4.5475e-01, 4.4861e-01, 4.1476e-01,\n",
       "             4.1262e-01, 3.6714e-01, 3.5268e-01, 3.5116e-01, 3.3143e-01, 3.2318e-01,\n",
       "             3.1627e-01, 2.9906e-01, 2.9016e-01, 2.5032e-01, 2.4424e-01, 2.2481e-01,\n",
       "             2.1605e-01, 1.9615e-01, 1.9031e-01, 1.7479e-01, 1.6427e-01, 1.5932e-01,\n",
       "             1.2004e-01, 8.0746e-02, 7.7505e-02, 7.3993e-02, 5.6781e-02, 4.7744e-02,\n",
       "             4.7442e-02, 3.6288e-02, 2.9941e-02, 2.3918e-02, 1.4726e-02, 1.2548e-02,\n",
       "             1.1024e-02, 7.2811e-03, 6.9464e-03, 5.1327e-03, 4.4790e-03, 3.4883e-03,\n",
       "             3.2338e-03, 3.1267e-03, 3.0805e-03, 3.0772e-03, 2.5541e-03, 2.1386e-03,\n",
       "             1.7856e-03, 1.7125e-03, 1.6529e-03, 1.4799e-03, 1.3038e-03, 1.2765e-03,\n",
       "             1.2568e-03, 1.2094e-03, 1.0032e-03, 9.3085e-04, 6.8114e-04, 6.7546e-04,\n",
       "             5.9819e-04, 5.8845e-04, 3.7878e-04, 3.1906e-04, 3.0933e-04, 2.9431e-04,\n",
       "             2.5167e-04, 2.4271e-04, 1.9148e-04, 1.8605e-04, 1.3951e-04, 1.1680e-04,\n",
       "             1.0449e-04, 1.0408e-04, 8.7410e-05, 8.4152e-05, 7.7820e-05, 7.0115e-05,\n",
       "             6.2726e-05, 6.0249e-05, 5.1524e-05, 5.1313e-05, 5.1107e-05, 3.7104e-05,\n",
       "             2.5763e-05, 2.3976e-05, 2.0699e-05, 1.9628e-05, 1.6198e-05, 1.5798e-05,\n",
       "             1.5100e-05, 1.4036e-05, 1.3947e-05, 1.3137e-05, 1.2290e-05, 1.1462e-05,\n",
       "             1.1153e-05, 9.8735e-06, 9.4571e-06, 9.1280e-06, 8.0432e-06, 7.3487e-06,\n",
       "             6.1999e-06, 6.0435e-06, 5.9848e-06, 5.4191e-06, 4.8621e-06, 3.8307e-06,\n",
       "             3.7394e-06, 2.7483e-06, 2.6689e-06, 2.3328e-06, 2.1916e-06, 2.0860e-06,\n",
       "             2.0426e-06, 1.6262e-06, 1.3876e-06, 1.3723e-06, 1.1720e-06, 1.0915e-06,\n",
       "             1.0206e-06, 8.7618e-07, 8.0690e-07, 7.3134e-07, 6.8477e-07, 6.5805e-07,\n",
       "             6.2887e-07, 6.0901e-07, 5.3681e-07, 5.1637e-07, 4.8691e-07, 4.7592e-07,\n",
       "             4.2648e-07, 4.1511e-07, 3.0958e-07, 2.7425e-07, 2.6628e-07, 2.5798e-07,\n",
       "             2.5152e-07, 2.3480e-07, 2.1557e-07, 2.0915e-07, 2.0810e-07, 2.0539e-07,\n",
       "             2.0326e-07, 1.9147e-07, 1.8271e-07, 1.7508e-07, 1.6481e-07, 1.6462e-07,\n",
       "             1.6058e-07, 1.5934e-07, 1.5278e-07, 1.4674e-07, 1.4173e-07, 1.3175e-07,\n",
       "             1.0872e-07, 9.9187e-08, 7.5714e-08, 6.6292e-08, 6.4657e-08, 6.3316e-08,\n",
       "             6.0547e-08, 5.4240e-08, 4.9678e-08, 4.4644e-08, 4.3812e-08, 4.0117e-08,\n",
       "             3.7943e-08, 3.6890e-08, 3.6864e-08, 3.5823e-08, 3.4727e-08, 3.1844e-08,\n",
       "             2.8012e-08, 2.5434e-08, 2.3903e-08, 2.3007e-08, 2.0867e-08, 1.9465e-08,\n",
       "             1.8240e-08, 1.6362e-08, 1.5344e-08, 1.5131e-08, 1.4249e-08, 1.3960e-08,\n",
       "             1.2475e-08, 1.2287e-08, 1.1889e-08, 1.1340e-08, 9.9627e-09, 9.7106e-09,\n",
       "             9.5872e-09, 9.1852e-09, 8.7978e-09, 8.5040e-09, 7.5546e-09, 7.4998e-09,\n",
       "             7.1587e-09, 7.0364e-09, 6.9961e-09, 6.7620e-09, 6.1192e-09, 5.9898e-09,\n",
       "             5.5379e-09, 5.1393e-09, 4.6927e-09, 4.0943e-09, 3.6806e-09, 3.4936e-09,\n",
       "             3.4305e-09, 3.0533e-09, 2.8772e-09, 2.8179e-09, 2.5037e-09, 2.2963e-09,\n",
       "             2.1820e-09, 1.7508e-09, 1.7001e-09, 1.6145e-09, 1.5068e-09, 1.4498e-09,\n",
       "             1.4323e-09, 1.4182e-09, 1.4000e-09, 1.3514e-09, 1.2825e-09, 1.2622e-09,\n",
       "             1.2433e-09, 1.2096e-09, 1.2032e-09, 1.1431e-09, 1.1365e-09, 1.0926e-09,\n",
       "             9.8657e-10, 9.5721e-10, 8.4951e-10, 8.4165e-10, 8.3734e-10, 8.1993e-10,\n",
       "             7.6377e-10, 7.6197e-10, 6.9018e-10, 6.6606e-10, 6.4284e-10, 6.2750e-10,\n",
       "             6.2553e-10, 6.1923e-10, 6.1076e-10, 5.9699e-10, 5.7369e-10, 4.6375e-10,\n",
       "             4.5587e-10, 4.3496e-10, 4.3314e-10, 4.2454e-10, 3.8816e-10, 3.4933e-10,\n",
       "             3.4700e-10, 3.4288e-10, 3.4143e-10, 3.0353e-10, 2.9851e-10, 2.7758e-10,\n",
       "             2.5951e-10, 2.3011e-10, 2.1222e-10, 1.8993e-10, 1.8992e-10, 1.8929e-10,\n",
       "             1.8149e-10, 1.6925e-10, 1.6670e-10, 1.6399e-10, 1.6140e-10, 1.4961e-10,\n",
       "             1.4751e-10, 1.3188e-10, 1.3128e-10, 1.2582e-10, 1.1849e-10, 1.1680e-10,\n",
       "             1.1555e-10, 1.1436e-10, 9.4106e-11, 9.3529e-11, 9.2695e-11, 9.0956e-11,\n",
       "             8.6579e-11, 8.5701e-11, 5.7733e-11, 4.9161e-11, 4.5034e-11, 4.4180e-11,\n",
       "             3.9947e-11, 3.8105e-11, 3.7153e-11, 3.6801e-11, 3.4260e-11, 3.1776e-11,\n",
       "             3.0642e-11, 3.0402e-11, 2.9459e-11, 2.8090e-11, 2.7871e-11, 2.6827e-11,\n",
       "             2.4956e-11, 2.4579e-11, 2.4359e-11, 2.0325e-11, 1.8569e-11, 1.7682e-11,\n",
       "             1.7364e-11, 1.6408e-11, 1.6049e-11, 1.5552e-11, 1.5111e-11, 1.2909e-11,\n",
       "             1.2268e-11, 1.2037e-11, 1.2001e-11, 1.1745e-11, 9.7578e-12, 9.3940e-12,\n",
       "             8.8518e-12, 6.7811e-12, 6.0550e-12, 5.6355e-12, 5.4720e-12, 4.3277e-12,\n",
       "             4.2298e-12, 4.0758e-12, 3.9037e-12, 3.4314e-12, 2.9898e-12, 2.4336e-12,\n",
       "             2.3719e-12, 2.3610e-12, 2.2809e-12, 1.9870e-12, 1.4110e-12, 1.3446e-12,\n",
       "             1.3242e-12, 1.3136e-12, 1.2927e-12, 1.2035e-12, 1.2030e-12, 1.1780e-12,\n",
       "             1.1475e-12, 1.0048e-12, 8.6708e-13, 8.2131e-13, 6.9769e-13, 6.6621e-13,\n",
       "             6.6408e-13, 6.4631e-13, 5.6907e-13, 5.5538e-13, 5.1957e-13, 5.1243e-13,\n",
       "             5.0446e-13, 4.7717e-13, 4.6072e-13, 4.2784e-13, 4.2236e-13, 4.0307e-13,\n",
       "             4.0282e-13, 3.9861e-13, 3.8746e-13, 3.6381e-13, 2.9187e-13, 2.8218e-13,\n",
       "             2.7047e-13, 2.6692e-13, 2.3183e-13, 2.2973e-13, 2.1058e-13, 1.8673e-13,\n",
       "             1.8517e-13, 1.7164e-13, 1.1671e-13, 9.7569e-14, 9.2330e-14, 8.5313e-14,\n",
       "             8.3366e-14, 8.0828e-14, 7.5180e-14, 7.5124e-14, 6.8275e-14, 6.3002e-14,\n",
       "             5.2358e-14, 4.9771e-14, 3.7791e-14, 3.6990e-14, 3.3466e-14, 3.3309e-14,\n",
       "             3.2286e-14, 3.1606e-14, 2.3391e-14, 2.1107e-14, 2.0022e-14, 1.8082e-14,\n",
       "             1.5286e-14, 1.2903e-14, 1.1106e-14, 9.3340e-15, 7.4493e-15, 5.2855e-15,\n",
       "             5.2296e-15, 5.0040e-15, 4.0884e-15, 3.9522e-15, 2.6068e-15, 2.2437e-15,\n",
       "             2.1318e-15, 1.7707e-15, 1.5725e-15, 1.5229e-15, 1.4057e-15, 1.3509e-15,\n",
       "             1.1155e-15, 9.3343e-16, 8.3585e-16, 8.2832e-16, 7.1953e-16, 6.8383e-16,\n",
       "             6.5350e-16, 6.4039e-16, 6.0690e-16, 4.4995e-16, 4.4798e-16, 3.8324e-16,\n",
       "             3.5236e-16, 3.2839e-16, 3.0915e-16, 2.5585e-16, 2.5529e-16, 2.2204e-16,\n",
       "             1.6107e-16, 1.6082e-16, 1.4154e-16, 1.2242e-16, 3.3759e-17, 1.4603e-17,\n",
       "             5.6937e-18, 5.6347e-18, 5.2501e-18, 4.5471e-18, 4.4963e-18, 4.4037e-18,\n",
       "             3.8650e-18, 3.0725e-18, 1.8441e-18, 1.7534e-18, 6.7114e-19, 1.7589e-19,\n",
       "             2.4331e-20, 3.3215e-21, 1.3167e-21, 1.5582e-22, 6.4714e-23, 6.5863e-24,\n",
       "             1.3684e-24])}},\n",
       "   {'fpr': np.float64(0.07929515418502203),\n",
       "    'tpr': np.float64(0.9837837837837838),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0198, 0.0198, 0.0198, 0.0220, 0.0220, 0.0220, 0.0242, 0.0242,\n",
       "             0.0242, 0.0242, 0.0242, 0.0242, 0.0264, 0.0264, 0.0264, 0.0264, 0.0286,\n",
       "             0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286,\n",
       "             0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286, 0.0286,\n",
       "             0.0286, 0.0286, 0.0286, 0.0308, 0.0308, 0.0308, 0.0308, 0.0308, 0.0308,\n",
       "             0.0308, 0.0308, 0.0308, 0.0308, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330,\n",
       "             0.0330, 0.0330, 0.0330, 0.0330, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352,\n",
       "             0.0352, 0.0352, 0.0374, 0.0374, 0.0374, 0.0396, 0.0396, 0.0419, 0.0441,\n",
       "             0.0441, 0.0441, 0.0441, 0.0463, 0.0463, 0.0463, 0.0463, 0.0463, 0.0463,\n",
       "             0.0463, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485,\n",
       "             0.0507, 0.0507, 0.0507, 0.0507, 0.0507, 0.0529, 0.0529, 0.0529, 0.0529,\n",
       "             0.0529, 0.0551, 0.0551, 0.0551, 0.0573, 0.0595, 0.0617, 0.0617, 0.0639,\n",
       "             0.0639, 0.0661, 0.0683, 0.0683, 0.0705, 0.0705, 0.0727, 0.0727, 0.0727,\n",
       "             0.0749, 0.0771, 0.0793, 0.0793, 0.0793, 0.0815, 0.0837, 0.0837, 0.0837,\n",
       "             0.0859, 0.0881, 0.0903, 0.0903, 0.0925, 0.0947, 0.0969, 0.0991, 0.1013,\n",
       "             0.1035, 0.1057, 0.1079, 0.1101, 0.1123, 0.1145, 0.1167, 0.1189, 0.1189,\n",
       "             0.1211, 0.1233, 0.1233, 0.1256, 0.1278, 0.1278, 0.1278, 0.1278, 0.1300,\n",
       "             0.1300, 0.1322, 0.1344, 0.1366, 0.1388, 0.1410, 0.1432, 0.1454, 0.1454,\n",
       "             0.1476, 0.1498, 0.1520, 0.1542, 0.1564, 0.1586, 0.1586, 0.1608, 0.1630,\n",
       "             0.1652, 0.1674, 0.1696, 0.1718, 0.1740, 0.1762, 0.1784, 0.1806, 0.1828,\n",
       "             0.1850, 0.1850, 0.1850, 0.1872, 0.1894, 0.1916, 0.1938, 0.1960, 0.1982,\n",
       "             0.2004, 0.2026, 0.2048, 0.2070, 0.2093, 0.2115, 0.2137, 0.2137, 0.2159,\n",
       "             0.2181, 0.2181, 0.2203, 0.2225, 0.2247, 0.2269, 0.2291, 0.2313, 0.2335,\n",
       "             0.2357, 0.2379, 0.2401, 0.2401, 0.2423, 0.2445, 0.2467, 0.2489, 0.2511,\n",
       "             0.2533, 0.2555, 0.2577, 0.2599, 0.2621, 0.2643, 0.2665, 0.2687, 0.2709,\n",
       "             0.2731, 0.2753, 0.2775, 0.2797, 0.2819, 0.2841, 0.2863, 0.2885, 0.2907,\n",
       "             0.2930, 0.2952, 0.2974, 0.2996, 0.3018, 0.3040, 0.3062, 0.3084, 0.3084,\n",
       "             0.3106, 0.3128, 0.3150, 0.3172, 0.3194, 0.3216, 0.3238, 0.3260, 0.3282,\n",
       "             0.3304, 0.3326, 0.3348, 0.3370, 0.3392, 0.3414, 0.3436, 0.3458, 0.3480,\n",
       "             0.3502, 0.3524, 0.3546, 0.3568, 0.3590, 0.3612, 0.3634, 0.3656, 0.3678,\n",
       "             0.3700, 0.3722, 0.3744, 0.3767, 0.3789, 0.3811, 0.3833, 0.3855, 0.3877,\n",
       "             0.3899, 0.3921, 0.3943, 0.3965, 0.3987, 0.4009, 0.4031, 0.4053, 0.4075,\n",
       "             0.4097, 0.4119, 0.4141, 0.4163, 0.4185, 0.4207, 0.4229, 0.4251, 0.4273,\n",
       "             0.4295, 0.4317, 0.4339, 0.4361, 0.4383, 0.4405, 0.4427, 0.4449, 0.4471,\n",
       "             0.4493, 0.4515, 0.4537, 0.4559, 0.4559, 0.4581, 0.4604, 0.4626, 0.4648,\n",
       "             0.4670, 0.4692, 0.4714, 0.4736, 0.4758, 0.4780, 0.4802, 0.4824, 0.4846,\n",
       "             0.4868, 0.4890, 0.4912, 0.4934, 0.4956, 0.4978, 0.5000, 0.5022, 0.5044,\n",
       "             0.5066, 0.5088, 0.5110, 0.5132, 0.5154, 0.5176, 0.5198, 0.5220, 0.5242,\n",
       "             0.5264, 0.5286, 0.5308, 0.5330, 0.5352, 0.5374, 0.5396, 0.5419, 0.5441,\n",
       "             0.5463, 0.5485, 0.5507, 0.5529, 0.5551, 0.5573, 0.5595, 0.5617, 0.5639,\n",
       "             0.5661, 0.5683, 0.5705, 0.5727, 0.5749, 0.5771, 0.5793, 0.5815, 0.5837,\n",
       "             0.5859, 0.5881, 0.5881, 0.5903, 0.5925, 0.5947, 0.5969, 0.5991, 0.6013,\n",
       "             0.6035, 0.6057, 0.6079, 0.6101, 0.6123, 0.6145, 0.6167, 0.6189, 0.6211,\n",
       "             0.6233, 0.6256, 0.6278, 0.6300, 0.6322, 0.6344, 0.6366, 0.6388, 0.6410,\n",
       "             0.6432, 0.6454, 0.6476, 0.6498, 0.6520, 0.6542, 0.6564, 0.6586, 0.6608,\n",
       "             0.6630, 0.6652, 0.6674, 0.6696, 0.6718, 0.6740, 0.6762, 0.6784, 0.6806,\n",
       "             0.6828, 0.6850, 0.6872, 0.6894, 0.6916, 0.6938, 0.6960, 0.6982, 0.7004,\n",
       "             0.7026, 0.7048, 0.7070, 0.7093, 0.7115, 0.7137, 0.7159, 0.7181, 0.7203,\n",
       "             0.7225, 0.7247, 0.7269, 0.7291, 0.7313, 0.7335, 0.7357, 0.7379, 0.7401,\n",
       "             0.7423, 0.7445, 0.7467, 0.7489, 0.7511, 0.7533, 0.7555, 0.7577, 0.7599,\n",
       "             0.7621, 0.7643, 0.7665, 0.7687, 0.7709, 0.7731, 0.7753, 0.7775, 0.7797,\n",
       "             0.7819, 0.7841, 0.7863, 0.7885, 0.7907, 0.7930, 0.7952, 0.7974, 0.7996,\n",
       "             0.8018, 0.8040, 0.8062, 0.8084, 0.8106, 0.8128, 0.8150, 0.8172, 0.8194,\n",
       "             0.8216, 0.8238, 0.8260, 0.8282, 0.8304, 0.8326, 0.8348, 0.8370, 0.8392,\n",
       "             0.8414, 0.8436, 0.8458, 0.8480, 0.8502, 0.8524, 0.8546, 0.8568, 0.8590,\n",
       "             0.8612, 0.8634, 0.8656, 0.8678, 0.8700, 0.8722, 0.8744, 0.8767, 0.8789,\n",
       "             0.8811, 0.8833, 0.8855, 0.8877, 0.8899, 0.8921, 0.8943, 0.8965, 0.8987,\n",
       "             0.9009, 0.9031, 0.9053, 0.9075, 0.9097, 0.9119, 0.9141, 0.9163, 0.9185,\n",
       "             0.9207, 0.9229, 0.9251, 0.9273, 0.9295, 0.9317, 0.9339, 0.9361, 0.9383,\n",
       "             0.9405, 0.9427, 0.9449, 0.9471, 0.9493, 0.9515, 0.9537, 0.9559, 0.9581,\n",
       "             0.9604, 0.9626, 0.9648, 0.9670, 0.9692, 0.9714, 0.9736, 0.9758, 0.9780,\n",
       "             0.9802, 0.9824, 0.9846, 0.9868, 0.9890, 0.9912, 0.9934, 0.9956, 0.9978,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8703, 0.8950, 0.9019, 0.9042, 0.9058, 0.9073, 0.9073, 0.9097,\n",
       "             0.9120, 0.9127, 0.9135, 0.9151, 0.9158, 0.9166, 0.9174, 0.9181, 0.9181,\n",
       "             0.9189, 0.9197, 0.9205, 0.9212, 0.9220, 0.9236, 0.9251, 0.9266, 0.9274,\n",
       "             0.9282, 0.9290, 0.9297, 0.9313, 0.9320, 0.9336, 0.9344, 0.9351, 0.9359,\n",
       "             0.9367, 0.9375, 0.9382, 0.9390, 0.9398, 0.9405, 0.9413, 0.9421, 0.9429,\n",
       "             0.9436, 0.9444, 0.9452, 0.9459, 0.9459, 0.9467, 0.9475, 0.9483, 0.9490,\n",
       "             0.9498, 0.9506, 0.9514, 0.9521, 0.9521, 0.9529, 0.9537, 0.9544, 0.9552,\n",
       "             0.9560, 0.9568, 0.9568, 0.9575, 0.9583, 0.9583, 0.9591, 0.9591, 0.9591,\n",
       "             0.9598, 0.9606, 0.9614, 0.9614, 0.9622, 0.9629, 0.9637, 0.9645, 0.9653,\n",
       "             0.9660, 0.9660, 0.9668, 0.9676, 0.9683, 0.9691, 0.9699, 0.9707, 0.9714,\n",
       "             0.9714, 0.9722, 0.9730, 0.9737, 0.9745, 0.9745, 0.9753, 0.9761, 0.9768,\n",
       "             0.9776, 0.9776, 0.9784, 0.9792, 0.9792, 0.9792, 0.9792, 0.9799, 0.9799,\n",
       "             0.9807, 0.9807, 0.9807, 0.9815, 0.9815, 0.9822, 0.9822, 0.9830, 0.9838,\n",
       "             0.9838, 0.9838, 0.9838, 0.9846, 0.9853, 0.9853, 0.9853, 0.9861, 0.9869,\n",
       "             0.9869, 0.9869, 0.9869, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876,\n",
       "             0.9876, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876, 0.9884,\n",
       "             0.9884, 0.9884, 0.9892, 0.9892, 0.9892, 0.9900, 0.9907, 0.9915, 0.9915,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961,\n",
       "             0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9989e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9975e-01, 9.9974e-01, 9.9967e-01, 9.9966e-01, 9.9964e-01,\n",
       "             9.9961e-01, 9.9948e-01, 9.9943e-01, 9.9939e-01, 9.9935e-01, 9.9932e-01,\n",
       "             9.9918e-01, 9.9891e-01, 9.9863e-01, 9.9857e-01, 9.9854e-01, 9.9842e-01,\n",
       "             9.9832e-01, 9.9816e-01, 9.9795e-01, 9.9712e-01, 9.9669e-01, 9.9644e-01,\n",
       "             9.9466e-01, 9.9140e-01, 9.8504e-01, 9.8173e-01, 9.8073e-01, 9.7773e-01,\n",
       "             9.7740e-01, 9.7548e-01, 9.5557e-01, 9.4642e-01, 9.4273e-01, 9.2892e-01,\n",
       "             9.1683e-01, 9.1586e-01, 9.0633e-01, 8.9667e-01, 8.9072e-01, 8.5462e-01,\n",
       "             8.2625e-01, 8.0994e-01, 7.9968e-01, 7.1937e-01, 6.9380e-01, 6.4985e-01,\n",
       "             4.8488e-01, 4.8072e-01, 4.6816e-01, 4.6621e-01, 4.6376e-01, 4.2218e-01,\n",
       "             3.4128e-01, 3.3445e-01, 3.2638e-01, 3.0377e-01, 2.9919e-01, 2.8761e-01,\n",
       "             2.3709e-01, 2.0261e-01, 1.9947e-01, 1.6227e-01, 1.5464e-01, 1.1286e-01,\n",
       "             7.6782e-02, 7.4784e-02, 7.4007e-02, 5.7486e-02, 5.4621e-02, 5.1516e-02,\n",
       "             4.6337e-02, 4.5220e-02, 4.1764e-02, 3.5975e-02, 3.3226e-02, 3.2324e-02,\n",
       "             3.0361e-02, 2.3609e-02, 1.9031e-02, 1.6130e-02, 1.5926e-02, 1.0826e-02,\n",
       "             7.5930e-03, 6.8535e-03, 6.8159e-03, 5.8958e-03, 5.3852e-03, 4.9977e-03,\n",
       "             4.8535e-03, 4.7287e-03, 4.4501e-03, 3.4642e-03, 3.2990e-03, 2.9877e-03,\n",
       "             2.9268e-03, 1.7941e-03, 1.6959e-03, 1.6266e-03, 1.2853e-03, 1.2217e-03,\n",
       "             1.0866e-03, 8.5623e-04, 8.2302e-04, 7.0742e-04, 6.0195e-04, 5.6346e-04,\n",
       "             3.7611e-04, 2.6560e-04, 1.5411e-04, 1.4725e-04, 1.3073e-04, 1.2768e-04,\n",
       "             1.2274e-04, 1.1379e-04, 1.0975e-04, 1.0020e-04, 9.8008e-05, 9.4302e-05,\n",
       "             7.5954e-05, 7.5950e-05, 6.1970e-05, 5.6509e-05, 5.4924e-05, 4.6751e-05,\n",
       "             4.4320e-05, 3.6486e-05, 3.5802e-05, 3.4581e-05, 3.3466e-05, 3.0559e-05,\n",
       "             3.0009e-05, 2.3440e-05, 1.6905e-05, 1.6257e-05, 1.5483e-05, 1.3457e-05,\n",
       "             1.0669e-05, 9.8731e-06, 8.5023e-06, 8.1779e-06, 8.1531e-06, 6.3817e-06,\n",
       "             4.3808e-06, 4.1148e-06, 4.0365e-06, 3.8639e-06, 3.5551e-06, 3.1904e-06,\n",
       "             2.7001e-06, 2.2701e-06, 1.8197e-06, 1.7900e-06, 1.7814e-06, 1.7445e-06,\n",
       "             1.6615e-06, 1.5930e-06, 1.5256e-06, 1.3118e-06, 1.2571e-06, 1.2210e-06,\n",
       "             1.2082e-06, 9.6159e-07, 8.1859e-07, 8.0726e-07, 5.8853e-07, 5.6602e-07,\n",
       "             5.1142e-07, 3.6457e-07, 3.5205e-07, 2.9260e-07, 2.8328e-07, 2.5767e-07,\n",
       "             2.5270e-07, 2.4552e-07, 2.1868e-07, 2.1360e-07, 1.8714e-07, 1.5101e-07,\n",
       "             1.4987e-07, 1.4873e-07, 1.3053e-07, 1.2365e-07, 1.1664e-07, 1.1108e-07,\n",
       "             1.0955e-07, 9.9409e-08, 9.6663e-08, 9.0312e-08, 8.9578e-08, 8.7484e-08,\n",
       "             8.6438e-08, 8.4029e-08, 8.2579e-08, 7.6038e-08, 6.5365e-08, 5.6777e-08,\n",
       "             4.9312e-08, 4.8234e-08, 4.4417e-08, 4.4039e-08, 4.3746e-08, 4.3497e-08,\n",
       "             4.3195e-08, 3.9667e-08, 3.8629e-08, 3.0578e-08, 2.9422e-08, 2.5354e-08,\n",
       "             2.3667e-08, 2.1399e-08, 2.1333e-08, 1.9860e-08, 1.8051e-08, 1.7801e-08,\n",
       "             1.4811e-08, 1.4054e-08, 1.3985e-08, 1.2974e-08, 1.0993e-08, 1.0947e-08,\n",
       "             1.0198e-08, 9.4567e-09, 8.9065e-09, 8.8694e-09, 8.1711e-09, 7.2431e-09,\n",
       "             6.9403e-09, 6.4996e-09, 6.2450e-09, 5.9935e-09, 5.8384e-09, 5.6507e-09,\n",
       "             5.5644e-09, 5.2673e-09, 4.7751e-09, 4.7052e-09, 4.4500e-09, 4.2023e-09,\n",
       "             4.1981e-09, 3.9927e-09, 3.9264e-09, 3.8422e-09, 3.7264e-09, 3.3397e-09,\n",
       "             3.2796e-09, 3.1552e-09, 2.9576e-09, 2.7736e-09, 2.6373e-09, 2.6106e-09,\n",
       "             1.8463e-09, 1.7245e-09, 1.7117e-09, 1.6640e-09, 1.6598e-09, 1.4557e-09,\n",
       "             1.3706e-09, 1.3340e-09, 1.2212e-09, 1.1320e-09, 1.1006e-09, 1.0531e-09,\n",
       "             9.2518e-10, 8.6433e-10, 7.6022e-10, 7.1707e-10, 7.0257e-10, 6.4864e-10,\n",
       "             5.7759e-10, 5.6973e-10, 5.4473e-10, 5.3144e-10, 5.1743e-10, 4.8085e-10,\n",
       "             3.9746e-10, 3.9621e-10, 3.7852e-10, 3.5632e-10, 3.0849e-10, 3.0644e-10,\n",
       "             3.0250e-10, 3.0110e-10, 2.8790e-10, 2.7560e-10, 2.6999e-10, 2.6524e-10,\n",
       "             2.6380e-10, 2.5766e-10, 1.7911e-10, 1.7833e-10, 1.7466e-10, 1.6487e-10,\n",
       "             1.6372e-10, 1.6158e-10, 1.2694e-10, 1.2588e-10, 1.2106e-10, 1.1134e-10,\n",
       "             1.0171e-10, 1.0070e-10, 9.9093e-11, 9.1475e-11, 8.8011e-11, 8.6836e-11,\n",
       "             7.3903e-11, 7.1488e-11, 6.4478e-11, 6.4308e-11, 6.2926e-11, 5.8153e-11,\n",
       "             5.3842e-11, 5.1185e-11, 5.0339e-11, 4.7846e-11, 4.2997e-11, 4.2900e-11,\n",
       "             3.8114e-11, 3.6588e-11, 3.1004e-11, 2.9995e-11, 2.9678e-11, 2.7325e-11,\n",
       "             2.7306e-11, 2.5503e-11, 2.5133e-11, 2.4810e-11, 2.4458e-11, 2.2023e-11,\n",
       "             1.7178e-11, 1.7055e-11, 1.6951e-11, 1.6222e-11, 1.6188e-11, 1.5681e-11,\n",
       "             1.3676e-11, 1.2839e-11, 1.0347e-11, 9.1079e-12, 8.8769e-12, 7.6248e-12,\n",
       "             7.0572e-12, 5.4018e-12, 3.6158e-12, 3.3810e-12, 3.2675e-12, 3.1413e-12,\n",
       "             2.7239e-12, 2.4597e-12, 2.3826e-12, 2.3474e-12, 2.3099e-12, 2.1488e-12,\n",
       "             2.0653e-12, 1.9489e-12, 1.7511e-12, 1.6482e-12, 1.6355e-12, 1.6042e-12,\n",
       "             1.3890e-12, 1.3816e-12, 1.2353e-12, 1.1615e-12, 1.1519e-12, 1.0400e-12,\n",
       "             1.0380e-12, 1.0379e-12, 1.0198e-12, 1.0188e-12, 1.0097e-12, 8.6005e-13,\n",
       "             8.1309e-13, 7.3258e-13, 6.9126e-13, 6.3767e-13, 6.2025e-13, 6.0860e-13,\n",
       "             5.8581e-13, 4.6746e-13, 4.6284e-13, 4.0947e-13, 3.7513e-13, 2.6566e-13,\n",
       "             2.4528e-13, 2.1678e-13, 2.1562e-13, 1.9949e-13, 1.9086e-13, 1.7257e-13,\n",
       "             1.5909e-13, 1.4183e-13, 1.2423e-13, 1.2320e-13, 1.1098e-13, 9.2155e-14,\n",
       "             8.4599e-14, 7.3887e-14, 7.1018e-14, 6.9745e-14, 6.2769e-14, 6.0706e-14,\n",
       "             6.0585e-14, 5.5838e-14, 5.0069e-14, 4.6619e-14, 4.6597e-14, 3.7561e-14,\n",
       "             2.3607e-14, 2.3487e-14, 1.8809e-14, 1.8705e-14, 1.5886e-14, 1.2823e-14,\n",
       "             1.1402e-14, 1.0317e-14, 9.0628e-15, 7.4490e-15, 5.3923e-15, 4.2954e-15,\n",
       "             3.8680e-15, 3.5462e-15, 3.4595e-15, 3.3248e-15, 3.1317e-15, 3.0402e-15,\n",
       "             2.8323e-15, 2.0166e-15, 1.2247e-15, 1.1857e-15, 1.0689e-15, 1.0002e-15,\n",
       "             9.7652e-16, 8.6447e-16, 6.4696e-16, 5.8226e-16, 5.4611e-16, 4.9880e-16,\n",
       "             4.5191e-16, 4.1348e-16, 3.0669e-16, 2.6726e-16, 2.6380e-16, 2.5513e-16,\n",
       "             2.3475e-16, 1.6304e-16, 1.6163e-16, 1.5584e-16, 1.4253e-16, 1.1347e-16,\n",
       "             9.0440e-17, 8.9125e-17, 5.2763e-17, 4.6433e-17, 4.4963e-17, 4.4265e-17,\n",
       "             2.7813e-17, 1.6603e-17, 1.5259e-17, 1.4506e-17, 9.1103e-18, 5.0161e-18,\n",
       "             4.9320e-18, 4.5716e-18, 3.6639e-18, 2.6238e-18, 2.0976e-18, 1.5649e-18,\n",
       "             1.1949e-18, 1.1694e-18, 1.1487e-18, 1.0751e-18, 8.5037e-19, 5.4754e-19,\n",
       "             4.1737e-19, 3.5541e-19, 2.2543e-19, 2.0923e-19, 1.7231e-19, 1.6258e-19,\n",
       "             1.2661e-20, 4.4486e-21, 3.2368e-23, 2.3713e-25, 1.0057e-25, 1.0691e-26,\n",
       "             2.9312e-27])}},\n",
       "   {'fpr': np.float64(0.07268722466960352),\n",
       "    'tpr': np.float64(0.983011583011583),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0110, 0.0110, 0.0110,\n",
       "             0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
       "             0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
       "             0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
       "             0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0176, 0.0176,\n",
       "             0.0198, 0.0198, 0.0198, 0.0198, 0.0198, 0.0198, 0.0198, 0.0198, 0.0198,\n",
       "             0.0198, 0.0198, 0.0198, 0.0220, 0.0220, 0.0220, 0.0220, 0.0242, 0.0242,\n",
       "             0.0242, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264,\n",
       "             0.0286, 0.0286, 0.0308, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330,\n",
       "             0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0352, 0.0374,\n",
       "             0.0374, 0.0374, 0.0374, 0.0396, 0.0396, 0.0396, 0.0396, 0.0396, 0.0396,\n",
       "             0.0419, 0.0441, 0.0441, 0.0463, 0.0463, 0.0463, 0.0463, 0.0463, 0.0463,\n",
       "             0.0485, 0.0485, 0.0507, 0.0507, 0.0507, 0.0507, 0.0507, 0.0529, 0.0551,\n",
       "             0.0573, 0.0595, 0.0595, 0.0595, 0.0617, 0.0617, 0.0639, 0.0661, 0.0683,\n",
       "             0.0705, 0.0705, 0.0705, 0.0727, 0.0727, 0.0749, 0.0771, 0.0793, 0.0815,\n",
       "             0.0837, 0.0837, 0.0837, 0.0859, 0.0881, 0.0881, 0.0903, 0.0903, 0.0925,\n",
       "             0.0947, 0.0969, 0.0991, 0.1013, 0.1035, 0.1057, 0.1057, 0.1057, 0.1079,\n",
       "             0.1079, 0.1101, 0.1123, 0.1123, 0.1123, 0.1145, 0.1167, 0.1167, 0.1189,\n",
       "             0.1189, 0.1211, 0.1233, 0.1233, 0.1256, 0.1278, 0.1278, 0.1300, 0.1322,\n",
       "             0.1344, 0.1366, 0.1366, 0.1388, 0.1410, 0.1432, 0.1454, 0.1476, 0.1498,\n",
       "             0.1520, 0.1520, 0.1542, 0.1564, 0.1586, 0.1608, 0.1630, 0.1652, 0.1674,\n",
       "             0.1696, 0.1718, 0.1740, 0.1762, 0.1784, 0.1806, 0.1828, 0.1850, 0.1872,\n",
       "             0.1894, 0.1916, 0.1938, 0.1960, 0.1982, 0.2004, 0.2026, 0.2048, 0.2070,\n",
       "             0.2093, 0.2115, 0.2137, 0.2159, 0.2181, 0.2203, 0.2225, 0.2247, 0.2247,\n",
       "             0.2269, 0.2269, 0.2291, 0.2291, 0.2313, 0.2335, 0.2357, 0.2379, 0.2401,\n",
       "             0.2423, 0.2445, 0.2467, 0.2489, 0.2489, 0.2511, 0.2533, 0.2533, 0.2555,\n",
       "             0.2577, 0.2599, 0.2621, 0.2643, 0.2665, 0.2687, 0.2709, 0.2731, 0.2753,\n",
       "             0.2775, 0.2797, 0.2819, 0.2841, 0.2863, 0.2885, 0.2907, 0.2930, 0.2952,\n",
       "             0.2974, 0.2996, 0.3018, 0.3040, 0.3062, 0.3084, 0.3106, 0.3128, 0.3150,\n",
       "             0.3172, 0.3194, 0.3216, 0.3238, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348,\n",
       "             0.3370, 0.3392, 0.3414, 0.3436, 0.3458, 0.3480, 0.3502, 0.3524, 0.3546,\n",
       "             0.3568, 0.3590, 0.3612, 0.3634, 0.3656, 0.3678, 0.3700, 0.3722, 0.3744,\n",
       "             0.3767, 0.3789, 0.3811, 0.3833, 0.3855, 0.3877, 0.3899, 0.3921, 0.3943,\n",
       "             0.3965, 0.3987, 0.4009, 0.4031, 0.4053, 0.4075, 0.4097, 0.4119, 0.4141,\n",
       "             0.4163, 0.4185, 0.4207, 0.4229, 0.4251, 0.4273, 0.4295, 0.4317, 0.4339,\n",
       "             0.4361, 0.4383, 0.4405, 0.4427, 0.4449, 0.4471, 0.4493, 0.4515, 0.4537,\n",
       "             0.4559, 0.4581, 0.4604, 0.4626, 0.4648, 0.4670, 0.4692, 0.4714, 0.4736,\n",
       "             0.4758, 0.4780, 0.4802, 0.4824, 0.4846, 0.4868, 0.4890, 0.4912, 0.4934,\n",
       "             0.4956, 0.4978, 0.5000, 0.5022, 0.5044, 0.5066, 0.5088, 0.5110, 0.5132,\n",
       "             0.5154, 0.5176, 0.5198, 0.5220, 0.5242, 0.5264, 0.5286, 0.5308, 0.5330,\n",
       "             0.5330, 0.5352, 0.5374, 0.5396, 0.5419, 0.5441, 0.5463, 0.5485, 0.5507,\n",
       "             0.5529, 0.5551, 0.5573, 0.5595, 0.5617, 0.5639, 0.5661, 0.5683, 0.5705,\n",
       "             0.5727, 0.5749, 0.5771, 0.5793, 0.5815, 0.5837, 0.5859, 0.5881, 0.5903,\n",
       "             0.5925, 0.5947, 0.5969, 0.5991, 0.6013, 0.6035, 0.6057, 0.6079, 0.6101,\n",
       "             0.6123, 0.6145, 0.6167, 0.6189, 0.6211, 0.6233, 0.6256, 0.6278, 0.6300,\n",
       "             0.6322, 0.6344, 0.6366, 0.6388, 0.6388, 0.6410, 0.6432, 0.6454, 0.6476,\n",
       "             0.6498, 0.6520, 0.6542, 0.6564, 0.6586, 0.6608, 0.6630, 0.6652, 0.6674,\n",
       "             0.6696, 0.6718, 0.6740, 0.6762, 0.6784, 0.6806, 0.6828, 0.6850, 0.6872,\n",
       "             0.6894, 0.6916, 0.6938, 0.6960, 0.6982, 0.7004, 0.7026, 0.7048, 0.7070,\n",
       "             0.7093, 0.7115, 0.7137, 0.7159, 0.7181, 0.7203, 0.7225, 0.7247, 0.7269,\n",
       "             0.7291, 0.7313, 0.7335, 0.7357, 0.7379, 0.7401, 0.7423, 0.7445, 0.7467,\n",
       "             0.7489, 0.7511, 0.7533, 0.7555, 0.7577, 0.7599, 0.7621, 0.7643, 0.7665,\n",
       "             0.7687, 0.7709, 0.7731, 0.7753, 0.7775, 0.7797, 0.7819, 0.7841, 0.7863,\n",
       "             0.7885, 0.7907, 0.7930, 0.7952, 0.7974, 0.7996, 0.8018, 0.8040, 0.8062,\n",
       "             0.8084, 0.8106, 0.8128, 0.8150, 0.8172, 0.8194, 0.8216, 0.8238, 0.8260,\n",
       "             0.8282, 0.8304, 0.8326, 0.8348, 0.8370, 0.8392, 0.8414, 0.8436, 0.8458,\n",
       "             0.8480, 0.8502, 0.8524, 0.8546, 0.8568, 0.8590, 0.8612, 0.8634, 0.8656,\n",
       "             0.8678, 0.8700, 0.8722, 0.8744, 0.8767, 0.8789, 0.8811, 0.8833, 0.8855,\n",
       "             0.8877, 0.8899, 0.8921, 0.8943, 0.8965, 0.8987, 0.9009, 0.9031, 0.9053,\n",
       "             0.9075, 0.9097, 0.9119, 0.9141, 0.9163, 0.9185, 0.9207, 0.9229, 0.9251,\n",
       "             0.9273, 0.9295, 0.9317, 0.9339, 0.9361, 0.9383, 0.9405, 0.9427, 0.9449,\n",
       "             0.9471, 0.9493, 0.9515, 0.9537, 0.9559, 0.9581, 0.9604, 0.9626, 0.9648,\n",
       "             0.9670, 0.9692, 0.9714, 0.9736, 0.9758, 0.9780, 0.9802, 0.9824, 0.9846,\n",
       "             0.9868, 0.9890, 0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8054, 0.8347, 0.8463, 0.8602, 0.8664, 0.8718, 0.8757, 0.8772,\n",
       "             0.8811, 0.8834, 0.8865, 0.8880, 0.8896, 0.8911, 0.8919, 0.8934, 0.8942,\n",
       "             0.8958, 0.8965, 0.8973, 0.8981, 0.8996, 0.9004, 0.9019, 0.9027, 0.9042,\n",
       "             0.9058, 0.9066, 0.9081, 0.9089, 0.9097, 0.9104, 0.9120, 0.9120, 0.9127,\n",
       "             0.9135, 0.9143, 0.9158, 0.9181, 0.9189, 0.9197, 0.9205, 0.9212, 0.9220,\n",
       "             0.9220, 0.9228, 0.9236, 0.9243, 0.9251, 0.9259, 0.9266, 0.9274, 0.9282,\n",
       "             0.9290, 0.9297, 0.9305, 0.9313, 0.9320, 0.9328, 0.9336, 0.9344, 0.9351,\n",
       "             0.9351, 0.9359, 0.9367, 0.9382, 0.9390, 0.9398, 0.9405, 0.9413, 0.9421,\n",
       "             0.9429, 0.9436, 0.9444, 0.9444, 0.9452, 0.9459, 0.9467, 0.9467, 0.9475,\n",
       "             0.9483, 0.9483, 0.9490, 0.9498, 0.9506, 0.9514, 0.9521, 0.9529, 0.9537,\n",
       "             0.9537, 0.9544, 0.9544, 0.9544, 0.9552, 0.9560, 0.9568, 0.9575, 0.9583,\n",
       "             0.9591, 0.9598, 0.9606, 0.9614, 0.9622, 0.9629, 0.9637, 0.9637, 0.9637,\n",
       "             0.9645, 0.9653, 0.9660, 0.9660, 0.9668, 0.9676, 0.9683, 0.9691, 0.9699,\n",
       "             0.9699, 0.9699, 0.9707, 0.9707, 0.9714, 0.9722, 0.9730, 0.9737, 0.9745,\n",
       "             0.9745, 0.9753, 0.9753, 0.9761, 0.9768, 0.9776, 0.9784, 0.9784, 0.9784,\n",
       "             0.9784, 0.9784, 0.9792, 0.9799, 0.9799, 0.9807, 0.9807, 0.9807, 0.9807,\n",
       "             0.9807, 0.9815, 0.9822, 0.9822, 0.9830, 0.9830, 0.9830, 0.9830, 0.9830,\n",
       "             0.9830, 0.9838, 0.9846, 0.9846, 0.9846, 0.9853, 0.9853, 0.9861, 0.9861,\n",
       "             0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9869, 0.9876, 0.9876,\n",
       "             0.9884, 0.9884, 0.9884, 0.9892, 0.9900, 0.9900, 0.9900, 0.9907, 0.9907,\n",
       "             0.9915, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954,\n",
       "             0.9954, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9990e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9985e-01, 9.9983e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9966e-01, 9.9962e-01,\n",
       "             9.9961e-01, 9.9958e-01, 9.9956e-01, 9.9954e-01, 9.9945e-01, 9.9944e-01,\n",
       "             9.9944e-01, 9.9941e-01, 9.9941e-01, 9.9940e-01, 9.9940e-01, 9.9929e-01,\n",
       "             9.9922e-01, 9.9915e-01, 9.9913e-01, 9.9907e-01, 9.9907e-01, 9.9890e-01,\n",
       "             9.9884e-01, 9.9884e-01, 9.9880e-01, 9.9879e-01, 9.9864e-01, 9.9859e-01,\n",
       "             9.9857e-01, 9.9825e-01, 9.9818e-01, 9.9816e-01, 9.9800e-01, 9.9715e-01,\n",
       "             9.9715e-01, 9.9711e-01, 9.9697e-01, 9.9686e-01, 9.9633e-01, 9.9613e-01,\n",
       "             9.9609e-01, 9.9528e-01, 9.9315e-01, 9.9132e-01, 9.8995e-01, 9.8937e-01,\n",
       "             9.8868e-01, 9.8821e-01, 9.8790e-01, 9.8705e-01, 9.8534e-01, 9.8511e-01,\n",
       "             9.8478e-01, 9.8298e-01, 9.7417e-01, 9.7112e-01, 9.6685e-01, 9.6087e-01,\n",
       "             9.5968e-01, 9.4535e-01, 9.3462e-01, 9.3355e-01, 9.2176e-01, 8.8527e-01,\n",
       "             7.9124e-01, 7.0831e-01, 7.0123e-01, 6.9527e-01, 6.8384e-01, 6.6370e-01,\n",
       "             6.4451e-01, 6.2453e-01, 5.2594e-01, 5.2049e-01, 5.1257e-01, 4.7342e-01,\n",
       "             4.5451e-01, 4.4509e-01, 3.8545e-01, 2.8982e-01, 2.8824e-01, 2.7641e-01,\n",
       "             2.5197e-01, 2.4553e-01, 2.4522e-01, 2.1482e-01, 1.8108e-01, 1.6118e-01,\n",
       "             1.4827e-01, 1.0098e-01, 6.0346e-02, 4.1747e-02, 3.4156e-02, 3.0928e-02,\n",
       "             2.7249e-02, 2.3449e-02, 1.9537e-02, 1.7771e-02, 1.7188e-02, 9.4605e-03,\n",
       "             8.0690e-03, 7.3960e-03, 7.3102e-03, 4.6310e-03, 3.5458e-03, 3.2970e-03,\n",
       "             2.6906e-03, 2.5304e-03, 2.1216e-03, 2.1098e-03, 2.0962e-03, 2.0535e-03,\n",
       "             1.9695e-03, 1.7664e-03, 1.4994e-03, 1.3001e-03, 1.2680e-03, 1.0415e-03,\n",
       "             7.4045e-04, 7.2205e-04, 5.9329e-04, 4.5974e-04, 3.8175e-04, 3.1013e-04,\n",
       "             3.0680e-04, 3.0543e-04, 2.5328e-04, 2.1936e-04, 2.0329e-04, 1.7554e-04,\n",
       "             1.7054e-04, 1.3138e-04, 1.1487e-04, 8.6645e-05, 7.8376e-05, 5.2611e-05,\n",
       "             5.1587e-05, 4.3721e-05, 3.0997e-05, 3.0748e-05, 2.8510e-05, 2.8316e-05,\n",
       "             2.8106e-05, 2.1726e-05, 1.8274e-05, 1.7087e-05, 1.6917e-05, 1.3865e-05,\n",
       "             1.3640e-05, 1.3073e-05, 1.1190e-05, 9.7829e-06, 9.2630e-06, 8.2723e-06,\n",
       "             7.6635e-06, 6.8050e-06, 5.4750e-06, 5.2917e-06, 4.8874e-06, 4.3996e-06,\n",
       "             4.2784e-06, 4.1239e-06, 4.0847e-06, 3.7768e-06, 2.4582e-06, 2.4508e-06,\n",
       "             2.0694e-06, 1.6773e-06, 1.5646e-06, 1.3710e-06, 1.1944e-06, 1.0285e-06,\n",
       "             9.4295e-07, 8.8714e-07, 8.7375e-07, 8.3516e-07, 7.9252e-07, 7.8896e-07,\n",
       "             6.9155e-07, 6.1945e-07, 5.7181e-07, 5.3222e-07, 4.4273e-07, 3.5566e-07,\n",
       "             3.4096e-07, 3.3819e-07, 3.0148e-07, 2.4541e-07, 2.4416e-07, 2.4245e-07,\n",
       "             2.1925e-07, 2.1762e-07, 2.0248e-07, 1.9800e-07, 1.9453e-07, 1.8122e-07,\n",
       "             1.8004e-07, 1.7013e-07, 1.4649e-07, 1.2817e-07, 1.2709e-07, 1.1075e-07,\n",
       "             8.3625e-08, 6.2182e-08, 3.6046e-08, 3.5777e-08, 3.5017e-08, 3.4606e-08,\n",
       "             3.2661e-08, 3.1286e-08, 2.8798e-08, 2.8460e-08, 2.8302e-08, 2.7785e-08,\n",
       "             2.4547e-08, 2.2789e-08, 2.0717e-08, 1.9873e-08, 1.8408e-08, 1.7361e-08,\n",
       "             1.5244e-08, 1.2930e-08, 1.2242e-08, 1.1779e-08, 1.0940e-08, 1.0712e-08,\n",
       "             1.0552e-08, 1.0517e-08, 1.0244e-08, 9.0491e-09, 8.3624e-09, 8.0684e-09,\n",
       "             8.0390e-09, 7.9464e-09, 7.7888e-09, 6.8752e-09, 6.8241e-09, 6.3946e-09,\n",
       "             5.6642e-09, 5.3958e-09, 5.0602e-09, 4.7916e-09, 4.4056e-09, 4.3329e-09,\n",
       "             4.1006e-09, 4.0856e-09, 3.7121e-09, 2.6757e-09, 2.6357e-09, 2.6246e-09,\n",
       "             2.5025e-09, 2.4316e-09, 2.3539e-09, 2.2811e-09, 2.0736e-09, 2.0254e-09,\n",
       "             1.9203e-09, 1.8724e-09, 1.6603e-09, 1.5968e-09, 1.5381e-09, 1.5292e-09,\n",
       "             1.3577e-09, 1.1980e-09, 1.1623e-09, 1.1513e-09, 1.1502e-09, 9.7755e-10,\n",
       "             8.2479e-10, 8.0321e-10, 7.5367e-10, 7.4665e-10, 7.3723e-10, 6.7667e-10,\n",
       "             6.6952e-10, 6.4660e-10, 6.2335e-10, 6.2075e-10, 6.0802e-10, 4.0332e-10,\n",
       "             4.0115e-10, 3.7868e-10, 3.7082e-10, 3.4200e-10, 3.1405e-10, 3.1335e-10,\n",
       "             2.6696e-10, 2.6602e-10, 2.4688e-10, 2.3752e-10, 2.2624e-10, 2.1639e-10,\n",
       "             2.1592e-10, 2.0803e-10, 1.6495e-10, 1.3360e-10, 1.3071e-10, 1.2889e-10,\n",
       "             1.2856e-10, 1.1748e-10, 1.1543e-10, 1.0440e-10, 1.0085e-10, 9.6835e-11,\n",
       "             9.4469e-11, 9.2753e-11, 8.9299e-11, 8.6735e-11, 8.4979e-11, 8.2543e-11,\n",
       "             8.1532e-11, 8.0300e-11, 7.7484e-11, 7.7426e-11, 7.6335e-11, 7.3884e-11,\n",
       "             6.8245e-11, 6.4888e-11, 6.3031e-11, 6.1296e-11, 5.3046e-11, 4.5152e-11,\n",
       "             4.2284e-11, 4.1425e-11, 4.1162e-11, 4.0386e-11, 3.9859e-11, 3.7897e-11,\n",
       "             3.0974e-11, 2.6889e-11, 2.5194e-11, 1.9195e-11, 1.8103e-11, 1.6486e-11,\n",
       "             1.4776e-11, 1.4726e-11, 1.3950e-11, 1.3261e-11, 1.0715e-11, 9.0731e-12,\n",
       "             8.5234e-12, 6.8656e-12, 6.8439e-12, 6.7005e-12, 6.5869e-12, 6.2824e-12,\n",
       "             5.3632e-12, 5.1159e-12, 5.0946e-12, 4.7837e-12, 4.5672e-12, 3.7191e-12,\n",
       "             3.6541e-12, 2.4656e-12, 2.3382e-12, 2.3293e-12, 2.3237e-12, 2.1535e-12,\n",
       "             1.8282e-12, 1.7640e-12, 1.7051e-12, 1.6798e-12, 1.4380e-12, 1.4051e-12,\n",
       "             1.3142e-12, 1.2846e-12, 1.1312e-12, 1.0970e-12, 1.0679e-12, 1.0291e-12,\n",
       "             9.2656e-13, 8.5198e-13, 8.4258e-13, 8.3912e-13, 8.1991e-13, 6.2803e-13,\n",
       "             6.0791e-13, 5.5094e-13, 5.4574e-13, 5.2872e-13, 5.2164e-13, 5.1425e-13,\n",
       "             4.1339e-13, 3.9359e-13, 3.7984e-13, 3.5903e-13, 3.5592e-13, 3.2798e-13,\n",
       "             2.8687e-13, 2.6513e-13, 2.4604e-13, 2.3190e-13, 2.3130e-13, 2.2665e-13,\n",
       "             2.2524e-13, 2.2274e-13, 2.2108e-13, 2.1558e-13, 2.0503e-13, 2.0374e-13,\n",
       "             1.8021e-13, 1.6955e-13, 1.6605e-13, 1.5789e-13, 1.4871e-13, 1.0723e-13,\n",
       "             1.0624e-13, 1.0368e-13, 9.6989e-14, 8.2917e-14, 6.8338e-14, 6.2360e-14,\n",
       "             5.1440e-14, 4.7565e-14, 4.0502e-14, 3.6565e-14, 2.9318e-14, 2.7748e-14,\n",
       "             2.7687e-14, 2.7528e-14, 2.7404e-14, 2.0562e-14, 2.0059e-14, 1.8857e-14,\n",
       "             1.8467e-14, 1.7969e-14, 1.5221e-14, 1.5032e-14, 1.4229e-14, 1.3817e-14,\n",
       "             1.1131e-14, 1.0718e-14, 1.0109e-14, 1.0005e-14, 9.4244e-15, 8.6066e-15,\n",
       "             8.4880e-15, 8.4775e-15, 7.8722e-15, 7.7997e-15, 7.3758e-15, 7.2316e-15,\n",
       "             6.1318e-15, 6.1063e-15, 5.5933e-15, 5.5172e-15, 5.3405e-15, 4.6095e-15,\n",
       "             4.1453e-15, 3.9244e-15, 2.7359e-15, 2.4467e-15, 2.0739e-15, 2.0322e-15,\n",
       "             1.8474e-15, 1.7653e-15, 1.3089e-15, 9.3360e-16, 9.0221e-16, 8.2370e-16,\n",
       "             7.8822e-16, 7.2727e-16, 6.0552e-16, 5.4978e-16, 4.6955e-16, 4.5254e-16,\n",
       "             4.2208e-16, 3.1939e-16, 2.1194e-16, 2.0145e-16, 1.8332e-16, 1.6251e-16,\n",
       "             1.3630e-16, 1.0677e-16, 1.0397e-16, 5.9828e-17, 3.2844e-17, 2.6050e-17,\n",
       "             2.5668e-17, 2.2972e-17, 2.2697e-17, 1.6371e-17, 1.6370e-17, 1.4518e-17,\n",
       "             9.8677e-18, 8.9030e-18, 8.6872e-18, 8.1081e-18, 7.1030e-18, 6.8614e-18,\n",
       "             6.6996e-18, 6.2971e-18, 3.4917e-18, 2.1479e-18, 1.8568e-18, 1.5675e-18,\n",
       "             1.0458e-18, 9.8886e-19, 5.9876e-19, 4.7492e-19, 4.3707e-19, 3.1714e-19,\n",
       "             2.9925e-19, 1.3444e-19, 9.9318e-20, 8.6824e-20, 8.3838e-20, 7.4327e-20,\n",
       "             6.7729e-20, 2.9328e-21, 1.4409e-21, 1.2049e-21, 6.4169e-22, 4.9874e-22,\n",
       "             4.8798e-22, 3.4612e-23, 3.8041e-25, 2.4201e-26])}},\n",
       "   {'fpr': np.float64(0.06387665198237885),\n",
       "    'tpr': np.float64(0.9845559845559846),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0110, 0.0110, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
       "             0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
       "             0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0176, 0.0176, 0.0176, 0.0176,\n",
       "             0.0176, 0.0176, 0.0176, 0.0176, 0.0198, 0.0198, 0.0198, 0.0198, 0.0220,\n",
       "             0.0220, 0.0220, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242,\n",
       "             0.0242, 0.0242, 0.0242, 0.0242, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264,\n",
       "             0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264, 0.0264,\n",
       "             0.0286, 0.0286, 0.0286, 0.0286, 0.0308, 0.0308, 0.0308, 0.0308, 0.0308,\n",
       "             0.0308, 0.0308, 0.0308, 0.0308, 0.0308, 0.0308, 0.0308, 0.0308, 0.0308,\n",
       "             0.0308, 0.0308, 0.0308, 0.0330, 0.0330, 0.0352, 0.0374, 0.0374, 0.0374,\n",
       "             0.0396, 0.0396, 0.0396, 0.0396, 0.0419, 0.0419, 0.0441, 0.0463, 0.0463,\n",
       "             0.0463, 0.0485, 0.0507, 0.0507, 0.0507, 0.0529, 0.0551, 0.0551, 0.0573,\n",
       "             0.0573, 0.0573, 0.0595, 0.0617, 0.0617, 0.0617, 0.0617, 0.0639, 0.0639,\n",
       "             0.0639, 0.0661, 0.0683, 0.0683, 0.0705, 0.0727, 0.0749, 0.0771, 0.0793,\n",
       "             0.0815, 0.0837, 0.0859, 0.0859, 0.0881, 0.0881, 0.0903, 0.0925, 0.0925,\n",
       "             0.0947, 0.0947, 0.0947, 0.0969, 0.0991, 0.0991, 0.0991, 0.1013, 0.1035,\n",
       "             0.1057, 0.1057, 0.1079, 0.1079, 0.1101, 0.1123, 0.1145, 0.1167, 0.1189,\n",
       "             0.1211, 0.1211, 0.1233, 0.1233, 0.1256, 0.1278, 0.1300, 0.1322, 0.1322,\n",
       "             0.1344, 0.1366, 0.1388, 0.1410, 0.1432, 0.1454, 0.1454, 0.1476, 0.1498,\n",
       "             0.1520, 0.1542, 0.1564, 0.1586, 0.1608, 0.1630, 0.1652, 0.1652, 0.1674,\n",
       "             0.1696, 0.1718, 0.1740, 0.1740, 0.1762, 0.1784, 0.1784, 0.1806, 0.1828,\n",
       "             0.1850, 0.1872, 0.1894, 0.1916, 0.1938, 0.1960, 0.1982, 0.2004, 0.2026,\n",
       "             0.2048, 0.2070, 0.2093, 0.2115, 0.2137, 0.2159, 0.2181, 0.2203, 0.2225,\n",
       "             0.2247, 0.2269, 0.2291, 0.2313, 0.2335, 0.2335, 0.2357, 0.2379, 0.2401,\n",
       "             0.2423, 0.2445, 0.2467, 0.2489, 0.2511, 0.2533, 0.2555, 0.2577, 0.2599,\n",
       "             0.2621, 0.2643, 0.2665, 0.2687, 0.2709, 0.2731, 0.2753, 0.2775, 0.2797,\n",
       "             0.2819, 0.2841, 0.2863, 0.2885, 0.2907, 0.2930, 0.2952, 0.2974, 0.2996,\n",
       "             0.2996, 0.3018, 0.3040, 0.3062, 0.3084, 0.3106, 0.3128, 0.3150, 0.3172,\n",
       "             0.3194, 0.3216, 0.3238, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370,\n",
       "             0.3392, 0.3414, 0.3436, 0.3458, 0.3480, 0.3502, 0.3524, 0.3546, 0.3568,\n",
       "             0.3590, 0.3612, 0.3634, 0.3656, 0.3678, 0.3700, 0.3722, 0.3744, 0.3767,\n",
       "             0.3789, 0.3811, 0.3833, 0.3855, 0.3877, 0.3899, 0.3921, 0.3943, 0.3965,\n",
       "             0.3987, 0.4009, 0.4031, 0.4053, 0.4075, 0.4097, 0.4119, 0.4141, 0.4163,\n",
       "             0.4185, 0.4207, 0.4229, 0.4251, 0.4273, 0.4295, 0.4317, 0.4339, 0.4361,\n",
       "             0.4383, 0.4405, 0.4427, 0.4449, 0.4471, 0.4493, 0.4515, 0.4537, 0.4559,\n",
       "             0.4581, 0.4604, 0.4626, 0.4648, 0.4670, 0.4692, 0.4714, 0.4736, 0.4758,\n",
       "             0.4780, 0.4802, 0.4824, 0.4846, 0.4868, 0.4890, 0.4912, 0.4934, 0.4956,\n",
       "             0.4978, 0.5000, 0.5022, 0.5044, 0.5066, 0.5088, 0.5110, 0.5132, 0.5154,\n",
       "             0.5176, 0.5198, 0.5220, 0.5242, 0.5264, 0.5286, 0.5308, 0.5330, 0.5352,\n",
       "             0.5374, 0.5396, 0.5419, 0.5441, 0.5463, 0.5485, 0.5507, 0.5529, 0.5551,\n",
       "             0.5573, 0.5595, 0.5617, 0.5639, 0.5661, 0.5683, 0.5683, 0.5705, 0.5727,\n",
       "             0.5749, 0.5771, 0.5793, 0.5815, 0.5837, 0.5859, 0.5881, 0.5903, 0.5925,\n",
       "             0.5947, 0.5969, 0.5991, 0.6013, 0.6035, 0.6057, 0.6079, 0.6101, 0.6123,\n",
       "             0.6145, 0.6167, 0.6189, 0.6211, 0.6233, 0.6256, 0.6278, 0.6300, 0.6322,\n",
       "             0.6344, 0.6366, 0.6388, 0.6410, 0.6432, 0.6454, 0.6476, 0.6498, 0.6520,\n",
       "             0.6542, 0.6564, 0.6586, 0.6608, 0.6630, 0.6652, 0.6674, 0.6696, 0.6718,\n",
       "             0.6740, 0.6762, 0.6784, 0.6806, 0.6828, 0.6850, 0.6872, 0.6894, 0.6916,\n",
       "             0.6938, 0.6960, 0.6982, 0.7004, 0.7026, 0.7048, 0.7070, 0.7093, 0.7115,\n",
       "             0.7137, 0.7159, 0.7181, 0.7203, 0.7225, 0.7247, 0.7269, 0.7291, 0.7313,\n",
       "             0.7335, 0.7357, 0.7379, 0.7401, 0.7423, 0.7445, 0.7467, 0.7489, 0.7511,\n",
       "             0.7533, 0.7555, 0.7577, 0.7599, 0.7621, 0.7643, 0.7665, 0.7687, 0.7709,\n",
       "             0.7731, 0.7753, 0.7775, 0.7797, 0.7819, 0.7841, 0.7863, 0.7885, 0.7907,\n",
       "             0.7930, 0.7952, 0.7974, 0.7996, 0.8018, 0.8040, 0.8062, 0.8084, 0.8106,\n",
       "             0.8128, 0.8150, 0.8172, 0.8194, 0.8216, 0.8238, 0.8260, 0.8282, 0.8304,\n",
       "             0.8326, 0.8348, 0.8370, 0.8392, 0.8414, 0.8436, 0.8458, 0.8480, 0.8502,\n",
       "             0.8524, 0.8546, 0.8568, 0.8590, 0.8612, 0.8634, 0.8656, 0.8678, 0.8700,\n",
       "             0.8722, 0.8744, 0.8767, 0.8789, 0.8811, 0.8833, 0.8855, 0.8877, 0.8899,\n",
       "             0.8921, 0.8943, 0.8965, 0.8987, 0.9009, 0.9031, 0.9053, 0.9075, 0.9097,\n",
       "             0.9119, 0.9141, 0.9163, 0.9185, 0.9207, 0.9229, 0.9251, 0.9273, 0.9295,\n",
       "             0.9317, 0.9339, 0.9361, 0.9383, 0.9405, 0.9427, 0.9449, 0.9471, 0.9493,\n",
       "             0.9515, 0.9537, 0.9559, 0.9581, 0.9604, 0.9626, 0.9648, 0.9670, 0.9692,\n",
       "             0.9714, 0.9736, 0.9758, 0.9780, 0.9802, 0.9824, 0.9846, 0.9868, 0.9890,\n",
       "             0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8672, 0.8834, 0.8857, 0.8942, 0.8981, 0.9027, 0.9058, 0.9073,\n",
       "             0.9112, 0.9127, 0.9158, 0.9174, 0.9189, 0.9205, 0.9212, 0.9220, 0.9228,\n",
       "             0.9236, 0.9243, 0.9251, 0.9259, 0.9266, 0.9274, 0.9282, 0.9290, 0.9297,\n",
       "             0.9305, 0.9313, 0.9320, 0.9328, 0.9328, 0.9336, 0.9344, 0.9351, 0.9351,\n",
       "             0.9367, 0.9375, 0.9375, 0.9382, 0.9390, 0.9398, 0.9405, 0.9413, 0.9421,\n",
       "             0.9429, 0.9436, 0.9444, 0.9452, 0.9452, 0.9459, 0.9467, 0.9475, 0.9483,\n",
       "             0.9490, 0.9498, 0.9506, 0.9514, 0.9521, 0.9529, 0.9537, 0.9544, 0.9552,\n",
       "             0.9552, 0.9560, 0.9568, 0.9575, 0.9575, 0.9583, 0.9591, 0.9598, 0.9606,\n",
       "             0.9614, 0.9622, 0.9629, 0.9637, 0.9645, 0.9653, 0.9660, 0.9668, 0.9676,\n",
       "             0.9683, 0.9691, 0.9699, 0.9699, 0.9707, 0.9707, 0.9707, 0.9714, 0.9722,\n",
       "             0.9722, 0.9730, 0.9737, 0.9745, 0.9745, 0.9753, 0.9753, 0.9753, 0.9761,\n",
       "             0.9768, 0.9768, 0.9768, 0.9776, 0.9784, 0.9784, 0.9784, 0.9792, 0.9792,\n",
       "             0.9799, 0.9807, 0.9807, 0.9807, 0.9815, 0.9822, 0.9830, 0.9830, 0.9838,\n",
       "             0.9846, 0.9846, 0.9846, 0.9853, 0.9853, 0.9853, 0.9853, 0.9853, 0.9853,\n",
       "             0.9853, 0.9853, 0.9853, 0.9861, 0.9861, 0.9869, 0.9869, 0.9869, 0.9876,\n",
       "             0.9876, 0.9884, 0.9892, 0.9892, 0.9892, 0.9900, 0.9907, 0.9907, 0.9907,\n",
       "             0.9907, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9931, 0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9981e-01, 9.9978e-01, 9.9976e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9967e-01, 9.9966e-01, 9.9963e-01, 9.9963e-01, 9.9957e-01, 9.9954e-01,\n",
       "             9.9934e-01, 9.9931e-01, 9.9924e-01, 9.9913e-01, 9.9906e-01, 9.9899e-01,\n",
       "             9.9876e-01, 9.9842e-01, 9.9805e-01, 9.9777e-01, 9.9777e-01, 9.9767e-01,\n",
       "             9.9749e-01, 9.9710e-01, 9.9665e-01, 9.9643e-01, 9.9557e-01, 9.9510e-01,\n",
       "             9.9180e-01, 9.9018e-01, 9.8816e-01, 9.8790e-01, 9.8414e-01, 9.8269e-01,\n",
       "             9.8140e-01, 9.8063e-01, 9.7765e-01, 9.7623e-01, 9.6743e-01, 9.6431e-01,\n",
       "             9.5356e-01, 8.7675e-01, 8.4958e-01, 8.1882e-01, 7.1248e-01, 7.1102e-01,\n",
       "             6.5107e-01, 6.4774e-01, 5.6636e-01, 5.3337e-01, 3.9876e-01, 3.7504e-01,\n",
       "             3.6660e-01, 3.2891e-01, 3.1949e-01, 2.8533e-01, 2.8272e-01, 2.8112e-01,\n",
       "             2.6039e-01, 2.3043e-01, 2.0392e-01, 1.9781e-01, 1.9305e-01, 1.6367e-01,\n",
       "             1.4251e-01, 1.4201e-01, 1.3537e-01, 1.3428e-01, 1.2720e-01, 1.0172e-01,\n",
       "             6.6853e-02, 6.0177e-02, 4.3277e-02, 3.6103e-02, 2.7238e-02, 2.6785e-02,\n",
       "             1.6464e-02, 1.1662e-02, 1.1084e-02, 1.1019e-02, 1.0807e-02, 1.0787e-02,\n",
       "             9.3893e-03, 9.1139e-03, 6.4575e-03, 6.3037e-03, 5.2917e-03, 4.5661e-03,\n",
       "             3.5548e-03, 2.7715e-03, 2.5126e-03, 2.1002e-03, 1.2677e-03, 1.2420e-03,\n",
       "             1.1348e-03, 8.3471e-04, 6.7985e-04, 6.5309e-04, 4.4708e-04, 4.2944e-04,\n",
       "             4.1963e-04, 3.6126e-04, 2.4134e-04, 2.1377e-04, 2.0142e-04, 2.0087e-04,\n",
       "             1.4472e-04, 1.3557e-04, 1.1887e-04, 9.2379e-05, 8.7595e-05, 8.2773e-05,\n",
       "             6.9644e-05, 6.2590e-05, 5.7466e-05, 5.6162e-05, 4.7559e-05, 4.6176e-05,\n",
       "             4.2734e-05, 2.2949e-05, 2.2163e-05, 2.1229e-05, 1.6878e-05, 1.5532e-05,\n",
       "             1.5177e-05, 1.4265e-05, 1.2444e-05, 1.2167e-05, 1.2067e-05, 1.0799e-05,\n",
       "             1.0423e-05, 9.6114e-06, 9.4638e-06, 7.8003e-06, 6.3881e-06, 6.2861e-06,\n",
       "             6.2073e-06, 6.1832e-06, 6.1709e-06, 6.1181e-06, 5.5747e-06, 4.7263e-06,\n",
       "             3.9383e-06, 3.7117e-06, 3.1723e-06, 3.0407e-06, 2.9764e-06, 2.6517e-06,\n",
       "             2.4173e-06, 2.3951e-06, 2.3723e-06, 2.1035e-06, 1.9830e-06, 1.5637e-06,\n",
       "             1.4771e-06, 1.1958e-06, 5.8420e-07, 5.3558e-07, 4.3087e-07, 4.0006e-07,\n",
       "             3.7727e-07, 3.6564e-07, 2.8042e-07, 2.6464e-07, 2.5344e-07, 2.3116e-07,\n",
       "             2.2124e-07, 2.1752e-07, 1.9280e-07, 1.5859e-07, 1.4574e-07, 1.4273e-07,\n",
       "             1.4097e-07, 1.2860e-07, 1.2592e-07, 1.1842e-07, 7.8034e-08, 6.2691e-08,\n",
       "             5.7659e-08, 5.1748e-08, 3.7136e-08, 3.5375e-08, 3.0105e-08, 2.9116e-08,\n",
       "             2.8897e-08, 2.4051e-08, 2.3841e-08, 2.2330e-08, 1.9724e-08, 1.9364e-08,\n",
       "             1.7898e-08, 1.4135e-08, 1.4054e-08, 1.2737e-08, 1.1448e-08, 1.1144e-08,\n",
       "             1.1048e-08, 1.0099e-08, 8.2343e-09, 8.0310e-09, 7.5933e-09, 6.8822e-09,\n",
       "             6.6811e-09, 5.7249e-09, 5.5746e-09, 5.3525e-09, 4.9295e-09, 4.8260e-09,\n",
       "             4.3223e-09, 3.9601e-09, 3.4256e-09, 3.3292e-09, 2.9799e-09, 2.2316e-09,\n",
       "             1.9956e-09, 1.9443e-09, 1.7873e-09, 1.7742e-09, 1.6837e-09, 1.2736e-09,\n",
       "             1.2630e-09, 1.1495e-09, 1.0570e-09, 1.0289e-09, 9.2733e-10, 8.5413e-10,\n",
       "             7.8030e-10, 7.7567e-10, 7.1950e-10, 6.9472e-10, 4.9441e-10, 4.7695e-10,\n",
       "             4.6653e-10, 3.8847e-10, 3.5206e-10, 2.2749e-10, 2.1256e-10, 2.1006e-10,\n",
       "             1.9612e-10, 1.9103e-10, 1.8392e-10, 1.6629e-10, 1.5856e-10, 1.5667e-10,\n",
       "             1.5241e-10, 1.4382e-10, 1.4233e-10, 1.1599e-10, 1.1308e-10, 1.0940e-10,\n",
       "             1.0312e-10, 1.0239e-10, 1.0003e-10, 8.5559e-11, 8.0884e-11, 7.8052e-11,\n",
       "             7.4252e-11, 5.4368e-11, 5.3309e-11, 5.3199e-11, 5.2228e-11, 4.9897e-11,\n",
       "             4.7506e-11, 4.6368e-11, 4.5624e-11, 4.4931e-11, 4.4842e-11, 4.3982e-11,\n",
       "             4.3748e-11, 4.3346e-11, 4.3149e-11, 4.1243e-11, 3.1976e-11, 3.1881e-11,\n",
       "             2.7386e-11, 2.3386e-11, 2.2662e-11, 2.2316e-11, 2.1351e-11, 2.0816e-11,\n",
       "             2.0362e-11, 2.0180e-11, 1.7689e-11, 1.7500e-11, 1.6696e-11, 1.6671e-11,\n",
       "             1.4556e-11, 1.4219e-11, 1.3467e-11, 1.2800e-11, 1.0967e-11, 1.0916e-11,\n",
       "             1.0765e-11, 9.4435e-12, 9.3013e-12, 9.1904e-12, 9.0557e-12, 8.5122e-12,\n",
       "             7.9790e-12, 7.5066e-12, 7.5057e-12, 6.8701e-12, 6.7056e-12, 6.0914e-12,\n",
       "             5.8097e-12, 5.3269e-12, 5.1709e-12, 4.3145e-12, 4.0734e-12, 3.6502e-12,\n",
       "             3.6464e-12, 3.4070e-12, 3.1570e-12, 2.7365e-12, 2.2899e-12, 2.2404e-12,\n",
       "             2.2179e-12, 2.1264e-12, 2.0932e-12, 2.0795e-12, 1.6837e-12, 1.6774e-12,\n",
       "             1.6333e-12, 1.6028e-12, 1.5559e-12, 1.5162e-12, 1.4332e-12, 1.3421e-12,\n",
       "             1.2254e-12, 1.0926e-12, 1.0280e-12, 9.3631e-13, 8.8079e-13, 7.6275e-13,\n",
       "             7.5417e-13, 7.3333e-13, 6.7911e-13, 6.3651e-13, 6.1361e-13, 6.0694e-13,\n",
       "             5.5935e-13, 4.8930e-13, 4.5385e-13, 3.8485e-13, 3.5512e-13, 3.2840e-13,\n",
       "             3.2493e-13, 2.2873e-13, 2.2066e-13, 2.1574e-13, 2.1339e-13, 2.1272e-13,\n",
       "             1.7213e-13, 1.6421e-13, 1.3706e-13, 1.1509e-13, 1.1119e-13, 1.0386e-13,\n",
       "             8.4620e-14, 7.0346e-14, 6.1338e-14, 4.7807e-14, 3.9149e-14, 3.1450e-14,\n",
       "             2.9803e-14, 2.7354e-14, 2.5624e-14, 2.4848e-14, 2.4564e-14, 2.1452e-14,\n",
       "             2.0448e-14, 2.0348e-14, 1.8449e-14, 1.8150e-14, 1.8053e-14, 1.7835e-14,\n",
       "             1.5154e-14, 1.4129e-14, 1.2817e-14, 1.0738e-14, 9.9817e-15, 9.9302e-15,\n",
       "             9.4478e-15, 9.4089e-15, 8.0006e-15, 7.6777e-15, 7.5481e-15, 7.2138e-15,\n",
       "             6.6421e-15, 6.5826e-15, 6.3605e-15, 5.8344e-15, 5.4673e-15, 4.5667e-15,\n",
       "             3.6341e-15, 2.8614e-15, 2.8191e-15, 2.7661e-15, 2.6701e-15, 2.4842e-15,\n",
       "             1.6590e-15, 1.4348e-15, 1.3619e-15, 1.2926e-15, 1.1551e-15, 1.1301e-15,\n",
       "             1.0204e-15, 7.3343e-16, 7.2399e-16, 6.8124e-16, 5.7372e-16, 5.3741e-16,\n",
       "             4.9218e-16, 3.9990e-16, 3.7261e-16, 2.8898e-16, 2.5496e-16, 2.4341e-16,\n",
       "             2.3486e-16, 2.1921e-16, 2.1085e-16, 1.9701e-16, 1.8511e-16, 1.7918e-16,\n",
       "             1.5849e-16, 1.4496e-16, 7.4678e-17, 5.4953e-17, 3.9710e-17, 3.8768e-17,\n",
       "             3.5202e-17, 3.4307e-17, 3.2273e-17, 2.9820e-17, 2.4262e-17, 1.8394e-17,\n",
       "             1.6402e-17, 1.4409e-17, 1.1364e-17, 9.9330e-18, 9.2619e-18, 8.8352e-18,\n",
       "             8.7708e-18, 8.0799e-18, 6.4046e-18, 6.1121e-18, 5.9688e-18, 4.2107e-18,\n",
       "             3.8162e-18, 3.2114e-18, 2.8963e-18, 2.4514e-18, 2.4505e-18, 2.4428e-18,\n",
       "             2.4281e-18, 2.3644e-18, 2.2143e-18, 2.1117e-18, 1.7670e-18, 1.2764e-18,\n",
       "             9.7570e-19, 9.0059e-19, 7.0520e-19, 3.8832e-19, 3.0327e-19, 2.6792e-19,\n",
       "             2.6054e-19, 2.5343e-19, 2.0106e-19, 1.8902e-19, 1.7412e-19, 1.5852e-19,\n",
       "             1.3411e-19, 1.1817e-19, 8.7593e-20, 6.1522e-20, 5.8846e-20, 4.8141e-20,\n",
       "             2.3167e-20, 1.9119e-20, 5.7103e-21, 4.3122e-21, 3.2560e-21, 2.2402e-21,\n",
       "             1.4854e-21, 5.3260e-22, 5.1604e-22, 4.5178e-22, 3.7483e-22, 3.2138e-22,\n",
       "             1.8813e-22, 7.6167e-23, 6.3377e-23, 1.7849e-25, 2.0166e-27])}},\n",
       "   {'fpr': np.float64(0.08149779735682819),\n",
       "    'tpr': np.float64(0.9907335907335907),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0154, 0.0176, 0.0176,\n",
       "             0.0198, 0.0198, 0.0220, 0.0220, 0.0242, 0.0242, 0.0242, 0.0242, 0.0242,\n",
       "             0.0264, 0.0264, 0.0264, 0.0264, 0.0286, 0.0286, 0.0308, 0.0330, 0.0330,\n",
       "             0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0330, 0.0352, 0.0352, 0.0352,\n",
       "             0.0352, 0.0352, 0.0374, 0.0396, 0.0419, 0.0419, 0.0419, 0.0419, 0.0419,\n",
       "             0.0419, 0.0441, 0.0441, 0.0463, 0.0485, 0.0485, 0.0485, 0.0485, 0.0507,\n",
       "             0.0507, 0.0529, 0.0529, 0.0529, 0.0529, 0.0529, 0.0551, 0.0551, 0.0551,\n",
       "             0.0573, 0.0573, 0.0573, 0.0573, 0.0595, 0.0617, 0.0617, 0.0639, 0.0661,\n",
       "             0.0661, 0.0683, 0.0705, 0.0705, 0.0705, 0.0727, 0.0749, 0.0749, 0.0771,\n",
       "             0.0793, 0.0815, 0.0837, 0.0859, 0.0859, 0.0881, 0.0903, 0.0925, 0.0947,\n",
       "             0.0969, 0.0991, 0.1013, 0.1035, 0.1057, 0.1079, 0.1101, 0.1123, 0.1145,\n",
       "             0.1167, 0.1189, 0.1211, 0.1233, 0.1256, 0.1278, 0.1300, 0.1322, 0.1344,\n",
       "             0.1366, 0.1388, 0.1410, 0.1432, 0.1454, 0.1454, 0.1476, 0.1498, 0.1520,\n",
       "             0.1542, 0.1564, 0.1586, 0.1586, 0.1608, 0.1630, 0.1652, 0.1674, 0.1696,\n",
       "             0.1718, 0.1740, 0.1740, 0.1762, 0.1784, 0.1784, 0.1806, 0.1828, 0.1850,\n",
       "             0.1872, 0.1894, 0.1916, 0.1938, 0.1960, 0.1982, 0.2004, 0.2026, 0.2048,\n",
       "             0.2070, 0.2093, 0.2115, 0.2115, 0.2137, 0.2159, 0.2181, 0.2181, 0.2203,\n",
       "             0.2203, 0.2225, 0.2247, 0.2269, 0.2291, 0.2313, 0.2335, 0.2357, 0.2379,\n",
       "             0.2401, 0.2423, 0.2445, 0.2467, 0.2489, 0.2511, 0.2511, 0.2533, 0.2555,\n",
       "             0.2577, 0.2599, 0.2621, 0.2643, 0.2665, 0.2687, 0.2709, 0.2731, 0.2753,\n",
       "             0.2775, 0.2797, 0.2819, 0.2841, 0.2863, 0.2885, 0.2907, 0.2930, 0.2952,\n",
       "             0.2974, 0.2996, 0.3018, 0.3040, 0.3062, 0.3084, 0.3106, 0.3128, 0.3150,\n",
       "             0.3172, 0.3194, 0.3194, 0.3216, 0.3216, 0.3238, 0.3260, 0.3282, 0.3304,\n",
       "             0.3326, 0.3348, 0.3370, 0.3392, 0.3414, 0.3436, 0.3458, 0.3480, 0.3502,\n",
       "             0.3524, 0.3546, 0.3568, 0.3590, 0.3612, 0.3634, 0.3656, 0.3678, 0.3700,\n",
       "             0.3722, 0.3744, 0.3767, 0.3789, 0.3811, 0.3833, 0.3855, 0.3877, 0.3899,\n",
       "             0.3921, 0.3943, 0.3965, 0.3987, 0.4009, 0.4031, 0.4053, 0.4075, 0.4097,\n",
       "             0.4119, 0.4141, 0.4163, 0.4185, 0.4207, 0.4229, 0.4251, 0.4273, 0.4295,\n",
       "             0.4317, 0.4339, 0.4361, 0.4383, 0.4405, 0.4427, 0.4449, 0.4471, 0.4493,\n",
       "             0.4515, 0.4537, 0.4559, 0.4581, 0.4604, 0.4626, 0.4648, 0.4670, 0.4692,\n",
       "             0.4714, 0.4736, 0.4758, 0.4780, 0.4802, 0.4824, 0.4846, 0.4868, 0.4890,\n",
       "             0.4912, 0.4934, 0.4956, 0.4978, 0.5000, 0.5022, 0.5044, 0.5066, 0.5088,\n",
       "             0.5110, 0.5132, 0.5154, 0.5176, 0.5198, 0.5220, 0.5242, 0.5264, 0.5286,\n",
       "             0.5308, 0.5330, 0.5352, 0.5374, 0.5396, 0.5419, 0.5441, 0.5463, 0.5485,\n",
       "             0.5507, 0.5529, 0.5551, 0.5573, 0.5595, 0.5617, 0.5639, 0.5661, 0.5683,\n",
       "             0.5705, 0.5727, 0.5749, 0.5771, 0.5793, 0.5815, 0.5837, 0.5859, 0.5881,\n",
       "             0.5903, 0.5903, 0.5925, 0.5947, 0.5969, 0.5991, 0.6013, 0.6035, 0.6057,\n",
       "             0.6079, 0.6101, 0.6123, 0.6145, 0.6167, 0.6189, 0.6211, 0.6233, 0.6256,\n",
       "             0.6278, 0.6300, 0.6322, 0.6344, 0.6366, 0.6388, 0.6410, 0.6432, 0.6454,\n",
       "             0.6476, 0.6498, 0.6520, 0.6542, 0.6564, 0.6586, 0.6608, 0.6630, 0.6652,\n",
       "             0.6674, 0.6696, 0.6718, 0.6740, 0.6762, 0.6784, 0.6806, 0.6828, 0.6850,\n",
       "             0.6872, 0.6894, 0.6916, 0.6938, 0.6960, 0.6982, 0.7004, 0.7026, 0.7048,\n",
       "             0.7070, 0.7093, 0.7115, 0.7137, 0.7159, 0.7181, 0.7203, 0.7225, 0.7247,\n",
       "             0.7269, 0.7291, 0.7313, 0.7335, 0.7357, 0.7379, 0.7401, 0.7423, 0.7445,\n",
       "             0.7467, 0.7489, 0.7511, 0.7533, 0.7555, 0.7577, 0.7599, 0.7621, 0.7643,\n",
       "             0.7665, 0.7687, 0.7709, 0.7731, 0.7753, 0.7775, 0.7797, 0.7819, 0.7841,\n",
       "             0.7863, 0.7885, 0.7907, 0.7930, 0.7952, 0.7974, 0.7996, 0.8018, 0.8040,\n",
       "             0.8062, 0.8084, 0.8106, 0.8128, 0.8150, 0.8172, 0.8194, 0.8216, 0.8238,\n",
       "             0.8260, 0.8282, 0.8304, 0.8326, 0.8348, 0.8370, 0.8392, 0.8414, 0.8436,\n",
       "             0.8458, 0.8480, 0.8502, 0.8524, 0.8546, 0.8568, 0.8590, 0.8612, 0.8634,\n",
       "             0.8656, 0.8678, 0.8700, 0.8722, 0.8744, 0.8767, 0.8789, 0.8811, 0.8833,\n",
       "             0.8855, 0.8877, 0.8899, 0.8921, 0.8943, 0.8965, 0.8987, 0.9009, 0.9031,\n",
       "             0.9053, 0.9075, 0.9097, 0.9119, 0.9141, 0.9163, 0.9185, 0.9207, 0.9229,\n",
       "             0.9251, 0.9273, 0.9295, 0.9317, 0.9339, 0.9361, 0.9383, 0.9405, 0.9427,\n",
       "             0.9449, 0.9471, 0.9493, 0.9515, 0.9537, 0.9559, 0.9581, 0.9604, 0.9626,\n",
       "             0.9648, 0.9670, 0.9692, 0.9714, 0.9736, 0.9758, 0.9780, 0.9802, 0.9824,\n",
       "             0.9846, 0.9868, 0.9890, 0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9351, 0.9421, 0.9444, 0.9467, 0.9498, 0.9514, 0.9537, 0.9544,\n",
       "             0.9544, 0.9552, 0.9552, 0.9560, 0.9560, 0.9568, 0.9591, 0.9598, 0.9606,\n",
       "             0.9606, 0.9614, 0.9622, 0.9629, 0.9629, 0.9637, 0.9637, 0.9637, 0.9645,\n",
       "             0.9653, 0.9660, 0.9668, 0.9676, 0.9683, 0.9691, 0.9691, 0.9699, 0.9707,\n",
       "             0.9714, 0.9722, 0.9722, 0.9722, 0.9722, 0.9730, 0.9737, 0.9745, 0.9753,\n",
       "             0.9761, 0.9761, 0.9768, 0.9768, 0.9768, 0.9776, 0.9784, 0.9792, 0.9792,\n",
       "             0.9799, 0.9799, 0.9807, 0.9815, 0.9822, 0.9830, 0.9830, 0.9838, 0.9846,\n",
       "             0.9846, 0.9853, 0.9861, 0.9869, 0.9869, 0.9869, 0.9876, 0.9876, 0.9876,\n",
       "             0.9884, 0.9884, 0.9884, 0.9892, 0.9900, 0.9900, 0.9900, 0.9907, 0.9907,\n",
       "             0.9907, 0.9907, 0.9907, 0.9907, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9938, 0.9938, 0.9938, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9988e-01, 9.9986e-01, 9.9986e-01, 9.9981e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9977e-01, 9.9957e-01, 9.9950e-01, 9.9948e-01,\n",
       "             9.9926e-01, 9.9923e-01, 9.9923e-01, 9.9919e-01, 9.9910e-01, 9.9879e-01,\n",
       "             9.9657e-01, 9.9527e-01, 9.9363e-01, 9.9205e-01, 9.8880e-01, 9.8016e-01,\n",
       "             9.7230e-01, 9.6957e-01, 9.6690e-01, 9.6436e-01, 9.6075e-01, 9.4259e-01,\n",
       "             9.4139e-01, 9.3861e-01, 9.3594e-01, 9.2983e-01, 9.1811e-01, 9.0471e-01,\n",
       "             9.0409e-01, 8.9506e-01, 8.7513e-01, 8.5334e-01, 5.2057e-01, 4.6768e-01,\n",
       "             4.3375e-01, 4.0658e-01, 4.0392e-01, 3.8519e-01, 3.0516e-01, 2.8188e-01,\n",
       "             2.5869e-01, 2.0388e-01, 1.9950e-01, 1.8169e-01, 1.7646e-01, 1.5902e-01,\n",
       "             1.4227e-01, 1.1731e-01, 1.0906e-01, 7.5765e-02, 6.2150e-02, 5.1106e-02,\n",
       "             4.0065e-02, 3.1073e-02, 2.1041e-02, 1.5245e-02, 1.3940e-02, 1.0677e-02,\n",
       "             9.4028e-03, 7.0376e-03, 6.3998e-03, 5.1294e-03, 3.8626e-03, 3.4624e-03,\n",
       "             2.9431e-03, 1.8118e-03, 1.7658e-03, 1.5840e-03, 1.4290e-03, 1.2338e-03,\n",
       "             8.8105e-04, 6.1477e-04, 5.0089e-04, 4.9408e-04, 4.4377e-04, 4.2715e-04,\n",
       "             3.9512e-04, 3.6636e-04, 2.6804e-04, 1.8268e-04, 1.4935e-04, 1.4060e-04,\n",
       "             1.2261e-04, 1.0893e-04, 1.0443e-04, 1.0099e-04, 9.4966e-05, 8.0351e-05,\n",
       "             7.0354e-05, 6.3126e-05, 5.2464e-05, 4.1661e-05, 3.6507e-05, 3.5422e-05,\n",
       "             3.1194e-05, 3.0012e-05, 2.6232e-05, 2.2789e-05, 2.2265e-05, 1.7582e-05,\n",
       "             1.5898e-05, 1.4126e-05, 1.3187e-05, 1.2070e-05, 9.5177e-06, 8.6320e-06,\n",
       "             8.2798e-06, 8.0765e-06, 7.4697e-06, 7.0393e-06, 6.6323e-06, 5.9228e-06,\n",
       "             5.5018e-06, 5.0625e-06, 4.6562e-06, 4.4522e-06, 4.3668e-06, 4.2751e-06,\n",
       "             3.4844e-06, 3.3263e-06, 2.8670e-06, 2.5570e-06, 2.5504e-06, 2.1236e-06,\n",
       "             1.9903e-06, 1.9367e-06, 1.8184e-06, 1.7607e-06, 1.7000e-06, 1.5657e-06,\n",
       "             1.5203e-06, 1.4649e-06, 1.3875e-06, 1.2854e-06, 1.2282e-06, 1.1294e-06,\n",
       "             7.7026e-07, 6.6067e-07, 6.1643e-07, 5.7078e-07, 5.2703e-07, 5.1456e-07,\n",
       "             4.7445e-07, 3.9830e-07, 3.7929e-07, 3.4883e-07, 3.4164e-07, 3.2210e-07,\n",
       "             2.6858e-07, 2.2699e-07, 2.0032e-07, 1.9948e-07, 1.6357e-07, 1.6174e-07,\n",
       "             1.1118e-07, 1.0134e-07, 9.1760e-08, 6.7243e-08, 4.5778e-08, 4.4037e-08,\n",
       "             4.3139e-08, 4.1398e-08, 2.6303e-08, 2.3952e-08, 2.1527e-08, 1.8204e-08,\n",
       "             1.6805e-08, 1.4297e-08, 1.3690e-08, 1.2635e-08, 1.0962e-08, 9.0412e-09,\n",
       "             8.8936e-09, 8.8603e-09, 6.5536e-09, 6.2852e-09, 6.1681e-09, 5.8191e-09,\n",
       "             5.3302e-09, 4.3327e-09, 4.0427e-09, 3.0895e-09, 3.0511e-09, 3.0276e-09,\n",
       "             2.7476e-09, 2.7154e-09, 2.3350e-09, 1.5273e-09, 1.3990e-09, 1.3877e-09,\n",
       "             1.3513e-09, 1.3483e-09, 1.3481e-09, 1.3322e-09, 1.2642e-09, 1.0932e-09,\n",
       "             1.0606e-09, 9.6689e-10, 9.1623e-10, 9.0302e-10, 7.3518e-10, 7.0531e-10,\n",
       "             6.9583e-10, 6.8408e-10, 6.7806e-10, 6.0607e-10, 5.8077e-10, 5.3491e-10,\n",
       "             5.2894e-10, 4.9883e-10, 3.9564e-10, 3.7338e-10, 3.5049e-10, 2.7550e-10,\n",
       "             2.3314e-10, 2.2588e-10, 1.9926e-10, 1.9808e-10, 1.8232e-10, 1.7392e-10,\n",
       "             1.6101e-10, 1.1953e-10, 1.1842e-10, 1.1545e-10, 9.8265e-11, 7.3178e-11,\n",
       "             7.2739e-11, 7.1984e-11, 5.1093e-11, 4.5375e-11, 3.9694e-11, 3.8877e-11,\n",
       "             3.0780e-11, 2.6367e-11, 2.5045e-11, 2.3491e-11, 2.1326e-11, 1.9898e-11,\n",
       "             1.8550e-11, 1.7610e-11, 1.5085e-11, 1.3112e-11, 1.1294e-11, 1.0627e-11,\n",
       "             9.4798e-12, 8.6157e-12, 7.2098e-12, 6.4222e-12, 6.3767e-12, 6.2911e-12,\n",
       "             5.5454e-12, 5.0624e-12, 4.8625e-12, 4.3068e-12, 4.1725e-12, 3.7462e-12,\n",
       "             3.2444e-12, 3.2227e-12, 2.9901e-12, 2.4243e-12, 2.1475e-12, 2.0869e-12,\n",
       "             1.5096e-12, 1.4964e-12, 1.4205e-12, 1.4105e-12, 1.3134e-12, 1.2743e-12,\n",
       "             1.2319e-12, 1.2275e-12, 1.2209e-12, 1.0505e-12, 1.0315e-12, 9.4444e-13,\n",
       "             9.4405e-13, 8.9012e-13, 8.8906e-13, 7.9359e-13, 5.7876e-13, 5.5595e-13,\n",
       "             5.3466e-13, 5.2311e-13, 5.1947e-13, 5.1474e-13, 4.6982e-13, 4.4185e-13,\n",
       "             3.8876e-13, 3.8760e-13, 3.8088e-13, 3.8011e-13, 3.6986e-13, 3.2753e-13,\n",
       "             3.0537e-13, 3.0417e-13, 2.8047e-13, 2.7882e-13, 2.3207e-13, 1.5667e-13,\n",
       "             1.5408e-13, 1.4206e-13, 1.0732e-13, 1.0633e-13, 1.0302e-13, 9.5511e-14,\n",
       "             8.7973e-14, 7.0500e-14, 7.0250e-14, 6.6906e-14, 6.0141e-14, 5.9522e-14,\n",
       "             5.3266e-14, 4.8005e-14, 4.7020e-14, 3.6404e-14, 3.5546e-14, 3.4924e-14,\n",
       "             3.2854e-14, 3.2050e-14, 3.0888e-14, 2.4569e-14, 2.4360e-14, 2.3841e-14,\n",
       "             1.9619e-14, 1.5857e-14, 1.4921e-14, 1.4169e-14, 1.3353e-14, 1.3077e-14,\n",
       "             1.1552e-14, 1.0313e-14, 9.3789e-15, 9.1694e-15, 8.8159e-15, 8.7755e-15,\n",
       "             8.1016e-15, 7.6227e-15, 4.4949e-15, 4.1144e-15, 3.7066e-15, 3.7042e-15,\n",
       "             2.7619e-15, 2.4550e-15, 2.3276e-15, 1.2788e-15, 1.2271e-15, 1.0872e-15,\n",
       "             1.0214e-15, 9.9188e-16, 9.1619e-16, 5.9512e-16, 5.2636e-16, 4.9438e-16,\n",
       "             4.6713e-16, 4.2895e-16, 3.5247e-16, 2.9602e-16, 2.8189e-16, 2.2647e-16,\n",
       "             1.9042e-16, 1.8889e-16, 1.7042e-16, 1.6394e-16, 1.6287e-16, 1.2259e-16,\n",
       "             1.1659e-16, 1.1436e-16, 1.0531e-16, 9.8478e-17, 9.2226e-17, 7.5960e-17,\n",
       "             6.2770e-17, 4.7750e-17, 4.6111e-17, 3.3657e-17, 2.7182e-17, 2.6874e-17,\n",
       "             2.2793e-17, 1.9308e-17, 1.6904e-17, 1.4033e-17, 1.3553e-17, 1.2251e-17,\n",
       "             1.1263e-17, 1.1244e-17, 1.0501e-17, 9.8741e-18, 8.8801e-18, 5.9745e-18,\n",
       "             4.8749e-18, 3.7942e-18, 2.9750e-18, 2.6469e-18, 1.9814e-18, 1.8906e-18,\n",
       "             1.7367e-18, 1.5157e-18, 1.5108e-18, 1.4467e-18, 1.2511e-18, 7.9446e-19,\n",
       "             6.5433e-19, 6.0695e-19, 5.5977e-19, 5.5569e-19, 5.2004e-19, 4.4879e-19,\n",
       "             4.1201e-19, 3.2538e-19, 2.9009e-19, 2.6085e-19, 1.9853e-19, 1.5452e-19,\n",
       "             1.4131e-19, 1.3794e-19, 7.7839e-20, 7.3934e-20, 6.7912e-20, 6.7812e-20,\n",
       "             6.7050e-20, 2.6098e-20, 1.0561e-20, 9.0482e-21, 8.3792e-21, 7.4411e-21,\n",
       "             6.4455e-21, 2.9916e-21, 1.4959e-21, 1.4197e-21, 1.1280e-21, 6.6692e-22,\n",
       "             5.2100e-22, 3.8690e-22, 3.7702e-22, 3.4954e-22, 3.0346e-22, 3.0287e-22,\n",
       "             2.1438e-22, 1.9430e-22, 1.7402e-22, 1.2797e-22, 1.1425e-22, 1.0950e-22,\n",
       "             1.0521e-22, 1.0219e-22, 5.7272e-23, 3.1277e-23, 2.2452e-23, 1.6951e-23,\n",
       "             1.3557e-23, 9.8819e-24, 7.6657e-24, 4.2217e-24, 1.7736e-24, 8.9795e-25,\n",
       "             4.9872e-25, 3.3277e-25, 1.3267e-25, 6.7551e-26, 1.9536e-26, 9.7559e-28,\n",
       "             6.3778e-29, 1.5019e-30])}},\n",
       "   {'fpr': np.float64(0.8392070484581498),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.5639, 0.5793, 0.5881, 0.5947, 0.6079, 0.6123, 0.6189, 0.6256,\n",
       "             0.6300, 0.6344, 0.6366, 0.6388, 0.6410, 0.6432, 0.6476, 0.6498, 0.6520,\n",
       "             0.6542, 0.6564, 0.6586, 0.6608, 0.6630, 0.6652, 0.6674, 0.6696, 0.6718,\n",
       "             0.6740, 0.6762, 0.6784, 0.6806, 0.6828, 0.6850, 0.6872, 0.6894, 0.6916,\n",
       "             0.6938, 0.6960, 0.6982, 0.7004, 0.7026, 0.7048, 0.7070, 0.7093, 0.7115,\n",
       "             0.7137, 0.7159, 0.7181, 0.7203, 0.7225, 0.7247, 0.7269, 0.7291, 0.7313,\n",
       "             0.7335, 0.7357, 0.7379, 0.7401, 0.7423, 0.7445, 0.7467, 0.7489, 0.7511,\n",
       "             0.7533, 0.7555, 0.7577, 0.7599, 0.7621, 0.7643, 0.7665, 0.7687, 0.7709,\n",
       "             0.7731, 0.7753, 0.7775, 0.7797, 0.7819, 0.7841, 0.7863, 0.7885, 0.7907,\n",
       "             0.7930, 0.7952, 0.7974, 0.7996, 0.8018, 0.8040, 0.8062, 0.8084, 0.8106,\n",
       "             0.8128, 0.8150, 0.8172, 0.8194, 0.8216, 0.8238, 0.8260, 0.8282, 0.8304,\n",
       "             0.8326, 0.8348, 0.8370, 0.8392, 0.8414, 0.8436, 0.8458, 0.8480, 0.8502,\n",
       "             0.8524, 0.8546, 0.8568, 0.8590, 0.8612, 0.8634, 0.8656, 0.8678, 0.8700,\n",
       "             0.8722, 0.8744, 0.8767, 0.8789, 0.8811, 0.8833, 0.8855, 0.8877, 0.8899,\n",
       "             0.8921, 0.8943, 0.8965, 0.8987, 0.9009, 0.9031, 0.9053, 0.9075, 0.9097,\n",
       "             0.9119, 0.9141, 0.9163, 0.9185, 0.9207, 0.9229, 0.9251, 0.9273, 0.9295,\n",
       "             0.9317, 0.9339, 0.9361, 0.9383, 0.9405, 0.9427, 0.9449, 0.9471, 0.9493,\n",
       "             0.9515, 0.9537, 0.9559, 0.9581, 0.9604, 0.9626, 0.9648, 0.9670, 0.9692,\n",
       "             0.9714, 0.9736, 0.9758, 0.9780, 0.9802, 0.9824, 0.9846, 0.9868, 0.9890,\n",
       "             0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9985, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9994e-01, 9.9994e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9985e-01,\n",
       "             9.9983e-01, 9.9980e-01, 9.9978e-01, 9.9977e-01, 9.9964e-01, 9.9961e-01,\n",
       "             9.9958e-01, 9.9941e-01, 9.9935e-01, 9.9933e-01, 9.9932e-01, 9.9917e-01,\n",
       "             9.9909e-01, 9.9893e-01, 9.9883e-01, 9.9835e-01, 9.9798e-01, 9.9775e-01,\n",
       "             9.9739e-01, 9.9692e-01, 9.9578e-01, 9.9569e-01, 9.9450e-01, 9.9399e-01,\n",
       "             9.9351e-01, 9.9096e-01, 9.8555e-01, 9.8377e-01, 9.8239e-01, 9.7823e-01,\n",
       "             9.7778e-01, 9.7186e-01, 9.7024e-01, 9.5342e-01, 9.4933e-01, 9.4580e-01,\n",
       "             9.3274e-01, 9.2064e-01, 9.1483e-01, 9.1467e-01, 9.0391e-01, 8.9743e-01,\n",
       "             8.7442e-01, 8.0996e-01, 8.0571e-01, 7.5388e-01, 7.4186e-01, 6.9949e-01,\n",
       "             6.9276e-01, 6.9077e-01, 6.6276e-01, 5.8417e-01, 5.6937e-01, 5.4304e-01,\n",
       "             5.1774e-01, 4.9949e-01, 4.7193e-01, 4.3109e-01, 3.7376e-01, 2.9622e-01,\n",
       "             1.8429e-01, 1.8081e-01, 1.7388e-01, 1.6192e-01, 1.6090e-01, 1.4338e-01,\n",
       "             1.2849e-01, 9.3104e-02, 7.7809e-02, 7.5097e-02, 5.3287e-02, 4.7379e-02,\n",
       "             3.8518e-02, 2.3196e-02, 1.6223e-02, 9.7364e-03, 9.4492e-03, 8.5676e-03,\n",
       "             8.3831e-03, 6.4793e-03, 4.6160e-03, 2.9065e-03, 2.6255e-03, 2.5137e-03,\n",
       "             2.4448e-03, 1.5389e-03, 1.5260e-03, 1.4250e-03, 1.2909e-03, 9.7884e-04,\n",
       "             6.5556e-04, 6.2436e-04, 5.4731e-04, 5.3062e-04, 4.0500e-04, 4.0112e-04,\n",
       "             3.3553e-04, 1.9824e-04, 1.3544e-04, 8.5778e-05, 8.2312e-05, 5.5480e-05,\n",
       "             2.7855e-05, 2.2915e-05, 1.5203e-05, 7.4233e-06, 3.0052e-06, 2.8981e-06,\n",
       "             9.8778e-07, 5.9253e-07, 2.1056e-07, 9.0802e-08, 7.2964e-08, 3.6138e-08,\n",
       "             2.7485e-08, 1.3905e-08, 1.0306e-08, 7.7598e-09, 7.5434e-09, 1.4890e-09,\n",
       "             6.0587e-10, 6.4435e-11, 5.5028e-11, 3.5837e-11, 8.2747e-12, 8.9548e-13,\n",
       "             1.9477e-15, 4.3838e-16])}}],\n",
       "  [{'fpr': np.float64(0.04565217391304348),\n",
       "    'tpr': np.float64(0.7613899613899614),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 2.3166e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9996e-01, 9.9991e-01,  ..., 3.8245e-06, 3.3140e-06,\n",
       "             6.4321e-07])}},\n",
       "   {'fpr': np.float64(0.021739130434782608),\n",
       "    'tpr': np.float64(0.9196911196911197),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9956e-01, 9.9926e-01,  ..., 1.6030e-07, 1.4235e-07,\n",
       "             3.4741e-08])}},\n",
       "   {'fpr': np.float64(0.030434782608695653),\n",
       "    'tpr': np.float64(0.9451737451737452),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9977e-01, 9.9962e-01,  ..., 2.5124e-07, 8.9586e-08,\n",
       "             2.9146e-08])}},\n",
       "   {'fpr': np.float64(0.05434782608695652),\n",
       "    'tpr': np.float64(0.9652509652509652),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9986e-01, 9.9984e-01,  ..., 4.3317e-07, 3.6441e-07,\n",
       "             3.3918e-07])}},\n",
       "   {'fpr': np.float64(0.02608695652173913),\n",
       "    'tpr': np.float64(0.9583011583011583),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.5540e-10, 1.4598e-10,\n",
       "             8.4803e-11])}},\n",
       "   {'fpr': np.float64(0.021739130434782608),\n",
       "    'tpr': np.float64(0.9544401544401544),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0950e-11, 4.6696e-12,\n",
       "             3.1493e-12])}},\n",
       "   {'fpr': np.float64(0.041304347826086954),\n",
       "    'tpr': np.float64(0.9683397683397683),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 6.8653e-10, 2.9269e-10,\n",
       "             1.3125e-10])}},\n",
       "   {'fpr': np.float64(0.0391304347826087),\n",
       "    'tpr': np.float64(0.9691119691119691),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.3127e-10, 2.3053e-10,\n",
       "             9.0260e-11])}},\n",
       "   {'fpr': np.float64(0.0891304347826087),\n",
       "    'tpr': np.float64(0.9899613899613899),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.9262e-09, 1.8056e-09,\n",
       "             1.4861e-09])}},\n",
       "   {'fpr': np.float64(0.02608695652173913),\n",
       "    'tpr': np.float64(0.9714285714285714),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7220e-04, 1.5444e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.7060e-12, 1.2314e-12,\n",
       "             9.4799e-13])}},\n",
       "   {'fpr': np.float64(0.09782608695652174),\n",
       "    'tpr': np.float64(0.9922779922779923),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0022, 0.0022, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087,\n",
       "             0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087,\n",
       "             0.0087, 0.0087, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0152, 0.0152, 0.0152,\n",
       "             0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0174, 0.0174,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0217,\n",
       "             0.0217, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239,\n",
       "             0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239,\n",
       "             0.0239, 0.0239, 0.0261, 0.0283, 0.0283, 0.0283, 0.0283, 0.0304, 0.0304,\n",
       "             0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0326, 0.0348, 0.0348, 0.0348, 0.0348, 0.0348, 0.0370, 0.0391, 0.0413,\n",
       "             0.0435, 0.0435, 0.0435, 0.0457, 0.0478, 0.0478, 0.0478, 0.0500, 0.0500,\n",
       "             0.0522, 0.0522, 0.0522, 0.0543, 0.0543, 0.0543, 0.0543, 0.0565, 0.0587,\n",
       "             0.0587, 0.0587, 0.0587, 0.0609, 0.0609, 0.0609, 0.0609, 0.0630, 0.0652,\n",
       "             0.0674, 0.0674, 0.0674, 0.0674, 0.0674, 0.0674, 0.0696, 0.0717, 0.0717,\n",
       "             0.0717, 0.0717, 0.0717, 0.0739, 0.0739, 0.0761, 0.0761, 0.0761, 0.0783,\n",
       "             0.0804, 0.0826, 0.0826, 0.0826, 0.0848, 0.0870, 0.0870, 0.0891, 0.0891,\n",
       "             0.0913, 0.0913, 0.0935, 0.0935, 0.0957, 0.0978, 0.0978, 0.1000, 0.1022,\n",
       "             0.1043, 0.1065, 0.1087, 0.1109, 0.1130, 0.1152, 0.1174, 0.1196, 0.1217,\n",
       "             0.1239, 0.1239, 0.1239, 0.1261, 0.1283, 0.1304, 0.1326, 0.1348, 0.1370,\n",
       "             0.1391, 0.1391, 0.1413, 0.1435, 0.1435, 0.1457, 0.1478, 0.1500, 0.1522,\n",
       "             0.1522, 0.1543, 0.1565, 0.1587, 0.1609, 0.1630, 0.1630, 0.1652, 0.1674,\n",
       "             0.1696, 0.1717, 0.1739, 0.1761, 0.1783, 0.1804, 0.1826, 0.1848, 0.1870,\n",
       "             0.1891, 0.1913, 0.1935, 0.1957, 0.1978, 0.2000, 0.2022, 0.2043, 0.2065,\n",
       "             0.2087, 0.2109, 0.2130, 0.2152, 0.2174, 0.2196, 0.2217, 0.2239, 0.2261,\n",
       "             0.2283, 0.2304, 0.2326, 0.2348, 0.2370, 0.2391, 0.2413, 0.2435, 0.2457,\n",
       "             0.2478, 0.2500, 0.2522, 0.2543, 0.2565, 0.2587, 0.2609, 0.2630, 0.2652,\n",
       "             0.2674, 0.2696, 0.2717, 0.2739, 0.2761, 0.2783, 0.2804, 0.2826, 0.2848,\n",
       "             0.2870, 0.2891, 0.2891, 0.2913, 0.2935, 0.2957, 0.2978, 0.3000, 0.3022,\n",
       "             0.3043, 0.3065, 0.3087, 0.3109, 0.3130, 0.3130, 0.3152, 0.3174, 0.3196,\n",
       "             0.3217, 0.3239, 0.3261, 0.3283, 0.3304, 0.3326, 0.3348, 0.3370, 0.3391,\n",
       "             0.3413, 0.3435, 0.3457, 0.3478, 0.3500, 0.3522, 0.3543, 0.3565, 0.3587,\n",
       "             0.3609, 0.3630, 0.3652, 0.3674, 0.3696, 0.3717, 0.3739, 0.3761, 0.3783,\n",
       "             0.3804, 0.3826, 0.3848, 0.3870, 0.3870, 0.3891, 0.3913, 0.3935, 0.3957,\n",
       "             0.3978, 0.4000, 0.4022, 0.4043, 0.4065, 0.4087, 0.4109, 0.4130, 0.4152,\n",
       "             0.4174, 0.4196, 0.4217, 0.4239, 0.4261, 0.4283, 0.4304, 0.4326, 0.4348,\n",
       "             0.4370, 0.4391, 0.4413, 0.4435, 0.4457, 0.4478, 0.4500, 0.4522, 0.4543,\n",
       "             0.4565, 0.4587, 0.4609, 0.4630, 0.4652, 0.4674, 0.4696, 0.4717, 0.4739,\n",
       "             0.4761, 0.4783, 0.4804, 0.4826, 0.4848, 0.4870, 0.4891, 0.4913, 0.4935,\n",
       "             0.4957, 0.4978, 0.5000, 0.5022, 0.5043, 0.5065, 0.5087, 0.5109, 0.5130,\n",
       "             0.5152, 0.5174, 0.5196, 0.5217, 0.5239, 0.5261, 0.5283, 0.5304, 0.5326,\n",
       "             0.5348, 0.5370, 0.5391, 0.5413, 0.5435, 0.5457, 0.5478, 0.5500, 0.5522,\n",
       "             0.5543, 0.5565, 0.5587, 0.5609, 0.5630, 0.5652, 0.5674, 0.5696, 0.5717,\n",
       "             0.5739, 0.5761, 0.5783, 0.5804, 0.5826, 0.5848, 0.5870, 0.5891, 0.5913,\n",
       "             0.5935, 0.5957, 0.5978, 0.6000, 0.6022, 0.6043, 0.6065, 0.6087, 0.6109,\n",
       "             0.6130, 0.6152, 0.6174, 0.6196, 0.6217, 0.6239, 0.6261, 0.6283, 0.6304,\n",
       "             0.6326, 0.6348, 0.6370, 0.6391, 0.6413, 0.6435, 0.6457, 0.6478, 0.6500,\n",
       "             0.6522, 0.6543, 0.6565, 0.6587, 0.6609, 0.6630, 0.6652, 0.6674, 0.6696,\n",
       "             0.6717, 0.6739, 0.6761, 0.6783, 0.6804, 0.6826, 0.6848, 0.6870, 0.6891,\n",
       "             0.6913, 0.6935, 0.6957, 0.6978, 0.7000, 0.7022, 0.7043, 0.7065, 0.7087,\n",
       "             0.7109, 0.7130, 0.7152, 0.7174, 0.7196, 0.7217, 0.7239, 0.7261, 0.7283,\n",
       "             0.7304, 0.7326, 0.7348, 0.7370, 0.7391, 0.7413, 0.7435, 0.7457, 0.7478,\n",
       "             0.7500, 0.7522, 0.7543, 0.7565, 0.7587, 0.7609, 0.7630, 0.7652, 0.7674,\n",
       "             0.7696, 0.7717, 0.7739, 0.7761, 0.7783, 0.7804, 0.7826, 0.7848, 0.7870,\n",
       "             0.7891, 0.7913, 0.7935, 0.7957, 0.7978, 0.8000, 0.8022, 0.8043, 0.8065,\n",
       "             0.8087, 0.8109, 0.8130, 0.8152, 0.8174, 0.8196, 0.8217, 0.8239, 0.8261,\n",
       "             0.8283, 0.8304, 0.8326, 0.8348, 0.8370, 0.8391, 0.8413, 0.8435, 0.8457,\n",
       "             0.8478, 0.8500, 0.8522, 0.8543, 0.8565, 0.8587, 0.8609, 0.8630, 0.8652,\n",
       "             0.8674, 0.8696, 0.8717, 0.8739, 0.8761, 0.8783, 0.8804, 0.8826, 0.8848,\n",
       "             0.8870, 0.8891, 0.8913, 0.8935, 0.8957, 0.8978, 0.9000, 0.9022, 0.9043,\n",
       "             0.9065, 0.9087, 0.9109, 0.9130, 0.9152, 0.9174, 0.9196, 0.9217, 0.9239,\n",
       "             0.9261, 0.9283, 0.9304, 0.9326, 0.9348, 0.9370, 0.9391, 0.9413, 0.9435,\n",
       "             0.9457, 0.9478, 0.9500, 0.9522, 0.9543, 0.9565, 0.9587, 0.9609, 0.9630,\n",
       "             0.9652, 0.9674, 0.9696, 0.9717, 0.9739, 0.9761, 0.9783, 0.9804, 0.9826,\n",
       "             0.9848, 0.9870, 0.9891, 0.9913, 0.9935, 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.1166, 0.1900, 0.2510, 0.2888, 0.3243, 0.3568, 0.3753, 0.3969,\n",
       "             0.4170, 0.4324, 0.4502, 0.4610, 0.4695, 0.4842, 0.4942, 0.5104, 0.5205,\n",
       "             0.5236, 0.5320, 0.5390, 0.5467, 0.5544, 0.5583, 0.5645, 0.5722, 0.5792,\n",
       "             0.5853, 0.5892, 0.5915, 0.5977, 0.6015, 0.6039, 0.6069, 0.6100, 0.6147,\n",
       "             0.6154, 0.6216, 0.6239, 0.6270, 0.6317, 0.6332, 0.6355, 0.6371, 0.6402,\n",
       "             0.6409, 0.6440, 0.6456, 0.6494, 0.6517, 0.6533, 0.6541, 0.6564, 0.6571,\n",
       "             0.6595, 0.6633, 0.6641, 0.6664, 0.6680, 0.6710, 0.6726, 0.6734, 0.6749,\n",
       "             0.6772, 0.6788, 0.6826, 0.6834, 0.6865, 0.6888, 0.6896, 0.6903, 0.6919,\n",
       "             0.6958, 0.6973, 0.6988, 0.7004, 0.7019, 0.7035, 0.7058, 0.7073, 0.7097,\n",
       "             0.7104, 0.7112, 0.7120, 0.7135, 0.7143, 0.7151, 0.7181, 0.7189, 0.7205,\n",
       "             0.7236, 0.7259, 0.7266, 0.7282, 0.7290, 0.7297, 0.7313, 0.7320, 0.7328,\n",
       "             0.7351, 0.7359, 0.7375, 0.7382, 0.7405, 0.7429, 0.7436, 0.7444, 0.7452,\n",
       "             0.7459, 0.7467, 0.7490, 0.7506, 0.7521, 0.7544, 0.7552, 0.7568, 0.7583,\n",
       "             0.7591, 0.7598, 0.7606, 0.7622, 0.7629, 0.7645, 0.7653, 0.7668, 0.7683,\n",
       "             0.7691, 0.7699, 0.7707, 0.7714, 0.7722, 0.7730, 0.7737, 0.7753, 0.7761,\n",
       "             0.7768, 0.7776, 0.7784, 0.7792, 0.7799, 0.7807, 0.7815, 0.7822, 0.7830,\n",
       "             0.7846, 0.7876, 0.7884, 0.7892, 0.7900, 0.7907, 0.7923, 0.7938, 0.7946,\n",
       "             0.7954, 0.7969, 0.7977, 0.7985, 0.7992, 0.8008, 0.8023, 0.8031, 0.8039,\n",
       "             0.8046, 0.8054, 0.8062, 0.8069, 0.8077, 0.8085, 0.8093, 0.8100, 0.8108,\n",
       "             0.8116, 0.8124, 0.8139, 0.8147, 0.8154, 0.8162, 0.8170, 0.8178, 0.8185,\n",
       "             0.8201, 0.8208, 0.8216, 0.8224, 0.8232, 0.8239, 0.8255, 0.8263, 0.8270,\n",
       "             0.8278, 0.8286, 0.8293, 0.8301, 0.8317, 0.8324, 0.8332, 0.8340, 0.8347,\n",
       "             0.8355, 0.8363, 0.8371, 0.8378, 0.8386, 0.8394, 0.8409, 0.8425, 0.8432,\n",
       "             0.8440, 0.8448, 0.8456, 0.8463, 0.8471, 0.8479, 0.8486, 0.8494, 0.8502,\n",
       "             0.8510, 0.8517, 0.8525, 0.8533, 0.8541, 0.8556, 0.8564, 0.8571, 0.8579,\n",
       "             0.8587, 0.8595, 0.8602, 0.8610, 0.8618, 0.8625, 0.8625, 0.8633, 0.8641,\n",
       "             0.8649, 0.8656, 0.8664, 0.8672, 0.8680, 0.8687, 0.8695, 0.8703, 0.8710,\n",
       "             0.8718, 0.8726, 0.8734, 0.8741, 0.8749, 0.8757, 0.8764, 0.8772, 0.8780,\n",
       "             0.8788, 0.8795, 0.8803, 0.8811, 0.8819, 0.8826, 0.8826, 0.8834, 0.8842,\n",
       "             0.8849, 0.8857, 0.8865, 0.8873, 0.8880, 0.8888, 0.8896, 0.8896, 0.8903,\n",
       "             0.8911, 0.8927, 0.8934, 0.8942, 0.8950, 0.8958, 0.8965, 0.8973, 0.8981,\n",
       "             0.8988, 0.8996, 0.9004, 0.9012, 0.9019, 0.9027, 0.9035, 0.9035, 0.9042,\n",
       "             0.9050, 0.9058, 0.9066, 0.9073, 0.9081, 0.9089, 0.9097, 0.9104, 0.9112,\n",
       "             0.9120, 0.9127, 0.9135, 0.9143, 0.9151, 0.9158, 0.9166, 0.9174, 0.9181,\n",
       "             0.9189, 0.9197, 0.9205, 0.9212, 0.9220, 0.9228, 0.9236, 0.9243, 0.9251,\n",
       "             0.9259, 0.9266, 0.9274, 0.9282, 0.9290, 0.9297, 0.9305, 0.9313, 0.9320,\n",
       "             0.9328, 0.9336, 0.9344, 0.9351, 0.9359, 0.9367, 0.9375, 0.9382, 0.9382,\n",
       "             0.9390, 0.9390, 0.9398, 0.9405, 0.9413, 0.9421, 0.9429, 0.9436, 0.9444,\n",
       "             0.9452, 0.9459, 0.9467, 0.9475, 0.9483, 0.9490, 0.9498, 0.9506, 0.9514,\n",
       "             0.9521, 0.9529, 0.9529, 0.9529, 0.9537, 0.9544, 0.9552, 0.9552, 0.9560,\n",
       "             0.9568, 0.9575, 0.9583, 0.9591, 0.9598, 0.9598, 0.9606, 0.9614, 0.9622,\n",
       "             0.9629, 0.9629, 0.9637, 0.9645, 0.9653, 0.9660, 0.9660, 0.9660, 0.9660,\n",
       "             0.9660, 0.9668, 0.9676, 0.9676, 0.9676, 0.9683, 0.9691, 0.9691, 0.9699,\n",
       "             0.9699, 0.9707, 0.9714, 0.9714, 0.9722, 0.9730, 0.9737, 0.9737, 0.9737,\n",
       "             0.9745, 0.9753, 0.9761, 0.9761, 0.9768, 0.9776, 0.9784, 0.9784, 0.9784,\n",
       "             0.9784, 0.9792, 0.9799, 0.9807, 0.9815, 0.9822, 0.9822, 0.9822, 0.9830,\n",
       "             0.9838, 0.9846, 0.9853, 0.9853, 0.9861, 0.9861, 0.9869, 0.9876, 0.9876,\n",
       "             0.9876, 0.9876, 0.9884, 0.9892, 0.9892, 0.9892, 0.9900, 0.9900, 0.9907,\n",
       "             0.9907, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9938, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9967e-01, 9.9967e-01, 9.9966e-01, 9.9964e-01,\n",
       "             9.9964e-01, 9.9963e-01, 9.9962e-01, 9.9961e-01, 9.9960e-01, 9.9959e-01,\n",
       "             9.9956e-01, 9.9955e-01, 9.9954e-01, 9.9954e-01, 9.9951e-01, 9.9950e-01,\n",
       "             9.9948e-01, 9.9947e-01, 9.9947e-01, 9.9942e-01, 9.9941e-01, 9.9940e-01,\n",
       "             9.9940e-01, 9.9939e-01, 9.9939e-01, 9.9938e-01, 9.9937e-01, 9.9935e-01,\n",
       "             9.9935e-01, 9.9934e-01, 9.9931e-01, 9.9928e-01, 9.9923e-01, 9.9923e-01,\n",
       "             9.9922e-01, 9.9919e-01, 9.9919e-01, 9.9916e-01, 9.9914e-01, 9.9914e-01,\n",
       "             9.9912e-01, 9.9912e-01, 9.9910e-01, 9.9905e-01, 9.9901e-01, 9.9898e-01,\n",
       "             9.9889e-01, 9.9883e-01, 9.9874e-01, 9.9869e-01, 9.9852e-01, 9.9833e-01,\n",
       "             9.9826e-01, 9.9818e-01, 9.9815e-01, 9.9812e-01, 9.9805e-01, 9.9758e-01,\n",
       "             9.9750e-01, 9.9735e-01, 9.9734e-01, 9.9731e-01, 9.9712e-01, 9.9699e-01,\n",
       "             9.9696e-01, 9.9696e-01, 9.9674e-01, 9.9665e-01, 9.9654e-01, 9.9646e-01,\n",
       "             9.9634e-01, 9.9590e-01, 9.9582e-01, 9.9555e-01, 9.9502e-01, 9.9468e-01,\n",
       "             9.9343e-01, 9.9318e-01, 9.9270e-01, 9.9265e-01, 9.9098e-01, 9.9079e-01,\n",
       "             9.9066e-01, 9.9033e-01, 9.8786e-01, 9.8752e-01, 9.8602e-01, 9.8575e-01,\n",
       "             9.8475e-01, 9.8407e-01, 9.8284e-01, 9.8157e-01, 9.7644e-01, 9.7612e-01,\n",
       "             9.7352e-01, 9.7128e-01, 9.7108e-01, 9.6969e-01, 9.6403e-01, 9.6111e-01,\n",
       "             9.6068e-01, 9.5469e-01, 9.5021e-01, 9.5007e-01, 9.4995e-01, 9.4453e-01,\n",
       "             9.4354e-01, 9.4211e-01, 9.1796e-01, 9.1537e-01, 9.1420e-01, 9.1268e-01,\n",
       "             9.1082e-01, 8.9464e-01, 8.6950e-01, 8.6525e-01, 8.6353e-01, 8.6237e-01,\n",
       "             8.5459e-01, 8.3728e-01, 8.3679e-01, 8.3679e-01, 8.3445e-01, 7.8971e-01,\n",
       "             7.8740e-01, 7.8280e-01, 7.6842e-01, 7.4228e-01, 7.1490e-01, 6.8957e-01,\n",
       "             5.9197e-01, 5.4600e-01, 5.3702e-01, 5.2599e-01, 5.2242e-01, 5.1734e-01,\n",
       "             4.5799e-01, 4.4980e-01, 4.4821e-01, 4.4051e-01, 4.2743e-01, 4.1232e-01,\n",
       "             4.0986e-01, 3.7036e-01, 3.6691e-01, 3.1009e-01, 3.0538e-01, 2.9536e-01,\n",
       "             2.7431e-01, 2.7255e-01, 2.4027e-01, 2.1806e-01, 2.0800e-01, 2.0648e-01,\n",
       "             1.9721e-01, 1.8982e-01, 1.8813e-01, 1.5508e-01, 1.4544e-01, 1.3909e-01,\n",
       "             1.3753e-01, 1.2912e-01, 1.2562e-01, 1.2551e-01, 1.1966e-01, 1.1820e-01,\n",
       "             1.1225e-01, 1.0283e-01, 9.9855e-02, 9.9153e-02, 7.9775e-02, 7.8379e-02,\n",
       "             7.8018e-02, 7.6596e-02, 6.5132e-02, 6.3535e-02, 5.3420e-02, 5.0058e-02,\n",
       "             4.5772e-02, 4.4953e-02, 4.1841e-02, 4.1511e-02, 3.7237e-02, 3.6742e-02,\n",
       "             3.5995e-02, 3.2426e-02, 2.9202e-02, 2.8928e-02, 2.7182e-02, 2.5501e-02,\n",
       "             2.4477e-02, 2.2691e-02, 2.2626e-02, 2.2274e-02, 1.9670e-02, 1.9391e-02,\n",
       "             1.8748e-02, 1.7840e-02, 1.7407e-02, 1.6468e-02, 1.6387e-02, 1.2050e-02,\n",
       "             1.1935e-02, 1.1407e-02, 1.1215e-02, 1.0935e-02, 9.9749e-03, 9.6118e-03,\n",
       "             8.3804e-03, 7.5141e-03, 7.4946e-03, 7.2719e-03, 6.5954e-03, 6.4222e-03,\n",
       "             6.2322e-03, 5.7688e-03, 5.5554e-03, 4.6269e-03, 4.3719e-03, 4.3619e-03,\n",
       "             3.8319e-03, 3.0379e-03, 3.0068e-03, 2.7829e-03, 2.5357e-03, 2.3539e-03,\n",
       "             2.2634e-03, 2.2278e-03, 2.0046e-03, 1.8217e-03, 1.7973e-03, 1.5045e-03,\n",
       "             1.4860e-03, 1.2411e-03, 1.1454e-03, 1.1411e-03, 1.0527e-03, 1.0308e-03,\n",
       "             9.3788e-04, 8.4195e-04, 8.3972e-04, 7.5047e-04, 7.3634e-04, 7.3215e-04,\n",
       "             7.2326e-04, 7.0620e-04, 6.9246e-04, 6.1504e-04, 5.7775e-04, 5.7159e-04,\n",
       "             5.4466e-04, 5.1796e-04, 4.4894e-04, 4.1593e-04, 4.1536e-04, 4.0308e-04,\n",
       "             3.8853e-04, 3.7185e-04, 3.6883e-04, 3.6675e-04, 3.2470e-04, 3.1657e-04,\n",
       "             2.8420e-04, 2.5623e-04, 1.9317e-04, 1.9033e-04, 1.8747e-04, 1.8512e-04,\n",
       "             1.8084e-04, 1.7619e-04, 1.7489e-04, 1.6732e-04, 1.4639e-04, 1.4152e-04,\n",
       "             1.2725e-04, 1.1898e-04, 1.1697e-04, 1.1539e-04, 1.1470e-04, 1.1442e-04,\n",
       "             1.1244e-04, 1.0223e-04, 9.1198e-05, 8.4134e-05, 7.7801e-05, 7.5690e-05,\n",
       "             7.5486e-05, 7.1102e-05, 7.0548e-05, 7.0464e-05, 6.2983e-05, 6.0881e-05,\n",
       "             5.7676e-05, 5.7263e-05, 5.4940e-05, 5.2658e-05, 5.1811e-05, 3.9374e-05,\n",
       "             3.9339e-05, 3.9174e-05, 3.7560e-05, 3.6794e-05, 3.6637e-05, 3.6473e-05,\n",
       "             3.5433e-05, 3.4830e-05, 3.4509e-05, 3.2635e-05, 3.1136e-05, 2.9789e-05,\n",
       "             2.6688e-05, 2.6571e-05, 2.6394e-05, 2.4922e-05, 2.3234e-05, 2.2046e-05,\n",
       "             1.8147e-05, 1.6594e-05, 1.6266e-05, 1.6191e-05, 1.4262e-05, 1.4211e-05,\n",
       "             1.3656e-05, 1.3232e-05, 1.3163e-05, 1.3005e-05, 1.2533e-05, 1.2174e-05,\n",
       "             1.1702e-05, 1.1221e-05, 1.0625e-05, 1.0339e-05, 1.0143e-05, 9.7594e-06,\n",
       "             9.1719e-06, 8.7191e-06, 8.3740e-06, 8.2095e-06, 8.1028e-06, 7.8893e-06,\n",
       "             7.4977e-06, 7.1466e-06, 7.0780e-06, 6.9041e-06, 6.6308e-06, 5.8058e-06,\n",
       "             5.8023e-06, 5.2139e-06, 5.1401e-06, 5.0460e-06, 4.8247e-06, 4.8081e-06,\n",
       "             4.7799e-06, 4.6510e-06, 4.1702e-06, 3.9423e-06, 3.9416e-06, 3.8484e-06,\n",
       "             3.6877e-06, 3.6222e-06, 3.5141e-06, 3.3559e-06, 3.2986e-06, 2.9780e-06,\n",
       "             2.9513e-06, 2.6332e-06, 2.5717e-06, 2.5137e-06, 2.4979e-06, 2.3872e-06,\n",
       "             2.3573e-06, 2.3351e-06, 2.3309e-06, 2.2190e-06, 2.0318e-06, 2.0311e-06,\n",
       "             2.0043e-06, 1.8099e-06, 1.7168e-06, 1.7006e-06, 1.5940e-06, 1.5925e-06,\n",
       "             1.5351e-06, 1.3262e-06, 1.2895e-06, 1.1628e-06, 1.0583e-06, 9.7570e-07,\n",
       "             9.5618e-07, 8.8936e-07, 8.8379e-07, 8.3998e-07, 8.3666e-07, 8.0742e-07,\n",
       "             7.8870e-07, 7.6359e-07, 7.4141e-07, 7.3089e-07, 6.7116e-07, 6.5486e-07,\n",
       "             5.9484e-07, 4.8206e-07, 4.7892e-07, 4.3748e-07, 4.3589e-07, 4.3018e-07,\n",
       "             4.2219e-07, 3.6888e-07, 3.4216e-07, 3.2498e-07, 3.2104e-07, 3.1348e-07,\n",
       "             3.0047e-07, 2.8643e-07, 2.8060e-07, 2.6962e-07, 2.6357e-07, 2.5836e-07,\n",
       "             2.3719e-07, 2.3026e-07, 2.1716e-07, 2.0120e-07, 2.0005e-07, 1.9850e-07,\n",
       "             1.9622e-07, 1.9605e-07, 1.8227e-07, 1.8037e-07, 1.7760e-07, 1.7460e-07,\n",
       "             1.4145e-07, 1.1762e-07, 9.7011e-08, 9.2494e-08, 8.5749e-08, 8.2222e-08,\n",
       "             7.9464e-08, 7.9060e-08, 7.3231e-08, 6.4696e-08, 5.5055e-08, 5.1344e-08,\n",
       "             4.8077e-08, 4.2884e-08, 4.2543e-08, 4.0044e-08, 3.8476e-08, 3.8379e-08,\n",
       "             3.3255e-08, 3.3047e-08, 3.2319e-08, 2.9101e-08, 2.8910e-08, 2.6073e-08,\n",
       "             2.3434e-08, 2.2688e-08, 2.2333e-08, 1.9722e-08, 1.7948e-08, 1.6716e-08,\n",
       "             1.6591e-08, 1.5344e-08, 1.3705e-08, 1.3170e-08, 1.2395e-08, 1.1974e-08,\n",
       "             1.1905e-08, 1.0903e-08, 1.0269e-08, 9.8528e-09, 9.7319e-09, 9.4418e-09,\n",
       "             8.8985e-09, 8.8831e-09, 8.7261e-09, 7.7573e-09, 7.1105e-09, 6.7727e-09,\n",
       "             5.2980e-09, 4.7938e-09, 4.4186e-09, 4.2917e-09, 3.5990e-09, 3.5371e-09,\n",
       "             3.2861e-09, 3.1637e-09, 3.1537e-09, 3.0953e-09, 3.0526e-09, 3.0221e-09,\n",
       "             2.5071e-09, 2.4437e-09, 2.4304e-09, 2.3265e-09, 2.2872e-09, 2.0974e-09,\n",
       "             1.6777e-09, 1.6412e-09, 1.6133e-09, 1.5835e-09, 1.4841e-09, 1.1808e-09,\n",
       "             1.1343e-09, 8.5298e-10, 8.3973e-10, 7.9123e-10, 6.3974e-10, 6.3904e-10,\n",
       "             4.8508e-10, 4.1759e-10, 3.9676e-10, 3.9171e-10, 3.6051e-10, 3.3370e-10,\n",
       "             2.8353e-10, 2.6788e-10, 2.4134e-10, 2.1473e-10, 1.9897e-10, 1.6903e-10,\n",
       "             1.6137e-10, 1.4454e-10, 1.1625e-10, 1.1117e-10, 9.5737e-11, 8.1561e-11,\n",
       "             7.4725e-11, 7.3802e-11, 6.8791e-11, 6.5211e-11, 6.2699e-11, 4.8128e-11,\n",
       "             4.3447e-11, 4.1262e-11, 3.7028e-11, 3.4903e-11, 3.3857e-11, 2.7630e-11,\n",
       "             2.5378e-11, 2.3457e-11, 2.1622e-11, 1.4398e-11, 1.0267e-11, 7.7970e-12,\n",
       "             7.5859e-12, 7.2615e-12, 7.0311e-12, 6.9693e-12, 6.3993e-12, 5.4644e-12,\n",
       "             5.1417e-12, 2.1878e-12, 1.7359e-12, 1.2665e-12, 1.2058e-12, 6.9389e-13,\n",
       "             5.6049e-13, 5.5606e-13, 9.5977e-14, 2.8153e-14, 2.6260e-14])}},\n",
       "   {'fpr': np.float64(0.06304347826086956),\n",
       "    'tpr': np.float64(0.9783783783783784),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087,\n",
       "             0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087,\n",
       "             0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087,\n",
       "             0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0109,\n",
       "             0.0109, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0239, 0.0239,\n",
       "             0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0283, 0.0283, 0.0304,\n",
       "             0.0304, 0.0326, 0.0348, 0.0348, 0.0348, 0.0370, 0.0391, 0.0391, 0.0391,\n",
       "             0.0413, 0.0435, 0.0435, 0.0435, 0.0435, 0.0457, 0.0478, 0.0500, 0.0522,\n",
       "             0.0522, 0.0543, 0.0543, 0.0543, 0.0543, 0.0565, 0.0565, 0.0565, 0.0587,\n",
       "             0.0587, 0.0609, 0.0609, 0.0630, 0.0630, 0.0630, 0.0630, 0.0630, 0.0652,\n",
       "             0.0652, 0.0674, 0.0696, 0.0696, 0.0696, 0.0696, 0.0696, 0.0696, 0.0717,\n",
       "             0.0717, 0.0739, 0.0761, 0.0783, 0.0804, 0.0826, 0.0826, 0.0848, 0.0870,\n",
       "             0.0891, 0.0913, 0.0935, 0.0957, 0.0978, 0.1000, 0.1022, 0.1043, 0.1065,\n",
       "             0.1087, 0.1109, 0.1130, 0.1130, 0.1152, 0.1174, 0.1196, 0.1196, 0.1217,\n",
       "             0.1239, 0.1261, 0.1283, 0.1283, 0.1304, 0.1326, 0.1348, 0.1370, 0.1391,\n",
       "             0.1413, 0.1435, 0.1435, 0.1457, 0.1478, 0.1500, 0.1522, 0.1543, 0.1543,\n",
       "             0.1565, 0.1587, 0.1609, 0.1630, 0.1630, 0.1652, 0.1652, 0.1652, 0.1674,\n",
       "             0.1674, 0.1696, 0.1696, 0.1696, 0.1717, 0.1739, 0.1761, 0.1783, 0.1804,\n",
       "             0.1826, 0.1848, 0.1870, 0.1891, 0.1913, 0.1913, 0.1935, 0.1935, 0.1935,\n",
       "             0.1957, 0.1978, 0.1978, 0.2000, 0.2022, 0.2043, 0.2065, 0.2087, 0.2109,\n",
       "             0.2130, 0.2152, 0.2152, 0.2174, 0.2196, 0.2217, 0.2239, 0.2261, 0.2283,\n",
       "             0.2304, 0.2326, 0.2348, 0.2370, 0.2391, 0.2413, 0.2435, 0.2457, 0.2478,\n",
       "             0.2500, 0.2522, 0.2543, 0.2565, 0.2587, 0.2609, 0.2630, 0.2652, 0.2674,\n",
       "             0.2696, 0.2717, 0.2739, 0.2761, 0.2783, 0.2804, 0.2826, 0.2848, 0.2870,\n",
       "             0.2891, 0.2913, 0.2935, 0.2957, 0.2978, 0.3000, 0.3022, 0.3043, 0.3065,\n",
       "             0.3087, 0.3109, 0.3130, 0.3152, 0.3174, 0.3196, 0.3217, 0.3239, 0.3261,\n",
       "             0.3283, 0.3304, 0.3326, 0.3348, 0.3370, 0.3391, 0.3413, 0.3435, 0.3457,\n",
       "             0.3478, 0.3500, 0.3522, 0.3543, 0.3565, 0.3587, 0.3609, 0.3630, 0.3652,\n",
       "             0.3674, 0.3696, 0.3717, 0.3739, 0.3761, 0.3783, 0.3804, 0.3826, 0.3848,\n",
       "             0.3870, 0.3891, 0.3913, 0.3913, 0.3935, 0.3957, 0.3978, 0.4000, 0.4022,\n",
       "             0.4043, 0.4065, 0.4087, 0.4109, 0.4130, 0.4152, 0.4174, 0.4196, 0.4217,\n",
       "             0.4239, 0.4261, 0.4283, 0.4304, 0.4326, 0.4348, 0.4370, 0.4391, 0.4413,\n",
       "             0.4435, 0.4457, 0.4478, 0.4500, 0.4522, 0.4543, 0.4565, 0.4587, 0.4609,\n",
       "             0.4630, 0.4652, 0.4674, 0.4696, 0.4717, 0.4739, 0.4761, 0.4783, 0.4804,\n",
       "             0.4826, 0.4848, 0.4870, 0.4891, 0.4913, 0.4935, 0.4957, 0.4978, 0.5000,\n",
       "             0.5022, 0.5043, 0.5065, 0.5087, 0.5109, 0.5130, 0.5152, 0.5174, 0.5196,\n",
       "             0.5217, 0.5239, 0.5261, 0.5283, 0.5304, 0.5326, 0.5348, 0.5370, 0.5391,\n",
       "             0.5413, 0.5435, 0.5457, 0.5478, 0.5500, 0.5522, 0.5543, 0.5565, 0.5587,\n",
       "             0.5609, 0.5630, 0.5652, 0.5674, 0.5696, 0.5717, 0.5739, 0.5761, 0.5783,\n",
       "             0.5804, 0.5826, 0.5848, 0.5870, 0.5891, 0.5913, 0.5935, 0.5957, 0.5978,\n",
       "             0.6000, 0.6022, 0.6043, 0.6065, 0.6087, 0.6109, 0.6130, 0.6152, 0.6174,\n",
       "             0.6196, 0.6217, 0.6239, 0.6261, 0.6283, 0.6304, 0.6326, 0.6348, 0.6370,\n",
       "             0.6391, 0.6413, 0.6435, 0.6457, 0.6478, 0.6500, 0.6522, 0.6543, 0.6565,\n",
       "             0.6587, 0.6609, 0.6630, 0.6652, 0.6674, 0.6696, 0.6717, 0.6739, 0.6761,\n",
       "             0.6783, 0.6804, 0.6826, 0.6848, 0.6870, 0.6891, 0.6913, 0.6935, 0.6957,\n",
       "             0.6978, 0.7000, 0.7022, 0.7043, 0.7065, 0.7087, 0.7109, 0.7130, 0.7152,\n",
       "             0.7174, 0.7196, 0.7217, 0.7239, 0.7261, 0.7283, 0.7304, 0.7326, 0.7348,\n",
       "             0.7370, 0.7391, 0.7413, 0.7435, 0.7457, 0.7478, 0.7500, 0.7522, 0.7543,\n",
       "             0.7565, 0.7587, 0.7609, 0.7630, 0.7652, 0.7674, 0.7696, 0.7717, 0.7739,\n",
       "             0.7761, 0.7783, 0.7804, 0.7826, 0.7848, 0.7870, 0.7891, 0.7913, 0.7935,\n",
       "             0.7957, 0.7978, 0.8000, 0.8022, 0.8043, 0.8065, 0.8087, 0.8109, 0.8130,\n",
       "             0.8152, 0.8174, 0.8196, 0.8217, 0.8239, 0.8261, 0.8283, 0.8304, 0.8326,\n",
       "             0.8348, 0.8370, 0.8391, 0.8413, 0.8435, 0.8457, 0.8478, 0.8500, 0.8522,\n",
       "             0.8543, 0.8565, 0.8587, 0.8609, 0.8630, 0.8652, 0.8674, 0.8696, 0.8717,\n",
       "             0.8739, 0.8761, 0.8783, 0.8804, 0.8826, 0.8848, 0.8870, 0.8891, 0.8913,\n",
       "             0.8935, 0.8957, 0.8978, 0.9000, 0.9022, 0.9043, 0.9065, 0.9087, 0.9109,\n",
       "             0.9130, 0.9152, 0.9174, 0.9196, 0.9217, 0.9239, 0.9261, 0.9283, 0.9304,\n",
       "             0.9326, 0.9348, 0.9370, 0.9391, 0.9413, 0.9435, 0.9457, 0.9478, 0.9500,\n",
       "             0.9522, 0.9543, 0.9565, 0.9587, 0.9609, 0.9630, 0.9652, 0.9674, 0.9696,\n",
       "             0.9717, 0.9739, 0.9761, 0.9783, 0.9804, 0.9826, 0.9848, 0.9870, 0.9891,\n",
       "             0.9913, 0.9935, 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.3699, 0.4718, 0.5127, 0.5382, 0.5606, 0.5722, 0.5861, 0.6000,\n",
       "             0.6085, 0.6162, 0.6224, 0.6332, 0.6363, 0.6394, 0.6456, 0.6494, 0.6525,\n",
       "             0.6556, 0.6618, 0.6664, 0.6703, 0.6734, 0.6757, 0.6772, 0.6795, 0.6842,\n",
       "             0.6849, 0.6873, 0.6903, 0.6919, 0.6942, 0.6965, 0.6981, 0.6996, 0.7004,\n",
       "             0.7035, 0.7058, 0.7073, 0.7089, 0.7097, 0.7104, 0.7112, 0.7120, 0.7127,\n",
       "             0.7135, 0.7143, 0.7166, 0.7174, 0.7189, 0.7220, 0.7243, 0.7259, 0.7266,\n",
       "             0.7290, 0.7305, 0.7313, 0.7328, 0.7336, 0.7344, 0.7359, 0.7367, 0.7375,\n",
       "             0.7405, 0.7413, 0.7421, 0.7436, 0.7444, 0.7452, 0.7459, 0.7467, 0.7490,\n",
       "             0.7506, 0.7514, 0.7521, 0.7537, 0.7544, 0.7568, 0.7575, 0.7583, 0.7591,\n",
       "             0.7598, 0.7606, 0.7614, 0.7622, 0.7629, 0.7637, 0.7645, 0.7653, 0.7660,\n",
       "             0.7668, 0.7676, 0.7683, 0.7691, 0.7691, 0.7707, 0.7714, 0.7722, 0.7730,\n",
       "             0.7737, 0.7745, 0.7753, 0.7761, 0.7768, 0.7776, 0.7784, 0.7792, 0.7799,\n",
       "             0.7807, 0.7822, 0.7830, 0.7838, 0.7846, 0.7853, 0.7861, 0.7869, 0.7876,\n",
       "             0.7884, 0.7892, 0.7900, 0.7907, 0.7915, 0.7923, 0.7931, 0.7938, 0.7946,\n",
       "             0.7954, 0.7961, 0.7969, 0.7977, 0.7992, 0.8000, 0.8008, 0.8015, 0.8023,\n",
       "             0.8031, 0.8039, 0.8046, 0.8054, 0.8069, 0.8077, 0.8085, 0.8093, 0.8100,\n",
       "             0.8108, 0.8116, 0.8124, 0.8131, 0.8139, 0.8154, 0.8162, 0.8170, 0.8178,\n",
       "             0.8185, 0.8193, 0.8201, 0.8208, 0.8224, 0.8232, 0.8239, 0.8247, 0.8255,\n",
       "             0.8263, 0.8270, 0.8278, 0.8286, 0.8293, 0.8301, 0.8309, 0.8317, 0.8324,\n",
       "             0.8332, 0.8340, 0.8347, 0.8363, 0.8371, 0.8378, 0.8386, 0.8394, 0.8402,\n",
       "             0.8409, 0.8417, 0.8425, 0.8432, 0.8440, 0.8448, 0.8456, 0.8463, 0.8471,\n",
       "             0.8479, 0.8486, 0.8494, 0.8502, 0.8510, 0.8517, 0.8525, 0.8533, 0.8541,\n",
       "             0.8548, 0.8556, 0.8564, 0.8571, 0.8579, 0.8595, 0.8602, 0.8610, 0.8618,\n",
       "             0.8625, 0.8633, 0.8641, 0.8656, 0.8664, 0.8672, 0.8687, 0.8695, 0.8703,\n",
       "             0.8710, 0.8718, 0.8726, 0.8734, 0.8741, 0.8749, 0.8757, 0.8764, 0.8772,\n",
       "             0.8780, 0.8788, 0.8795, 0.8803, 0.8811, 0.8819, 0.8826, 0.8834, 0.8842,\n",
       "             0.8849, 0.8857, 0.8865, 0.8873, 0.8880, 0.8888, 0.8896, 0.8903, 0.8911,\n",
       "             0.8919, 0.8927, 0.8934, 0.8942, 0.8950, 0.8950, 0.8958, 0.8965, 0.8973,\n",
       "             0.8981, 0.8988, 0.8996, 0.9004, 0.9012, 0.9019, 0.9027, 0.9035, 0.9042,\n",
       "             0.9050, 0.9058, 0.9066, 0.9066, 0.9073, 0.9081, 0.9089, 0.9097, 0.9104,\n",
       "             0.9112, 0.9120, 0.9127, 0.9135, 0.9143, 0.9151, 0.9158, 0.9166, 0.9174,\n",
       "             0.9181, 0.9189, 0.9197, 0.9205, 0.9212, 0.9220, 0.9228, 0.9236, 0.9243,\n",
       "             0.9251, 0.9259, 0.9266, 0.9274, 0.9282, 0.9290, 0.9297, 0.9305, 0.9305,\n",
       "             0.9313, 0.9313, 0.9320, 0.9328, 0.9336, 0.9344, 0.9351, 0.9359, 0.9367,\n",
       "             0.9367, 0.9375, 0.9382, 0.9390, 0.9398, 0.9405, 0.9413, 0.9421, 0.9429,\n",
       "             0.9429, 0.9436, 0.9444, 0.9452, 0.9459, 0.9459, 0.9467, 0.9475, 0.9483,\n",
       "             0.9483, 0.9490, 0.9498, 0.9506, 0.9514, 0.9521, 0.9529, 0.9529, 0.9537,\n",
       "             0.9544, 0.9552, 0.9560, 0.9568, 0.9575, 0.9583, 0.9591, 0.9598, 0.9598,\n",
       "             0.9606, 0.9614, 0.9622, 0.9629, 0.9637, 0.9645, 0.9645, 0.9653, 0.9653,\n",
       "             0.9660, 0.9660, 0.9660, 0.9668, 0.9676, 0.9676, 0.9676, 0.9683, 0.9691,\n",
       "             0.9691, 0.9691, 0.9699, 0.9707, 0.9714, 0.9714, 0.9714, 0.9714, 0.9714,\n",
       "             0.9722, 0.9722, 0.9730, 0.9737, 0.9745, 0.9745, 0.9753, 0.9761, 0.9761,\n",
       "             0.9768, 0.9768, 0.9776, 0.9776, 0.9784, 0.9792, 0.9799, 0.9807, 0.9807,\n",
       "             0.9815, 0.9815, 0.9815, 0.9822, 0.9830, 0.9838, 0.9846, 0.9853, 0.9853,\n",
       "             0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9869, 0.9869, 0.9869,\n",
       "             0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869,\n",
       "             0.9869, 0.9869, 0.9869, 0.9876, 0.9876, 0.9876, 0.9876, 0.9884, 0.9884,\n",
       "             0.9884, 0.9884, 0.9884, 0.9892, 0.9892, 0.9892, 0.9892, 0.9892, 0.9892,\n",
       "             0.9892, 0.9892, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9907,\n",
       "             0.9907, 0.9907, 0.9907, 0.9907, 0.9915, 0.9915, 0.9923, 0.9931, 0.9931,\n",
       "             0.9938, 0.9938, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9969, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9977e-01,\n",
       "             9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01,\n",
       "             9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9967e-01,\n",
       "             9.9967e-01, 9.9964e-01, 9.9964e-01, 9.9964e-01, 9.9963e-01, 9.9963e-01,\n",
       "             9.9960e-01, 9.9954e-01, 9.9954e-01, 9.9954e-01, 9.9953e-01, 9.9952e-01,\n",
       "             9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9947e-01, 9.9947e-01, 9.9947e-01,\n",
       "             9.9946e-01, 9.9944e-01, 9.9944e-01, 9.9936e-01, 9.9933e-01, 9.9933e-01,\n",
       "             9.9931e-01, 9.9931e-01, 9.9930e-01, 9.9928e-01, 9.9927e-01, 9.9925e-01,\n",
       "             9.9925e-01, 9.9924e-01, 9.9923e-01, 9.9919e-01, 9.9918e-01, 9.9918e-01,\n",
       "             9.9914e-01, 9.9913e-01, 9.9912e-01, 9.9909e-01, 9.9906e-01, 9.9898e-01,\n",
       "             9.9898e-01, 9.9890e-01, 9.9889e-01, 9.9887e-01, 9.9874e-01, 9.9874e-01,\n",
       "             9.9861e-01, 9.9850e-01, 9.9847e-01, 9.9843e-01, 9.9817e-01, 9.9800e-01,\n",
       "             9.9764e-01, 9.9763e-01, 9.9749e-01, 9.9740e-01, 9.9729e-01, 9.9702e-01,\n",
       "             9.9697e-01, 9.9660e-01, 9.9613e-01, 9.9582e-01, 9.9577e-01, 9.9573e-01,\n",
       "             9.9559e-01, 9.9547e-01, 9.9535e-01, 9.9502e-01, 9.9465e-01, 9.9447e-01,\n",
       "             9.9444e-01, 9.9428e-01, 9.9416e-01, 9.9415e-01, 9.9394e-01, 9.9393e-01,\n",
       "             9.9346e-01, 9.9248e-01, 9.9206e-01, 9.9139e-01, 9.9097e-01, 9.8949e-01,\n",
       "             9.8949e-01, 9.8905e-01, 9.8891e-01, 9.8739e-01, 9.8520e-01, 9.8511e-01,\n",
       "             9.8412e-01, 9.8367e-01, 9.8241e-01, 9.7877e-01, 9.7863e-01, 9.7697e-01,\n",
       "             9.7546e-01, 9.7146e-01, 9.7005e-01, 9.6426e-01, 9.6077e-01, 9.5891e-01,\n",
       "             9.5707e-01, 9.5478e-01, 9.4823e-01, 9.4639e-01, 9.4592e-01, 9.3991e-01,\n",
       "             9.3540e-01, 9.3032e-01, 9.2737e-01, 9.1124e-01, 9.1098e-01, 9.0839e-01,\n",
       "             9.0557e-01, 8.9887e-01, 8.6504e-01, 8.6011e-01, 8.5702e-01, 8.5148e-01,\n",
       "             8.5121e-01, 8.4557e-01, 8.2594e-01, 8.1952e-01, 8.0813e-01, 7.9775e-01,\n",
       "             7.7476e-01, 7.5343e-01, 7.4748e-01, 7.2164e-01, 7.0288e-01, 6.7898e-01,\n",
       "             6.7541e-01, 6.5808e-01, 6.4710e-01, 6.4241e-01, 6.2840e-01, 5.6993e-01,\n",
       "             5.6615e-01, 5.6019e-01, 5.4681e-01, 5.4265e-01, 5.2232e-01, 3.7932e-01,\n",
       "             3.6249e-01, 3.4261e-01, 3.3010e-01, 3.2282e-01, 3.0185e-01, 3.0120e-01,\n",
       "             2.8565e-01, 2.6744e-01, 2.3303e-01, 2.0273e-01, 1.5480e-01, 1.4684e-01,\n",
       "             1.2318e-01, 1.2129e-01, 1.1223e-01, 1.1034e-01, 9.7824e-02, 9.5769e-02,\n",
       "             9.1973e-02, 8.6605e-02, 7.7764e-02, 7.7448e-02, 7.0513e-02, 5.9444e-02,\n",
       "             5.8365e-02, 5.6715e-02, 4.4793e-02, 4.4369e-02, 3.8510e-02, 3.5109e-02,\n",
       "             3.3842e-02, 3.1782e-02, 3.1389e-02, 3.0707e-02, 2.9686e-02, 2.8663e-02,\n",
       "             2.7203e-02, 2.6441e-02, 2.5518e-02, 2.5440e-02, 2.4276e-02, 2.3113e-02,\n",
       "             2.2190e-02, 2.1998e-02, 2.0149e-02, 1.8706e-02, 1.6532e-02, 1.4799e-02,\n",
       "             7.0266e-03, 6.0648e-03, 5.9300e-03, 5.5668e-03, 5.2398e-03, 5.0348e-03,\n",
       "             4.5803e-03, 4.3912e-03, 4.2392e-03, 3.9740e-03, 3.7549e-03, 3.6472e-03,\n",
       "             3.1081e-03, 2.5492e-03, 2.1932e-03, 2.1573e-03, 2.1401e-03, 2.0636e-03,\n",
       "             1.8889e-03, 1.7731e-03, 1.6803e-03, 1.6164e-03, 1.6054e-03, 1.5736e-03,\n",
       "             1.5452e-03, 1.2040e-03, 1.1543e-03, 1.0697e-03, 1.0631e-03, 1.0193e-03,\n",
       "             1.0094e-03, 9.6953e-04, 9.5124e-04, 9.3103e-04, 8.4746e-04, 7.9396e-04,\n",
       "             7.8600e-04, 7.1448e-04, 6.0517e-04, 4.8258e-04, 4.7071e-04, 4.5182e-04,\n",
       "             4.2702e-04, 4.2170e-04, 3.5428e-04, 3.4750e-04, 3.3478e-04, 2.7584e-04,\n",
       "             2.5974e-04, 2.5576e-04, 2.5010e-04, 2.0167e-04, 1.7778e-04, 1.5804e-04,\n",
       "             1.3451e-04, 1.2227e-04, 1.2024e-04, 1.1891e-04, 1.0090e-04, 7.8116e-05,\n",
       "             7.7993e-05, 7.7680e-05, 7.4455e-05, 6.7475e-05, 6.2630e-05, 6.0168e-05,\n",
       "             5.9715e-05, 5.9279e-05, 5.1446e-05, 5.1221e-05, 5.0517e-05, 4.7984e-05,\n",
       "             4.4722e-05, 4.3834e-05, 4.2397e-05, 3.9265e-05, 3.6848e-05, 3.2672e-05,\n",
       "             2.4133e-05, 2.4063e-05, 2.3691e-05, 2.3498e-05, 2.1940e-05, 2.0011e-05,\n",
       "             1.7973e-05, 1.6625e-05, 1.5906e-05, 1.4840e-05, 1.3422e-05, 1.2986e-05,\n",
       "             1.2017e-05, 1.1898e-05, 1.1744e-05, 1.1340e-05, 1.0381e-05, 1.0278e-05,\n",
       "             9.6098e-06, 8.2368e-06, 7.5316e-06, 7.4459e-06, 6.2913e-06, 5.9804e-06,\n",
       "             5.7200e-06, 5.2236e-06, 4.6236e-06, 4.5803e-06, 4.3155e-06, 3.9257e-06,\n",
       "             3.8419e-06, 3.5178e-06, 3.1859e-06, 3.1510e-06, 2.8749e-06, 2.7665e-06,\n",
       "             2.6470e-06, 2.6446e-06, 2.5505e-06, 2.4854e-06, 2.3755e-06, 2.3748e-06,\n",
       "             2.1818e-06, 2.1708e-06, 2.0036e-06, 1.9443e-06, 1.9339e-06, 1.8622e-06,\n",
       "             1.8250e-06, 1.7921e-06, 1.7470e-06, 1.7311e-06, 1.6576e-06, 1.6028e-06,\n",
       "             1.4572e-06, 1.2498e-06, 1.2434e-06, 1.2398e-06, 1.2149e-06, 1.1540e-06,\n",
       "             1.0861e-06, 1.0391e-06, 9.5203e-07, 9.3935e-07, 8.9218e-07, 8.5249e-07,\n",
       "             8.4260e-07, 7.8388e-07, 7.6112e-07, 7.4179e-07, 7.3243e-07, 7.2844e-07,\n",
       "             7.1550e-07, 7.0279e-07, 6.9695e-07, 6.5534e-07, 6.5193e-07, 6.4167e-07,\n",
       "             5.3257e-07, 5.1964e-07, 5.0942e-07, 4.4153e-07, 3.8862e-07, 3.8391e-07,\n",
       "             3.4791e-07, 3.2744e-07, 3.2459e-07, 3.1262e-07, 2.9869e-07, 2.5678e-07,\n",
       "             2.5613e-07, 2.5538e-07, 2.5105e-07, 2.4242e-07, 2.3362e-07, 2.2721e-07,\n",
       "             2.2596e-07, 2.1364e-07, 2.1348e-07, 2.1034e-07, 2.0922e-07, 2.0720e-07,\n",
       "             2.0252e-07, 1.6530e-07, 1.5597e-07, 1.5348e-07, 1.4675e-07, 1.4168e-07,\n",
       "             1.3326e-07, 1.1248e-07, 1.0929e-07, 1.0701e-07, 1.0356e-07, 1.0029e-07,\n",
       "             8.9916e-08, 8.6817e-08, 8.3267e-08, 7.7681e-08, 7.6436e-08, 7.2965e-08,\n",
       "             7.1389e-08, 6.5957e-08, 6.1522e-08, 6.0691e-08, 5.8938e-08, 5.8513e-08,\n",
       "             5.4534e-08, 5.1238e-08, 5.0595e-08, 4.8844e-08, 4.6038e-08, 4.5833e-08,\n",
       "             4.3881e-08, 4.3173e-08, 4.3032e-08, 4.2216e-08, 3.8392e-08, 3.6061e-08,\n",
       "             3.5923e-08, 3.2660e-08, 3.0583e-08, 3.0057e-08, 2.8720e-08, 2.8122e-08,\n",
       "             2.4284e-08, 2.2207e-08, 2.2130e-08, 2.2092e-08, 2.0898e-08, 2.0494e-08,\n",
       "             2.0365e-08, 1.9613e-08, 1.8607e-08, 1.6782e-08, 1.5680e-08, 1.5648e-08,\n",
       "             1.4769e-08, 1.4571e-08, 1.4527e-08, 1.3616e-08, 1.3226e-08, 1.3105e-08,\n",
       "             1.1985e-08, 1.1256e-08, 1.1144e-08, 1.1045e-08, 1.0937e-08, 9.9549e-09,\n",
       "             9.4010e-09, 8.2281e-09, 8.0232e-09, 7.4144e-09, 7.3488e-09, 5.7634e-09,\n",
       "             5.6273e-09, 5.5883e-09, 5.4870e-09, 5.3996e-09, 5.3418e-09, 4.5985e-09,\n",
       "             4.3523e-09, 4.1383e-09, 3.8399e-09, 3.1122e-09, 2.7979e-09, 2.7383e-09,\n",
       "             2.7065e-09, 2.5823e-09, 2.5025e-09, 2.3533e-09, 1.4929e-09, 1.3302e-09,\n",
       "             1.2949e-09, 1.1297e-09, 1.1278e-09, 1.1190e-09, 1.0843e-09, 1.0576e-09,\n",
       "             1.0321e-09, 9.9768e-10, 9.7263e-10, 8.2079e-10, 7.3641e-10, 5.7544e-10,\n",
       "             5.1718e-10, 5.0229e-10, 4.8786e-10, 4.8714e-10, 4.8076e-10, 4.5819e-10,\n",
       "             4.1489e-10, 4.1486e-10, 3.8811e-10, 3.6221e-10, 3.0475e-10, 2.7502e-10,\n",
       "             2.6913e-10, 2.6591e-10, 2.3750e-10, 2.3445e-10, 2.2661e-10, 2.1391e-10,\n",
       "             2.0314e-10, 1.9706e-10, 1.8194e-10, 1.6863e-10, 1.6825e-10, 1.5689e-10,\n",
       "             1.4722e-10, 1.3812e-10, 1.2049e-10, 1.1342e-10, 1.0841e-10, 1.0576e-10,\n",
       "             9.7629e-11, 9.5474e-11, 8.6360e-11, 8.3873e-11, 8.1992e-11, 7.6041e-11,\n",
       "             6.7319e-11, 5.5223e-11, 5.3151e-11, 5.1753e-11, 4.7599e-11, 4.6682e-11,\n",
       "             4.0486e-11, 3.0553e-11, 2.8900e-11, 2.8692e-11, 2.7515e-11, 2.5749e-11,\n",
       "             2.3443e-11, 2.2686e-11, 2.2477e-11, 2.1430e-11, 2.1294e-11, 2.1161e-11,\n",
       "             1.8535e-11, 1.6815e-11, 1.2508e-11, 1.1554e-11, 1.0351e-11, 9.0444e-12,\n",
       "             8.9620e-12, 8.1908e-12, 8.1048e-12, 7.8502e-12, 6.7650e-12, 6.2405e-12,\n",
       "             6.1608e-12, 6.0437e-12, 5.9733e-12, 5.4347e-12, 4.8075e-12, 4.5115e-12,\n",
       "             4.3457e-12, 3.5790e-12, 3.5705e-12, 3.3325e-12, 2.6346e-12, 2.0641e-12,\n",
       "             1.9539e-12, 1.8664e-12, 1.7972e-12, 1.7949e-12, 1.2986e-12, 1.2364e-12,\n",
       "             1.1012e-12, 8.1968e-13, 7.0869e-13, 5.1936e-13, 3.8325e-13, 2.6326e-13,\n",
       "             2.3376e-13, 2.2427e-13, 1.8699e-13, 1.4920e-13, 1.4583e-13, 1.3828e-13,\n",
       "             1.2014e-13, 1.0814e-13, 1.0486e-13, 1.0415e-13, 9.3586e-14, 9.2675e-14,\n",
       "             9.1171e-14, 7.2380e-14, 5.5881e-14, 5.3028e-14, 3.2048e-14, 2.9363e-14,\n",
       "             2.5521e-14, 2.1497e-14, 1.9855e-14, 1.2879e-14, 1.2235e-14, 1.2057e-14,\n",
       "             1.1852e-14, 1.0616e-14, 8.7956e-15, 6.8887e-15, 5.7765e-15, 3.5053e-15,\n",
       "             1.4645e-15, 1.0997e-15, 1.0791e-15, 5.8970e-16, 3.4274e-16, 2.7056e-16,\n",
       "             1.8156e-17, 1.9361e-18])}},\n",
       "   {'fpr': np.float64(0.07391304347826087),\n",
       "    'tpr': np.float64(0.9814671814671815),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.1398, 0.2139,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.1795e-13, 5.3924e-14,\n",
       "             2.5258e-14])}},\n",
       "   {'fpr': np.float64(0.13478260869565217),\n",
       "    'tpr': np.float64(0.9899613899613899),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0065, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0109,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
       "             0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0174, 0.0174, 0.0174, 0.0174,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0196, 0.0196,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0239, 0.0239, 0.0239, 0.0239,\n",
       "             0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239,\n",
       "             0.0239, 0.0239, 0.0261, 0.0261, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283,\n",
       "             0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283,\n",
       "             0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0304, 0.0304, 0.0304, 0.0304,\n",
       "             0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304,\n",
       "             0.0326, 0.0326, 0.0326, 0.0326, 0.0348, 0.0348, 0.0348, 0.0348, 0.0348,\n",
       "             0.0348, 0.0348, 0.0348, 0.0348, 0.0348, 0.0348, 0.0348, 0.0370, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0413, 0.0435, 0.0457, 0.0457, 0.0457,\n",
       "             0.0457, 0.0457, 0.0457, 0.0478, 0.0478, 0.0478, 0.0478, 0.0500, 0.0500,\n",
       "             0.0522, 0.0522, 0.0522, 0.0522, 0.0522, 0.0522, 0.0522, 0.0522, 0.0522,\n",
       "             0.0543, 0.0565, 0.0565, 0.0565, 0.0587, 0.0587, 0.0587, 0.0587, 0.0587,\n",
       "             0.0587, 0.0587, 0.0587, 0.0609, 0.0609, 0.0609, 0.0609, 0.0609, 0.0630,\n",
       "             0.0630, 0.0630, 0.0652, 0.0674, 0.0696, 0.0717, 0.0717, 0.0739, 0.0761,\n",
       "             0.0783, 0.0783, 0.0804, 0.0804, 0.0826, 0.0848, 0.0870, 0.0870, 0.0891,\n",
       "             0.0913, 0.0935, 0.0957, 0.0957, 0.0957, 0.0978, 0.1000, 0.1022, 0.1022,\n",
       "             0.1043, 0.1065, 0.1065, 0.1087, 0.1109, 0.1130, 0.1130, 0.1152, 0.1174,\n",
       "             0.1196, 0.1217, 0.1239, 0.1261, 0.1283, 0.1283, 0.1304, 0.1326, 0.1326,\n",
       "             0.1348, 0.1348, 0.1370, 0.1391, 0.1391, 0.1413, 0.1413, 0.1435, 0.1457,\n",
       "             0.1478, 0.1500, 0.1522, 0.1543, 0.1565, 0.1587, 0.1609, 0.1609, 0.1630,\n",
       "             0.1652, 0.1674, 0.1674, 0.1696, 0.1717, 0.1739, 0.1761, 0.1783, 0.1804,\n",
       "             0.1804, 0.1826, 0.1848, 0.1870, 0.1891, 0.1913, 0.1935, 0.1957, 0.1978,\n",
       "             0.1978, 0.2000, 0.2000, 0.2022, 0.2022, 0.2043, 0.2065, 0.2087, 0.2109,\n",
       "             0.2130, 0.2152, 0.2174, 0.2196, 0.2217, 0.2239, 0.2261, 0.2283, 0.2304,\n",
       "             0.2326, 0.2348, 0.2370, 0.2391, 0.2413, 0.2435, 0.2457, 0.2478, 0.2500,\n",
       "             0.2522, 0.2543, 0.2565, 0.2587, 0.2609, 0.2630, 0.2652, 0.2674, 0.2696,\n",
       "             0.2696, 0.2717, 0.2717, 0.2739, 0.2761, 0.2783, 0.2804, 0.2826, 0.2848,\n",
       "             0.2870, 0.2891, 0.2913, 0.2935, 0.2957, 0.2978, 0.3000, 0.3022, 0.3043,\n",
       "             0.3065, 0.3087, 0.3109, 0.3130, 0.3152, 0.3174, 0.3196, 0.3217, 0.3239,\n",
       "             0.3261, 0.3283, 0.3304, 0.3326, 0.3348, 0.3370, 0.3370, 0.3391, 0.3413,\n",
       "             0.3435, 0.3457, 0.3478, 0.3500, 0.3522, 0.3543, 0.3565, 0.3587, 0.3609,\n",
       "             0.3630, 0.3652, 0.3674, 0.3696, 0.3717, 0.3739, 0.3761, 0.3783, 0.3804,\n",
       "             0.3826, 0.3848, 0.3870, 0.3891, 0.3913, 0.3935, 0.3957, 0.3978, 0.4000,\n",
       "             0.4022, 0.4043, 0.4065, 0.4087, 0.4109, 0.4130, 0.4152, 0.4174, 0.4196,\n",
       "             0.4217, 0.4239, 0.4261, 0.4283, 0.4304, 0.4326, 0.4348, 0.4370, 0.4391,\n",
       "             0.4413, 0.4435, 0.4457, 0.4478, 0.4500, 0.4522, 0.4543, 0.4565, 0.4587,\n",
       "             0.4609, 0.4630, 0.4652, 0.4674, 0.4696, 0.4717, 0.4739, 0.4761, 0.4783,\n",
       "             0.4804, 0.4826, 0.4826, 0.4848, 0.4870, 0.4891, 0.4913, 0.4935, 0.4957,\n",
       "             0.4978, 0.5000, 0.5022, 0.5043, 0.5065, 0.5087, 0.5109, 0.5130, 0.5152,\n",
       "             0.5174, 0.5196, 0.5217, 0.5239, 0.5261, 0.5283, 0.5304, 0.5326, 0.5348,\n",
       "             0.5370, 0.5391, 0.5413, 0.5435, 0.5457, 0.5478, 0.5500, 0.5522, 0.5543,\n",
       "             0.5565, 0.5587, 0.5609, 0.5630, 0.5652, 0.5674, 0.5696, 0.5717, 0.5739,\n",
       "             0.5761, 0.5783, 0.5804, 0.5826, 0.5848, 0.5870, 0.5891, 0.5913, 0.5935,\n",
       "             0.5957, 0.5978, 0.6000, 0.6022, 0.6043, 0.6065, 0.6087, 0.6109, 0.6130,\n",
       "             0.6152, 0.6174, 0.6196, 0.6217, 0.6239, 0.6261, 0.6283, 0.6304, 0.6326,\n",
       "             0.6348, 0.6370, 0.6391, 0.6413, 0.6435, 0.6457, 0.6478, 0.6500, 0.6522,\n",
       "             0.6543, 0.6565, 0.6587, 0.6609, 0.6630, 0.6652, 0.6674, 0.6696, 0.6717,\n",
       "             0.6739, 0.6761, 0.6783, 0.6804, 0.6826, 0.6848, 0.6870, 0.6891, 0.6913,\n",
       "             0.6935, 0.6957, 0.6978, 0.7000, 0.7022, 0.7043, 0.7065, 0.7087, 0.7109,\n",
       "             0.7130, 0.7152, 0.7174, 0.7196, 0.7217, 0.7239, 0.7261, 0.7283, 0.7304,\n",
       "             0.7326, 0.7348, 0.7370, 0.7391, 0.7413, 0.7435, 0.7457, 0.7478, 0.7500,\n",
       "             0.7522, 0.7543, 0.7565, 0.7587, 0.7609, 0.7630, 0.7652, 0.7674, 0.7696,\n",
       "             0.7717, 0.7739, 0.7761, 0.7783, 0.7804, 0.7826, 0.7848, 0.7870, 0.7891,\n",
       "             0.7913, 0.7935, 0.7957, 0.7978, 0.8000, 0.8022, 0.8043, 0.8065, 0.8087,\n",
       "             0.8109, 0.8130, 0.8152, 0.8174, 0.8196, 0.8217, 0.8239, 0.8261, 0.8283,\n",
       "             0.8304, 0.8326, 0.8348, 0.8370, 0.8391, 0.8413, 0.8435, 0.8457, 0.8478,\n",
       "             0.8500, 0.8522, 0.8543, 0.8565, 0.8587, 0.8609, 0.8630, 0.8652, 0.8674,\n",
       "             0.8696, 0.8717, 0.8739, 0.8761, 0.8783, 0.8804, 0.8826, 0.8848, 0.8870,\n",
       "             0.8891, 0.8913, 0.8935, 0.8957, 0.8978, 0.9000, 0.9022, 0.9043, 0.9065,\n",
       "             0.9087, 0.9109, 0.9130, 0.9152, 0.9174, 0.9196, 0.9217, 0.9239, 0.9261,\n",
       "             0.9283, 0.9304, 0.9326, 0.9348, 0.9370, 0.9391, 0.9413, 0.9435, 0.9457,\n",
       "             0.9478, 0.9500, 0.9522, 0.9543, 0.9565, 0.9587, 0.9609, 0.9630, 0.9652,\n",
       "             0.9674, 0.9696, 0.9717, 0.9739, 0.9761, 0.9783, 0.9804, 0.9826, 0.9848,\n",
       "             0.9870, 0.9891, 0.9913, 0.9935, 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.4077, 0.5328, 0.5985, 0.6371, 0.6618, 0.6842, 0.6965, 0.7081,\n",
       "             0.7143, 0.7220, 0.7290, 0.7351, 0.7398, 0.7459, 0.7521, 0.7575, 0.7645,\n",
       "             0.7668, 0.7714, 0.7730, 0.7761, 0.7807, 0.7838, 0.7876, 0.7892, 0.7931,\n",
       "             0.7938, 0.7961, 0.7985, 0.7992, 0.8015, 0.8031, 0.8054, 0.8069, 0.8085,\n",
       "             0.8116, 0.8124, 0.8162, 0.8170, 0.8185, 0.8201, 0.8239, 0.8255, 0.8263,\n",
       "             0.8270, 0.8301, 0.8317, 0.8324, 0.8332, 0.8340, 0.8347, 0.8355, 0.8371,\n",
       "             0.8386, 0.8409, 0.8417, 0.8432, 0.8440, 0.8440, 0.8456, 0.8463, 0.8486,\n",
       "             0.8502, 0.8510, 0.8517, 0.8533, 0.8541, 0.8556, 0.8564, 0.8571, 0.8587,\n",
       "             0.8595, 0.8602, 0.8618, 0.8625, 0.8633, 0.8641, 0.8656, 0.8664, 0.8680,\n",
       "             0.8695, 0.8703, 0.8718, 0.8726, 0.8741, 0.8764, 0.8772, 0.8780, 0.8788,\n",
       "             0.8795, 0.8803, 0.8826, 0.8842, 0.8849, 0.8857, 0.8865, 0.8880, 0.8888,\n",
       "             0.8896, 0.8903, 0.8911, 0.8919, 0.8927, 0.8934, 0.8942, 0.8950, 0.8965,\n",
       "             0.8973, 0.8981, 0.8988, 0.8996, 0.9004, 0.9012, 0.9019, 0.9027, 0.9035,\n",
       "             0.9042, 0.9050, 0.9058, 0.9066, 0.9073, 0.9073, 0.9081, 0.9089, 0.9097,\n",
       "             0.9104, 0.9112, 0.9120, 0.9135, 0.9143, 0.9151, 0.9158, 0.9166, 0.9174,\n",
       "             0.9181, 0.9189, 0.9189, 0.9197, 0.9197, 0.9205, 0.9212, 0.9220, 0.9228,\n",
       "             0.9236, 0.9243, 0.9251, 0.9259, 0.9266, 0.9274, 0.9282, 0.9290, 0.9297,\n",
       "             0.9305, 0.9313, 0.9320, 0.9328, 0.9336, 0.9336, 0.9344, 0.9351, 0.9359,\n",
       "             0.9367, 0.9375, 0.9382, 0.9390, 0.9398, 0.9405, 0.9413, 0.9421, 0.9429,\n",
       "             0.9429, 0.9436, 0.9444, 0.9452, 0.9452, 0.9459, 0.9467, 0.9475, 0.9483,\n",
       "             0.9490, 0.9498, 0.9506, 0.9514, 0.9521, 0.9529, 0.9537, 0.9537, 0.9537,\n",
       "             0.9544, 0.9552, 0.9560, 0.9568, 0.9568, 0.9568, 0.9568, 0.9575, 0.9583,\n",
       "             0.9591, 0.9598, 0.9606, 0.9606, 0.9614, 0.9622, 0.9629, 0.9629, 0.9637,\n",
       "             0.9637, 0.9645, 0.9653, 0.9660, 0.9668, 0.9676, 0.9683, 0.9691, 0.9699,\n",
       "             0.9699, 0.9699, 0.9707, 0.9714, 0.9714, 0.9722, 0.9730, 0.9737, 0.9745,\n",
       "             0.9753, 0.9761, 0.9768, 0.9768, 0.9776, 0.9784, 0.9792, 0.9799, 0.9799,\n",
       "             0.9807, 0.9815, 0.9815, 0.9815, 0.9815, 0.9815, 0.9822, 0.9822, 0.9822,\n",
       "             0.9822, 0.9830, 0.9830, 0.9838, 0.9838, 0.9838, 0.9838, 0.9846, 0.9846,\n",
       "             0.9846, 0.9846, 0.9846, 0.9853, 0.9861, 0.9861, 0.9861, 0.9861, 0.9869,\n",
       "             0.9869, 0.9869, 0.9876, 0.9876, 0.9876, 0.9876, 0.9884, 0.9884, 0.9884,\n",
       "             0.9884, 0.9884, 0.9884, 0.9884, 0.9884, 0.9892, 0.9892, 0.9892, 0.9900,\n",
       "             0.9900, 0.9907, 0.9907, 0.9907, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9954, 0.9954, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9980e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9975e-01, 9.9974e-01, 9.9972e-01,\n",
       "             9.9969e-01, 9.9967e-01, 9.9966e-01, 9.9959e-01, 9.9956e-01, 9.9955e-01,\n",
       "             9.9952e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01, 9.9949e-01, 9.9931e-01,\n",
       "             9.9930e-01, 9.9917e-01, 9.9912e-01, 9.9911e-01, 9.9895e-01, 9.9894e-01,\n",
       "             9.9883e-01, 9.9877e-01, 9.9848e-01, 9.9811e-01, 9.9806e-01, 9.9800e-01,\n",
       "             9.9787e-01, 9.9786e-01, 9.9777e-01, 9.9736e-01, 9.9668e-01, 9.9658e-01,\n",
       "             9.9632e-01, 9.9615e-01, 9.9575e-01, 9.9537e-01, 9.9524e-01, 9.9414e-01,\n",
       "             9.9369e-01, 9.9266e-01, 9.9250e-01, 9.9135e-01, 9.9126e-01, 9.9107e-01,\n",
       "             9.8865e-01, 9.8858e-01, 9.8756e-01, 9.8572e-01, 9.8459e-01, 9.8293e-01,\n",
       "             9.8039e-01, 9.8035e-01, 9.7999e-01, 9.7897e-01, 9.7792e-01, 9.7424e-01,\n",
       "             9.6957e-01, 9.6422e-01, 9.6333e-01, 9.6128e-01, 9.5929e-01, 9.5499e-01,\n",
       "             9.5437e-01, 9.4821e-01, 9.4123e-01, 9.3838e-01, 9.1315e-01, 9.0153e-01,\n",
       "             8.8290e-01, 8.8022e-01, 8.7670e-01, 8.6080e-01, 8.4662e-01, 8.1763e-01,\n",
       "             7.9785e-01, 7.3149e-01, 7.3076e-01, 7.1252e-01, 6.8837e-01, 6.8373e-01,\n",
       "             6.6482e-01, 6.4240e-01, 6.0544e-01, 5.9672e-01, 5.6229e-01, 5.5074e-01,\n",
       "             5.3915e-01, 5.3333e-01, 5.2564e-01, 5.0802e-01, 4.4362e-01, 4.3629e-01,\n",
       "             4.1298e-01, 3.9081e-01, 3.6661e-01, 3.6334e-01, 3.4814e-01, 3.1034e-01,\n",
       "             3.1034e-01, 3.0918e-01, 3.0278e-01, 2.8998e-01, 2.7879e-01, 2.7795e-01,\n",
       "             2.5187e-01, 2.3676e-01, 2.3554e-01, 2.2856e-01, 2.0174e-01, 1.9101e-01,\n",
       "             1.8927e-01, 1.6331e-01, 1.5948e-01, 1.5644e-01, 1.4907e-01, 1.4219e-01,\n",
       "             1.1019e-01, 1.0624e-01, 9.5627e-02, 9.2864e-02, 9.0943e-02, 8.9015e-02,\n",
       "             6.4270e-02, 5.5917e-02, 5.2249e-02, 4.9888e-02, 4.8002e-02, 4.5588e-02,\n",
       "             4.4923e-02, 4.2843e-02, 4.2450e-02, 3.4378e-02, 2.1896e-02, 1.9372e-02,\n",
       "             1.9143e-02, 1.7921e-02, 1.4715e-02, 1.4228e-02, 1.4127e-02, 1.3756e-02,\n",
       "             1.3421e-02, 1.2909e-02, 1.1751e-02, 1.0950e-02, 1.0531e-02, 1.0119e-02,\n",
       "             9.0260e-03, 8.5272e-03, 7.6873e-03, 6.8344e-03, 5.5004e-03, 5.2558e-03,\n",
       "             3.2787e-03, 3.1567e-03, 3.1474e-03, 2.5155e-03, 2.3512e-03, 2.1598e-03,\n",
       "             1.8969e-03, 1.6759e-03, 1.4966e-03, 1.4412e-03, 1.3034e-03, 1.2815e-03,\n",
       "             1.1063e-03, 1.0842e-03, 9.8049e-04, 8.5732e-04, 8.2512e-04, 7.6914e-04,\n",
       "             6.7580e-04, 6.6412e-04, 5.8885e-04, 4.8712e-04, 4.6892e-04, 4.4513e-04,\n",
       "             4.2044e-04, 3.9424e-04, 3.5880e-04, 3.5780e-04, 3.5053e-04, 3.4318e-04,\n",
       "             3.2903e-04, 3.2539e-04, 3.2277e-04, 2.8385e-04, 2.7403e-04, 2.7002e-04,\n",
       "             2.3617e-04, 2.2874e-04, 2.1619e-04, 1.9146e-04, 1.7721e-04, 1.7504e-04,\n",
       "             1.4582e-04, 1.4091e-04, 1.3252e-04, 1.2837e-04, 1.2396e-04, 9.3500e-05,\n",
       "             8.7360e-05, 7.5095e-05, 7.2068e-05, 5.8307e-05, 5.6289e-05, 5.5654e-05,\n",
       "             4.6686e-05, 4.3191e-05, 4.2299e-05, 4.0004e-05, 3.9108e-05, 3.7006e-05,\n",
       "             3.6959e-05, 3.4393e-05, 3.3120e-05, 3.1922e-05, 3.1723e-05, 2.7619e-05,\n",
       "             2.5147e-05, 2.4300e-05, 2.3117e-05, 2.1676e-05, 1.7877e-05, 1.7372e-05,\n",
       "             1.7144e-05, 1.6359e-05, 1.5784e-05, 1.5389e-05, 1.5160e-05, 1.2892e-05,\n",
       "             1.2479e-05, 1.1568e-05, 1.1391e-05, 1.0053e-05, 9.6695e-06, 9.6243e-06,\n",
       "             9.5491e-06, 8.6883e-06, 8.6180e-06, 8.0422e-06, 7.8941e-06, 7.5938e-06,\n",
       "             7.4183e-06, 7.0489e-06, 7.0380e-06, 6.3334e-06, 6.0942e-06, 5.8109e-06,\n",
       "             5.3537e-06, 5.1942e-06, 2.9701e-06, 2.7498e-06, 2.7362e-06, 2.6603e-06,\n",
       "             2.6083e-06, 2.5438e-06, 2.5056e-06, 2.4373e-06, 2.3956e-06, 2.1367e-06,\n",
       "             2.1282e-06, 2.1157e-06, 1.8074e-06, 1.6827e-06, 1.4900e-06, 1.4734e-06,\n",
       "             1.3977e-06, 1.3429e-06, 1.3353e-06, 1.0563e-06, 9.9657e-07, 8.3674e-07,\n",
       "             7.4764e-07, 7.4733e-07, 7.3891e-07, 6.8532e-07, 6.7905e-07, 6.2482e-07,\n",
       "             6.1605e-07, 5.3178e-07, 5.0582e-07, 4.7741e-07, 4.6701e-07, 4.3522e-07,\n",
       "             3.5258e-07, 3.4623e-07, 3.2679e-07, 2.9637e-07, 2.6884e-07, 2.6779e-07,\n",
       "             2.4343e-07, 2.4101e-07, 2.3052e-07, 2.2991e-07, 2.0877e-07, 1.6938e-07,\n",
       "             1.6718e-07, 1.5850e-07, 1.5790e-07, 1.4649e-07, 9.0858e-08, 8.9833e-08,\n",
       "             8.3938e-08, 7.9553e-08, 7.6417e-08, 6.9160e-08, 6.7254e-08, 6.5167e-08,\n",
       "             6.2168e-08, 5.7691e-08, 5.6412e-08, 5.6319e-08, 5.6193e-08, 5.0875e-08,\n",
       "             4.6140e-08, 4.4440e-08, 3.9326e-08, 3.4462e-08, 3.4378e-08, 3.0354e-08,\n",
       "             2.9309e-08, 2.7837e-08, 2.3010e-08, 1.9999e-08, 1.9427e-08, 1.6811e-08,\n",
       "             1.6605e-08, 1.6487e-08, 1.4558e-08, 1.4537e-08, 1.3631e-08, 1.2651e-08,\n",
       "             1.2433e-08, 1.1747e-08, 1.1122e-08, 1.1035e-08, 1.0854e-08, 1.0845e-08,\n",
       "             1.0391e-08, 9.6124e-09, 9.6030e-09, 8.6124e-09, 8.1375e-09, 7.9053e-09,\n",
       "             6.9046e-09, 6.7896e-09, 5.9678e-09, 4.2861e-09, 3.9937e-09, 3.4196e-09,\n",
       "             2.9328e-09, 2.9228e-09, 2.8007e-09, 2.6524e-09, 2.6406e-09, 2.6246e-09,\n",
       "             2.5492e-09, 2.2135e-09, 2.1773e-09, 2.0974e-09, 2.0103e-09, 1.6299e-09,\n",
       "             1.5731e-09, 1.5149e-09, 1.5129e-09, 1.5038e-09, 1.4952e-09, 1.4563e-09,\n",
       "             1.4489e-09, 1.2276e-09, 1.1858e-09, 1.0952e-09, 1.0909e-09, 1.0715e-09,\n",
       "             9.7807e-10, 8.8752e-10, 7.1551e-10, 6.6132e-10, 6.6049e-10, 5.8112e-10,\n",
       "             5.3824e-10, 5.2166e-10, 5.1022e-10, 4.6285e-10, 3.9315e-10, 3.6312e-10,\n",
       "             3.5820e-10, 3.2616e-10, 3.0911e-10, 2.8459e-10, 2.8458e-10, 2.6253e-10,\n",
       "             2.1458e-10, 2.0806e-10, 1.8812e-10, 1.8040e-10, 1.6070e-10, 1.5427e-10,\n",
       "             1.2280e-10, 1.0929e-10, 1.0422e-10, 9.6294e-11, 8.0848e-11, 8.0495e-11,\n",
       "             7.8404e-11, 7.2084e-11, 7.0537e-11, 6.2242e-11, 4.5494e-11, 3.5902e-11,\n",
       "             3.4507e-11, 3.0896e-11, 2.7690e-11, 1.9187e-11, 1.8564e-11, 1.6319e-11,\n",
       "             1.5392e-11, 1.1618e-11, 1.0468e-11, 9.2888e-12, 7.8321e-12, 7.3815e-12,\n",
       "             6.9418e-12, 4.4921e-12, 4.0287e-12, 3.4763e-12, 3.3481e-12, 3.1602e-12,\n",
       "             2.8993e-12, 2.7616e-12, 2.6273e-12, 2.6087e-12, 2.3790e-12, 2.3664e-12,\n",
       "             1.6445e-12, 1.5750e-12, 1.5568e-12, 1.1004e-12, 9.9756e-13, 9.0108e-13,\n",
       "             6.6331e-13, 6.2963e-13, 5.8867e-13, 5.6736e-13, 4.5099e-13, 3.9149e-13,\n",
       "             2.3318e-13, 2.2919e-13, 2.2543e-13, 1.7074e-13, 1.4821e-13, 1.3344e-13,\n",
       "             1.3110e-13, 1.2005e-13, 1.1416e-13, 8.5379e-14, 8.2964e-14, 7.6504e-14,\n",
       "             7.2284e-14, 6.0714e-14, 4.3905e-14, 3.2479e-14, 2.8985e-14, 2.8514e-14,\n",
       "             2.1625e-14, 2.0552e-14, 1.0411e-14, 5.6099e-15, 5.5758e-15, 4.4700e-15,\n",
       "             3.6062e-15, 3.4365e-15, 3.3472e-15, 3.0482e-15, 2.3611e-15, 1.6130e-15,\n",
       "             1.5447e-15, 1.0595e-15, 9.1912e-16, 6.3155e-16, 6.2868e-16, 6.0071e-16,\n",
       "             5.2756e-16, 4.8181e-16, 4.7103e-16, 4.6625e-16, 2.0292e-16, 1.9448e-16,\n",
       "             1.5471e-16, 1.1997e-16, 6.6143e-17, 2.6054e-17, 2.3616e-17, 1.9727e-17,\n",
       "             1.4791e-17, 6.4837e-18, 4.9295e-18, 1.2805e-18, 1.1924e-18, 1.0102e-18,\n",
       "             1.5829e-19])}},\n",
       "   {'fpr': np.float64(0.10434782608695652),\n",
       "    'tpr': np.float64(0.9814671814671815),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0043, 0.0043, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087,\n",
       "             0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0087,\n",
       "             0.0109, 0.0109, 0.0109, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0152, 0.0152,\n",
       "             0.0152, 0.0152, 0.0152, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0196, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0217, 0.0217, 0.0239, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0283, 0.0283, 0.0283, 0.0304, 0.0304,\n",
       "             0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304,\n",
       "             0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304,\n",
       "             0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304, 0.0304,\n",
       "             0.0304, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0348, 0.0348, 0.0348, 0.0348, 0.0370, 0.0370, 0.0370, 0.0370, 0.0391,\n",
       "             0.0391, 0.0413, 0.0413, 0.0413, 0.0413, 0.0413, 0.0435, 0.0435, 0.0435,\n",
       "             0.0435, 0.0435, 0.0457, 0.0478, 0.0500, 0.0522, 0.0543, 0.0543, 0.0543,\n",
       "             0.0543, 0.0543, 0.0543, 0.0543, 0.0543, 0.0565, 0.0565, 0.0565, 0.0565,\n",
       "             0.0565, 0.0587, 0.0587, 0.0587, 0.0587, 0.0609, 0.0609, 0.0609, 0.0630,\n",
       "             0.0630, 0.0630, 0.0630, 0.0630, 0.0630, 0.0630, 0.0652, 0.0652, 0.0652,\n",
       "             0.0674, 0.0674, 0.0696, 0.0717, 0.0717, 0.0739, 0.0761, 0.0761, 0.0783,\n",
       "             0.0783, 0.0783, 0.0804, 0.0804, 0.0826, 0.0848, 0.0870, 0.0891, 0.0913,\n",
       "             0.0935, 0.0935, 0.0935, 0.0935, 0.0935, 0.0957, 0.0978, 0.0978, 0.0978,\n",
       "             0.0978, 0.1000, 0.1022, 0.1043, 0.1043, 0.1043, 0.1043, 0.1065, 0.1087,\n",
       "             0.1087, 0.1109, 0.1130, 0.1130, 0.1130, 0.1152, 0.1174, 0.1196, 0.1217,\n",
       "             0.1217, 0.1239, 0.1261, 0.1283, 0.1304, 0.1326, 0.1348, 0.1348, 0.1348,\n",
       "             0.1370, 0.1391, 0.1413, 0.1435, 0.1457, 0.1478, 0.1500, 0.1522, 0.1543,\n",
       "             0.1565, 0.1565, 0.1565, 0.1587, 0.1609, 0.1630, 0.1652, 0.1652, 0.1674,\n",
       "             0.1674, 0.1674, 0.1674, 0.1674, 0.1674, 0.1696, 0.1717, 0.1739, 0.1739,\n",
       "             0.1761, 0.1761, 0.1761, 0.1783, 0.1804, 0.1826, 0.1848, 0.1870, 0.1891,\n",
       "             0.1913, 0.1935, 0.1957, 0.1978, 0.2000, 0.2022, 0.2022, 0.2043, 0.2065,\n",
       "             0.2087, 0.2109, 0.2130, 0.2152, 0.2174, 0.2196, 0.2217, 0.2239, 0.2261,\n",
       "             0.2283, 0.2304, 0.2326, 0.2348, 0.2370, 0.2370, 0.2391, 0.2413, 0.2435,\n",
       "             0.2457, 0.2478, 0.2500, 0.2522, 0.2543, 0.2565, 0.2565, 0.2587, 0.2609,\n",
       "             0.2630, 0.2652, 0.2674, 0.2696, 0.2717, 0.2739, 0.2761, 0.2783, 0.2804,\n",
       "             0.2826, 0.2848, 0.2870, 0.2891, 0.2913, 0.2935, 0.2957, 0.2978, 0.3000,\n",
       "             0.3022, 0.3043, 0.3065, 0.3065, 0.3087, 0.3109, 0.3130, 0.3152, 0.3174,\n",
       "             0.3196, 0.3217, 0.3239, 0.3261, 0.3283, 0.3304, 0.3326, 0.3348, 0.3370,\n",
       "             0.3370, 0.3391, 0.3413, 0.3435, 0.3457, 0.3478, 0.3500, 0.3522, 0.3543,\n",
       "             0.3565, 0.3587, 0.3609, 0.3630, 0.3652, 0.3674, 0.3696, 0.3717, 0.3739,\n",
       "             0.3761, 0.3783, 0.3804, 0.3826, 0.3848, 0.3870, 0.3891, 0.3913, 0.3935,\n",
       "             0.3957, 0.3957, 0.3978, 0.4000, 0.4022, 0.4043, 0.4065, 0.4087, 0.4109,\n",
       "             0.4130, 0.4152, 0.4174, 0.4196, 0.4217, 0.4239, 0.4261, 0.4283, 0.4304,\n",
       "             0.4326, 0.4348, 0.4370, 0.4391, 0.4413, 0.4435, 0.4457, 0.4478, 0.4500,\n",
       "             0.4522, 0.4543, 0.4565, 0.4587, 0.4609, 0.4630, 0.4652, 0.4674, 0.4696,\n",
       "             0.4717, 0.4739, 0.4761, 0.4783, 0.4804, 0.4826, 0.4848, 0.4870, 0.4891,\n",
       "             0.4913, 0.4935, 0.4957, 0.4978, 0.5000, 0.5022, 0.5043, 0.5065, 0.5087,\n",
       "             0.5109, 0.5130, 0.5152, 0.5174, 0.5196, 0.5217, 0.5239, 0.5261, 0.5283,\n",
       "             0.5304, 0.5326, 0.5348, 0.5370, 0.5391, 0.5413, 0.5435, 0.5457, 0.5478,\n",
       "             0.5500, 0.5522, 0.5543, 0.5565, 0.5587, 0.5609, 0.5630, 0.5652, 0.5674,\n",
       "             0.5696, 0.5717, 0.5739, 0.5761, 0.5783, 0.5804, 0.5826, 0.5848, 0.5870,\n",
       "             0.5891, 0.5913, 0.5935, 0.5957, 0.5978, 0.6000, 0.6022, 0.6043, 0.6065,\n",
       "             0.6087, 0.6109, 0.6130, 0.6152, 0.6174, 0.6196, 0.6217, 0.6239, 0.6261,\n",
       "             0.6283, 0.6304, 0.6326, 0.6348, 0.6370, 0.6391, 0.6413, 0.6435, 0.6457,\n",
       "             0.6478, 0.6500, 0.6522, 0.6543, 0.6565, 0.6587, 0.6609, 0.6630, 0.6652,\n",
       "             0.6674, 0.6696, 0.6717, 0.6739, 0.6761, 0.6783, 0.6804, 0.6826, 0.6848,\n",
       "             0.6870, 0.6891, 0.6913, 0.6935, 0.6957, 0.6978, 0.7000, 0.7022, 0.7043,\n",
       "             0.7065, 0.7087, 0.7109, 0.7130, 0.7152, 0.7174, 0.7196, 0.7217, 0.7239,\n",
       "             0.7261, 0.7283, 0.7304, 0.7326, 0.7348, 0.7370, 0.7391, 0.7413, 0.7435,\n",
       "             0.7457, 0.7478, 0.7500, 0.7522, 0.7543, 0.7565, 0.7587, 0.7609, 0.7630,\n",
       "             0.7652, 0.7674, 0.7696, 0.7717, 0.7739, 0.7761, 0.7783, 0.7804, 0.7826,\n",
       "             0.7848, 0.7870, 0.7891, 0.7913, 0.7935, 0.7957, 0.7978, 0.8000, 0.8022,\n",
       "             0.8043, 0.8065, 0.8087, 0.8109, 0.8130, 0.8152, 0.8174, 0.8196, 0.8217,\n",
       "             0.8239, 0.8261, 0.8283, 0.8304, 0.8326, 0.8348, 0.8370, 0.8391, 0.8413,\n",
       "             0.8435, 0.8457, 0.8478, 0.8500, 0.8522, 0.8543, 0.8565, 0.8587, 0.8609,\n",
       "             0.8630, 0.8652, 0.8674, 0.8696, 0.8717, 0.8739, 0.8761, 0.8783, 0.8804,\n",
       "             0.8826, 0.8848, 0.8870, 0.8891, 0.8913, 0.8935, 0.8957, 0.8978, 0.9000,\n",
       "             0.9022, 0.9043, 0.9065, 0.9087, 0.9109, 0.9130, 0.9152, 0.9174, 0.9196,\n",
       "             0.9217, 0.9239, 0.9261, 0.9283, 0.9304, 0.9326, 0.9348, 0.9370, 0.9391,\n",
       "             0.9413, 0.9435, 0.9457, 0.9478, 0.9500, 0.9522, 0.9543, 0.9565, 0.9587,\n",
       "             0.9609, 0.9630, 0.9652, 0.9674, 0.9696, 0.9717, 0.9739, 0.9761, 0.9783,\n",
       "             0.9804, 0.9826, 0.9848, 0.9870, 0.9891, 0.9913, 0.9935, 0.9957, 0.9978,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.6695, 0.7266, 0.7560, 0.7691, 0.7807, 0.7900, 0.7985, 0.8046,\n",
       "             0.8077, 0.8131, 0.8178, 0.8208, 0.8216, 0.8255, 0.8286, 0.8324, 0.8363,\n",
       "             0.8402, 0.8425, 0.8432, 0.8440, 0.8463, 0.8471, 0.8479, 0.8517, 0.8525,\n",
       "             0.8533, 0.8541, 0.8556, 0.8556, 0.8571, 0.8587, 0.8595, 0.8610, 0.8633,\n",
       "             0.8656, 0.8672, 0.8680, 0.8687, 0.8695, 0.8703, 0.8710, 0.8718, 0.8726,\n",
       "             0.8734, 0.8741, 0.8764, 0.8772, 0.8780, 0.8788, 0.8795, 0.8795, 0.8811,\n",
       "             0.8819, 0.8826, 0.8834, 0.8834, 0.8842, 0.8849, 0.8857, 0.8865, 0.8873,\n",
       "             0.8888, 0.8896, 0.8903, 0.8911, 0.8919, 0.8927, 0.8927, 0.8934, 0.8950,\n",
       "             0.8958, 0.8965, 0.8965, 0.8973, 0.8973, 0.8973, 0.8981, 0.8988, 0.8996,\n",
       "             0.9004, 0.9019, 0.9027, 0.9035, 0.9042, 0.9050, 0.9058, 0.9066, 0.9073,\n",
       "             0.9081, 0.9089, 0.9097, 0.9104, 0.9104, 0.9112, 0.9120, 0.9120, 0.9127,\n",
       "             0.9135, 0.9143, 0.9151, 0.9158, 0.9166, 0.9174, 0.9181, 0.9189, 0.9197,\n",
       "             0.9205, 0.9212, 0.9220, 0.9228, 0.9236, 0.9243, 0.9251, 0.9259, 0.9266,\n",
       "             0.9274, 0.9282, 0.9290, 0.9297, 0.9305, 0.9313, 0.9320, 0.9328, 0.9336,\n",
       "             0.9344, 0.9344, 0.9351, 0.9359, 0.9367, 0.9375, 0.9382, 0.9390, 0.9398,\n",
       "             0.9398, 0.9405, 0.9413, 0.9421, 0.9421, 0.9429, 0.9436, 0.9444, 0.9444,\n",
       "             0.9452, 0.9452, 0.9459, 0.9467, 0.9475, 0.9483, 0.9483, 0.9490, 0.9498,\n",
       "             0.9506, 0.9514, 0.9514, 0.9514, 0.9514, 0.9514, 0.9514, 0.9521, 0.9529,\n",
       "             0.9537, 0.9544, 0.9552, 0.9560, 0.9568, 0.9568, 0.9575, 0.9583, 0.9591,\n",
       "             0.9598, 0.9598, 0.9606, 0.9614, 0.9622, 0.9622, 0.9629, 0.9637, 0.9637,\n",
       "             0.9645, 0.9653, 0.9660, 0.9668, 0.9676, 0.9683, 0.9683, 0.9691, 0.9699,\n",
       "             0.9699, 0.9707, 0.9707, 0.9707, 0.9714, 0.9714, 0.9714, 0.9722, 0.9722,\n",
       "             0.9730, 0.9737, 0.9737, 0.9745, 0.9745, 0.9745, 0.9745, 0.9745, 0.9745,\n",
       "             0.9745, 0.9753, 0.9761, 0.9768, 0.9776, 0.9776, 0.9776, 0.9784, 0.9792,\n",
       "             0.9799, 0.9799, 0.9799, 0.9799, 0.9807, 0.9815, 0.9822, 0.9822, 0.9822,\n",
       "             0.9830, 0.9830, 0.9830, 0.9838, 0.9846, 0.9846, 0.9846, 0.9846, 0.9846,\n",
       "             0.9853, 0.9853, 0.9853, 0.9853, 0.9853, 0.9853, 0.9853, 0.9861, 0.9869,\n",
       "             0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869,\n",
       "             0.9869, 0.9876, 0.9884, 0.9884, 0.9884, 0.9884, 0.9884, 0.9892, 0.9892,\n",
       "             0.9900, 0.9907, 0.9915, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9938,\n",
       "             0.9938, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9978e-01, 9.9976e-01, 9.9970e-01, 9.9970e-01,\n",
       "             9.9967e-01, 9.9965e-01, 9.9962e-01, 9.9960e-01, 9.9957e-01, 9.9954e-01,\n",
       "             9.9951e-01, 9.9949e-01, 9.9936e-01, 9.9934e-01, 9.9933e-01, 9.9932e-01,\n",
       "             9.9926e-01, 9.9925e-01, 9.9922e-01, 9.9918e-01, 9.9912e-01, 9.9907e-01,\n",
       "             9.9907e-01, 9.9870e-01, 9.9851e-01, 9.9844e-01, 9.9830e-01, 9.9808e-01,\n",
       "             9.9808e-01, 9.9807e-01, 9.9800e-01, 9.9757e-01, 9.9722e-01, 9.9684e-01,\n",
       "             9.9674e-01, 9.9644e-01, 9.9635e-01, 9.9620e-01, 9.9612e-01, 9.9599e-01,\n",
       "             9.9591e-01, 9.9504e-01, 9.9482e-01, 9.9467e-01, 9.9452e-01, 9.9432e-01,\n",
       "             9.9396e-01, 9.9384e-01, 9.9374e-01, 9.9288e-01, 9.9256e-01, 9.8975e-01,\n",
       "             9.8956e-01, 9.8935e-01, 9.8645e-01, 9.8443e-01, 9.8441e-01, 9.8425e-01,\n",
       "             9.8141e-01, 9.7967e-01, 9.7947e-01, 9.7936e-01, 9.7871e-01, 9.7463e-01,\n",
       "             9.7070e-01, 9.7044e-01, 9.6893e-01, 9.6758e-01, 9.6628e-01, 9.6143e-01,\n",
       "             9.6048e-01, 9.4834e-01, 9.4697e-01, 9.4261e-01, 9.3242e-01, 9.2255e-01,\n",
       "             9.1703e-01, 9.0828e-01, 9.0369e-01, 8.8808e-01, 8.8802e-01, 8.8682e-01,\n",
       "             8.8487e-01, 8.5680e-01, 8.3186e-01, 8.1546e-01, 8.0247e-01, 7.9744e-01,\n",
       "             7.9110e-01, 7.7461e-01, 7.6897e-01, 7.2562e-01, 7.0332e-01, 6.9833e-01,\n",
       "             6.6780e-01, 6.5709e-01, 6.1811e-01, 5.9036e-01, 5.8086e-01, 5.1016e-01,\n",
       "             4.9886e-01, 4.8100e-01, 4.5690e-01, 4.2634e-01, 3.0990e-01, 2.8051e-01,\n",
       "             2.6033e-01, 2.4320e-01, 2.2588e-01, 2.1157e-01, 2.0734e-01, 1.9549e-01,\n",
       "             1.5607e-01, 1.5175e-01, 1.4441e-01, 1.4303e-01, 1.3968e-01, 1.2672e-01,\n",
       "             8.0028e-02, 7.3323e-02, 7.0149e-02, 6.4390e-02, 6.1093e-02, 5.8760e-02,\n",
       "             5.4002e-02, 4.7230e-02, 4.4557e-02, 4.2838e-02, 3.7356e-02, 3.5423e-02,\n",
       "             3.4947e-02, 3.1982e-02, 2.0488e-02, 1.9918e-02, 1.9110e-02, 1.7885e-02,\n",
       "             1.7033e-02, 1.6444e-02, 1.5765e-02, 1.4366e-02, 1.2791e-02, 1.1608e-02,\n",
       "             1.0715e-02, 9.5983e-03, 8.9700e-03, 7.9764e-03, 7.8355e-03, 7.5975e-03,\n",
       "             6.7837e-03, 6.4300e-03, 5.9655e-03, 4.0359e-03, 3.2966e-03, 3.0273e-03,\n",
       "             2.8982e-03, 2.7874e-03, 2.6909e-03, 2.5642e-03, 2.1490e-03, 2.1299e-03,\n",
       "             1.9652e-03, 1.6976e-03, 1.6786e-03, 1.5342e-03, 1.4461e-03, 1.2972e-03,\n",
       "             6.6266e-04, 5.8907e-04, 5.5870e-04, 5.2718e-04, 4.9736e-04, 4.9484e-04,\n",
       "             4.4011e-04, 3.9581e-04, 3.8831e-04, 3.8052e-04, 3.7263e-04, 3.5496e-04,\n",
       "             3.1366e-04, 2.8511e-04, 2.8436e-04, 2.7839e-04, 2.7811e-04, 2.5299e-04,\n",
       "             1.7957e-04, 1.6858e-04, 1.6229e-04, 1.4614e-04, 1.3491e-04, 1.3412e-04,\n",
       "             1.2779e-04, 1.1020e-04, 1.0281e-04, 8.6371e-05, 8.1183e-05, 8.0081e-05,\n",
       "             7.1808e-05, 7.1172e-05, 6.9976e-05, 6.9767e-05, 6.8446e-05, 6.7393e-05,\n",
       "             6.5992e-05, 6.3169e-05, 5.7284e-05, 5.6414e-05, 5.1917e-05, 4.6663e-05,\n",
       "             3.8555e-05, 3.7011e-05, 3.3463e-05, 3.1524e-05, 3.1072e-05, 3.0064e-05,\n",
       "             2.7960e-05, 2.6523e-05, 1.7923e-05, 1.7752e-05, 1.5785e-05, 1.5518e-05,\n",
       "             1.3356e-05, 1.1880e-05, 1.1616e-05, 8.9588e-06, 8.4787e-06, 7.7680e-06,\n",
       "             7.5135e-06, 7.5049e-06, 6.4475e-06, 5.8472e-06, 4.4487e-06, 4.1666e-06,\n",
       "             3.5960e-06, 2.9188e-06, 2.7737e-06, 2.6815e-06, 2.6287e-06, 2.6195e-06,\n",
       "             2.4292e-06, 2.3506e-06, 2.1626e-06, 2.1397e-06, 2.0908e-06, 1.7667e-06,\n",
       "             1.5219e-06, 1.4163e-06, 1.3774e-06, 1.3488e-06, 1.3173e-06, 1.2803e-06,\n",
       "             1.1330e-06, 1.0253e-06, 1.0160e-06, 9.9453e-07, 9.7640e-07, 9.4995e-07,\n",
       "             9.4289e-07, 9.3580e-07, 8.4425e-07, 6.6091e-07, 6.1851e-07, 6.0030e-07,\n",
       "             4.6010e-07, 4.1759e-07, 4.0925e-07, 3.6476e-07, 3.5536e-07, 3.5269e-07,\n",
       "             3.3864e-07, 3.2075e-07, 2.6014e-07, 2.3018e-07, 2.2856e-07, 2.1150e-07,\n",
       "             1.9232e-07, 1.8201e-07, 1.8142e-07, 1.6172e-07, 1.4087e-07, 1.3669e-07,\n",
       "             1.2781e-07, 1.0808e-07, 9.7817e-08, 9.3573e-08, 9.1760e-08, 9.1723e-08,\n",
       "             9.0003e-08, 8.2347e-08, 7.4881e-08, 7.1183e-08, 5.6466e-08, 5.5379e-08,\n",
       "             5.3120e-08, 5.1904e-08, 5.0162e-08, 4.7337e-08, 4.1192e-08, 3.9089e-08,\n",
       "             3.8117e-08, 3.7912e-08, 3.0534e-08, 3.0126e-08, 2.8919e-08, 2.7592e-08,\n",
       "             2.6431e-08, 2.5124e-08, 2.3556e-08, 2.2881e-08, 2.2430e-08, 2.1014e-08,\n",
       "             2.0803e-08, 2.0580e-08, 1.7764e-08, 1.7473e-08, 1.6664e-08, 1.6387e-08,\n",
       "             1.5125e-08, 1.5120e-08, 1.4579e-08, 1.3927e-08, 1.3911e-08, 1.2833e-08,\n",
       "             1.2241e-08, 1.0544e-08, 8.3153e-09, 7.5921e-09, 7.2091e-09, 7.1858e-09,\n",
       "             7.1604e-09, 6.7928e-09, 6.3164e-09, 5.8501e-09, 5.2769e-09, 5.0276e-09,\n",
       "             4.2734e-09, 3.7825e-09, 3.4897e-09, 3.4871e-09, 2.7341e-09, 2.2579e-09,\n",
       "             2.1787e-09, 2.0486e-09, 1.7032e-09, 1.6496e-09, 1.3339e-09, 1.0554e-09,\n",
       "             9.2894e-10, 9.1979e-10, 8.7785e-10, 8.3013e-10, 7.8327e-10, 7.6787e-10,\n",
       "             6.6890e-10, 6.3773e-10, 6.1425e-10, 5.2356e-10, 5.0287e-10, 4.5103e-10,\n",
       "             4.5000e-10, 4.2423e-10, 3.9568e-10, 3.3537e-10, 3.3219e-10, 3.1757e-10,\n",
       "             3.0345e-10, 2.9843e-10, 2.7250e-10, 2.6851e-10, 2.5960e-10, 2.4436e-10,\n",
       "             2.1154e-10, 1.9280e-10, 1.7614e-10, 1.6053e-10, 1.5447e-10, 1.5419e-10,\n",
       "             1.4838e-10, 1.4464e-10, 1.4047e-10, 1.2635e-10, 1.1800e-10, 9.6199e-11,\n",
       "             7.0026e-11, 5.7823e-11, 5.7662e-11, 5.6624e-11, 5.5490e-11, 4.7548e-11,\n",
       "             4.6738e-11, 4.2409e-11, 4.1476e-11, 3.9696e-11, 3.6935e-11, 3.4737e-11,\n",
       "             3.2597e-11, 2.6387e-11, 2.4673e-11, 2.3544e-11, 2.2883e-11, 2.1588e-11,\n",
       "             2.1098e-11, 1.9972e-11, 1.9810e-11, 1.8363e-11, 1.8231e-11, 1.7693e-11,\n",
       "             1.5626e-11, 1.2688e-11, 1.1351e-11, 1.0548e-11, 1.0496e-11, 9.5197e-12,\n",
       "             8.8725e-12, 7.5654e-12, 7.5463e-12, 7.5217e-12, 6.9952e-12, 6.8024e-12,\n",
       "             6.4461e-12, 6.4203e-12, 5.5459e-12, 5.3034e-12, 4.9441e-12, 4.2719e-12,\n",
       "             3.2851e-12, 3.0449e-12, 3.0372e-12, 2.9664e-12, 2.9440e-12, 2.5598e-12,\n",
       "             2.4966e-12, 2.4953e-12, 2.3996e-12, 2.2322e-12, 2.1764e-12, 2.1625e-12,\n",
       "             2.0305e-12, 1.5728e-12, 1.4873e-12, 1.3338e-12, 1.1596e-12, 1.1290e-12,\n",
       "             9.8429e-13, 7.1802e-13, 7.0614e-13, 6.3253e-13, 6.2946e-13, 6.2726e-13,\n",
       "             5.8946e-13, 5.7798e-13, 5.3785e-13, 4.6499e-13, 4.3929e-13, 4.0130e-13,\n",
       "             3.4022e-13, 1.4495e-13, 1.2262e-13, 8.4197e-14, 7.3903e-14, 7.1223e-14,\n",
       "             5.8678e-14, 5.0872e-14, 4.8042e-14, 4.5940e-14, 3.4642e-14, 3.0503e-14,\n",
       "             2.9013e-14, 2.8500e-14, 2.8112e-14, 2.6191e-14, 2.4968e-14, 2.0637e-14,\n",
       "             1.9428e-14, 1.9403e-14, 1.6908e-14, 1.4460e-14, 1.4099e-14, 1.1503e-14,\n",
       "             8.6645e-15, 7.5655e-15, 7.4330e-15, 5.4966e-15, 4.3889e-15, 3.3218e-15,\n",
       "             3.0655e-15, 2.1444e-15, 1.9756e-15, 1.9510e-15, 1.2342e-15, 1.0999e-15,\n",
       "             9.2720e-16, 7.7613e-16, 6.4658e-16, 6.2771e-16, 6.1928e-16, 4.7663e-16,\n",
       "             4.0646e-16, 2.9888e-16, 2.9181e-16, 2.8171e-16, 2.7779e-16, 2.2181e-16,\n",
       "             1.6424e-16, 9.2233e-17, 7.4103e-17, 5.1120e-17, 4.5114e-17, 3.9971e-17,\n",
       "             3.9092e-17, 3.3465e-17, 3.3199e-17, 3.0347e-17, 1.6543e-17, 1.3795e-17,\n",
       "             1.2248e-17, 9.6352e-18, 9.5290e-18, 7.2290e-18, 4.6462e-18, 3.9604e-18,\n",
       "             3.8308e-18, 3.0800e-18, 2.6759e-18, 1.7622e-18, 1.7267e-18, 5.1381e-19,\n",
       "             4.3810e-19, 2.7722e-19, 2.2183e-19, 2.0381e-19, 1.6232e-19, 1.0676e-19,\n",
       "             7.6316e-21, 3.5385e-21, 2.4832e-21, 1.6175e-21])}},\n",
       "   {'fpr': np.float64(0.10869565217391304),\n",
       "    'tpr': np.float64(0.9845559845559846),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043,\n",
       "             0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0043, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0087, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0152, 0.0152, 0.0152, 0.0152,\n",
       "             0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
       "             0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0196, 0.0196, 0.0217,\n",
       "             0.0217, 0.0217, 0.0239, 0.0239, 0.0239, 0.0239, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0283, 0.0283, 0.0304, 0.0304, 0.0304, 0.0326, 0.0326,\n",
       "             0.0326, 0.0348, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0413, 0.0435, 0.0457, 0.0478, 0.0478,\n",
       "             0.0478, 0.0500, 0.0500, 0.0500, 0.0522, 0.0543, 0.0543, 0.0565, 0.0565,\n",
       "             0.0565, 0.0587, 0.0609, 0.0630, 0.0652, 0.0674, 0.0674, 0.0674, 0.0696,\n",
       "             0.0696, 0.0717, 0.0739, 0.0739, 0.0739, 0.0739, 0.0761, 0.0761, 0.0761,\n",
       "             0.0761, 0.0761, 0.0761, 0.0783, 0.0804, 0.0826, 0.0826, 0.0848, 0.0870,\n",
       "             0.0870, 0.0891, 0.0913, 0.0913, 0.0935, 0.0935, 0.0957, 0.0978, 0.1000,\n",
       "             0.1022, 0.1043, 0.1043, 0.1043, 0.1043, 0.1065, 0.1087, 0.1087, 0.1087,\n",
       "             0.1087, 0.1087, 0.1087, 0.1087, 0.1087, 0.1109, 0.1130, 0.1152, 0.1152,\n",
       "             0.1152, 0.1174, 0.1174, 0.1196, 0.1217, 0.1239, 0.1261, 0.1261, 0.1283,\n",
       "             0.1304, 0.1304, 0.1326, 0.1326, 0.1348, 0.1370, 0.1370, 0.1391, 0.1413,\n",
       "             0.1435, 0.1435, 0.1457, 0.1478, 0.1500, 0.1522, 0.1543, 0.1543, 0.1565,\n",
       "             0.1587, 0.1609, 0.1630, 0.1630, 0.1652, 0.1674, 0.1696, 0.1717, 0.1739,\n",
       "             0.1761, 0.1783, 0.1804, 0.1804, 0.1826, 0.1848, 0.1870, 0.1891, 0.1913,\n",
       "             0.1913, 0.1935, 0.1957, 0.1978, 0.2000, 0.2000, 0.2022, 0.2043, 0.2065,\n",
       "             0.2087, 0.2109, 0.2130, 0.2152, 0.2174, 0.2196, 0.2217, 0.2239, 0.2261,\n",
       "             0.2283, 0.2304, 0.2326, 0.2326, 0.2348, 0.2370, 0.2391, 0.2413, 0.2435,\n",
       "             0.2457, 0.2478, 0.2500, 0.2522, 0.2543, 0.2565, 0.2587, 0.2609, 0.2630,\n",
       "             0.2652, 0.2674, 0.2696, 0.2717, 0.2739, 0.2739, 0.2761, 0.2783, 0.2804,\n",
       "             0.2826, 0.2848, 0.2870, 0.2891, 0.2913, 0.2935, 0.2957, 0.2978, 0.3000,\n",
       "             0.3022, 0.3043, 0.3065, 0.3087, 0.3109, 0.3130, 0.3152, 0.3174, 0.3196,\n",
       "             0.3217, 0.3239, 0.3261, 0.3283, 0.3304, 0.3326, 0.3348, 0.3370, 0.3391,\n",
       "             0.3413, 0.3435, 0.3457, 0.3478, 0.3500, 0.3522, 0.3543, 0.3565, 0.3587,\n",
       "             0.3609, 0.3630, 0.3652, 0.3674, 0.3696, 0.3717, 0.3739, 0.3761, 0.3783,\n",
       "             0.3804, 0.3826, 0.3848, 0.3870, 0.3891, 0.3913, 0.3935, 0.3957, 0.3957,\n",
       "             0.3978, 0.4000, 0.4000, 0.4022, 0.4022, 0.4043, 0.4065, 0.4087, 0.4109,\n",
       "             0.4130, 0.4152, 0.4174, 0.4196, 0.4217, 0.4239, 0.4261, 0.4283, 0.4304,\n",
       "             0.4326, 0.4348, 0.4370, 0.4391, 0.4413, 0.4435, 0.4457, 0.4478, 0.4500,\n",
       "             0.4522, 0.4543, 0.4565, 0.4587, 0.4609, 0.4630, 0.4652, 0.4674, 0.4696,\n",
       "             0.4717, 0.4739, 0.4761, 0.4783, 0.4804, 0.4826, 0.4848, 0.4870, 0.4891,\n",
       "             0.4913, 0.4935, 0.4957, 0.4978, 0.5000, 0.5022, 0.5043, 0.5065, 0.5087,\n",
       "             0.5109, 0.5130, 0.5152, 0.5174, 0.5196, 0.5217, 0.5239, 0.5261, 0.5283,\n",
       "             0.5304, 0.5326, 0.5348, 0.5370, 0.5391, 0.5413, 0.5435, 0.5457, 0.5478,\n",
       "             0.5500, 0.5522, 0.5543, 0.5565, 0.5587, 0.5609, 0.5630, 0.5652, 0.5652,\n",
       "             0.5674, 0.5696, 0.5717, 0.5739, 0.5761, 0.5783, 0.5804, 0.5826, 0.5848,\n",
       "             0.5870, 0.5891, 0.5913, 0.5935, 0.5957, 0.5978, 0.6000, 0.6022, 0.6043,\n",
       "             0.6065, 0.6087, 0.6109, 0.6130, 0.6152, 0.6174, 0.6196, 0.6217, 0.6239,\n",
       "             0.6261, 0.6283, 0.6304, 0.6326, 0.6348, 0.6370, 0.6391, 0.6413, 0.6435,\n",
       "             0.6457, 0.6478, 0.6500, 0.6522, 0.6543, 0.6565, 0.6587, 0.6609, 0.6630,\n",
       "             0.6652, 0.6674, 0.6696, 0.6717, 0.6739, 0.6761, 0.6783, 0.6804, 0.6826,\n",
       "             0.6848, 0.6870, 0.6891, 0.6913, 0.6935, 0.6957, 0.6978, 0.7000, 0.7022,\n",
       "             0.7043, 0.7065, 0.7087, 0.7109, 0.7130, 0.7152, 0.7174, 0.7196, 0.7217,\n",
       "             0.7239, 0.7261, 0.7283, 0.7304, 0.7326, 0.7348, 0.7370, 0.7391, 0.7413,\n",
       "             0.7435, 0.7457, 0.7478, 0.7500, 0.7522, 0.7543, 0.7565, 0.7587, 0.7609,\n",
       "             0.7630, 0.7652, 0.7674, 0.7696, 0.7717, 0.7739, 0.7761, 0.7783, 0.7804,\n",
       "             0.7826, 0.7848, 0.7870, 0.7891, 0.7913, 0.7935, 0.7957, 0.7978, 0.8000,\n",
       "             0.8022, 0.8043, 0.8065, 0.8087, 0.8109, 0.8130, 0.8152, 0.8174, 0.8196,\n",
       "             0.8217, 0.8239, 0.8261, 0.8283, 0.8304, 0.8326, 0.8348, 0.8370, 0.8391,\n",
       "             0.8413, 0.8435, 0.8457, 0.8478, 0.8500, 0.8522, 0.8543, 0.8565, 0.8587,\n",
       "             0.8609, 0.8630, 0.8652, 0.8674, 0.8696, 0.8717, 0.8739, 0.8761, 0.8783,\n",
       "             0.8804, 0.8826, 0.8848, 0.8870, 0.8891, 0.8913, 0.8935, 0.8957, 0.8978,\n",
       "             0.9000, 0.9022, 0.9043, 0.9065, 0.9087, 0.9109, 0.9130, 0.9152, 0.9174,\n",
       "             0.9196, 0.9217, 0.9239, 0.9261, 0.9283, 0.9304, 0.9326, 0.9348, 0.9370,\n",
       "             0.9391, 0.9413, 0.9435, 0.9457, 0.9478, 0.9500, 0.9522, 0.9543, 0.9565,\n",
       "             0.9587, 0.9609, 0.9630, 0.9652, 0.9674, 0.9696, 0.9717, 0.9739, 0.9761,\n",
       "             0.9783, 0.9804, 0.9826, 0.9848, 0.9870, 0.9891, 0.9913, 0.9935, 0.9957,\n",
       "             0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.4664, 0.5552, 0.6015, 0.6301, 0.6533, 0.6625, 0.6741, 0.6857,\n",
       "             0.6927, 0.6973, 0.7042, 0.7066, 0.7127, 0.7151, 0.7181, 0.7205, 0.7236,\n",
       "             0.7259, 0.7297, 0.7320, 0.7367, 0.7390, 0.7413, 0.7421, 0.7444, 0.7467,\n",
       "             0.7475, 0.7490, 0.7498, 0.7506, 0.7529, 0.7560, 0.7568, 0.7591, 0.7598,\n",
       "             0.7629, 0.7637, 0.7660, 0.7691, 0.7722, 0.7745, 0.7753, 0.7761, 0.7768,\n",
       "             0.7784, 0.7807, 0.7815, 0.7822, 0.7838, 0.7846, 0.7853, 0.7861, 0.7876,\n",
       "             0.7892, 0.7907, 0.7915, 0.7923, 0.7931, 0.7946, 0.7969, 0.7977, 0.7985,\n",
       "             0.8000, 0.8015, 0.8031, 0.8046, 0.8054, 0.8062, 0.8077, 0.8085, 0.8093,\n",
       "             0.8100, 0.8116, 0.8131, 0.8139, 0.8147, 0.8154, 0.8162, 0.8178, 0.8185,\n",
       "             0.8201, 0.8208, 0.8216, 0.8224, 0.8232, 0.8239, 0.8247, 0.8255, 0.8263,\n",
       "             0.8270, 0.8278, 0.8286, 0.8293, 0.8301, 0.8309, 0.8317, 0.8324, 0.8332,\n",
       "             0.8340, 0.8347, 0.8355, 0.8363, 0.8371, 0.8386, 0.8394, 0.8402, 0.8409,\n",
       "             0.8417, 0.8425, 0.8432, 0.8440, 0.8448, 0.8456, 0.8463, 0.8471, 0.8486,\n",
       "             0.8494, 0.8502, 0.8510, 0.8517, 0.8525, 0.8533, 0.8541, 0.8548, 0.8556,\n",
       "             0.8564, 0.8571, 0.8579, 0.8587, 0.8595, 0.8618, 0.8625, 0.8633, 0.8641,\n",
       "             0.8649, 0.8656, 0.8664, 0.8672, 0.8680, 0.8687, 0.8695, 0.8703, 0.8710,\n",
       "             0.8718, 0.8726, 0.8734, 0.8741, 0.8749, 0.8757, 0.8764, 0.8772, 0.8780,\n",
       "             0.8788, 0.8795, 0.8803, 0.8811, 0.8819, 0.8826, 0.8834, 0.8842, 0.8849,\n",
       "             0.8857, 0.8865, 0.8873, 0.8880, 0.8888, 0.8896, 0.8903, 0.8911, 0.8919,\n",
       "             0.8927, 0.8934, 0.8934, 0.8934, 0.8942, 0.8950, 0.8958, 0.8965, 0.8973,\n",
       "             0.8981, 0.8996, 0.9004, 0.9012, 0.9019, 0.9027, 0.9035, 0.9042, 0.9050,\n",
       "             0.9058, 0.9066, 0.9073, 0.9081, 0.9089, 0.9089, 0.9097, 0.9104, 0.9112,\n",
       "             0.9120, 0.9127, 0.9135, 0.9143, 0.9151, 0.9151, 0.9158, 0.9166, 0.9174,\n",
       "             0.9181, 0.9189, 0.9197, 0.9205, 0.9212, 0.9220, 0.9228, 0.9236, 0.9243,\n",
       "             0.9251, 0.9259, 0.9266, 0.9274, 0.9282, 0.9290, 0.9297, 0.9305, 0.9313,\n",
       "             0.9313, 0.9320, 0.9328, 0.9336, 0.9344, 0.9351, 0.9351, 0.9359, 0.9359,\n",
       "             0.9367, 0.9375, 0.9375, 0.9382, 0.9390, 0.9398, 0.9398, 0.9405, 0.9413,\n",
       "             0.9421, 0.9429, 0.9429, 0.9436, 0.9436, 0.9444, 0.9452, 0.9452, 0.9459,\n",
       "             0.9467, 0.9467, 0.9467, 0.9475, 0.9483, 0.9490, 0.9498, 0.9506, 0.9514,\n",
       "             0.9514, 0.9521, 0.9529, 0.9537, 0.9544, 0.9552, 0.9560, 0.9568, 0.9575,\n",
       "             0.9583, 0.9591, 0.9598, 0.9606, 0.9606, 0.9606, 0.9606, 0.9606, 0.9614,\n",
       "             0.9622, 0.9622, 0.9629, 0.9637, 0.9637, 0.9637, 0.9645, 0.9645, 0.9653,\n",
       "             0.9660, 0.9660, 0.9660, 0.9660, 0.9660, 0.9660, 0.9668, 0.9676, 0.9676,\n",
       "             0.9683, 0.9683, 0.9683, 0.9691, 0.9699, 0.9707, 0.9707, 0.9714, 0.9722,\n",
       "             0.9730, 0.9737, 0.9745, 0.9745, 0.9745, 0.9745, 0.9753, 0.9753, 0.9753,\n",
       "             0.9761, 0.9761, 0.9761, 0.9768, 0.9768, 0.9776, 0.9776, 0.9776, 0.9776,\n",
       "             0.9776, 0.9776, 0.9784, 0.9792, 0.9799, 0.9799, 0.9799, 0.9807, 0.9815,\n",
       "             0.9822, 0.9830, 0.9838, 0.9846, 0.9853, 0.9853, 0.9853, 0.9853, 0.9861,\n",
       "             0.9869, 0.9869, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876, 0.9884, 0.9884,\n",
       "             0.9884, 0.9892, 0.9892, 0.9900, 0.9900, 0.9900, 0.9907, 0.9907, 0.9907,\n",
       "             0.9907, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9976e-01, 9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9965e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9963e-01, 9.9962e-01, 9.9953e-01, 9.9949e-01, 9.9949e-01,\n",
       "             9.9948e-01, 9.9939e-01, 9.9939e-01, 9.9938e-01, 9.9938e-01, 9.9935e-01,\n",
       "             9.9930e-01, 9.9929e-01, 9.9928e-01, 9.9927e-01, 9.9924e-01, 9.9923e-01,\n",
       "             9.9919e-01, 9.9918e-01, 9.9917e-01, 9.9913e-01, 9.9907e-01, 9.9898e-01,\n",
       "             9.9897e-01, 9.9890e-01, 9.9889e-01, 9.9887e-01, 9.9881e-01, 9.9880e-01,\n",
       "             9.9879e-01, 9.9873e-01, 9.9871e-01, 9.9861e-01, 9.9857e-01, 9.9855e-01,\n",
       "             9.9839e-01, 9.9825e-01, 9.9808e-01, 9.9807e-01, 9.9789e-01, 9.9770e-01,\n",
       "             9.9754e-01, 9.9734e-01, 9.9731e-01, 9.9714e-01, 9.9711e-01, 9.9710e-01,\n",
       "             9.9692e-01, 9.9673e-01, 9.9669e-01, 9.9665e-01, 9.9583e-01, 9.9580e-01,\n",
       "             9.9570e-01, 9.9529e-01, 9.9498e-01, 9.9469e-01, 9.9431e-01, 9.9415e-01,\n",
       "             9.9396e-01, 9.9345e-01, 9.9290e-01, 9.9133e-01, 9.9128e-01, 9.9093e-01,\n",
       "             9.9070e-01, 9.9032e-01, 9.8922e-01, 9.8879e-01, 9.8834e-01, 9.8831e-01,\n",
       "             9.8731e-01, 9.8699e-01, 9.8613e-01, 9.8601e-01, 9.8382e-01, 9.8272e-01,\n",
       "             9.8217e-01, 9.8116e-01, 9.8112e-01, 9.7930e-01, 9.7813e-01, 9.7406e-01,\n",
       "             9.7393e-01, 9.7168e-01, 9.6926e-01, 9.6916e-01, 9.6364e-01, 9.5953e-01,\n",
       "             9.5797e-01, 9.5486e-01, 9.5143e-01, 9.4806e-01, 9.4561e-01, 9.4340e-01,\n",
       "             9.2259e-01, 9.2012e-01, 9.1804e-01, 9.1127e-01, 8.8497e-01, 8.8189e-01,\n",
       "             8.7566e-01, 8.7501e-01, 8.7482e-01, 8.7374e-01, 8.6521e-01, 8.6186e-01,\n",
       "             8.4161e-01, 8.3477e-01, 8.2298e-01, 8.2138e-01, 7.8693e-01, 7.8372e-01,\n",
       "             7.4248e-01, 6.9808e-01, 6.9304e-01, 6.6393e-01, 6.5129e-01, 6.3784e-01,\n",
       "             6.2090e-01, 5.7552e-01, 5.5077e-01, 5.4072e-01, 5.2573e-01, 5.2478e-01,\n",
       "             5.1134e-01, 4.7071e-01, 4.4153e-01, 4.2261e-01, 3.4567e-01, 3.0647e-01,\n",
       "             3.0432e-01, 2.4569e-01, 2.4249e-01, 2.3155e-01, 2.1005e-01, 1.8327e-01,\n",
       "             1.7131e-01, 1.7092e-01, 1.5688e-01, 1.4872e-01, 1.4312e-01, 1.3590e-01,\n",
       "             1.3133e-01, 1.2068e-01, 1.1889e-01, 1.0978e-01, 9.8720e-02, 7.8553e-02,\n",
       "             7.1567e-02, 6.7109e-02, 6.1227e-02, 5.2792e-02, 3.9851e-02, 3.8486e-02,\n",
       "             3.7502e-02, 3.1010e-02, 2.9396e-02, 2.6793e-02, 2.5127e-02, 2.4527e-02,\n",
       "             2.4040e-02, 1.9530e-02, 1.8875e-02, 1.8194e-02, 1.7420e-02, 1.6777e-02,\n",
       "             1.6665e-02, 1.4015e-02, 1.3037e-02, 1.0954e-02, 1.0371e-02, 1.0351e-02,\n",
       "             9.7460e-03, 9.2133e-03, 9.2009e-03, 8.5234e-03, 8.5087e-03, 8.1522e-03,\n",
       "             8.1004e-03, 5.8562e-03, 5.8269e-03, 5.3186e-03, 4.5946e-03, 3.9019e-03,\n",
       "             3.5745e-03, 3.2990e-03, 2.8333e-03, 2.7135e-03, 1.8539e-03, 1.8483e-03,\n",
       "             1.7991e-03, 1.7377e-03, 1.7138e-03, 1.6550e-03, 1.5089e-03, 1.3248e-03,\n",
       "             1.2980e-03, 1.0820e-03, 9.7158e-04, 7.3602e-04, 6.5723e-04, 6.5703e-04,\n",
       "             5.7425e-04, 5.5451e-04, 5.2104e-04, 5.1138e-04, 5.0825e-04, 5.0730e-04,\n",
       "             4.2410e-04, 3.8543e-04, 3.7896e-04, 3.7440e-04, 3.7244e-04, 3.6357e-04,\n",
       "             3.6352e-04, 3.6128e-04, 3.5003e-04, 3.2470e-04, 2.8595e-04, 2.7788e-04,\n",
       "             2.7196e-04, 2.6163e-04, 2.5337e-04, 2.4042e-04, 2.3575e-04, 2.2380e-04,\n",
       "             2.1561e-04, 2.1421e-04, 2.1100e-04, 2.1037e-04, 1.9213e-04, 1.8489e-04,\n",
       "             1.8374e-04, 1.7370e-04, 1.6980e-04, 1.6229e-04, 1.6019e-04, 1.4969e-04,\n",
       "             1.4310e-04, 1.3508e-04, 1.3420e-04, 1.2926e-04, 1.2750e-04, 1.1210e-04,\n",
       "             1.0459e-04, 9.6380e-05, 9.2654e-05, 8.8260e-05, 8.6617e-05, 8.2204e-05,\n",
       "             7.4299e-05, 6.4034e-05, 5.8480e-05, 5.8169e-05, 5.7531e-05, 5.4757e-05,\n",
       "             5.2185e-05, 5.1127e-05, 4.9354e-05, 4.8593e-05, 4.1626e-05, 4.1223e-05,\n",
       "             3.9650e-05, 3.6300e-05, 3.5080e-05, 3.3608e-05, 2.9762e-05, 2.9623e-05,\n",
       "             2.8105e-05, 2.7950e-05, 2.7683e-05, 2.6738e-05, 2.6198e-05, 2.3175e-05,\n",
       "             2.1145e-05, 2.0562e-05, 2.0159e-05, 1.6372e-05, 1.6228e-05, 1.6161e-05,\n",
       "             1.5239e-05, 1.5176e-05, 1.4914e-05, 1.4539e-05, 1.3944e-05, 1.3322e-05,\n",
       "             1.1713e-05, 1.1409e-05, 1.1148e-05, 1.1104e-05, 1.0985e-05, 9.0699e-06,\n",
       "             9.0124e-06, 8.1706e-06, 8.1284e-06, 7.2571e-06, 6.3533e-06, 5.5313e-06,\n",
       "             5.4296e-06, 5.3459e-06, 5.3356e-06, 5.0221e-06, 4.9665e-06, 4.8739e-06,\n",
       "             4.5603e-06, 4.5546e-06, 4.5187e-06, 4.0852e-06, 3.6421e-06, 3.5670e-06,\n",
       "             3.5422e-06, 3.5253e-06, 3.5244e-06, 3.4666e-06, 3.4336e-06, 3.2859e-06,\n",
       "             3.2587e-06, 2.6333e-06, 2.4329e-06, 2.2342e-06, 2.1792e-06, 1.8563e-06,\n",
       "             1.7120e-06, 1.6950e-06, 1.6850e-06, 1.6584e-06, 1.5989e-06, 1.5171e-06,\n",
       "             1.5060e-06, 1.4161e-06, 1.3762e-06, 1.3079e-06, 1.2981e-06, 1.0780e-06,\n",
       "             1.0495e-06, 1.0156e-06, 9.4234e-07, 8.7475e-07, 8.0039e-07, 7.4574e-07,\n",
       "             6.0233e-07, 5.2670e-07, 5.2373e-07, 5.1148e-07, 4.8088e-07, 4.7713e-07,\n",
       "             4.6172e-07, 4.0799e-07, 4.0671e-07, 4.0577e-07, 4.0482e-07, 3.8705e-07,\n",
       "             3.7282e-07, 3.4971e-07, 3.4681e-07, 3.3678e-07, 2.6201e-07, 2.5625e-07,\n",
       "             2.2349e-07, 2.1898e-07, 1.9812e-07, 1.9055e-07, 1.7817e-07, 1.5976e-07,\n",
       "             1.5550e-07, 1.5316e-07, 1.5269e-07, 1.5039e-07, 1.4951e-07, 1.4469e-07,\n",
       "             1.3935e-07, 1.3173e-07, 1.3013e-07, 1.2442e-07, 1.0401e-07, 1.0299e-07,\n",
       "             8.2720e-08, 7.5769e-08, 7.3832e-08, 7.1221e-08, 6.8359e-08, 6.7894e-08,\n",
       "             6.7246e-08, 6.6631e-08, 5.9097e-08, 5.2082e-08, 4.8877e-08, 4.7625e-08,\n",
       "             4.1079e-08, 4.0173e-08, 3.3321e-08, 3.2653e-08, 3.1331e-08, 2.7812e-08,\n",
       "             2.5971e-08, 2.3127e-08, 2.2779e-08, 2.0360e-08, 1.2210e-08, 1.1309e-08,\n",
       "             1.0928e-08, 9.6354e-09, 8.7908e-09, 8.6705e-09, 8.5839e-09, 8.1863e-09,\n",
       "             7.3144e-09, 7.2318e-09, 7.0942e-09, 7.0116e-09, 6.8986e-09, 5.8469e-09,\n",
       "             5.5607e-09, 5.5392e-09, 5.5220e-09, 5.0282e-09, 3.9624e-09, 3.8885e-09,\n",
       "             3.8644e-09, 3.4159e-09, 3.3731e-09, 3.3661e-09, 3.0949e-09, 3.0947e-09,\n",
       "             3.0707e-09, 2.8204e-09, 2.3610e-09, 2.2969e-09, 1.4411e-09, 1.3932e-09,\n",
       "             1.2866e-09, 1.2631e-09, 1.1802e-09, 1.1659e-09, 1.0580e-09, 1.0197e-09,\n",
       "             9.6540e-10, 9.6015e-10, 8.9869e-10, 7.7245e-10, 7.6278e-10, 6.8670e-10,\n",
       "             6.8473e-10, 6.6347e-10, 5.4969e-10, 3.9746e-10, 3.2139e-10, 2.9842e-10,\n",
       "             2.9202e-10, 2.8800e-10, 2.5533e-10, 2.5258e-10, 2.5109e-10, 1.8871e-10,\n",
       "             1.7696e-10, 1.6472e-10, 1.6063e-10, 1.5473e-10, 1.4917e-10, 1.4826e-10,\n",
       "             1.2278e-10, 1.1562e-10, 1.0122e-10, 9.8660e-11, 9.7899e-11, 7.6706e-11,\n",
       "             7.5363e-11, 6.1827e-11, 5.4782e-11, 5.3052e-11, 5.0261e-11, 4.7920e-11,\n",
       "             4.4317e-11, 4.4251e-11, 4.1101e-11, 3.1742e-11, 3.0214e-11, 2.2803e-11,\n",
       "             2.2557e-11, 1.5276e-11, 1.4082e-11, 7.9208e-12, 7.8903e-12, 6.8545e-12,\n",
       "             5.0348e-12, 4.0729e-12, 3.7045e-12, 2.4404e-12, 2.0404e-12, 1.9869e-12,\n",
       "             1.9074e-12, 1.6520e-12, 1.4879e-12, 1.3338e-12, 1.3026e-12, 1.2908e-12,\n",
       "             1.2880e-12, 1.2611e-12, 1.1282e-12, 9.7083e-13, 9.3561e-13, 8.3153e-13,\n",
       "             7.3823e-13, 7.2903e-13, 5.2435e-13, 4.7841e-13, 4.1133e-13, 4.0896e-13,\n",
       "             2.2859e-13, 1.9133e-13, 1.7094e-13, 1.6031e-13, 1.1477e-13, 1.0846e-13,\n",
       "             1.0233e-13, 9.1870e-14, 5.9034e-14, 5.1929e-14, 4.6043e-14, 4.3500e-14,\n",
       "             3.7454e-14, 3.4176e-14, 3.1541e-14, 2.5536e-14, 2.3770e-14, 2.2673e-14,\n",
       "             2.2511e-14, 2.1285e-14, 1.8655e-14, 1.8237e-14, 1.6244e-14, 1.5162e-14,\n",
       "             1.3928e-14, 5.9774e-15, 3.0770e-15, 2.8517e-15, 2.6397e-15, 1.6494e-15,\n",
       "             1.2759e-15, 1.2542e-15, 1.2152e-15, 1.1176e-15, 6.2381e-16, 4.7684e-16,\n",
       "             3.9706e-16, 3.1838e-16, 2.3483e-16, 9.5550e-17, 7.0702e-17, 1.8498e-17,\n",
       "             1.7921e-17, 7.4825e-18, 2.4450e-18, 1.9751e-18, 1.9384e-19])}},\n",
       "   {'fpr': np.float64(0.11304347826086956),\n",
       "    'tpr': np.float64(0.9891891891891892),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0065, 0.0065, 0.0109, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0152, 0.0174, 0.0196, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "             0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "             0.0217, 0.0217, 0.0217, 0.0239, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283,\n",
       "             0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283,\n",
       "             0.0283, 0.0283, 0.0283, 0.0283, 0.0304, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0348,\n",
       "             0.0348, 0.0348, 0.0370, 0.0370, 0.0391, 0.0413, 0.0435, 0.0435, 0.0457,\n",
       "             0.0457, 0.0457, 0.0478, 0.0478, 0.0478, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "             0.0500, 0.0500, 0.0522, 0.0543, 0.0543, 0.0565, 0.0565, 0.0587, 0.0609,\n",
       "             0.0609, 0.0630, 0.0630, 0.0652, 0.0674, 0.0696, 0.0717, 0.0717, 0.0739,\n",
       "             0.0761, 0.0761, 0.0761, 0.0761, 0.0783, 0.0783, 0.0783, 0.0804, 0.0826,\n",
       "             0.0848, 0.0870, 0.0891, 0.0891, 0.0913, 0.0935, 0.0935, 0.0957, 0.0978,\n",
       "             0.1000, 0.1000, 0.1022, 0.1022, 0.1022, 0.1022, 0.1043, 0.1065, 0.1065,\n",
       "             0.1065, 0.1087, 0.1109, 0.1130, 0.1130, 0.1152, 0.1174, 0.1196, 0.1196,\n",
       "             0.1196, 0.1196, 0.1217, 0.1239, 0.1261, 0.1283, 0.1283, 0.1304, 0.1326,\n",
       "             0.1348, 0.1370, 0.1391, 0.1413, 0.1435, 0.1457, 0.1478, 0.1500, 0.1522,\n",
       "             0.1543, 0.1565, 0.1587, 0.1609, 0.1630, 0.1652, 0.1674, 0.1696, 0.1717,\n",
       "             0.1739, 0.1761, 0.1761, 0.1761, 0.1783, 0.1804, 0.1826, 0.1848, 0.1870,\n",
       "             0.1891, 0.1891, 0.1913, 0.1913, 0.1913, 0.1935, 0.1957, 0.1978, 0.2000,\n",
       "             0.2022, 0.2043, 0.2065, 0.2087, 0.2109, 0.2130, 0.2152, 0.2174, 0.2196,\n",
       "             0.2217, 0.2239, 0.2261, 0.2283, 0.2283, 0.2304, 0.2326, 0.2348, 0.2370,\n",
       "             0.2391, 0.2413, 0.2435, 0.2457, 0.2478, 0.2500, 0.2522, 0.2543, 0.2565,\n",
       "             0.2587, 0.2609, 0.2630, 0.2652, 0.2674, 0.2696, 0.2717, 0.2739, 0.2761,\n",
       "             0.2783, 0.2804, 0.2826, 0.2848, 0.2870, 0.2891, 0.2913, 0.2935, 0.2957,\n",
       "             0.2978, 0.3000, 0.3022, 0.3043, 0.3065, 0.3087, 0.3109, 0.3130, 0.3152,\n",
       "             0.3152, 0.3174, 0.3196, 0.3196, 0.3217, 0.3239, 0.3261, 0.3283, 0.3304,\n",
       "             0.3326, 0.3348, 0.3370, 0.3391, 0.3413, 0.3435, 0.3457, 0.3457, 0.3478,\n",
       "             0.3500, 0.3522, 0.3543, 0.3565, 0.3587, 0.3609, 0.3630, 0.3652, 0.3674,\n",
       "             0.3696, 0.3717, 0.3739, 0.3761, 0.3783, 0.3804, 0.3826, 0.3848, 0.3870,\n",
       "             0.3891, 0.3913, 0.3935, 0.3957, 0.3978, 0.4000, 0.4022, 0.4043, 0.4065,\n",
       "             0.4087, 0.4109, 0.4130, 0.4152, 0.4174, 0.4196, 0.4217, 0.4239, 0.4261,\n",
       "             0.4283, 0.4304, 0.4326, 0.4348, 0.4370, 0.4391, 0.4413, 0.4435, 0.4457,\n",
       "             0.4478, 0.4500, 0.4522, 0.4543, 0.4565, 0.4587, 0.4609, 0.4630, 0.4652,\n",
       "             0.4674, 0.4696, 0.4717, 0.4739, 0.4761, 0.4783, 0.4804, 0.4826, 0.4848,\n",
       "             0.4870, 0.4891, 0.4913, 0.4935, 0.4957, 0.4978, 0.5000, 0.5022, 0.5043,\n",
       "             0.5065, 0.5087, 0.5109, 0.5130, 0.5152, 0.5174, 0.5196, 0.5217, 0.5239,\n",
       "             0.5261, 0.5283, 0.5283, 0.5304, 0.5326, 0.5348, 0.5370, 0.5391, 0.5413,\n",
       "             0.5435, 0.5457, 0.5478, 0.5500, 0.5522, 0.5543, 0.5565, 0.5587, 0.5609,\n",
       "             0.5630, 0.5652, 0.5674, 0.5696, 0.5717, 0.5739, 0.5761, 0.5783, 0.5804,\n",
       "             0.5826, 0.5848, 0.5870, 0.5891, 0.5913, 0.5935, 0.5957, 0.5978, 0.6000,\n",
       "             0.6022, 0.6043, 0.6065, 0.6087, 0.6109, 0.6130, 0.6152, 0.6174, 0.6196,\n",
       "             0.6217, 0.6239, 0.6261, 0.6283, 0.6304, 0.6326, 0.6348, 0.6370, 0.6391,\n",
       "             0.6413, 0.6435, 0.6457, 0.6478, 0.6500, 0.6522, 0.6543, 0.6565, 0.6587,\n",
       "             0.6609, 0.6630, 0.6652, 0.6674, 0.6696, 0.6717, 0.6739, 0.6761, 0.6783,\n",
       "             0.6804, 0.6826, 0.6848, 0.6870, 0.6891, 0.6913, 0.6935, 0.6957, 0.6978,\n",
       "             0.7000, 0.7022, 0.7043, 0.7065, 0.7087, 0.7109, 0.7130, 0.7152, 0.7174,\n",
       "             0.7196, 0.7217, 0.7239, 0.7261, 0.7283, 0.7304, 0.7326, 0.7348, 0.7370,\n",
       "             0.7391, 0.7413, 0.7435, 0.7457, 0.7478, 0.7500, 0.7522, 0.7543, 0.7565,\n",
       "             0.7587, 0.7609, 0.7630, 0.7652, 0.7674, 0.7696, 0.7717, 0.7739, 0.7761,\n",
       "             0.7783, 0.7804, 0.7826, 0.7848, 0.7870, 0.7891, 0.7913, 0.7935, 0.7957,\n",
       "             0.7978, 0.8000, 0.8022, 0.8043, 0.8065, 0.8087, 0.8109, 0.8130, 0.8152,\n",
       "             0.8174, 0.8196, 0.8217, 0.8239, 0.8261, 0.8283, 0.8304, 0.8326, 0.8348,\n",
       "             0.8370, 0.8391, 0.8413, 0.8435, 0.8457, 0.8478, 0.8500, 0.8522, 0.8543,\n",
       "             0.8565, 0.8587, 0.8609, 0.8630, 0.8652, 0.8674, 0.8696, 0.8717, 0.8739,\n",
       "             0.8761, 0.8783, 0.8804, 0.8826, 0.8848, 0.8870, 0.8891, 0.8913, 0.8935,\n",
       "             0.8957, 0.8978, 0.9000, 0.9022, 0.9043, 0.9065, 0.9087, 0.9109, 0.9130,\n",
       "             0.9152, 0.9174, 0.9196, 0.9217, 0.9239, 0.9261, 0.9283, 0.9304, 0.9326,\n",
       "             0.9348, 0.9370, 0.9391, 0.9413, 0.9435, 0.9457, 0.9478, 0.9500, 0.9522,\n",
       "             0.9543, 0.9565, 0.9587, 0.9609, 0.9630, 0.9652, 0.9674, 0.9696, 0.9717,\n",
       "             0.9739, 0.9761, 0.9783, 0.9804, 0.9826, 0.9848, 0.9870, 0.9891, 0.9913,\n",
       "             0.9935, 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7521, 0.7923, 0.8131, 0.8224, 0.8340, 0.8409, 0.8463, 0.8525,\n",
       "             0.8571, 0.8602, 0.8625, 0.8641, 0.8664, 0.8680, 0.8710, 0.8757, 0.8811,\n",
       "             0.8842, 0.8857, 0.8865, 0.8873, 0.8880, 0.8888, 0.8896, 0.8911, 0.8919,\n",
       "             0.8927, 0.8942, 0.8950, 0.8973, 0.8981, 0.8988, 0.8996, 0.9004, 0.9012,\n",
       "             0.9019, 0.9027, 0.9035, 0.9050, 0.9058, 0.9066, 0.9081, 0.9097, 0.9104,\n",
       "             0.9104, 0.9112, 0.9120, 0.9120, 0.9120, 0.9135, 0.9143, 0.9151, 0.9158,\n",
       "             0.9166, 0.9189, 0.9197, 0.9205, 0.9212, 0.9220, 0.9228, 0.9236, 0.9243,\n",
       "             0.9251, 0.9259, 0.9266, 0.9266, 0.9274, 0.9282, 0.9290, 0.9297, 0.9305,\n",
       "             0.9313, 0.9320, 0.9328, 0.9344, 0.9359, 0.9367, 0.9375, 0.9382, 0.9390,\n",
       "             0.9398, 0.9405, 0.9413, 0.9421, 0.9421, 0.9421, 0.9429, 0.9436, 0.9444,\n",
       "             0.9452, 0.9459, 0.9467, 0.9475, 0.9483, 0.9490, 0.9498, 0.9506, 0.9514,\n",
       "             0.9521, 0.9529, 0.9537, 0.9544, 0.9552, 0.9560, 0.9568, 0.9575, 0.9583,\n",
       "             0.9591, 0.9598, 0.9606, 0.9614, 0.9622, 0.9629, 0.9637, 0.9645, 0.9645,\n",
       "             0.9653, 0.9660, 0.9660, 0.9668, 0.9668, 0.9668, 0.9668, 0.9676, 0.9676,\n",
       "             0.9683, 0.9691, 0.9691, 0.9699, 0.9707, 0.9707, 0.9714, 0.9722, 0.9730,\n",
       "             0.9737, 0.9745, 0.9745, 0.9745, 0.9753, 0.9753, 0.9761, 0.9761, 0.9761,\n",
       "             0.9768, 0.9768, 0.9776, 0.9776, 0.9776, 0.9776, 0.9776, 0.9784, 0.9784,\n",
       "             0.9784, 0.9792, 0.9799, 0.9807, 0.9807, 0.9815, 0.9822, 0.9822, 0.9822,\n",
       "             0.9822, 0.9822, 0.9822, 0.9830, 0.9830, 0.9830, 0.9838, 0.9838, 0.9838,\n",
       "             0.9838, 0.9846, 0.9846, 0.9853, 0.9861, 0.9869, 0.9869, 0.9869, 0.9876,\n",
       "             0.9884, 0.9884, 0.9884, 0.9884, 0.9892, 0.9892, 0.9892, 0.9892, 0.9900,\n",
       "             0.9907, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9946, 0.9946, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9977e-01, 9.9975e-01, 9.9972e-01, 9.9970e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9949e-01, 9.9949e-01, 9.9945e-01, 9.9942e-01, 9.9941e-01,\n",
       "             9.9932e-01, 9.9931e-01, 9.9924e-01, 9.9917e-01, 9.9911e-01, 9.9905e-01,\n",
       "             9.9883e-01, 9.9877e-01, 9.9864e-01, 9.9862e-01, 9.9831e-01, 9.9812e-01,\n",
       "             9.9804e-01, 9.9787e-01, 9.9769e-01, 9.9757e-01, 9.9717e-01, 9.9708e-01,\n",
       "             9.9603e-01, 9.9543e-01, 9.9516e-01, 9.9405e-01, 9.9351e-01, 9.9342e-01,\n",
       "             9.9203e-01, 9.9194e-01, 9.9143e-01, 9.9072e-01, 9.9017e-01, 9.8865e-01,\n",
       "             9.8586e-01, 9.8579e-01, 9.8285e-01, 9.7974e-01, 9.7796e-01, 9.7510e-01,\n",
       "             9.6271e-01, 9.5963e-01, 9.5164e-01, 9.4672e-01, 9.4097e-01, 9.3020e-01,\n",
       "             9.2412e-01, 9.2357e-01, 9.2063e-01, 9.1079e-01, 9.1076e-01, 9.0974e-01,\n",
       "             8.9559e-01, 8.8537e-01, 8.7254e-01, 8.5764e-01, 8.3221e-01, 8.2206e-01,\n",
       "             7.7577e-01, 7.7472e-01, 7.5333e-01, 7.3591e-01, 7.2253e-01, 6.8301e-01,\n",
       "             6.6291e-01, 6.4540e-01, 6.1907e-01, 5.3559e-01, 5.0625e-01, 4.9572e-01,\n",
       "             4.4909e-01, 4.2327e-01, 3.3797e-01, 2.9018e-01, 2.5779e-01, 2.4282e-01,\n",
       "             2.2679e-01, 2.1950e-01, 2.0316e-01, 1.9467e-01, 1.7799e-01, 1.7375e-01,\n",
       "             1.2825e-01, 1.1896e-01, 1.1875e-01, 1.0765e-01, 1.0559e-01, 9.5859e-02,\n",
       "             9.3266e-02, 8.9693e-02, 8.8924e-02, 8.3515e-02, 8.3296e-02, 4.3637e-02,\n",
       "             3.5167e-02, 3.1816e-02, 2.9955e-02, 2.4206e-02, 2.4056e-02, 1.6373e-02,\n",
       "             1.6025e-02, 1.5986e-02, 1.5528e-02, 1.2177e-02, 1.1050e-02, 1.0816e-02,\n",
       "             9.6754e-03, 8.9550e-03, 8.5835e-03, 8.5616e-03, 8.4280e-03, 7.6344e-03,\n",
       "             6.9291e-03, 5.9987e-03, 5.9445e-03, 5.3245e-03, 3.3960e-03, 2.4437e-03,\n",
       "             1.8220e-03, 1.4413e-03, 1.2436e-03, 9.4785e-04, 9.3134e-04, 9.0668e-04,\n",
       "             9.0523e-04, 6.2546e-04, 6.1595e-04, 5.5347e-04, 4.6723e-04, 3.8941e-04,\n",
       "             3.5152e-04, 3.2492e-04, 3.2428e-04, 2.9107e-04, 2.7332e-04, 2.6924e-04,\n",
       "             2.6004e-04, 2.4786e-04, 2.4234e-04, 2.0020e-04, 1.7448e-04, 1.5884e-04,\n",
       "             1.4711e-04, 1.4228e-04, 1.4087e-04, 1.3454e-04, 1.3174e-04, 1.2650e-04,\n",
       "             1.2286e-04, 8.1989e-05, 7.8438e-05, 7.6675e-05, 7.3385e-05, 6.6245e-05,\n",
       "             6.5571e-05, 4.1551e-05, 3.4403e-05, 2.0809e-05, 2.0804e-05, 2.0358e-05,\n",
       "             1.9015e-05, 1.6784e-05, 1.5205e-05, 1.4705e-05, 1.4568e-05, 1.0598e-05,\n",
       "             9.6372e-06, 8.8010e-06, 7.4733e-06, 7.3652e-06, 7.0261e-06, 6.0206e-06,\n",
       "             5.3996e-06, 5.0718e-06, 4.9709e-06, 3.2343e-06, 3.0487e-06, 2.6733e-06,\n",
       "             2.0218e-06, 1.9708e-06, 1.9223e-06, 1.9215e-06, 1.3657e-06, 1.2100e-06,\n",
       "             1.1849e-06, 1.1336e-06, 9.6098e-07, 7.6382e-07, 6.2915e-07, 5.6666e-07,\n",
       "             4.7331e-07, 3.8870e-07, 3.7603e-07, 3.4228e-07, 3.1971e-07, 2.8413e-07,\n",
       "             2.7664e-07, 2.7268e-07, 2.1057e-07, 1.9727e-07, 1.9225e-07, 1.7429e-07,\n",
       "             1.3926e-07, 1.3376e-07, 1.0629e-07, 9.9449e-08, 8.6665e-08, 8.2977e-08,\n",
       "             8.0464e-08, 7.6548e-08, 7.3466e-08, 6.4353e-08, 5.7155e-08, 5.2084e-08,\n",
       "             4.6310e-08, 4.3873e-08, 4.3459e-08, 4.2738e-08, 3.9494e-08, 3.7847e-08,\n",
       "             3.0968e-08, 3.0865e-08, 2.4748e-08, 1.8331e-08, 1.8130e-08, 1.7955e-08,\n",
       "             1.3550e-08, 1.3461e-08, 1.3287e-08, 1.2507e-08, 1.2280e-08, 1.0587e-08,\n",
       "             9.5826e-09, 8.6376e-09, 8.0101e-09, 7.3296e-09, 6.7038e-09, 6.2483e-09,\n",
       "             5.9165e-09, 4.7276e-09, 4.6619e-09, 4.5121e-09, 4.1637e-09, 3.9043e-09,\n",
       "             3.6904e-09, 3.5877e-09, 3.4250e-09, 3.2819e-09, 3.0634e-09, 2.8890e-09,\n",
       "             2.8647e-09, 2.5240e-09, 2.4829e-09, 2.1879e-09, 2.0902e-09, 2.0765e-09,\n",
       "             1.5712e-09, 1.3902e-09, 1.2713e-09, 1.0172e-09, 9.9064e-10, 9.7235e-10,\n",
       "             8.3304e-10, 8.3148e-10, 7.0308e-10, 6.7145e-10, 5.1552e-10, 4.8903e-10,\n",
       "             4.6275e-10, 4.4170e-10, 3.6442e-10, 3.3487e-10, 3.1021e-10, 2.8394e-10,\n",
       "             2.7625e-10, 2.3107e-10, 2.1254e-10, 2.0976e-10, 2.0963e-10, 1.9211e-10,\n",
       "             1.8855e-10, 1.5565e-10, 1.5544e-10, 1.4780e-10, 1.3776e-10, 1.3080e-10,\n",
       "             1.3023e-10, 1.2561e-10, 8.7583e-11, 7.8756e-11, 7.2386e-11, 6.8166e-11,\n",
       "             6.3162e-11, 6.0207e-11, 6.0048e-11, 5.4268e-11, 5.2937e-11, 4.7332e-11,\n",
       "             3.9942e-11, 3.8930e-11, 3.8040e-11, 3.5169e-11, 3.2757e-11, 3.0449e-11,\n",
       "             2.8897e-11, 2.8290e-11, 2.8016e-11, 2.7456e-11, 2.6556e-11, 2.6538e-11,\n",
       "             1.9105e-11, 1.9061e-11, 1.8183e-11, 1.8134e-11, 1.5392e-11, 1.1649e-11,\n",
       "             1.1543e-11, 9.5812e-12, 9.5292e-12, 8.7341e-12, 8.4909e-12, 7.2912e-12,\n",
       "             6.5331e-12, 6.3365e-12, 6.2841e-12, 5.6227e-12, 4.5629e-12, 4.5201e-12,\n",
       "             4.2994e-12, 4.0305e-12, 2.7537e-12, 1.7480e-12, 1.5170e-12, 1.3537e-12,\n",
       "             1.2966e-12, 1.1183e-12, 8.9979e-13, 8.4250e-13, 8.4115e-13, 8.2739e-13,\n",
       "             7.8564e-13, 4.7899e-13, 4.6755e-13, 4.6257e-13, 4.5594e-13, 4.4632e-13,\n",
       "             3.8074e-13, 3.4676e-13, 3.4634e-13, 2.6439e-13, 2.4791e-13, 2.1554e-13,\n",
       "             2.0217e-13, 2.0041e-13, 1.8142e-13, 1.6319e-13, 1.3888e-13, 1.3375e-13,\n",
       "             1.2084e-13, 1.1106e-13, 1.0694e-13, 1.0617e-13, 8.3216e-14, 7.8321e-14,\n",
       "             6.3012e-14, 6.1495e-14, 5.8464e-14, 5.3895e-14, 4.8043e-14, 4.7321e-14,\n",
       "             4.6649e-14, 3.2528e-14, 3.1542e-14, 3.1429e-14, 2.5918e-14, 2.5574e-14,\n",
       "             2.4859e-14, 2.2775e-14, 2.2554e-14, 2.1285e-14, 1.9084e-14, 1.8364e-14,\n",
       "             1.7576e-14, 1.7124e-14, 1.6312e-14, 1.5727e-14, 1.4631e-14, 1.1037e-14,\n",
       "             1.0760e-14, 1.0441e-14, 9.6025e-15, 9.4723e-15, 8.7341e-15, 8.0114e-15,\n",
       "             7.7539e-15, 3.9484e-15, 3.6543e-15, 3.1608e-15, 2.0503e-15, 1.8951e-15,\n",
       "             1.5902e-15, 1.5752e-15, 1.3214e-15, 1.2006e-15, 1.0783e-15, 9.7880e-16,\n",
       "             8.4217e-16, 6.7741e-16, 5.0008e-16, 3.8877e-16, 3.5486e-16, 2.2494e-16,\n",
       "             2.0342e-16, 1.5253e-16, 1.4791e-16, 1.1318e-16, 1.0369e-16, 8.8653e-17,\n",
       "             6.0996e-17, 3.5291e-17, 2.5144e-17, 1.4230e-17, 1.2295e-17, 1.1740e-17,\n",
       "             9.2048e-18, 8.2617e-18, 5.5878e-18, 4.7942e-18, 3.3763e-18, 1.8037e-18,\n",
       "             1.6582e-18, 1.2715e-18, 1.2630e-18, 1.0372e-18, 8.4224e-19, 3.8803e-19,\n",
       "             3.7067e-19, 3.6107e-19, 2.4583e-19, 2.0922e-19, 2.0243e-19, 1.2363e-19,\n",
       "             1.2228e-19, 1.1649e-19, 1.0345e-19, 9.4429e-20, 6.0890e-20, 5.7238e-20,\n",
       "             5.2122e-20, 4.8422e-20, 2.2610e-20, 2.0497e-20, 1.9743e-20, 1.8422e-20,\n",
       "             1.4786e-20, 5.4481e-21, 4.9965e-21, 4.3222e-21, 4.0098e-21, 2.2444e-21,\n",
       "             2.1748e-21, 1.4524e-21, 1.4327e-21, 9.2419e-22, 7.5562e-22, 6.1337e-22,\n",
       "             6.0848e-22, 5.8434e-22, 4.5675e-22, 4.1321e-22, 3.5257e-22, 1.5826e-22,\n",
       "             1.1938e-22, 1.0941e-22, 7.9210e-23, 3.2688e-23, 2.7323e-23, 1.8250e-23,\n",
       "             1.5317e-23, 6.3012e-24, 5.9235e-24, 1.0711e-24, 6.6611e-25, 1.2116e-25,\n",
       "             1.0070e-25, 8.8099e-26, 5.2543e-26, 2.0538e-26, 1.2588e-26, 1.9924e-27,\n",
       "             6.4692e-31])}},\n",
       "   {'fpr': np.float64(0.02608695652173913),\n",
       "    'tpr': np.float64(0.9691119691119691),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0043, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0087, 0.0087, 0.0109, 0.0109, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
       "             0.0152, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174,\n",
       "             0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174,\n",
       "             0.0174, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0217, 0.0217, 0.0217,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0239,\n",
       "             0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239,\n",
       "             0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239,\n",
       "             0.0261, 0.0261, 0.0283, 0.0283, 0.0283, 0.0304, 0.0304, 0.0326, 0.0326,\n",
       "             0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0348, 0.0348, 0.0370,\n",
       "             0.0370, 0.0370, 0.0391, 0.0391, 0.0413, 0.0435, 0.0435, 0.0457, 0.0457,\n",
       "             0.0457, 0.0478, 0.0500, 0.0500, 0.0500, 0.0500, 0.0522, 0.0543, 0.0565,\n",
       "             0.0587, 0.0609, 0.0630, 0.0652, 0.0652, 0.0674, 0.0696, 0.0717, 0.0717,\n",
       "             0.0739, 0.0761, 0.0761, 0.0783, 0.0804, 0.0826, 0.0848, 0.0870, 0.0891,\n",
       "             0.0891, 0.0891, 0.0913, 0.0935, 0.0935, 0.0957, 0.0957, 0.0978, 0.1000,\n",
       "             0.1022, 0.1043, 0.1043, 0.1043, 0.1065, 0.1087, 0.1109, 0.1109, 0.1109,\n",
       "             0.1130, 0.1130, 0.1152, 0.1174, 0.1174, 0.1196, 0.1217, 0.1239, 0.1261,\n",
       "             0.1283, 0.1283, 0.1304, 0.1326, 0.1348, 0.1348, 0.1370, 0.1391, 0.1413,\n",
       "             0.1435, 0.1457, 0.1478, 0.1500, 0.1522, 0.1543, 0.1565, 0.1587, 0.1609,\n",
       "             0.1630, 0.1652, 0.1674, 0.1696, 0.1717, 0.1739, 0.1761, 0.1783, 0.1804,\n",
       "             0.1826, 0.1848, 0.1870, 0.1891, 0.1913, 0.1935, 0.1957, 0.1978, 0.2000,\n",
       "             0.2022, 0.2043, 0.2065, 0.2087, 0.2109, 0.2130, 0.2152, 0.2174, 0.2196,\n",
       "             0.2217, 0.2239, 0.2261, 0.2283, 0.2304, 0.2326, 0.2348, 0.2370, 0.2391,\n",
       "             0.2413, 0.2435, 0.2457, 0.2478, 0.2500, 0.2522, 0.2543, 0.2565, 0.2587,\n",
       "             0.2609, 0.2630, 0.2652, 0.2674, 0.2696, 0.2717, 0.2739, 0.2761, 0.2783,\n",
       "             0.2804, 0.2826, 0.2848, 0.2870, 0.2891, 0.2913, 0.2935, 0.2957, 0.2978,\n",
       "             0.3000, 0.3022, 0.3043, 0.3065, 0.3087, 0.3109, 0.3130, 0.3152, 0.3174,\n",
       "             0.3196, 0.3217, 0.3239, 0.3261, 0.3283, 0.3304, 0.3326, 0.3348, 0.3370,\n",
       "             0.3391, 0.3413, 0.3435, 0.3457, 0.3457, 0.3478, 0.3478, 0.3500, 0.3522,\n",
       "             0.3543, 0.3565, 0.3587, 0.3609, 0.3630, 0.3652, 0.3674, 0.3696, 0.3696,\n",
       "             0.3717, 0.3739, 0.3761, 0.3783, 0.3804, 0.3826, 0.3848, 0.3870, 0.3891,\n",
       "             0.3913, 0.3935, 0.3957, 0.3978, 0.4000, 0.4022, 0.4043, 0.4065, 0.4087,\n",
       "             0.4109, 0.4130, 0.4152, 0.4174, 0.4196, 0.4217, 0.4239, 0.4261, 0.4283,\n",
       "             0.4304, 0.4326, 0.4348, 0.4370, 0.4391, 0.4413, 0.4435, 0.4457, 0.4478,\n",
       "             0.4500, 0.4522, 0.4543, 0.4565, 0.4587, 0.4609, 0.4630, 0.4652, 0.4674,\n",
       "             0.4696, 0.4717, 0.4739, 0.4739, 0.4761, 0.4783, 0.4804, 0.4826, 0.4848,\n",
       "             0.4870, 0.4891, 0.4913, 0.4935, 0.4957, 0.4978, 0.5000, 0.5022, 0.5043,\n",
       "             0.5065, 0.5087, 0.5109, 0.5130, 0.5152, 0.5174, 0.5196, 0.5217, 0.5239,\n",
       "             0.5261, 0.5283, 0.5304, 0.5326, 0.5348, 0.5370, 0.5391, 0.5413, 0.5435,\n",
       "             0.5457, 0.5478, 0.5500, 0.5522, 0.5543, 0.5565, 0.5587, 0.5609, 0.5630,\n",
       "             0.5652, 0.5674, 0.5696, 0.5717, 0.5739, 0.5761, 0.5783, 0.5804, 0.5826,\n",
       "             0.5848, 0.5870, 0.5891, 0.5913, 0.5935, 0.5957, 0.5978, 0.6000, 0.6022,\n",
       "             0.6043, 0.6065, 0.6087, 0.6109, 0.6130, 0.6152, 0.6174, 0.6196, 0.6217,\n",
       "             0.6239, 0.6261, 0.6283, 0.6304, 0.6326, 0.6348, 0.6370, 0.6391, 0.6413,\n",
       "             0.6435, 0.6457, 0.6478, 0.6500, 0.6522, 0.6543, 0.6565, 0.6587, 0.6609,\n",
       "             0.6630, 0.6652, 0.6674, 0.6696, 0.6717, 0.6739, 0.6761, 0.6783, 0.6804,\n",
       "             0.6826, 0.6848, 0.6870, 0.6891, 0.6913, 0.6935, 0.6957, 0.6978, 0.7000,\n",
       "             0.7022, 0.7043, 0.7065, 0.7087, 0.7109, 0.7130, 0.7152, 0.7174, 0.7196,\n",
       "             0.7217, 0.7239, 0.7261, 0.7283, 0.7304, 0.7326, 0.7348, 0.7370, 0.7391,\n",
       "             0.7413, 0.7435, 0.7457, 0.7478, 0.7500, 0.7522, 0.7543, 0.7565, 0.7587,\n",
       "             0.7609, 0.7630, 0.7652, 0.7674, 0.7696, 0.7717, 0.7739, 0.7761, 0.7783,\n",
       "             0.7804, 0.7826, 0.7848, 0.7870, 0.7891, 0.7913, 0.7935, 0.7957, 0.7978,\n",
       "             0.8000, 0.8022, 0.8043, 0.8065, 0.8087, 0.8109, 0.8130, 0.8152, 0.8174,\n",
       "             0.8196, 0.8217, 0.8239, 0.8261, 0.8283, 0.8304, 0.8326, 0.8348, 0.8370,\n",
       "             0.8391, 0.8413, 0.8435, 0.8457, 0.8478, 0.8500, 0.8522, 0.8543, 0.8565,\n",
       "             0.8587, 0.8609, 0.8630, 0.8652, 0.8674, 0.8696, 0.8717, 0.8739, 0.8761,\n",
       "             0.8783, 0.8804, 0.8826, 0.8848, 0.8870, 0.8891, 0.8913, 0.8935, 0.8957,\n",
       "             0.8978, 0.9000, 0.9022, 0.9043, 0.9065, 0.9087, 0.9109, 0.9130, 0.9152,\n",
       "             0.9174, 0.9196, 0.9217, 0.9239, 0.9261, 0.9283, 0.9304, 0.9326, 0.9348,\n",
       "             0.9370, 0.9391, 0.9413, 0.9435, 0.9457, 0.9478, 0.9500, 0.9522, 0.9543,\n",
       "             0.9565, 0.9587, 0.9609, 0.9630, 0.9652, 0.9674, 0.9696, 0.9717, 0.9739,\n",
       "             0.9761, 0.9783, 0.9804, 0.9826, 0.9848, 0.9870, 0.9891, 0.9913, 0.9935,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8541, 0.8710, 0.8780, 0.8795, 0.8826, 0.8849, 0.8857, 0.8880,\n",
       "             0.8911, 0.8927, 0.8934, 0.8934, 0.8942, 0.8965, 0.8988, 0.8996, 0.9012,\n",
       "             0.9019, 0.9027, 0.9035, 0.9042, 0.9050, 0.9058, 0.9066, 0.9073, 0.9089,\n",
       "             0.9097, 0.9104, 0.9112, 0.9120, 0.9135, 0.9143, 0.9151, 0.9158, 0.9166,\n",
       "             0.9174, 0.9174, 0.9181, 0.9189, 0.9197, 0.9205, 0.9212, 0.9220, 0.9228,\n",
       "             0.9236, 0.9236, 0.9243, 0.9251, 0.9259, 0.9266, 0.9274, 0.9282, 0.9290,\n",
       "             0.9305, 0.9313, 0.9320, 0.9328, 0.9336, 0.9344, 0.9351, 0.9359, 0.9367,\n",
       "             0.9375, 0.9382, 0.9390, 0.9398, 0.9405, 0.9413, 0.9421, 0.9429, 0.9436,\n",
       "             0.9444, 0.9444, 0.9452, 0.9459, 0.9467, 0.9475, 0.9475, 0.9483, 0.9490,\n",
       "             0.9498, 0.9506, 0.9514, 0.9521, 0.9529, 0.9537, 0.9544, 0.9552, 0.9552,\n",
       "             0.9560, 0.9568, 0.9575, 0.9583, 0.9591, 0.9598, 0.9606, 0.9614, 0.9622,\n",
       "             0.9629, 0.9637, 0.9645, 0.9653, 0.9660, 0.9668, 0.9676, 0.9683, 0.9691,\n",
       "             0.9691, 0.9699, 0.9699, 0.9707, 0.9714, 0.9714, 0.9722, 0.9722, 0.9730,\n",
       "             0.9737, 0.9745, 0.9753, 0.9761, 0.9768, 0.9776, 0.9776, 0.9784, 0.9784,\n",
       "             0.9792, 0.9799, 0.9799, 0.9807, 0.9807, 0.9807, 0.9815, 0.9815, 0.9822,\n",
       "             0.9830, 0.9830, 0.9830, 0.9838, 0.9846, 0.9853, 0.9853, 0.9853, 0.9853,\n",
       "             0.9853, 0.9853, 0.9853, 0.9853, 0.9861, 0.9861, 0.9861, 0.9861, 0.9869,\n",
       "             0.9869, 0.9869, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876, 0.9876,\n",
       "             0.9884, 0.9892, 0.9892, 0.9892, 0.9900, 0.9900, 0.9907, 0.9907, 0.9907,\n",
       "             0.9907, 0.9907, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9938,\n",
       "             0.9938, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9995e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9988e-01, 9.9981e-01, 9.9980e-01, 9.9976e-01, 9.9975e-01, 9.9974e-01,\n",
       "             9.9970e-01, 9.9963e-01, 9.9960e-01, 9.9959e-01, 9.9949e-01, 9.9942e-01,\n",
       "             9.9941e-01, 9.9939e-01, 9.9933e-01, 9.9922e-01, 9.9906e-01, 9.9900e-01,\n",
       "             9.9898e-01, 9.9874e-01, 9.9870e-01, 9.9865e-01, 9.9860e-01, 9.9860e-01,\n",
       "             9.9857e-01, 9.9849e-01, 9.9848e-01, 9.9832e-01, 9.9829e-01, 9.9816e-01,\n",
       "             9.9811e-01, 9.9769e-01, 9.9768e-01, 9.9753e-01, 9.9744e-01, 9.9706e-01,\n",
       "             9.9685e-01, 9.9622e-01, 9.9406e-01, 9.9373e-01, 9.9195e-01, 9.8607e-01,\n",
       "             9.8508e-01, 9.8151e-01, 9.7833e-01, 9.7606e-01, 9.6840e-01, 9.5777e-01,\n",
       "             9.3993e-01, 9.3480e-01, 9.1284e-01, 9.0406e-01, 8.9691e-01, 8.8546e-01,\n",
       "             8.7966e-01, 8.7166e-01, 8.1592e-01, 7.7158e-01, 7.1160e-01, 6.1620e-01,\n",
       "             5.9733e-01, 4.9293e-01, 4.5908e-01, 4.3219e-01, 4.1216e-01, 3.5273e-01,\n",
       "             3.3792e-01, 3.1516e-01, 3.0841e-01, 2.3570e-01, 2.1488e-01, 1.8634e-01,\n",
       "             1.4931e-01, 1.4772e-01, 1.2308e-01, 8.6950e-02, 8.4271e-02, 7.3281e-02,\n",
       "             5.4660e-02, 4.1134e-02, 3.4321e-02, 3.3765e-02, 2.2735e-02, 1.9916e-02,\n",
       "             1.8085e-02, 1.5827e-02, 1.3677e-02, 1.1183e-02, 8.2416e-03, 8.2109e-03,\n",
       "             5.8447e-03, 5.0101e-03, 3.8422e-03, 2.6062e-03, 1.7099e-03, 1.1670e-03,\n",
       "             1.0774e-03, 6.8515e-04, 6.2455e-04, 3.1091e-04, 2.8985e-04, 2.4459e-04,\n",
       "             2.1684e-04, 1.8291e-04, 1.8186e-04, 1.5243e-04, 9.8418e-05, 7.1116e-05,\n",
       "             6.8271e-05, 6.5384e-05, 6.5305e-05, 5.2119e-05, 4.7022e-05, 4.5648e-05,\n",
       "             3.2230e-05, 2.9350e-05, 2.4384e-05, 2.3618e-05, 1.0659e-05, 1.0015e-05,\n",
       "             9.6587e-06, 4.6143e-06, 3.0759e-06, 2.5080e-06, 2.4107e-06, 2.2201e-06,\n",
       "             2.2027e-06, 1.8510e-06, 9.2373e-07, 7.2403e-07, 5.9629e-07, 5.6532e-07,\n",
       "             4.1240e-07, 3.7973e-07, 3.0030e-07, 2.0443e-07, 1.9193e-07, 1.8754e-07,\n",
       "             1.0878e-07, 8.7659e-08, 6.9528e-08, 6.4697e-08, 5.3882e-08, 4.8736e-08,\n",
       "             4.7824e-08, 3.2301e-08, 2.9711e-08, 2.7960e-08, 2.2907e-08, 1.7002e-08,\n",
       "             1.5890e-08, 1.5114e-08, 1.4635e-08, 1.3233e-08, 9.8609e-09, 4.9710e-09,\n",
       "             3.6697e-09, 3.3514e-09, 2.8577e-09, 9.1575e-10, 6.9883e-10, 6.9136e-10,\n",
       "             5.2297e-10, 4.5928e-10, 4.0929e-10, 3.2737e-10, 2.2997e-10, 2.0942e-10,\n",
       "             1.8114e-10, 1.7099e-10, 1.2846e-10, 1.2825e-10, 1.0407e-10, 9.2574e-11,\n",
       "             6.1007e-11, 5.1485e-11, 4.5443e-11, 3.7871e-11, 3.3738e-11, 2.3679e-11,\n",
       "             1.6117e-11, 1.5940e-11, 1.5747e-11, 1.5271e-11, 1.2645e-11, 1.1909e-11,\n",
       "             4.1892e-12, 2.8110e-12, 1.5336e-12, 1.4957e-12, 1.2983e-12, 1.2534e-12,\n",
       "             1.1407e-12, 1.0615e-12, 7.2139e-13, 6.9052e-13, 6.3605e-13, 5.4217e-13,\n",
       "             4.9542e-13, 4.0114e-13, 3.9480e-13, 3.9447e-13, 3.9204e-13, 3.7695e-13,\n",
       "             3.0100e-13, 2.8101e-13, 2.4361e-13, 1.9711e-13, 1.9066e-13, 1.8228e-13,\n",
       "             1.7851e-13, 1.7746e-13, 1.7207e-13, 1.6183e-13, 1.5669e-13, 1.5309e-13,\n",
       "             1.4340e-13, 1.3612e-13, 1.2835e-13, 1.2285e-13, 1.1663e-13, 1.1598e-13,\n",
       "             1.0180e-13, 8.9371e-14, 8.9261e-14, 5.5232e-14, 4.5906e-14, 4.1714e-14,\n",
       "             3.7211e-14, 2.6393e-14, 2.1947e-14, 1.7610e-14, 1.5521e-14, 1.4494e-14,\n",
       "             1.3164e-14, 1.1373e-14, 1.0961e-14, 1.0196e-14, 9.9770e-15, 9.2376e-15,\n",
       "             8.7765e-15, 7.0802e-15, 6.7288e-15, 6.4101e-15, 5.5407e-15, 4.9389e-15,\n",
       "             4.5767e-15, 3.9475e-15, 3.2807e-15, 2.8591e-15, 2.8382e-15, 2.7577e-15,\n",
       "             2.6565e-15, 2.5576e-15, 2.4948e-15, 2.4820e-15, 2.2345e-15, 2.1171e-15,\n",
       "             2.0495e-15, 1.9492e-15, 1.7480e-15, 1.2679e-15, 1.0262e-15, 6.6446e-16,\n",
       "             6.3726e-16, 5.8614e-16, 5.6743e-16, 4.6996e-16, 4.1420e-16, 3.3900e-16,\n",
       "             3.2199e-16, 3.1951e-16, 2.6358e-16, 2.5652e-16, 2.4271e-16, 1.8001e-16,\n",
       "             1.7072e-16, 1.2830e-16, 1.1200e-16, 1.0583e-16, 8.3619e-17, 7.8914e-17,\n",
       "             7.8463e-17, 6.7878e-17, 6.1261e-17, 5.2359e-17, 3.7909e-17, 3.3243e-17,\n",
       "             2.7169e-17, 2.6892e-17, 2.6054e-17, 2.5802e-17, 2.0240e-17, 1.8585e-17,\n",
       "             1.7549e-17, 1.5728e-17, 1.5555e-17, 1.4080e-17, 1.1481e-17, 1.1196e-17,\n",
       "             9.1402e-18, 9.1334e-18, 8.0025e-18, 7.7291e-18, 6.4157e-18, 6.3697e-18,\n",
       "             5.3843e-18, 5.2904e-18, 4.9330e-18, 4.7997e-18, 3.6205e-18, 1.9663e-18,\n",
       "             1.8979e-18, 1.8382e-18, 1.7635e-18, 1.4088e-18, 1.3683e-18, 1.2476e-18,\n",
       "             1.2348e-18, 1.2305e-18, 1.0625e-18, 8.0325e-19, 6.3790e-19, 5.5033e-19,\n",
       "             4.7676e-19, 3.3537e-19, 3.2979e-19, 2.8906e-19, 2.8656e-19, 2.5545e-19,\n",
       "             2.5193e-19, 2.3243e-19, 2.1245e-19, 2.0150e-19, 1.7097e-19, 1.4240e-19,\n",
       "             1.3839e-19, 9.5129e-20, 9.5098e-20, 7.4345e-20, 5.6885e-20, 5.4864e-20,\n",
       "             5.4508e-20, 5.1095e-20, 4.9284e-20, 4.8192e-20, 4.7497e-20, 4.6056e-20,\n",
       "             4.4070e-20, 4.0651e-20, 3.7849e-20, 2.1184e-20, 1.9623e-20, 1.7939e-20,\n",
       "             1.6149e-20, 1.5904e-20, 1.3214e-20, 1.0667e-20, 1.0592e-20, 9.6657e-21,\n",
       "             9.3552e-21, 8.4801e-21, 8.2442e-21, 7.2123e-21, 6.7194e-21, 6.0867e-21,\n",
       "             5.7034e-21, 5.0194e-21, 3.7000e-21, 3.5817e-21, 3.2641e-21, 3.1828e-21,\n",
       "             2.5780e-21, 2.5596e-21, 2.3944e-21, 1.4212e-21, 1.3086e-21, 1.2783e-21,\n",
       "             9.6566e-22, 8.6247e-22, 6.9448e-22, 6.2604e-22, 5.5404e-22, 5.2792e-22,\n",
       "             4.1819e-22, 3.5310e-22, 3.1471e-22, 2.9388e-22, 2.7037e-22, 2.4647e-22,\n",
       "             2.2188e-22, 2.2137e-22, 2.0720e-22, 1.8264e-22, 1.6875e-22, 1.4051e-22,\n",
       "             1.1892e-22, 1.1866e-22, 7.6298e-23, 6.6058e-23, 5.7702e-23, 5.6674e-23,\n",
       "             4.9340e-23, 4.8156e-23, 4.3788e-23, 3.4482e-23, 3.3730e-23, 2.6628e-23,\n",
       "             2.4619e-23, 2.3192e-23, 2.2670e-23, 1.9652e-23, 1.2493e-23, 1.0618e-23,\n",
       "             1.0220e-23, 6.7097e-24, 4.4478e-24, 4.1349e-24, 2.4485e-24, 2.3064e-24,\n",
       "             2.1260e-24, 1.6904e-24, 1.6131e-24, 1.5593e-24, 9.4274e-25, 8.2435e-25,\n",
       "             7.1867e-25, 6.0153e-25, 5.7211e-25, 4.9450e-25, 3.7680e-25, 3.6619e-25,\n",
       "             3.5932e-25, 2.0068e-25, 1.8548e-25, 1.7510e-25, 1.6011e-25, 1.5878e-25,\n",
       "             1.5227e-25, 1.4699e-25, 1.2840e-25, 9.9375e-26, 9.7037e-26, 9.4623e-26,\n",
       "             8.7476e-26, 6.9918e-26, 6.3406e-26, 5.4720e-26, 4.9008e-26, 4.4158e-26,\n",
       "             4.2265e-26, 3.7497e-26, 2.8749e-26, 2.8413e-26, 2.6123e-26, 2.3470e-26,\n",
       "             2.2003e-26, 2.1924e-26, 2.0323e-26, 1.8103e-26, 1.6340e-26, 1.5171e-26,\n",
       "             1.4959e-26, 9.0468e-27, 8.3951e-27, 7.5039e-27, 7.4383e-27, 4.0939e-27,\n",
       "             3.7055e-27, 3.2255e-27, 2.5492e-27, 2.4929e-27, 2.2602e-27, 2.2532e-27,\n",
       "             2.1890e-27, 1.7150e-27, 1.1963e-27, 9.2226e-28, 8.9359e-28, 8.6974e-28,\n",
       "             7.5724e-28, 5.3994e-28, 5.2310e-28, 4.6516e-28, 3.6856e-28, 3.0020e-28,\n",
       "             2.6984e-28, 1.2230e-28, 1.1056e-28, 7.9956e-29, 7.1892e-29, 5.7118e-29,\n",
       "             4.9829e-29, 4.7424e-29, 2.6673e-29, 2.1179e-29, 2.0847e-29, 1.5223e-29,\n",
       "             1.4255e-29, 1.3894e-29, 1.0385e-29, 7.3409e-30, 7.2251e-30, 5.1961e-30,\n",
       "             4.2247e-30, 3.4064e-30, 2.5919e-30, 2.5734e-30, 1.1579e-30, 4.9570e-31,\n",
       "             4.0408e-31, 2.8289e-31, 2.6278e-31, 2.4178e-31, 2.0974e-31, 1.7393e-31,\n",
       "             1.4540e-31, 1.2292e-31, 7.3989e-32, 5.3431e-32, 2.4832e-32, 2.0499e-32,\n",
       "             2.0234e-32, 8.8857e-33, 8.8053e-33, 7.1934e-33, 4.5579e-33, 2.7348e-33,\n",
       "             1.3283e-33, 1.0905e-33, 8.3777e-34, 6.7279e-34, 4.8774e-34, 1.7719e-34,\n",
       "             1.2333e-34, 1.1219e-34, 7.1102e-35, 1.4604e-35, 1.1152e-35, 7.6971e-36,\n",
       "             6.5145e-36, 6.0396e-36, 5.0052e-36, 3.9387e-36, 1.0478e-37, 1.7450e-38,\n",
       "             0.0000e+00])}},\n",
       "   {'fpr': np.float64(0.06521739130434782),\n",
       "    'tpr': np.float64(0.9868725868725868),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0130, 0.0196, 0.0196, 0.0196, 0.0217, 0.0217, 0.0239, 0.0239,\n",
       "             0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0283, 0.0283,\n",
       "             0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0304, 0.0304,\n",
       "             0.0326, 0.0326, 0.0326, 0.0326, 0.0348, 0.0348, 0.0348, 0.0348, 0.0348,\n",
       "             0.0348, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0391, 0.0413,\n",
       "             0.0413, 0.0435, 0.0435, 0.0435, 0.0435, 0.0457, 0.0457, 0.0478, 0.0500,\n",
       "             0.0500, 0.0522, 0.0543, 0.0565, 0.0565, 0.0587, 0.0609, 0.0630, 0.0630,\n",
       "             0.0630, 0.0652, 0.0674, 0.0696, 0.0717, 0.0717, 0.0739, 0.0761, 0.0761,\n",
       "             0.0783, 0.0804, 0.0804, 0.0804, 0.0826, 0.0848, 0.0870, 0.0891, 0.0913,\n",
       "             0.0913, 0.0935, 0.0957, 0.0957, 0.0978, 0.1000, 0.1022, 0.1043, 0.1043,\n",
       "             0.1065, 0.1087, 0.1109, 0.1109, 0.1130, 0.1152, 0.1174, 0.1196, 0.1217,\n",
       "             0.1239, 0.1261, 0.1283, 0.1283, 0.1304, 0.1326, 0.1326, 0.1348, 0.1370,\n",
       "             0.1391, 0.1413, 0.1435, 0.1457, 0.1478, 0.1500, 0.1522, 0.1543, 0.1565,\n",
       "             0.1587, 0.1609, 0.1630, 0.1652, 0.1674, 0.1674, 0.1696, 0.1717, 0.1739,\n",
       "             0.1761, 0.1783, 0.1804, 0.1826, 0.1848, 0.1870, 0.1870, 0.1891, 0.1913,\n",
       "             0.1935, 0.1957, 0.1978, 0.2000, 0.2022, 0.2043, 0.2065, 0.2087, 0.2109,\n",
       "             0.2130, 0.2152, 0.2174, 0.2196, 0.2217, 0.2239, 0.2239, 0.2261, 0.2283,\n",
       "             0.2304, 0.2326, 0.2348, 0.2370, 0.2391, 0.2413, 0.2435, 0.2457, 0.2478,\n",
       "             0.2500, 0.2522, 0.2543, 0.2543, 0.2565, 0.2587, 0.2609, 0.2630, 0.2652,\n",
       "             0.2674, 0.2696, 0.2696, 0.2717, 0.2739, 0.2761, 0.2783, 0.2804, 0.2826,\n",
       "             0.2848, 0.2870, 0.2891, 0.2913, 0.2935, 0.2957, 0.2978, 0.3000, 0.3022,\n",
       "             0.3043, 0.3065, 0.3087, 0.3109, 0.3130, 0.3152, 0.3174, 0.3196, 0.3217,\n",
       "             0.3239, 0.3261, 0.3283, 0.3304, 0.3326, 0.3348, 0.3370, 0.3391, 0.3413,\n",
       "             0.3435, 0.3457, 0.3478, 0.3500, 0.3522, 0.3543, 0.3565, 0.3587, 0.3609,\n",
       "             0.3630, 0.3652, 0.3652, 0.3674, 0.3696, 0.3717, 0.3739, 0.3761, 0.3783,\n",
       "             0.3804, 0.3826, 0.3848, 0.3870, 0.3891, 0.3913, 0.3935, 0.3957, 0.3978,\n",
       "             0.3978, 0.4000, 0.4022, 0.4043, 0.4065, 0.4087, 0.4109, 0.4130, 0.4152,\n",
       "             0.4174, 0.4196, 0.4217, 0.4239, 0.4261, 0.4283, 0.4304, 0.4326, 0.4348,\n",
       "             0.4370, 0.4391, 0.4413, 0.4435, 0.4457, 0.4478, 0.4500, 0.4522, 0.4543,\n",
       "             0.4565, 0.4587, 0.4609, 0.4630, 0.4652, 0.4674, 0.4696, 0.4717, 0.4739,\n",
       "             0.4761, 0.4783, 0.4804, 0.4826, 0.4848, 0.4870, 0.4891, 0.4913, 0.4935,\n",
       "             0.4957, 0.4978, 0.5000, 0.5022, 0.5043, 0.5065, 0.5087, 0.5109, 0.5130,\n",
       "             0.5152, 0.5174, 0.5196, 0.5217, 0.5239, 0.5261, 0.5283, 0.5304, 0.5326,\n",
       "             0.5348, 0.5370, 0.5391, 0.5413, 0.5435, 0.5457, 0.5478, 0.5500, 0.5522,\n",
       "             0.5543, 0.5565, 0.5587, 0.5609, 0.5630, 0.5652, 0.5674, 0.5696, 0.5717,\n",
       "             0.5739, 0.5761, 0.5783, 0.5804, 0.5826, 0.5848, 0.5870, 0.5891, 0.5913,\n",
       "             0.5935, 0.5957, 0.5978, 0.6000, 0.6022, 0.6043, 0.6065, 0.6087, 0.6109,\n",
       "             0.6130, 0.6152, 0.6174, 0.6196, 0.6217, 0.6239, 0.6261, 0.6283, 0.6304,\n",
       "             0.6326, 0.6348, 0.6370, 0.6391, 0.6413, 0.6435, 0.6457, 0.6478, 0.6500,\n",
       "             0.6522, 0.6543, 0.6565, 0.6587, 0.6609, 0.6630, 0.6652, 0.6674, 0.6696,\n",
       "             0.6717, 0.6739, 0.6761, 0.6783, 0.6804, 0.6826, 0.6848, 0.6870, 0.6891,\n",
       "             0.6913, 0.6935, 0.6957, 0.6978, 0.7000, 0.7022, 0.7043, 0.7065, 0.7087,\n",
       "             0.7109, 0.7130, 0.7152, 0.7174, 0.7196, 0.7217, 0.7239, 0.7261, 0.7283,\n",
       "             0.7304, 0.7326, 0.7348, 0.7370, 0.7391, 0.7413, 0.7435, 0.7457, 0.7478,\n",
       "             0.7500, 0.7522, 0.7543, 0.7565, 0.7587, 0.7609, 0.7630, 0.7652, 0.7674,\n",
       "             0.7696, 0.7717, 0.7739, 0.7761, 0.7783, 0.7804, 0.7826, 0.7848, 0.7870,\n",
       "             0.7891, 0.7913, 0.7935, 0.7957, 0.7978, 0.8000, 0.8022, 0.8043, 0.8065,\n",
       "             0.8087, 0.8109, 0.8130, 0.8152, 0.8174, 0.8196, 0.8217, 0.8239, 0.8261,\n",
       "             0.8283, 0.8304, 0.8326, 0.8348, 0.8370, 0.8391, 0.8413, 0.8435, 0.8457,\n",
       "             0.8478, 0.8500, 0.8522, 0.8543, 0.8565, 0.8587, 0.8609, 0.8630, 0.8652,\n",
       "             0.8674, 0.8696, 0.8717, 0.8739, 0.8761, 0.8783, 0.8804, 0.8826, 0.8848,\n",
       "             0.8870, 0.8891, 0.8913, 0.8935, 0.8957, 0.8978, 0.9000, 0.9022, 0.9043,\n",
       "             0.9065, 0.9087, 0.9109, 0.9130, 0.9152, 0.9174, 0.9196, 0.9217, 0.9239,\n",
       "             0.9261, 0.9283, 0.9304, 0.9326, 0.9348, 0.9370, 0.9391, 0.9413, 0.9435,\n",
       "             0.9457, 0.9478, 0.9500, 0.9522, 0.9543, 0.9565, 0.9587, 0.9609, 0.9630,\n",
       "             0.9652, 0.9674, 0.9696, 0.9717, 0.9739, 0.9761, 0.9783, 0.9804, 0.9826,\n",
       "             0.9848, 0.9870, 0.9891, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9359, 0.9398, 0.9452, 0.9459, 0.9459, 0.9483, 0.9490, 0.9498,\n",
       "             0.9514, 0.9521, 0.9537, 0.9544, 0.9552, 0.9560, 0.9568, 0.9575, 0.9575,\n",
       "             0.9583, 0.9591, 0.9598, 0.9606, 0.9614, 0.9622, 0.9629, 0.9629, 0.9637,\n",
       "             0.9645, 0.9653, 0.9660, 0.9668, 0.9676, 0.9683, 0.9691, 0.9691, 0.9699,\n",
       "             0.9699, 0.9707, 0.9714, 0.9722, 0.9722, 0.9730, 0.9737, 0.9745, 0.9753,\n",
       "             0.9761, 0.9761, 0.9768, 0.9776, 0.9784, 0.9792, 0.9799, 0.9799, 0.9799,\n",
       "             0.9807, 0.9807, 0.9815, 0.9822, 0.9830, 0.9830, 0.9838, 0.9838, 0.9838,\n",
       "             0.9846, 0.9846, 0.9846, 0.9846, 0.9853, 0.9853, 0.9853, 0.9853, 0.9861,\n",
       "             0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9876, 0.9876, 0.9876, 0.9884,\n",
       "             0.9884, 0.9884, 0.9892, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "             0.9907, 0.9907, 0.9907, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9938, 0.9938, 0.9938, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9991e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9977e-01, 9.9965e-01, 9.9965e-01, 9.9955e-01, 9.9940e-01, 9.9911e-01,\n",
       "             9.9906e-01, 9.9906e-01, 9.9882e-01, 9.9847e-01, 9.9841e-01, 9.9787e-01,\n",
       "             9.9783e-01, 9.9624e-01, 9.9285e-01, 9.8999e-01, 9.8623e-01, 9.8128e-01,\n",
       "             9.7754e-01, 9.6619e-01, 9.6532e-01, 9.6050e-01, 9.5842e-01, 9.5777e-01,\n",
       "             9.5112e-01, 9.5007e-01, 9.4774e-01, 9.4725e-01, 9.3998e-01, 9.1322e-01,\n",
       "             8.8532e-01, 8.3857e-01, 8.2975e-01, 7.8080e-01, 7.2317e-01, 7.0444e-01,\n",
       "             5.5720e-01, 5.0274e-01, 3.4416e-01, 3.1143e-01, 2.6675e-01, 2.5191e-01,\n",
       "             1.0082e-01, 7.9038e-02, 6.8702e-02, 3.4500e-02, 2.8887e-02, 2.6431e-02,\n",
       "             2.3701e-02, 1.6172e-02, 7.8278e-03, 4.9191e-03, 4.5466e-03, 4.4906e-03,\n",
       "             2.6193e-03, 2.6020e-03, 2.3862e-03, 2.0204e-03, 1.6728e-03, 1.5169e-03,\n",
       "             3.9510e-04, 3.5155e-04, 3.4830e-04, 3.3541e-04, 3.1661e-04, 3.1126e-04,\n",
       "             2.0620e-04, 1.4384e-04, 1.0237e-04, 7.0401e-05, 5.4101e-05, 4.8971e-05,\n",
       "             3.7783e-05, 2.1026e-05, 1.9424e-05, 1.5691e-05, 1.4424e-05, 1.2329e-05,\n",
       "             7.4009e-06, 6.1367e-06, 4.6171e-06, 4.5933e-06, 4.4446e-06, 2.5768e-06,\n",
       "             2.5507e-06, 2.5051e-06, 2.1416e-06, 2.0971e-06, 8.9404e-07, 8.8670e-07,\n",
       "             5.6787e-07, 5.3949e-07, 5.2971e-07, 5.2636e-07, 4.6926e-07, 3.4243e-07,\n",
       "             2.0617e-07, 1.8841e-07, 1.2881e-07, 1.0509e-07, 8.2927e-08, 5.4769e-08,\n",
       "             4.6561e-08, 3.8627e-08, 2.6958e-08, 2.0464e-08, 1.5881e-08, 1.5691e-08,\n",
       "             1.4916e-08, 1.2285e-08, 1.0120e-08, 8.6563e-09, 8.3914e-09, 7.7421e-09,\n",
       "             7.4753e-09, 7.3311e-09, 6.4677e-09, 5.6706e-09, 5.0388e-09, 3.9445e-09,\n",
       "             2.3276e-09, 2.0871e-09, 1.8508e-09, 1.6845e-09, 1.4909e-09, 1.4014e-09,\n",
       "             1.2077e-09, 1.0280e-09, 9.4023e-10, 8.6624e-10, 8.2908e-10, 6.8855e-10,\n",
       "             5.7571e-10, 5.2723e-10, 5.0453e-10, 4.8713e-10, 4.5611e-10, 3.7713e-10,\n",
       "             3.6473e-10, 3.4322e-10, 3.3269e-10, 3.1414e-10, 3.0045e-10, 2.2547e-10,\n",
       "             1.2499e-10, 1.1847e-10, 1.1828e-10, 1.1424e-10, 1.1252e-10, 9.7563e-11,\n",
       "             8.0262e-11, 7.9233e-11, 6.7876e-11, 5.9911e-11, 4.7793e-11, 3.5451e-11,\n",
       "             2.1116e-11, 1.3406e-11, 1.2996e-11, 1.2577e-11, 9.3953e-12, 7.5651e-12,\n",
       "             7.3558e-12, 5.5822e-12, 4.9532e-12, 4.0597e-12, 4.0577e-12, 3.6401e-12,\n",
       "             2.9547e-12, 2.7518e-12, 2.0955e-12, 2.0194e-12, 1.7983e-12, 1.1159e-12,\n",
       "             8.3189e-13, 7.6903e-13, 6.4155e-13, 4.6416e-13, 4.1595e-13, 4.1446e-13,\n",
       "             3.6546e-13, 3.3463e-13, 2.2477e-13, 1.5621e-13, 1.5351e-13, 1.3737e-13,\n",
       "             1.1688e-13, 1.1663e-13, 1.0170e-13, 8.5521e-14, 8.0864e-14, 7.6590e-14,\n",
       "             7.4068e-14, 6.6435e-14, 6.4392e-14, 6.4300e-14, 6.3013e-14, 5.9127e-14,\n",
       "             4.7936e-14, 4.5687e-14, 4.5628e-14, 3.3663e-14, 3.1501e-14, 3.1461e-14,\n",
       "             2.7327e-14, 2.4615e-14, 1.8808e-14, 1.7194e-14, 1.1568e-14, 9.5169e-15,\n",
       "             8.8799e-15, 7.6476e-15, 7.3699e-15, 5.3094e-15, 4.5154e-15, 3.5978e-15,\n",
       "             3.2794e-15, 3.2660e-15, 3.2520e-15, 2.4855e-15, 2.2471e-15, 2.1617e-15,\n",
       "             2.1085e-15, 1.7992e-15, 1.6370e-15, 1.5341e-15, 1.5259e-15, 1.5022e-15,\n",
       "             1.4278e-15, 1.2899e-15, 1.1741e-15, 1.1012e-15, 9.6166e-16, 9.1494e-16,\n",
       "             8.0678e-16, 7.6371e-16, 7.4407e-16, 7.1055e-16, 6.7682e-16, 6.6991e-16,\n",
       "             6.1494e-16, 5.4040e-16, 4.9397e-16, 4.6805e-16, 4.4193e-16, 4.2040e-16,\n",
       "             3.6531e-16, 3.5700e-16, 3.1523e-16, 2.5488e-16, 1.8776e-16, 1.4887e-16,\n",
       "             1.3867e-16, 1.3471e-16, 1.1244e-16, 1.1192e-16, 1.0975e-16, 7.1454e-17,\n",
       "             5.0113e-17, 4.5802e-17, 3.2301e-17, 2.4487e-17, 2.3047e-17, 2.2861e-17,\n",
       "             2.1451e-17, 2.1000e-17, 1.9085e-17, 1.6741e-17, 1.5166e-17, 1.2565e-17,\n",
       "             1.0812e-17, 1.0270e-17, 8.8755e-18, 8.7337e-18, 7.2036e-18, 6.5686e-18,\n",
       "             6.2160e-18, 4.9796e-18, 4.1678e-18, 3.6677e-18, 3.4506e-18, 3.0620e-18,\n",
       "             2.5250e-18, 2.0566e-18, 1.7197e-18, 1.4911e-18, 1.3299e-18, 1.2510e-18,\n",
       "             1.0682e-18, 7.8776e-19, 7.2503e-19, 5.7940e-19, 5.6138e-19, 4.8459e-19,\n",
       "             4.4389e-19, 3.9822e-19, 3.7798e-19, 3.0602e-19, 1.9839e-19, 1.4967e-19,\n",
       "             1.4371e-19, 1.2539e-19, 1.1682e-19, 9.8321e-20, 8.3317e-20, 7.9158e-20,\n",
       "             5.8753e-20, 5.8276e-20, 5.5230e-20, 4.8764e-20, 4.5777e-20, 4.3030e-20,\n",
       "             3.8545e-20, 2.5402e-20, 1.9432e-20, 1.6554e-20, 1.4311e-20, 1.3913e-20,\n",
       "             1.3850e-20, 1.3197e-20, 1.1911e-20, 1.0170e-20, 8.6419e-21, 8.5806e-21,\n",
       "             8.4341e-21, 7.1975e-21, 6.3388e-21, 6.2802e-21, 5.7889e-21, 4.1406e-21,\n",
       "             3.3721e-21, 2.9522e-21, 2.6827e-21, 2.6647e-21, 2.1641e-21, 2.1528e-21,\n",
       "             2.0909e-21, 1.2578e-21, 1.2267e-21, 5.6727e-22, 5.5338e-22, 5.0646e-22,\n",
       "             3.8317e-22, 3.3891e-22, 3.2927e-22, 2.8495e-22, 2.8087e-22, 2.0402e-22,\n",
       "             1.8532e-22, 1.8335e-22, 1.5399e-22, 1.1682e-22, 1.0363e-22, 9.4376e-23,\n",
       "             9.3248e-23, 9.1823e-23, 7.6959e-23, 7.5955e-23, 7.5010e-23, 7.2120e-23,\n",
       "             6.1935e-23, 5.5240e-23, 4.6456e-23, 3.5047e-23, 3.0272e-23, 2.5719e-23,\n",
       "             2.1925e-23, 2.1555e-23, 1.8772e-23, 1.8468e-23, 1.8154e-23, 1.3731e-23,\n",
       "             1.1417e-23, 9.1188e-24, 6.1712e-24, 5.6597e-24, 4.5028e-24, 3.3680e-24,\n",
       "             3.2565e-24, 2.9820e-24, 2.2410e-24, 2.2298e-24, 2.2127e-24, 2.2113e-24,\n",
       "             2.0735e-24, 1.9102e-24, 1.7295e-24, 1.3842e-24, 1.1521e-24, 1.1510e-24,\n",
       "             1.0345e-24, 5.7712e-25, 4.3777e-25, 3.7465e-25, 3.7054e-25, 3.0250e-25,\n",
       "             2.9895e-25, 2.7442e-25, 2.1426e-25, 1.9199e-25, 1.0314e-25, 9.0329e-26,\n",
       "             8.0368e-26, 7.3257e-26, 7.2553e-26, 3.7456e-26, 1.7787e-26, 1.5236e-26,\n",
       "             1.4798e-26, 1.1190e-26, 8.7289e-27, 7.5750e-27, 7.0486e-27, 6.5078e-27,\n",
       "             3.4715e-27, 2.6028e-27, 2.2335e-27, 1.6005e-27, 1.5422e-27, 1.2850e-27,\n",
       "             8.9796e-28, 6.1424e-28, 6.0146e-28, 5.7244e-28, 4.4299e-28, 3.9088e-28,\n",
       "             3.3724e-28, 2.6364e-28, 2.2428e-28, 1.3806e-28, 1.2795e-28, 1.0589e-28,\n",
       "             9.6081e-29, 8.1762e-29, 6.5473e-29, 4.5976e-29, 4.1008e-29, 2.3410e-29,\n",
       "             1.7468e-29, 1.7229e-30, 9.5838e-31, 6.0112e-31, 2.4205e-31, 1.2902e-31,\n",
       "             5.5845e-32, 4.4518e-32, 4.1609e-32, 4.0131e-32, 3.9992e-32, 3.8549e-32,\n",
       "             3.7986e-32, 3.2770e-32, 2.8656e-32, 2.5304e-32, 2.3522e-32, 1.8848e-32,\n",
       "             1.0734e-32, 5.4246e-33, 3.3495e-33, 2.2115e-33, 1.2658e-33, 9.9291e-34,\n",
       "             8.7631e-34, 7.2009e-34, 4.5042e-34, 3.0252e-34, 1.9508e-34, 2.5752e-35,\n",
       "             1.3152e-35, 1.0590e-35, 1.0225e-35, 2.9875e-36, 6.4115e-37, 3.2394e-37,\n",
       "             1.7648e-37, 1.4661e-37, 4.1005e-38, 2.5269e-38, 1.4912e-38, 8.5808e-39,\n",
       "             0.0000e+00])}},\n",
       "   {'fpr': np.float64(0.741304347826087),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.4913, 0.5130, 0.5152, 0.5283, 0.5370, 0.5413, 0.5435, 0.5478,\n",
       "             0.5500, 0.5522, 0.5543, 0.5565, 0.5609, 0.5630, 0.5652, 0.5674, 0.5696,\n",
       "             0.5717, 0.5739, 0.5761, 0.5783, 0.5804, 0.5848, 0.5870, 0.5891, 0.5913,\n",
       "             0.5935, 0.5957, 0.5978, 0.6000, 0.6022, 0.6043, 0.6065, 0.6087, 0.6109,\n",
       "             0.6130, 0.6152, 0.6174, 0.6196, 0.6217, 0.6239, 0.6261, 0.6283, 0.6304,\n",
       "             0.6326, 0.6348, 0.6370, 0.6391, 0.6413, 0.6435, 0.6457, 0.6478, 0.6500,\n",
       "             0.6522, 0.6543, 0.6565, 0.6587, 0.6609, 0.6630, 0.6652, 0.6674, 0.6696,\n",
       "             0.6717, 0.6739, 0.6761, 0.6783, 0.6804, 0.6826, 0.6848, 0.6870, 0.6891,\n",
       "             0.6913, 0.6935, 0.6957, 0.6978, 0.7000, 0.7022, 0.7043, 0.7065, 0.7087,\n",
       "             0.7109, 0.7130, 0.7152, 0.7174, 0.7196, 0.7217, 0.7239, 0.7261, 0.7283,\n",
       "             0.7304, 0.7326, 0.7348, 0.7370, 0.7391, 0.7413, 0.7435, 0.7457, 0.7478,\n",
       "             0.7500, 0.7522, 0.7543, 0.7565, 0.7587, 0.7609, 0.7630, 0.7652, 0.7674,\n",
       "             0.7696, 0.7717, 0.7739, 0.7761, 0.7783, 0.7804, 0.7826, 0.7848, 0.7870,\n",
       "             0.7891, 0.7913, 0.7935, 0.7957, 0.7978, 0.8000, 0.8022, 0.8043, 0.8065,\n",
       "             0.8087, 0.8109, 0.8130, 0.8152, 0.8174, 0.8196, 0.8217, 0.8239, 0.8261,\n",
       "             0.8283, 0.8304, 0.8326, 0.8348, 0.8370, 0.8391, 0.8413, 0.8435, 0.8457,\n",
       "             0.8478, 0.8500, 0.8522, 0.8543, 0.8565, 0.8587, 0.8609, 0.8630, 0.8652,\n",
       "             0.8674, 0.8696, 0.8717, 0.8739, 0.8761, 0.8783, 0.8804, 0.8826, 0.8848,\n",
       "             0.8870, 0.8891, 0.8913, 0.8935, 0.8957, 0.8978, 0.9000, 0.9022, 0.9043,\n",
       "             0.9065, 0.9087, 0.9109, 0.9130, 0.9152, 0.9174, 0.9196, 0.9217, 0.9239,\n",
       "             0.9261, 0.9283, 0.9304, 0.9326, 0.9348, 0.9370, 0.9391, 0.9413, 0.9435,\n",
       "             0.9457, 0.9478, 0.9500, 0.9522, 0.9543, 0.9565, 0.9587, 0.9609, 0.9630,\n",
       "             0.9652, 0.9674, 0.9696, 0.9717, 0.9739, 0.9761, 0.9783, 0.9804, 0.9826,\n",
       "             0.9848, 0.9870, 0.9891, 0.9913, 0.9935, 0.9957, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9990e-01, 9.9972e-01, 9.9970e-01, 9.9969e-01,\n",
       "             9.9967e-01, 9.9966e-01, 9.9962e-01, 9.9848e-01, 9.9830e-01, 9.9825e-01,\n",
       "             9.9818e-01, 9.9813e-01, 9.9751e-01, 9.9744e-01, 9.9731e-01, 9.9721e-01,\n",
       "             9.9676e-01, 9.9573e-01, 9.9403e-01, 9.9334e-01, 9.9272e-01, 9.9005e-01,\n",
       "             9.8938e-01, 9.8845e-01, 9.8223e-01, 9.8185e-01, 9.7987e-01, 9.7762e-01,\n",
       "             9.7687e-01, 9.6960e-01, 9.6785e-01, 9.5638e-01, 9.5627e-01, 9.5254e-01,\n",
       "             9.4358e-01, 9.3568e-01, 9.1761e-01, 9.0170e-01, 8.6460e-01, 7.9066e-01,\n",
       "             7.7111e-01, 7.4317e-01, 7.0993e-01, 6.8545e-01, 6.4655e-01, 5.3313e-01,\n",
       "             4.9485e-01, 4.8222e-01, 4.6707e-01, 4.5922e-01, 4.5643e-01, 3.3453e-01,\n",
       "             3.3427e-01, 3.1298e-01, 2.7611e-01, 2.3724e-01, 2.1706e-01, 1.9811e-01,\n",
       "             1.2602e-01, 1.1699e-01, 1.1214e-01, 1.0362e-01, 6.8904e-02, 5.5106e-02,\n",
       "             5.4338e-02, 5.1390e-02, 4.5274e-02, 4.2737e-02, 3.2944e-02, 2.6551e-02,\n",
       "             2.1392e-02, 2.1261e-02, 2.0305e-02, 2.0006e-02, 1.8941e-02, 1.7098e-02,\n",
       "             1.6092e-02, 1.5147e-02, 1.4307e-02, 1.3271e-02, 1.1203e-02, 8.0935e-03,\n",
       "             4.2304e-03, 2.4750e-03, 2.4682e-03, 1.9420e-03, 1.7348e-03, 1.5971e-03,\n",
       "             1.3355e-03, 1.2589e-03, 7.1565e-04, 5.8348e-04, 5.6587e-04, 3.5897e-04,\n",
       "             3.2527e-04, 3.0455e-04, 1.6859e-04, 9.8009e-05, 5.4496e-05, 5.3847e-05,\n",
       "             2.8966e-05, 2.3676e-05, 2.0742e-05, 2.0245e-05, 1.4141e-05, 9.9832e-06,\n",
       "             5.8400e-06, 3.1552e-06, 2.4203e-06, 2.3701e-06, 1.7734e-06, 1.3089e-06,\n",
       "             1.1524e-06, 9.2326e-07, 6.7427e-07, 6.5373e-07, 2.4115e-07, 2.1666e-07,\n",
       "             1.9616e-07, 7.1993e-08, 6.4879e-08, 6.0255e-08, 5.9829e-08, 5.7003e-08,\n",
       "             3.7840e-08, 3.0385e-08, 5.8572e-09, 2.7222e-09, 1.5618e-09, 1.1813e-09,\n",
       "             8.0011e-10, 4.1730e-10, 3.8188e-10, 2.3297e-10, 1.6039e-10, 1.4421e-10,\n",
       "             1.3563e-10, 7.5315e-11, 3.0579e-11, 1.5406e-11, 1.1719e-11, 7.3149e-12,\n",
       "             6.1317e-12, 1.4442e-12, 1.3214e-12, 9.1482e-13, 5.9728e-13, 2.9883e-13,\n",
       "             1.1634e-13, 9.4494e-14, 6.3572e-14, 1.9375e-14, 1.3513e-14, 1.0701e-14,\n",
       "             1.0250e-14, 8.9381e-15, 2.2873e-15, 7.7789e-16, 7.4754e-16, 6.0161e-16,\n",
       "             9.1610e-17, 2.8481e-17, 5.7145e-18, 1.2852e-20, 4.5006e-21])}}],\n",
       "  [{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.6864e-04, 1.5373e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 2.8950e-02, 2.3641e-02,  ..., 1.2428e-18, 6.5159e-19,\n",
       "             6.2564e-19])}},\n",
       "   {'fpr': np.float64(0.03286384976525822),\n",
       "    'tpr': np.float64(0.9023827824750192),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.6864e-04, 1.5373e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9987e-01, 9.9985e-01,  ..., 6.4365e-06, 4.9653e-06,\n",
       "             4.7541e-06])}},\n",
       "   {'fpr': np.float64(0.004694835680751174),\n",
       "    'tpr': np.float64(0.8562644119907763),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.6864e-04, 1.5373e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9982e-01,  ..., 2.9373e-07, 2.4074e-07,\n",
       "             1.8840e-07])}},\n",
       "   {'fpr': np.float64(0.014084507042253521),\n",
       "    'tpr': np.float64(0.9185242121445042),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.6864e-04, 1.5373e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9996e-01,  ..., 1.3191e-08, 1.1209e-08,\n",
       "             1.0585e-08])}},\n",
       "   {'fpr': np.float64(0.03051643192488263),\n",
       "    'tpr': np.float64(0.9638739431206764),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.6864e-04, 1.5373e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.0442e-08, 1.2057e-08,\n",
       "             9.0619e-09])}},\n",
       "   {'fpr': np.float64(0.03990610328638498),\n",
       "    'tpr': np.float64(0.9769408147578785),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.6864e-04, 1.5373e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.6213e-08, 1.3465e-08,\n",
       "             9.1398e-09])}},\n",
       "   {'fpr': np.float64(0.04460093896713615),\n",
       "    'tpr': np.float64(0.9754035357417371),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0023, 0.0061,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.9269e-11, 1.0828e-11,\n",
       "             7.5096e-12])}},\n",
       "   {'fpr': np.float64(0.11971830985915492),\n",
       "    'tpr': np.float64(0.9884704073789393),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0630, 0.1130,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.2218e-10, 1.0546e-10,\n",
       "             5.5588e-11])}},\n",
       "   {'fpr': np.float64(0.1056338028169014),\n",
       "    'tpr': np.float64(0.9877017678708686),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0184, 0.0384,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.9836e-10, 1.4566e-10,\n",
       "             9.6241e-11])}},\n",
       "   {'fpr': np.float64(0.03051643192488263),\n",
       "    'tpr': np.float64(0.9600307455803229),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0094, 0.0094, 0.0094,\n",
       "             0.0117, 0.0141, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0188, 0.0188,\n",
       "             0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0211, 0.0211, 0.0211,\n",
       "             0.0211, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235,\n",
       "             0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235,\n",
       "             0.0235, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258,\n",
       "             0.0258, 0.0258, 0.0258, 0.0282, 0.0282, 0.0305, 0.0305, 0.0305, 0.0305,\n",
       "             0.0305, 0.0329, 0.0352, 0.0376, 0.0376, 0.0376, 0.0399, 0.0399, 0.0399,\n",
       "             0.0399, 0.0399, 0.0423, 0.0446, 0.0446, 0.0446, 0.0446, 0.0446, 0.0446,\n",
       "             0.0446, 0.0446, 0.0446, 0.0446, 0.0446, 0.0446, 0.0469, 0.0493, 0.0493,\n",
       "             0.0493, 0.0516, 0.0516, 0.0540, 0.0540, 0.0540, 0.0540, 0.0563, 0.0563,\n",
       "             0.0587, 0.0587, 0.0610, 0.0634, 0.0634, 0.0657, 0.0657, 0.0657, 0.0681,\n",
       "             0.0681, 0.0681, 0.0681, 0.0704, 0.0728, 0.0751, 0.0775, 0.0798, 0.0822,\n",
       "             0.0845, 0.0869, 0.0869, 0.0869, 0.0892, 0.0915, 0.0915, 0.0939, 0.0962,\n",
       "             0.0962, 0.0986, 0.1009, 0.1033, 0.1056, 0.1080, 0.1103, 0.1127, 0.1150,\n",
       "             0.1174, 0.1197, 0.1221, 0.1244, 0.1268, 0.1268, 0.1291, 0.1291, 0.1291,\n",
       "             0.1315, 0.1338, 0.1338, 0.1362, 0.1385, 0.1408, 0.1408, 0.1432, 0.1432,\n",
       "             0.1455, 0.1479, 0.1502, 0.1526, 0.1549, 0.1573, 0.1596, 0.1620, 0.1643,\n",
       "             0.1667, 0.1667, 0.1690, 0.1714, 0.1737, 0.1761, 0.1761, 0.1784, 0.1808,\n",
       "             0.1831, 0.1854, 0.1878, 0.1901, 0.1925, 0.1948, 0.1948, 0.1972, 0.1972,\n",
       "             0.1995, 0.2019, 0.2042, 0.2066, 0.2089, 0.2113, 0.2136, 0.2160, 0.2183,\n",
       "             0.2207, 0.2230, 0.2254, 0.2277, 0.2277, 0.2300, 0.2324, 0.2347, 0.2371,\n",
       "             0.2394, 0.2418, 0.2441, 0.2465, 0.2488, 0.2512, 0.2535, 0.2559, 0.2582,\n",
       "             0.2606, 0.2629, 0.2653, 0.2676, 0.2700, 0.2723, 0.2746, 0.2770, 0.2793,\n",
       "             0.2817, 0.2840, 0.2864, 0.2887, 0.2911, 0.2934, 0.2958, 0.2981, 0.3005,\n",
       "             0.3028, 0.3052, 0.3075, 0.3099, 0.3122, 0.3146, 0.3169, 0.3192, 0.3216,\n",
       "             0.3239, 0.3263, 0.3286, 0.3310, 0.3333, 0.3357, 0.3380, 0.3404, 0.3427,\n",
       "             0.3451, 0.3474, 0.3498, 0.3521, 0.3545, 0.3568, 0.3592, 0.3615, 0.3638,\n",
       "             0.3662, 0.3685, 0.3709, 0.3732, 0.3756, 0.3779, 0.3803, 0.3826, 0.3850,\n",
       "             0.3873, 0.3897, 0.3920, 0.3944, 0.3967, 0.3991, 0.4014, 0.4038, 0.4061,\n",
       "             0.4085, 0.4108, 0.4131, 0.4155, 0.4178, 0.4202, 0.4225, 0.4249, 0.4272,\n",
       "             0.4296, 0.4319, 0.4343, 0.4366, 0.4390, 0.4413, 0.4437, 0.4460, 0.4484,\n",
       "             0.4507, 0.4531, 0.4554, 0.4577, 0.4601, 0.4624, 0.4648, 0.4671, 0.4695,\n",
       "             0.4718, 0.4742, 0.4765, 0.4789, 0.4812, 0.4836, 0.4859, 0.4883, 0.4906,\n",
       "             0.4930, 0.4953, 0.4977, 0.5000, 0.5023, 0.5047, 0.5070, 0.5094, 0.5117,\n",
       "             0.5141, 0.5164, 0.5188, 0.5211, 0.5235, 0.5258, 0.5282, 0.5305, 0.5329,\n",
       "             0.5352, 0.5376, 0.5399, 0.5423, 0.5446, 0.5469, 0.5493, 0.5516, 0.5540,\n",
       "             0.5563, 0.5587, 0.5610, 0.5634, 0.5657, 0.5681, 0.5704, 0.5728, 0.5751,\n",
       "             0.5775, 0.5798, 0.5822, 0.5845, 0.5869, 0.5892, 0.5915, 0.5939, 0.5962,\n",
       "             0.5986, 0.6009, 0.6033, 0.6056, 0.6080, 0.6103, 0.6127, 0.6150, 0.6174,\n",
       "             0.6197, 0.6221, 0.6244, 0.6268, 0.6291, 0.6315, 0.6338, 0.6362, 0.6385,\n",
       "             0.6408, 0.6432, 0.6455, 0.6479, 0.6502, 0.6526, 0.6549, 0.6573, 0.6596,\n",
       "             0.6620, 0.6643, 0.6667, 0.6690, 0.6714, 0.6737, 0.6761, 0.6761, 0.6784,\n",
       "             0.6808, 0.6831, 0.6854, 0.6878, 0.6901, 0.6925, 0.6948, 0.6972, 0.6995,\n",
       "             0.7019, 0.7042, 0.7066, 0.7089, 0.7113, 0.7136, 0.7160, 0.7183, 0.7207,\n",
       "             0.7230, 0.7254, 0.7277, 0.7300, 0.7324, 0.7347, 0.7371, 0.7394, 0.7418,\n",
       "             0.7441, 0.7465, 0.7488, 0.7512, 0.7512, 0.7535, 0.7559, 0.7582, 0.7606,\n",
       "             0.7629, 0.7653, 0.7676, 0.7700, 0.7723, 0.7746, 0.7770, 0.7793, 0.7817,\n",
       "             0.7840, 0.7864, 0.7887, 0.7911, 0.7934, 0.7958, 0.7981, 0.8005, 0.8028,\n",
       "             0.8052, 0.8075, 0.8099, 0.8122, 0.8146, 0.8169, 0.8192, 0.8216, 0.8239,\n",
       "             0.8263, 0.8286, 0.8310, 0.8333, 0.8357, 0.8380, 0.8404, 0.8427, 0.8451,\n",
       "             0.8474, 0.8498, 0.8521, 0.8545, 0.8568, 0.8592, 0.8615, 0.8638, 0.8662,\n",
       "             0.8685, 0.8709, 0.8732, 0.8756, 0.8779, 0.8803, 0.8826, 0.8850, 0.8873,\n",
       "             0.8897, 0.8920, 0.8944, 0.8967, 0.8991, 0.9014, 0.9038, 0.9061, 0.9085,\n",
       "             0.9108, 0.9131, 0.9155, 0.9178, 0.9202, 0.9225, 0.9249, 0.9272, 0.9296,\n",
       "             0.9319, 0.9343, 0.9366, 0.9390, 0.9413, 0.9437, 0.9460, 0.9484, 0.9507,\n",
       "             0.9531, 0.9554, 0.9577, 0.9601, 0.9624, 0.9648, 0.9671, 0.9695, 0.9718,\n",
       "             0.9742, 0.9765, 0.9789, 0.9812, 0.9836, 0.9859, 0.9883, 0.9906, 0.9930,\n",
       "             0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.5642, 0.6380, 0.6695, 0.6887, 0.7071, 0.7210, 0.7279, 0.7348,\n",
       "             0.7410, 0.7463, 0.7517, 0.7586, 0.7617, 0.7656, 0.7717, 0.7763, 0.7809,\n",
       "             0.7825, 0.7848, 0.7863, 0.7902, 0.7925, 0.7932, 0.7948, 0.7963, 0.7986,\n",
       "             0.8002, 0.8017, 0.8025, 0.8032, 0.8040, 0.8048, 0.8055, 0.8078, 0.8086,\n",
       "             0.8094, 0.8101, 0.8125, 0.8132, 0.8140, 0.8148, 0.8155, 0.8178, 0.8201,\n",
       "             0.8209, 0.8217, 0.8224, 0.8240, 0.8255, 0.8263, 0.8271, 0.8278, 0.8294,\n",
       "             0.8301, 0.8309, 0.8317, 0.8324, 0.8332, 0.8340, 0.8347, 0.8355, 0.8363,\n",
       "             0.8370, 0.8378, 0.8386, 0.8394, 0.8401, 0.8417, 0.8424, 0.8432, 0.8440,\n",
       "             0.8447, 0.8455, 0.8463, 0.8470, 0.8478, 0.8493, 0.8501, 0.8509, 0.8517,\n",
       "             0.8532, 0.8547, 0.8563, 0.8570, 0.8578, 0.8586, 0.8593, 0.8601, 0.8609,\n",
       "             0.8616, 0.8624, 0.8632, 0.8640, 0.8647, 0.8655, 0.8663, 0.8670, 0.8678,\n",
       "             0.8686, 0.8693, 0.8701, 0.8709, 0.8716, 0.8724, 0.8732, 0.8739, 0.8747,\n",
       "             0.8755, 0.8762, 0.8770, 0.8778, 0.8786, 0.8793, 0.8801, 0.8809, 0.8816,\n",
       "             0.8824, 0.8832, 0.8839, 0.8847, 0.8855, 0.8862, 0.8870, 0.8878, 0.8885,\n",
       "             0.8893, 0.8901, 0.8909, 0.8916, 0.8924, 0.8932, 0.8939, 0.8947, 0.8955,\n",
       "             0.8962, 0.8970, 0.8978, 0.8985, 0.8993, 0.9001, 0.9008, 0.9008, 0.9016,\n",
       "             0.9024, 0.9032, 0.9039, 0.9047, 0.9055, 0.9062, 0.9070, 0.9078, 0.9085,\n",
       "             0.9093, 0.9101, 0.9108, 0.9116, 0.9124, 0.9131, 0.9139, 0.9147, 0.9154,\n",
       "             0.9162, 0.9170, 0.9178, 0.9185, 0.9193, 0.9193, 0.9201, 0.9208, 0.9216,\n",
       "             0.9224, 0.9231, 0.9239, 0.9247, 0.9254, 0.9262, 0.9262, 0.9270, 0.9277,\n",
       "             0.9277, 0.9277, 0.9277, 0.9285, 0.9293, 0.9301, 0.9308, 0.9308, 0.9316,\n",
       "             0.9324, 0.9331, 0.9339, 0.9347, 0.9354, 0.9362, 0.9362, 0.9370, 0.9377,\n",
       "             0.9385, 0.9385, 0.9393, 0.9400, 0.9408, 0.9416, 0.9424, 0.9431, 0.9439,\n",
       "             0.9447, 0.9454, 0.9462, 0.9470, 0.9477, 0.9485, 0.9493, 0.9500, 0.9508,\n",
       "             0.9516, 0.9516, 0.9523, 0.9531, 0.9539, 0.9547, 0.9554, 0.9562, 0.9570,\n",
       "             0.9577, 0.9585, 0.9593, 0.9593, 0.9600, 0.9600, 0.9608, 0.9616, 0.9623,\n",
       "             0.9631, 0.9631, 0.9631, 0.9631, 0.9639, 0.9646, 0.9646, 0.9654, 0.9662,\n",
       "             0.9669, 0.9677, 0.9677, 0.9677, 0.9685, 0.9693, 0.9700, 0.9708, 0.9716,\n",
       "             0.9723, 0.9731, 0.9739, 0.9746, 0.9754, 0.9762, 0.9762, 0.9762, 0.9769,\n",
       "             0.9777, 0.9777, 0.9785, 0.9785, 0.9792, 0.9800, 0.9808, 0.9808, 0.9816,\n",
       "             0.9816, 0.9823, 0.9823, 0.9823, 0.9831, 0.9831, 0.9839, 0.9846, 0.9846,\n",
       "             0.9854, 0.9862, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869,\n",
       "             0.9869, 0.9869, 0.9877, 0.9885, 0.9885, 0.9885, 0.9892, 0.9892, 0.9892,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9908, 0.9908, 0.9915, 0.9923,\n",
       "             0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9939, 0.9939, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9969, 0.9969, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9972e-01, 9.9971e-01,\n",
       "             9.9968e-01, 9.9967e-01, 9.9967e-01, 9.9965e-01, 9.9965e-01, 9.9965e-01,\n",
       "             9.9963e-01, 9.9962e-01, 9.9951e-01, 9.9948e-01, 9.9948e-01, 9.9947e-01,\n",
       "             9.9942e-01, 9.9938e-01, 9.9937e-01, 9.9934e-01, 9.9915e-01, 9.9914e-01,\n",
       "             9.9907e-01, 9.9902e-01, 9.9901e-01, 9.9870e-01, 9.9869e-01, 9.9869e-01,\n",
       "             9.9868e-01, 9.9856e-01, 9.9847e-01, 9.9846e-01, 9.9823e-01, 9.9798e-01,\n",
       "             9.9750e-01, 9.9719e-01, 9.9709e-01, 9.9678e-01, 9.9677e-01, 9.9676e-01,\n",
       "             9.9613e-01, 9.9604e-01, 9.9555e-01, 9.9541e-01, 9.9516e-01, 9.9503e-01,\n",
       "             9.9388e-01, 9.9169e-01, 9.9093e-01, 9.8906e-01, 9.8177e-01, 9.7908e-01,\n",
       "             9.7871e-01, 9.7758e-01, 9.7736e-01, 9.7712e-01, 9.7635e-01, 9.7439e-01,\n",
       "             9.7391e-01, 9.7363e-01, 9.7327e-01, 9.6809e-01, 9.6794e-01, 9.6616e-01,\n",
       "             9.6331e-01, 9.6029e-01, 9.5993e-01, 9.5868e-01, 9.5766e-01, 9.5165e-01,\n",
       "             9.5110e-01, 9.4211e-01, 9.3344e-01, 9.3239e-01, 9.3070e-01, 9.2383e-01,\n",
       "             9.0118e-01, 8.8560e-01, 8.8543e-01, 8.5499e-01, 8.4927e-01, 8.2836e-01,\n",
       "             8.0957e-01, 7.9428e-01, 7.5380e-01, 7.0319e-01, 6.6974e-01, 6.6680e-01,\n",
       "             5.7403e-01, 5.6792e-01, 5.3703e-01, 4.9559e-01, 3.7613e-01, 3.6017e-01,\n",
       "             3.4877e-01, 3.4548e-01, 3.2276e-01, 3.1477e-01, 2.8704e-01, 2.6410e-01,\n",
       "             2.0789e-01, 2.0717e-01, 1.8710e-01, 1.6514e-01, 1.5981e-01, 1.5502e-01,\n",
       "             1.5015e-01, 1.4810e-01, 1.3895e-01, 1.2443e-01, 1.1388e-01, 1.1121e-01,\n",
       "             1.0672e-01, 1.0628e-01, 9.1161e-02, 8.2371e-02, 5.9431e-02, 4.7243e-02,\n",
       "             4.7206e-02, 3.5373e-02, 2.8721e-02, 2.2294e-02, 2.1574e-02, 2.1204e-02,\n",
       "             2.0665e-02, 1.6061e-02, 1.4732e-02, 1.4525e-02, 1.1912e-02, 9.9347e-03,\n",
       "             8.7798e-03, 8.1003e-03, 6.8859e-03, 6.2418e-03, 4.1398e-03, 2.8560e-03,\n",
       "             2.5869e-03, 1.5375e-03, 1.4248e-03, 1.3552e-03, 1.2090e-03, 9.0141e-04,\n",
       "             8.3065e-04, 5.9155e-04, 5.7788e-04, 5.4027e-04, 4.6878e-04, 3.5936e-04,\n",
       "             3.3341e-04, 3.0991e-04, 3.0952e-04, 2.2749e-04, 2.1601e-04, 1.8675e-04,\n",
       "             1.2189e-04, 1.0897e-04, 1.0770e-04, 8.3990e-05, 5.6223e-05, 5.5250e-05,\n",
       "             4.7919e-05, 4.5523e-05, 2.8542e-05, 2.6350e-05, 2.1947e-05, 2.0877e-05,\n",
       "             1.9323e-05, 1.8250e-05, 1.7389e-05, 1.4400e-05, 1.2596e-05, 1.0070e-05,\n",
       "             9.9167e-06, 7.7991e-06, 6.5140e-06, 6.3870e-06, 6.1520e-06, 5.7934e-06,\n",
       "             5.1726e-06, 4.9270e-06, 3.9629e-06, 3.9501e-06, 3.0635e-06, 2.8505e-06,\n",
       "             2.4597e-06, 2.3679e-06, 2.2429e-06, 2.2228e-06, 1.6200e-06, 1.5925e-06,\n",
       "             1.5828e-06, 1.3986e-06, 1.2081e-06, 8.8051e-07, 7.6647e-07, 7.3765e-07,\n",
       "             6.3293e-07, 5.8266e-07, 5.0877e-07, 4.3324e-07, 3.6757e-07, 3.4456e-07,\n",
       "             3.3255e-07, 2.7689e-07, 1.8133e-07, 1.2361e-07, 1.1725e-07, 8.7679e-08,\n",
       "             8.7638e-08, 8.5379e-08, 7.8052e-08, 6.3332e-08, 5.9180e-08, 5.2799e-08,\n",
       "             4.4106e-08, 3.1752e-08, 2.6990e-08, 2.1239e-08, 1.8114e-08, 1.6197e-08,\n",
       "             1.5889e-08, 1.5773e-08, 1.3806e-08, 1.2586e-08, 1.1705e-08, 1.0277e-08,\n",
       "             9.6643e-09, 9.3086e-09, 8.9929e-09, 8.4024e-09, 7.5289e-09, 6.1285e-09,\n",
       "             5.8473e-09, 5.1453e-09, 4.0036e-09, 2.7373e-09, 2.6060e-09, 2.2787e-09,\n",
       "             2.2189e-09, 1.9737e-09, 1.8341e-09, 1.7780e-09, 1.7633e-09, 1.2977e-09,\n",
       "             1.2930e-09, 1.2769e-09, 1.2060e-09, 1.1733e-09, 1.1601e-09, 9.1788e-10,\n",
       "             9.1176e-10, 8.9862e-10, 8.0801e-10, 7.0833e-10, 6.6443e-10, 6.5931e-10,\n",
       "             6.2131e-10, 5.8885e-10, 5.1355e-10, 4.9313e-10, 4.5134e-10, 3.9148e-10,\n",
       "             3.5063e-10, 3.2240e-10, 2.7326e-10, 2.1380e-10, 2.0563e-10, 1.8743e-10,\n",
       "             1.7953e-10, 1.7737e-10, 1.6067e-10, 1.5651e-10, 1.4311e-10, 1.3199e-10,\n",
       "             1.3113e-10, 1.2841e-10, 1.2150e-10, 1.1229e-10, 1.0246e-10, 9.5250e-11,\n",
       "             9.3346e-11, 9.2493e-11, 9.0750e-11, 8.2089e-11, 8.1912e-11, 6.3563e-11,\n",
       "             6.0394e-11, 5.8022e-11, 5.3488e-11, 5.2191e-11, 5.0309e-11, 4.7888e-11,\n",
       "             4.5054e-11, 3.4656e-11, 3.3804e-11, 2.9406e-11, 2.7252e-11, 2.6256e-11,\n",
       "             2.5777e-11, 2.4827e-11, 2.3261e-11, 2.1003e-11, 2.0146e-11, 1.9137e-11,\n",
       "             1.7495e-11, 1.5005e-11, 1.2291e-11, 1.1174e-11, 1.1137e-11, 1.0753e-11,\n",
       "             1.0613e-11, 1.0270e-11, 9.9635e-12, 9.1978e-12, 8.7166e-12, 7.4428e-12,\n",
       "             7.3320e-12, 6.9087e-12, 6.9052e-12, 6.4251e-12, 6.0582e-12, 5.5211e-12,\n",
       "             4.9721e-12, 4.5227e-12, 4.2628e-12, 4.2171e-12, 3.8455e-12, 3.2054e-12,\n",
       "             3.1745e-12, 3.1308e-12, 3.0062e-12, 2.7298e-12, 2.5870e-12, 2.0215e-12,\n",
       "             2.0166e-12, 1.7833e-12, 1.5726e-12, 1.5316e-12, 1.4487e-12, 1.3742e-12,\n",
       "             1.1982e-12, 1.1498e-12, 1.1347e-12, 1.1146e-12, 1.1104e-12, 1.0912e-12,\n",
       "             1.0441e-12, 9.6949e-13, 9.3478e-13, 8.9050e-13, 8.8259e-13, 8.7095e-13,\n",
       "             8.5199e-13, 8.4050e-13, 8.1696e-13, 7.7153e-13, 5.9268e-13, 5.8854e-13,\n",
       "             4.7248e-13, 4.6191e-13, 4.2020e-13, 3.4192e-13, 3.3131e-13, 3.2548e-13,\n",
       "             2.7688e-13, 2.6626e-13, 2.4227e-13, 2.0520e-13, 2.0428e-13, 1.9104e-13,\n",
       "             1.7638e-13, 1.6353e-13, 1.5938e-13, 1.3208e-13, 1.2792e-13, 1.2480e-13,\n",
       "             1.1980e-13, 1.1895e-13, 1.1301e-13, 1.1163e-13, 1.0535e-13, 1.0163e-13,\n",
       "             9.8934e-14, 9.7342e-14, 9.5680e-14, 9.3114e-14, 8.7790e-14, 6.9734e-14,\n",
       "             6.5931e-14, 5.9819e-14, 5.8474e-14, 5.1826e-14, 5.1622e-14, 4.5050e-14,\n",
       "             3.9114e-14, 2.9380e-14, 2.7257e-14, 2.7189e-14, 2.2872e-14, 1.8059e-14,\n",
       "             1.6299e-14, 1.5264e-14, 1.3177e-14, 1.2469e-14, 1.1316e-14, 9.9382e-15,\n",
       "             8.9887e-15, 7.6259e-15, 7.3426e-15, 6.9038e-15, 6.2074e-15, 6.0217e-15,\n",
       "             5.3469e-15, 4.6190e-15, 4.3209e-15, 4.2116e-15, 4.0734e-15, 3.7572e-15,\n",
       "             3.1994e-15, 3.1107e-15, 2.7834e-15, 2.7750e-15, 2.6655e-15, 2.6115e-15,\n",
       "             2.1345e-15, 2.0297e-15, 1.9478e-15, 1.9236e-15, 1.8718e-15, 1.7367e-15,\n",
       "             1.6956e-15, 1.6940e-15, 1.5990e-15, 1.5933e-15, 1.5279e-15, 1.3144e-15,\n",
       "             1.2883e-15, 1.0507e-15, 9.8831e-16, 9.5355e-16, 9.4590e-16, 9.2064e-16,\n",
       "             8.8539e-16, 8.1086e-16, 6.2470e-16, 5.8885e-16, 5.8807e-16, 5.8221e-16,\n",
       "             5.6605e-16, 5.2130e-16, 5.0997e-16, 4.9273e-16, 4.8751e-16, 4.8035e-16,\n",
       "             4.7539e-16, 4.7392e-16, 3.9797e-16, 3.9384e-16, 3.8159e-16, 3.0737e-16,\n",
       "             2.4783e-16, 2.4656e-16, 2.2816e-16, 2.2014e-16, 2.0927e-16, 2.0665e-16,\n",
       "             2.0471e-16, 1.9765e-16, 1.4081e-16, 1.1232e-16, 1.0625e-16, 8.9569e-17,\n",
       "             8.8298e-17, 8.7380e-17, 8.4677e-17, 7.4467e-17, 7.0561e-17, 5.9465e-17,\n",
       "             5.8462e-17, 4.6653e-17, 4.4086e-17, 4.3075e-17, 3.9460e-17, 3.9118e-17,\n",
       "             3.7939e-17, 3.1750e-17, 2.6200e-17, 2.4880e-17, 2.3804e-17, 1.5656e-17,\n",
       "             1.4511e-17, 1.1660e-17, 1.1441e-17, 1.1255e-17, 1.0285e-17, 8.8047e-18,\n",
       "             8.3825e-18, 7.9535e-18, 7.1630e-18, 5.5872e-18, 4.5104e-18, 4.2284e-18,\n",
       "             3.0931e-18, 2.6086e-18, 2.4892e-18, 2.2603e-18, 2.2119e-18, 2.1353e-18,\n",
       "             1.9639e-18, 1.7540e-18, 1.6358e-18, 1.0583e-18, 1.0170e-18, 9.6716e-19,\n",
       "             9.0356e-19, 8.7569e-19, 8.6988e-19, 8.2685e-19, 6.3287e-19, 5.9626e-19,\n",
       "             4.8295e-19, 4.3700e-19, 2.3124e-19, 2.0512e-19, 2.0152e-19, 1.9626e-19,\n",
       "             1.7350e-19, 1.1870e-19, 8.4440e-20, 6.2279e-20, 5.8151e-20, 5.0407e-20,\n",
       "             4.0239e-20, 2.2483e-20, 2.1748e-20, 1.6639e-20, 1.5988e-20, 2.0928e-21,\n",
       "             1.0759e-21, 9.0921e-22, 6.8139e-22, 6.2375e-22, 5.5903e-22, 5.2954e-22,\n",
       "             1.7232e-22, 3.2394e-23, 2.1116e-23, 7.4668e-24, 8.8601e-25, 7.2195e-25,\n",
       "             2.4889e-25, 1.2290e-25, 2.4765e-26, 2.3804e-27, 1.0578e-27, 7.9336e-28])}},\n",
       "   {'fpr': np.float64(0.04225352112676056),\n",
       "    'tpr': np.float64(0.9692544196771714),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0117, 0.0117, 0.0141,\n",
       "             0.0141, 0.0141, 0.0164, 0.0188, 0.0188, 0.0211, 0.0235, 0.0235, 0.0235,\n",
       "             0.0235, 0.0235, 0.0235, 0.0258, 0.0282, 0.0282, 0.0305, 0.0329, 0.0329,\n",
       "             0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0352, 0.0352, 0.0352,\n",
       "             0.0352, 0.0352, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376,\n",
       "             0.0376, 0.0399, 0.0399, 0.0423, 0.0423, 0.0423, 0.0423, 0.0423, 0.0423,\n",
       "             0.0423, 0.0446, 0.0446, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0493,\n",
       "             0.0516, 0.0516, 0.0540, 0.0563, 0.0587, 0.0610, 0.0610, 0.0610, 0.0634,\n",
       "             0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0681, 0.0704,\n",
       "             0.0704, 0.0728, 0.0751, 0.0775, 0.0798, 0.0798, 0.0798, 0.0822, 0.0845,\n",
       "             0.0845, 0.0845, 0.0845, 0.0869, 0.0892, 0.0915, 0.0939, 0.0962, 0.0986,\n",
       "             0.1009, 0.1009, 0.1033, 0.1056, 0.1080, 0.1103, 0.1127, 0.1127, 0.1127,\n",
       "             0.1150, 0.1150, 0.1174, 0.1197, 0.1221, 0.1244, 0.1268, 0.1268, 0.1268,\n",
       "             0.1291, 0.1315, 0.1338, 0.1362, 0.1385, 0.1408, 0.1432, 0.1455, 0.1479,\n",
       "             0.1502, 0.1526, 0.1549, 0.1573, 0.1596, 0.1620, 0.1643, 0.1667, 0.1690,\n",
       "             0.1714, 0.1737, 0.1761, 0.1784, 0.1808, 0.1831, 0.1854, 0.1878, 0.1901,\n",
       "             0.1925, 0.1948, 0.1972, 0.1972, 0.1995, 0.2019, 0.2042, 0.2066, 0.2066,\n",
       "             0.2089, 0.2113, 0.2113, 0.2136, 0.2160, 0.2183, 0.2183, 0.2207, 0.2230,\n",
       "             0.2254, 0.2277, 0.2300, 0.2300, 0.2324, 0.2347, 0.2371, 0.2394, 0.2418,\n",
       "             0.2441, 0.2465, 0.2488, 0.2488, 0.2512, 0.2535, 0.2559, 0.2582, 0.2606,\n",
       "             0.2629, 0.2653, 0.2653, 0.2676, 0.2700, 0.2700, 0.2723, 0.2746, 0.2770,\n",
       "             0.2793, 0.2817, 0.2840, 0.2864, 0.2887, 0.2911, 0.2934, 0.2958, 0.2981,\n",
       "             0.3005, 0.3028, 0.3052, 0.3075, 0.3099, 0.3122, 0.3146, 0.3169, 0.3192,\n",
       "             0.3216, 0.3239, 0.3263, 0.3286, 0.3310, 0.3333, 0.3357, 0.3380, 0.3404,\n",
       "             0.3427, 0.3451, 0.3474, 0.3498, 0.3521, 0.3545, 0.3568, 0.3592, 0.3615,\n",
       "             0.3638, 0.3662, 0.3685, 0.3709, 0.3732, 0.3756, 0.3779, 0.3803, 0.3826,\n",
       "             0.3850, 0.3873, 0.3897, 0.3920, 0.3944, 0.3967, 0.3991, 0.4014, 0.4038,\n",
       "             0.4061, 0.4085, 0.4108, 0.4131, 0.4155, 0.4178, 0.4202, 0.4225, 0.4249,\n",
       "             0.4272, 0.4296, 0.4319, 0.4343, 0.4366, 0.4390, 0.4413, 0.4437, 0.4460,\n",
       "             0.4484, 0.4507, 0.4531, 0.4554, 0.4577, 0.4601, 0.4624, 0.4648, 0.4671,\n",
       "             0.4695, 0.4718, 0.4742, 0.4765, 0.4789, 0.4812, 0.4836, 0.4859, 0.4883,\n",
       "             0.4906, 0.4930, 0.4953, 0.4977, 0.5000, 0.5023, 0.5047, 0.5070, 0.5094,\n",
       "             0.5117, 0.5141, 0.5164, 0.5188, 0.5211, 0.5235, 0.5258, 0.5282, 0.5305,\n",
       "             0.5329, 0.5352, 0.5376, 0.5399, 0.5423, 0.5446, 0.5469, 0.5493, 0.5516,\n",
       "             0.5540, 0.5563, 0.5587, 0.5610, 0.5634, 0.5657, 0.5681, 0.5704, 0.5728,\n",
       "             0.5751, 0.5775, 0.5798, 0.5798, 0.5822, 0.5845, 0.5869, 0.5892, 0.5915,\n",
       "             0.5939, 0.5962, 0.5986, 0.6009, 0.6033, 0.6056, 0.6080, 0.6080, 0.6103,\n",
       "             0.6127, 0.6150, 0.6174, 0.6197, 0.6221, 0.6244, 0.6268, 0.6291, 0.6315,\n",
       "             0.6338, 0.6362, 0.6385, 0.6408, 0.6432, 0.6455, 0.6479, 0.6502, 0.6526,\n",
       "             0.6549, 0.6573, 0.6596, 0.6620, 0.6643, 0.6667, 0.6690, 0.6714, 0.6737,\n",
       "             0.6761, 0.6784, 0.6808, 0.6831, 0.6854, 0.6878, 0.6901, 0.6925, 0.6948,\n",
       "             0.6972, 0.6995, 0.7019, 0.7042, 0.7066, 0.7089, 0.7113, 0.7136, 0.7160,\n",
       "             0.7183, 0.7207, 0.7230, 0.7254, 0.7277, 0.7300, 0.7324, 0.7347, 0.7371,\n",
       "             0.7394, 0.7418, 0.7441, 0.7465, 0.7488, 0.7512, 0.7535, 0.7559, 0.7582,\n",
       "             0.7606, 0.7629, 0.7653, 0.7676, 0.7700, 0.7723, 0.7746, 0.7770, 0.7793,\n",
       "             0.7817, 0.7840, 0.7864, 0.7887, 0.7911, 0.7934, 0.7958, 0.7981, 0.8005,\n",
       "             0.8028, 0.8052, 0.8075, 0.8099, 0.8122, 0.8146, 0.8169, 0.8192, 0.8216,\n",
       "             0.8239, 0.8263, 0.8286, 0.8310, 0.8333, 0.8357, 0.8380, 0.8404, 0.8427,\n",
       "             0.8451, 0.8474, 0.8498, 0.8521, 0.8545, 0.8568, 0.8592, 0.8615, 0.8638,\n",
       "             0.8662, 0.8685, 0.8709, 0.8732, 0.8756, 0.8779, 0.8803, 0.8826, 0.8850,\n",
       "             0.8873, 0.8897, 0.8920, 0.8944, 0.8967, 0.8991, 0.9014, 0.9038, 0.9061,\n",
       "             0.9085, 0.9108, 0.9131, 0.9155, 0.9178, 0.9202, 0.9225, 0.9249, 0.9272,\n",
       "             0.9296, 0.9319, 0.9343, 0.9366, 0.9390, 0.9413, 0.9437, 0.9460, 0.9484,\n",
       "             0.9507, 0.9531, 0.9554, 0.9577, 0.9601, 0.9624, 0.9648, 0.9671, 0.9695,\n",
       "             0.9718, 0.9742, 0.9765, 0.9789, 0.9812, 0.9836, 0.9859, 0.9883, 0.9906,\n",
       "             0.9930, 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.2052, 0.2905, 0.3359, 0.3597, 0.3851, 0.4028, 0.4251, 0.4389,\n",
       "             0.4527, 0.4681, 0.4758, 0.4819, 0.4935, 0.5019, 0.5058, 0.5111, 0.5127,\n",
       "             0.5188, 0.5234, 0.5327, 0.5373, 0.5442, 0.5496, 0.5519, 0.5527, 0.5542,\n",
       "             0.5573, 0.5603, 0.5634, 0.5673, 0.5696, 0.5719, 0.5749, 0.5765, 0.5780,\n",
       "             0.5842, 0.5880, 0.5903, 0.5926, 0.5957, 0.5988, 0.5995, 0.6026, 0.6042,\n",
       "             0.6080, 0.6111, 0.6141, 0.6157, 0.6172, 0.6195, 0.6211, 0.6234, 0.6241,\n",
       "             0.6257, 0.6272, 0.6303, 0.6311, 0.6318, 0.6341, 0.6349, 0.6357, 0.6380,\n",
       "             0.6387, 0.6403, 0.6418, 0.6426, 0.6449, 0.6457, 0.6472, 0.6487, 0.6503,\n",
       "             0.6526, 0.6556, 0.6572, 0.6580, 0.6587, 0.6618, 0.6649, 0.6656, 0.6664,\n",
       "             0.6679, 0.6687, 0.6695, 0.6703, 0.6718, 0.6733, 0.6741, 0.6756, 0.6764,\n",
       "             0.6795, 0.6826, 0.6833, 0.6841, 0.6849, 0.6856, 0.6879, 0.6895, 0.6918,\n",
       "             0.6925, 0.6941, 0.6949, 0.6964, 0.6972, 0.6979, 0.6995, 0.7018, 0.7025,\n",
       "             0.7033, 0.7048, 0.7056, 0.7064, 0.7071, 0.7079, 0.7087, 0.7095, 0.7102,\n",
       "             0.7110, 0.7118, 0.7125, 0.7133, 0.7141, 0.7156, 0.7164, 0.7171, 0.7179,\n",
       "             0.7187, 0.7194, 0.7210, 0.7218, 0.7225, 0.7233, 0.7241, 0.7248, 0.7256,\n",
       "             0.7264, 0.7271, 0.7279, 0.7287, 0.7294, 0.7302, 0.7310, 0.7317, 0.7341,\n",
       "             0.7348, 0.7364, 0.7379, 0.7394, 0.7402, 0.7410, 0.7425, 0.7433, 0.7440,\n",
       "             0.7448, 0.7456, 0.7463, 0.7471, 0.7479, 0.7487, 0.7494, 0.7502, 0.7510,\n",
       "             0.7517, 0.7525, 0.7533, 0.7540, 0.7548, 0.7556, 0.7563, 0.7571, 0.7579,\n",
       "             0.7586, 0.7594, 0.7602, 0.7610, 0.7617, 0.7625, 0.7633, 0.7640, 0.7656,\n",
       "             0.7663, 0.7671, 0.7679, 0.7686, 0.7694, 0.7702, 0.7717, 0.7725, 0.7733,\n",
       "             0.7740, 0.7748, 0.7756, 0.7763, 0.7771, 0.7779, 0.7786, 0.7794, 0.7802,\n",
       "             0.7809, 0.7817, 0.7825, 0.7832, 0.7840, 0.7848, 0.7855, 0.7863, 0.7871,\n",
       "             0.7879, 0.7886, 0.7894, 0.7902, 0.7909, 0.7909, 0.7917, 0.7925, 0.7932,\n",
       "             0.7940, 0.7948, 0.7955, 0.7963, 0.7971, 0.7986, 0.7994, 0.8002, 0.8009,\n",
       "             0.8017, 0.8032, 0.8040, 0.8048, 0.8055, 0.8063, 0.8071, 0.8078, 0.8078,\n",
       "             0.8086, 0.8094, 0.8101, 0.8109, 0.8117, 0.8132, 0.8140, 0.8148, 0.8155,\n",
       "             0.8163, 0.8171, 0.8186, 0.8194, 0.8201, 0.8209, 0.8217, 0.8224, 0.8232,\n",
       "             0.8240, 0.8248, 0.8255, 0.8263, 0.8271, 0.8278, 0.8286, 0.8294, 0.8301,\n",
       "             0.8309, 0.8317, 0.8332, 0.8340, 0.8347, 0.8355, 0.8363, 0.8370, 0.8378,\n",
       "             0.8386, 0.8394, 0.8401, 0.8409, 0.8417, 0.8424, 0.8432, 0.8440, 0.8447,\n",
       "             0.8455, 0.8463, 0.8470, 0.8478, 0.8486, 0.8493, 0.8501, 0.8509, 0.8517,\n",
       "             0.8524, 0.8532, 0.8540, 0.8547, 0.8555, 0.8563, 0.8570, 0.8578, 0.8586,\n",
       "             0.8593, 0.8601, 0.8609, 0.8616, 0.8624, 0.8632, 0.8640, 0.8647, 0.8655,\n",
       "             0.8663, 0.8670, 0.8678, 0.8686, 0.8693, 0.8701, 0.8709, 0.8716, 0.8724,\n",
       "             0.8732, 0.8739, 0.8747, 0.8755, 0.8762, 0.8770, 0.8778, 0.8786, 0.8793,\n",
       "             0.8801, 0.8809, 0.8816, 0.8824, 0.8832, 0.8839, 0.8847, 0.8855, 0.8862,\n",
       "             0.8870, 0.8878, 0.8885, 0.8893, 0.8901, 0.8909, 0.8916, 0.8924, 0.8932,\n",
       "             0.8939, 0.8947, 0.8955, 0.8962, 0.8970, 0.8978, 0.8985, 0.8993, 0.9001,\n",
       "             0.9008, 0.9016, 0.9024, 0.9032, 0.9039, 0.9047, 0.9055, 0.9062, 0.9070,\n",
       "             0.9078, 0.9085, 0.9093, 0.9101, 0.9108, 0.9116, 0.9124, 0.9131, 0.9139,\n",
       "             0.9147, 0.9154, 0.9162, 0.9170, 0.9178, 0.9185, 0.9193, 0.9201, 0.9208,\n",
       "             0.9216, 0.9224, 0.9231, 0.9239, 0.9247, 0.9254, 0.9262, 0.9270, 0.9277,\n",
       "             0.9285, 0.9293, 0.9301, 0.9308, 0.9316, 0.9324, 0.9331, 0.9339, 0.9347,\n",
       "             0.9354, 0.9362, 0.9370, 0.9370, 0.9377, 0.9385, 0.9393, 0.9400, 0.9408,\n",
       "             0.9416, 0.9424, 0.9431, 0.9439, 0.9447, 0.9454, 0.9454, 0.9462, 0.9462,\n",
       "             0.9470, 0.9477, 0.9477, 0.9477, 0.9485, 0.9485, 0.9485, 0.9493, 0.9500,\n",
       "             0.9508, 0.9516, 0.9523, 0.9523, 0.9523, 0.9531, 0.9531, 0.9531, 0.9539,\n",
       "             0.9547, 0.9554, 0.9562, 0.9570, 0.9577, 0.9585, 0.9585, 0.9593, 0.9600,\n",
       "             0.9608, 0.9616, 0.9616, 0.9623, 0.9631, 0.9639, 0.9646, 0.9654, 0.9662,\n",
       "             0.9669, 0.9669, 0.9677, 0.9677, 0.9685, 0.9693, 0.9700, 0.9708, 0.9716,\n",
       "             0.9723, 0.9723, 0.9731, 0.9731, 0.9739, 0.9746, 0.9754, 0.9762, 0.9762,\n",
       "             0.9762, 0.9769, 0.9769, 0.9769, 0.9769, 0.9769, 0.9777, 0.9785, 0.9785,\n",
       "             0.9785, 0.9792, 0.9800, 0.9808, 0.9816, 0.9823, 0.9831, 0.9831, 0.9831,\n",
       "             0.9839, 0.9839, 0.9839, 0.9839, 0.9839, 0.9846, 0.9854, 0.9854, 0.9854,\n",
       "             0.9862, 0.9869, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877,\n",
       "             0.9877, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9892, 0.9900,\n",
       "             0.9900, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9915, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9939,\n",
       "             0.9939, 0.9939, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01,\n",
       "             9.9968e-01, 9.9968e-01, 9.9967e-01, 9.9967e-01, 9.9966e-01, 9.9965e-01,\n",
       "             9.9963e-01, 9.9961e-01, 9.9961e-01, 9.9960e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9956e-01, 9.9956e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9955e-01,\n",
       "             9.9954e-01, 9.9954e-01, 9.9954e-01, 9.9954e-01, 9.9952e-01, 9.9947e-01,\n",
       "             9.9947e-01, 9.9947e-01, 9.9946e-01, 9.9946e-01, 9.9944e-01, 9.9943e-01,\n",
       "             9.9941e-01, 9.9939e-01, 9.9939e-01, 9.9937e-01, 9.9936e-01, 9.9936e-01,\n",
       "             9.9935e-01, 9.9934e-01, 9.9933e-01, 9.9932e-01, 9.9930e-01, 9.9929e-01,\n",
       "             9.9927e-01, 9.9926e-01, 9.9925e-01, 9.9916e-01, 9.9903e-01, 9.9902e-01,\n",
       "             9.9900e-01, 9.9899e-01, 9.9896e-01, 9.9895e-01, 9.9890e-01, 9.9889e-01,\n",
       "             9.9888e-01, 9.9886e-01, 9.9885e-01, 9.9873e-01, 9.9868e-01, 9.9861e-01,\n",
       "             9.9859e-01, 9.9851e-01, 9.9838e-01, 9.9835e-01, 9.9829e-01, 9.9821e-01,\n",
       "             9.9812e-01, 9.9809e-01, 9.9789e-01, 9.9781e-01, 9.9772e-01, 9.9771e-01,\n",
       "             9.9760e-01, 9.9737e-01, 9.9737e-01, 9.9721e-01, 9.9718e-01, 9.9707e-01,\n",
       "             9.9698e-01, 9.9694e-01, 9.9670e-01, 9.9647e-01, 9.9616e-01, 9.9596e-01,\n",
       "             9.9594e-01, 9.9585e-01, 9.9532e-01, 9.9527e-01, 9.9483e-01, 9.9478e-01,\n",
       "             9.9417e-01, 9.9124e-01, 9.9074e-01, 9.9070e-01, 9.9029e-01, 9.9025e-01,\n",
       "             9.9002e-01, 9.8931e-01, 9.8899e-01, 9.8679e-01, 9.8657e-01, 9.8613e-01,\n",
       "             9.8467e-01, 9.8338e-01, 9.7927e-01, 9.7867e-01, 9.7755e-01, 9.7560e-01,\n",
       "             9.7293e-01, 9.7259e-01, 9.7155e-01, 9.7140e-01, 9.7077e-01, 9.6651e-01,\n",
       "             9.6616e-01, 9.6435e-01, 9.6417e-01, 9.6324e-01, 9.6021e-01, 9.5691e-01,\n",
       "             9.5493e-01, 9.5357e-01, 9.5313e-01, 9.4824e-01, 9.4734e-01, 9.3742e-01,\n",
       "             9.3335e-01, 9.2862e-01, 9.2511e-01, 9.2300e-01, 9.1043e-01, 9.0968e-01,\n",
       "             8.9487e-01, 8.9213e-01, 8.7102e-01, 8.6113e-01, 8.5335e-01, 8.4386e-01,\n",
       "             8.2922e-01, 8.0365e-01, 7.9836e-01, 7.9749e-01, 7.7111e-01, 7.6229e-01,\n",
       "             7.1471e-01, 6.9643e-01, 6.7861e-01, 6.3495e-01, 6.1131e-01, 5.9401e-01,\n",
       "             5.8910e-01, 5.5788e-01, 5.4796e-01, 5.2944e-01, 5.2740e-01, 5.2672e-01,\n",
       "             4.9998e-01, 4.6471e-01, 4.2648e-01, 4.1598e-01, 3.8694e-01, 3.8120e-01,\n",
       "             2.7570e-01, 2.4495e-01, 2.3670e-01, 2.1526e-01, 2.1167e-01, 1.9459e-01,\n",
       "             1.9147e-01, 1.8731e-01, 1.7647e-01, 1.7604e-01, 1.7303e-01, 1.6700e-01,\n",
       "             1.2529e-01, 1.1454e-01, 1.1081e-01, 1.0691e-01, 1.0080e-01, 9.8177e-02,\n",
       "             7.3331e-02, 6.5368e-02, 6.4968e-02, 5.9016e-02, 5.7537e-02, 4.9080e-02,\n",
       "             4.8389e-02, 4.1047e-02, 4.0425e-02, 3.3081e-02, 3.2966e-02, 2.8897e-02,\n",
       "             2.8700e-02, 2.2305e-02, 2.2137e-02, 2.0770e-02, 1.5638e-02, 9.6844e-03,\n",
       "             8.9299e-03, 7.2945e-03, 6.1010e-03, 5.9036e-03, 5.7296e-03, 5.2161e-03,\n",
       "             4.8452e-03, 4.3515e-03, 2.7381e-03, 2.5592e-03, 2.5391e-03, 2.3240e-03,\n",
       "             2.1014e-03, 1.5949e-03, 1.5376e-03, 1.5228e-03, 1.4611e-03, 1.3951e-03,\n",
       "             1.1505e-03, 9.8909e-04, 9.6074e-04, 8.3349e-04, 8.2120e-04, 5.5053e-04,\n",
       "             4.3416e-04, 4.1295e-04, 4.1293e-04, 3.9243e-04, 3.5431e-04, 3.4353e-04,\n",
       "             3.1869e-04, 2.8930e-04, 2.7583e-04, 2.2822e-04, 2.2161e-04, 2.1807e-04,\n",
       "             1.8333e-04, 1.3840e-04, 1.2781e-04, 1.2085e-04, 1.1521e-04, 1.0284e-04,\n",
       "             1.0075e-04, 9.8576e-05, 8.3761e-05, 7.3608e-05, 7.0706e-05, 6.4429e-05,\n",
       "             6.3362e-05, 6.2932e-05, 5.9113e-05, 5.8983e-05, 5.4530e-05, 5.3217e-05,\n",
       "             4.8386e-05, 3.8480e-05, 3.8312e-05, 3.7595e-05, 3.5660e-05, 3.1799e-05,\n",
       "             3.1076e-05, 3.0965e-05, 3.0546e-05, 3.0236e-05, 2.9760e-05, 2.7454e-05,\n",
       "             2.6993e-05, 2.6565e-05, 2.2805e-05, 2.2636e-05, 1.8722e-05, 1.6697e-05,\n",
       "             1.4844e-05, 1.3509e-05, 1.1695e-05, 9.0040e-06, 8.6571e-06, 8.2807e-06,\n",
       "             7.1483e-06, 6.6015e-06, 6.4746e-06, 5.8685e-06, 5.7496e-06, 4.7480e-06,\n",
       "             4.7426e-06, 4.5289e-06, 3.4666e-06, 3.2161e-06, 3.0304e-06, 2.3939e-06,\n",
       "             2.1623e-06, 1.8457e-06, 1.8107e-06, 1.8003e-06, 1.7578e-06, 1.7179e-06,\n",
       "             1.6292e-06, 1.5863e-06, 1.5731e-06, 1.4230e-06, 1.3081e-06, 1.2307e-06,\n",
       "             1.2261e-06, 1.2163e-06, 1.2084e-06, 1.2009e-06, 1.1335e-06, 1.1066e-06,\n",
       "             9.6290e-07, 6.7910e-07, 6.3564e-07, 6.1938e-07, 5.8887e-07, 5.1997e-07,\n",
       "             4.8199e-07, 4.7627e-07, 4.5804e-07, 4.4977e-07, 4.3374e-07, 4.2264e-07,\n",
       "             3.2005e-07, 3.0321e-07, 2.8533e-07, 2.4711e-07, 2.3501e-07, 2.3035e-07,\n",
       "             2.2363e-07, 2.2052e-07, 2.1017e-07, 1.9546e-07, 1.9441e-07, 1.7931e-07,\n",
       "             1.4780e-07, 1.4525e-07, 1.4209e-07, 1.2395e-07, 1.1521e-07, 1.1463e-07,\n",
       "             9.7569e-08, 9.3367e-08, 8.6575e-08, 8.4071e-08, 8.0914e-08, 7.7031e-08,\n",
       "             7.6673e-08, 7.6205e-08, 7.3126e-08, 6.9011e-08, 6.4020e-08, 5.9707e-08,\n",
       "             5.1778e-08, 4.8254e-08, 4.4817e-08, 4.1148e-08, 3.9199e-08, 3.8151e-08,\n",
       "             3.6146e-08, 3.5459e-08, 3.0492e-08, 2.8752e-08, 2.7622e-08, 2.6660e-08,\n",
       "             2.6188e-08, 2.5967e-08, 2.5262e-08, 2.5254e-08, 2.4433e-08, 2.2592e-08,\n",
       "             2.2035e-08, 1.9961e-08, 1.9923e-08, 1.9912e-08, 1.9817e-08, 1.9634e-08,\n",
       "             1.9585e-08, 1.7928e-08, 1.6083e-08, 1.4421e-08, 1.3577e-08, 1.0070e-08,\n",
       "             9.9418e-09, 8.8792e-09, 8.8135e-09, 8.0749e-09, 8.0532e-09, 7.5364e-09,\n",
       "             7.3050e-09, 6.2056e-09, 4.9863e-09, 4.8561e-09, 4.8116e-09, 4.4028e-09,\n",
       "             4.2751e-09, 4.2749e-09, 4.1359e-09, 4.1141e-09, 3.9408e-09, 3.9065e-09,\n",
       "             3.8450e-09, 3.7387e-09, 3.7307e-09, 3.4416e-09, 3.1577e-09, 3.1322e-09,\n",
       "             3.0688e-09, 3.0165e-09, 2.8993e-09, 2.8794e-09, 2.7092e-09, 2.5486e-09,\n",
       "             2.3767e-09, 2.1694e-09, 2.1419e-09, 2.0306e-09, 1.7879e-09, 1.7604e-09,\n",
       "             1.6259e-09, 1.4304e-09, 1.3689e-09, 1.3361e-09, 1.3335e-09, 1.2913e-09,\n",
       "             1.1461e-09, 1.0296e-09, 9.5047e-10, 9.3546e-10, 9.0164e-10, 8.5226e-10,\n",
       "             7.9498e-10, 7.2918e-10, 7.0881e-10, 6.7903e-10, 6.5424e-10, 6.4058e-10,\n",
       "             6.3633e-10, 6.3196e-10, 6.2315e-10, 6.0620e-10, 5.8776e-10, 5.5939e-10,\n",
       "             5.1625e-10, 5.0589e-10, 4.9214e-10, 4.6250e-10, 4.4881e-10, 4.2136e-10,\n",
       "             3.6861e-10, 3.4373e-10, 3.2468e-10, 2.8445e-10, 2.8352e-10, 2.8341e-10,\n",
       "             2.8087e-10, 2.1418e-10, 2.1044e-10, 2.0985e-10, 2.0420e-10, 1.9900e-10,\n",
       "             1.9441e-10, 1.8620e-10, 1.7923e-10, 1.7847e-10, 1.6067e-10, 1.4610e-10,\n",
       "             1.3169e-10, 1.1809e-10, 1.1421e-10, 1.1047e-10, 1.0809e-10, 9.6366e-11,\n",
       "             9.5262e-11, 8.4224e-11, 7.5280e-11, 7.1352e-11, 6.9866e-11, 6.7176e-11,\n",
       "             5.3562e-11, 5.2347e-11, 5.2111e-11, 4.6215e-11, 4.4462e-11, 4.2253e-11,\n",
       "             3.9959e-11, 3.6915e-11, 3.2010e-11, 2.9157e-11, 2.8260e-11, 2.8237e-11,\n",
       "             2.4559e-11, 2.2842e-11, 2.0584e-11, 2.0175e-11, 1.9333e-11, 1.9212e-11,\n",
       "             1.9092e-11, 1.7248e-11, 1.4692e-11, 1.4299e-11, 1.3918e-11, 1.2766e-11,\n",
       "             1.2693e-11, 1.1937e-11, 1.1659e-11, 1.1058e-11, 1.0897e-11, 1.0600e-11,\n",
       "             1.0302e-11, 9.4089e-12, 8.0306e-12, 7.4368e-12, 6.8663e-12, 6.5248e-12,\n",
       "             6.4144e-12, 5.9214e-12, 5.8335e-12, 5.3517e-12, 4.7560e-12, 4.0312e-12,\n",
       "             3.9951e-12, 3.8709e-12, 3.7772e-12, 3.0790e-12, 2.9147e-12, 2.8389e-12,\n",
       "             2.7950e-12, 2.7644e-12, 2.0420e-12, 1.9588e-12, 1.6263e-12, 1.5775e-12,\n",
       "             1.4381e-12, 1.3928e-12, 1.3122e-12, 1.0942e-12, 1.0904e-12, 1.0381e-12,\n",
       "             8.1805e-13, 7.7230e-13, 7.6198e-13, 7.3968e-13, 6.6219e-13, 6.4062e-13,\n",
       "             6.1651e-13, 5.4533e-13, 5.1539e-13, 4.8793e-13, 4.4315e-13, 4.2528e-13,\n",
       "             4.1315e-13, 3.9477e-13, 3.6501e-13, 3.1564e-13, 1.8979e-13, 1.5884e-13,\n",
       "             1.5364e-13, 1.4861e-13, 1.2764e-13, 1.1869e-13, 1.0977e-13, 9.6752e-14,\n",
       "             8.1545e-14, 7.4093e-14, 6.7377e-14, 6.1377e-14, 4.9494e-14, 3.6554e-14,\n",
       "             3.1081e-14, 2.6877e-14, 2.2851e-14, 1.8193e-14, 1.7096e-14, 1.5308e-14,\n",
       "             1.2246e-14, 1.1347e-14, 1.0161e-14, 9.9351e-15, 8.7987e-15, 8.2025e-15,\n",
       "             6.6029e-15, 6.2713e-15, 5.0236e-15, 4.4084e-15, 3.8549e-15, 3.3678e-15,\n",
       "             1.2001e-15, 7.8712e-16, 2.9718e-16, 2.0299e-16, 1.8684e-16, 1.6944e-16,\n",
       "             1.3199e-16, 1.2313e-16, 1.0835e-16, 6.2811e-17, 1.7242e-17, 1.4082e-17,\n",
       "             1.3689e-17, 1.1743e-17, 9.8661e-18, 2.4980e-18, 2.7465e-19, 1.8628e-19,\n",
       "             1.6406e-19, 3.8359e-21, 2.2092e-21, 1.2263e-21])}},\n",
       "   {'fpr': np.float64(0.06338028169014084),\n",
       "    'tpr': np.float64(0.9815526518063028),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0117, 0.0117, 0.0141, 0.0141, 0.0141,\n",
       "             0.0141, 0.0141, 0.0141, 0.0141, 0.0164, 0.0164, 0.0164, 0.0164, 0.0188,\n",
       "             0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0211,\n",
       "             0.0211, 0.0211, 0.0211, 0.0235, 0.0235, 0.0258, 0.0282, 0.0282, 0.0282,\n",
       "             0.0282, 0.0282, 0.0282, 0.0282, 0.0305, 0.0305, 0.0305, 0.0305, 0.0305,\n",
       "             0.0329, 0.0329, 0.0352, 0.0376, 0.0376, 0.0399, 0.0423, 0.0423, 0.0423,\n",
       "             0.0423, 0.0423, 0.0423, 0.0446, 0.0446, 0.0469, 0.0469, 0.0469, 0.0493,\n",
       "             0.0493, 0.0493, 0.0493, 0.0516, 0.0516, 0.0516, 0.0516, 0.0516, 0.0540,\n",
       "             0.0540, 0.0540, 0.0540, 0.0540, 0.0563, 0.0587, 0.0587, 0.0587, 0.0587,\n",
       "             0.0587, 0.0587, 0.0587, 0.0587, 0.0610, 0.0634, 0.0657, 0.0657, 0.0681,\n",
       "             0.0704, 0.0728, 0.0728, 0.0751, 0.0775, 0.0775, 0.0798, 0.0798, 0.0798,\n",
       "             0.0822, 0.0845, 0.0845, 0.0869, 0.0892, 0.0915, 0.0939, 0.0939, 0.0962,\n",
       "             0.0986, 0.0986, 0.1009, 0.1033, 0.1056, 0.1080, 0.1103, 0.1127, 0.1150,\n",
       "             0.1174, 0.1197, 0.1221, 0.1244, 0.1268, 0.1291, 0.1315, 0.1338, 0.1362,\n",
       "             0.1385, 0.1385, 0.1385, 0.1408, 0.1432, 0.1455, 0.1455, 0.1479, 0.1502,\n",
       "             0.1526, 0.1549, 0.1573, 0.1596, 0.1596, 0.1620, 0.1643, 0.1667, 0.1667,\n",
       "             0.1690, 0.1714, 0.1737, 0.1761, 0.1784, 0.1808, 0.1831, 0.1854, 0.1878,\n",
       "             0.1901, 0.1925, 0.1948, 0.1948, 0.1972, 0.1995, 0.2019, 0.2019, 0.2042,\n",
       "             0.2066, 0.2089, 0.2089, 0.2089, 0.2113, 0.2136, 0.2160, 0.2183, 0.2207,\n",
       "             0.2230, 0.2254, 0.2277, 0.2300, 0.2324, 0.2324, 0.2347, 0.2371, 0.2394,\n",
       "             0.2418, 0.2441, 0.2465, 0.2488, 0.2512, 0.2535, 0.2535, 0.2559, 0.2559,\n",
       "             0.2582, 0.2606, 0.2629, 0.2653, 0.2676, 0.2700, 0.2723, 0.2746, 0.2770,\n",
       "             0.2770, 0.2793, 0.2817, 0.2840, 0.2864, 0.2887, 0.2911, 0.2934, 0.2958,\n",
       "             0.2958, 0.2981, 0.3005, 0.3028, 0.3052, 0.3075, 0.3099, 0.3122, 0.3146,\n",
       "             0.3169, 0.3192, 0.3216, 0.3239, 0.3263, 0.3286, 0.3310, 0.3333, 0.3357,\n",
       "             0.3380, 0.3404, 0.3427, 0.3451, 0.3474, 0.3498, 0.3521, 0.3545, 0.3568,\n",
       "             0.3592, 0.3615, 0.3638, 0.3662, 0.3685, 0.3709, 0.3732, 0.3756, 0.3779,\n",
       "             0.3803, 0.3826, 0.3850, 0.3873, 0.3897, 0.3920, 0.3944, 0.3967, 0.3991,\n",
       "             0.4014, 0.4038, 0.4061, 0.4085, 0.4108, 0.4131, 0.4155, 0.4178, 0.4202,\n",
       "             0.4225, 0.4249, 0.4272, 0.4296, 0.4319, 0.4343, 0.4366, 0.4390, 0.4413,\n",
       "             0.4437, 0.4460, 0.4484, 0.4507, 0.4531, 0.4554, 0.4577, 0.4601, 0.4624,\n",
       "             0.4648, 0.4671, 0.4695, 0.4718, 0.4742, 0.4765, 0.4789, 0.4812, 0.4836,\n",
       "             0.4859, 0.4883, 0.4906, 0.4930, 0.4953, 0.4977, 0.5000, 0.5023, 0.5047,\n",
       "             0.5070, 0.5094, 0.5117, 0.5141, 0.5164, 0.5188, 0.5211, 0.5235, 0.5258,\n",
       "             0.5282, 0.5305, 0.5329, 0.5352, 0.5376, 0.5399, 0.5423, 0.5446, 0.5469,\n",
       "             0.5493, 0.5516, 0.5540, 0.5563, 0.5587, 0.5610, 0.5634, 0.5657, 0.5681,\n",
       "             0.5704, 0.5728, 0.5751, 0.5775, 0.5798, 0.5822, 0.5845, 0.5869, 0.5892,\n",
       "             0.5915, 0.5939, 0.5962, 0.5986, 0.6009, 0.6033, 0.6056, 0.6080, 0.6103,\n",
       "             0.6127, 0.6150, 0.6174, 0.6197, 0.6221, 0.6244, 0.6268, 0.6291, 0.6315,\n",
       "             0.6338, 0.6362, 0.6385, 0.6408, 0.6432, 0.6455, 0.6479, 0.6502, 0.6526,\n",
       "             0.6549, 0.6573, 0.6596, 0.6620, 0.6643, 0.6667, 0.6690, 0.6714, 0.6737,\n",
       "             0.6761, 0.6784, 0.6808, 0.6831, 0.6854, 0.6878, 0.6901, 0.6925, 0.6948,\n",
       "             0.6972, 0.6995, 0.7019, 0.7019, 0.7042, 0.7066, 0.7089, 0.7113, 0.7136,\n",
       "             0.7160, 0.7183, 0.7207, 0.7230, 0.7254, 0.7277, 0.7300, 0.7324, 0.7347,\n",
       "             0.7371, 0.7394, 0.7418, 0.7441, 0.7465, 0.7488, 0.7512, 0.7535, 0.7559,\n",
       "             0.7582, 0.7606, 0.7629, 0.7653, 0.7676, 0.7700, 0.7723, 0.7746, 0.7770,\n",
       "             0.7793, 0.7817, 0.7840, 0.7864, 0.7887, 0.7911, 0.7934, 0.7958, 0.7981,\n",
       "             0.8005, 0.8028, 0.8052, 0.8075, 0.8099, 0.8122, 0.8146, 0.8169, 0.8192,\n",
       "             0.8216, 0.8239, 0.8263, 0.8286, 0.8310, 0.8333, 0.8357, 0.8380, 0.8404,\n",
       "             0.8427, 0.8451, 0.8474, 0.8498, 0.8521, 0.8545, 0.8568, 0.8592, 0.8615,\n",
       "             0.8638, 0.8662, 0.8685, 0.8709, 0.8732, 0.8756, 0.8779, 0.8803, 0.8826,\n",
       "             0.8850, 0.8850, 0.8873, 0.8897, 0.8920, 0.8944, 0.8967, 0.8991, 0.9014,\n",
       "             0.9038, 0.9061, 0.9085, 0.9108, 0.9131, 0.9155, 0.9178, 0.9202, 0.9225,\n",
       "             0.9249, 0.9272, 0.9296, 0.9319, 0.9343, 0.9366, 0.9390, 0.9413, 0.9437,\n",
       "             0.9460, 0.9484, 0.9507, 0.9531, 0.9554, 0.9577, 0.9601, 0.9624, 0.9648,\n",
       "             0.9671, 0.9695, 0.9718, 0.9742, 0.9765, 0.9789, 0.9812, 0.9836, 0.9859,\n",
       "             0.9883, 0.9906, 0.9930, 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.2798, 0.3636, 0.4058, 0.4435, 0.4596, 0.4758, 0.4912, 0.5012,\n",
       "             0.5142, 0.5242, 0.5327, 0.5404, 0.5480, 0.5511, 0.5580, 0.5619, 0.5642,\n",
       "             0.5673, 0.5680, 0.5688, 0.5711, 0.5742, 0.5819, 0.5826, 0.5872, 0.5911,\n",
       "             0.5972, 0.6018, 0.6026, 0.6034, 0.6088, 0.6095, 0.6134, 0.6164, 0.6203,\n",
       "             0.6218, 0.6234, 0.6249, 0.6272, 0.6311, 0.6341, 0.6349, 0.6364, 0.6387,\n",
       "             0.6395, 0.6410, 0.6434, 0.6464, 0.6495, 0.6518, 0.6533, 0.6541, 0.6549,\n",
       "             0.6572, 0.6587, 0.6603, 0.6610, 0.6618, 0.6626, 0.6633, 0.6649, 0.6656,\n",
       "             0.6672, 0.6679, 0.6695, 0.6703, 0.6710, 0.6726, 0.6733, 0.6756, 0.6772,\n",
       "             0.6779, 0.6787, 0.6795, 0.6802, 0.6810, 0.6818, 0.6826, 0.6841, 0.6856,\n",
       "             0.6864, 0.6872, 0.6879, 0.6895, 0.6902, 0.6910, 0.6918, 0.6925, 0.6941,\n",
       "             0.6956, 0.6964, 0.6972, 0.6987, 0.6995, 0.7002, 0.7010, 0.7025, 0.7033,\n",
       "             0.7041, 0.7064, 0.7071, 0.7079, 0.7087, 0.7095, 0.7102, 0.7110, 0.7118,\n",
       "             0.7125, 0.7141, 0.7148, 0.7156, 0.7164, 0.7171, 0.7179, 0.7187, 0.7194,\n",
       "             0.7202, 0.7210, 0.7218, 0.7225, 0.7233, 0.7241, 0.7248, 0.7256, 0.7271,\n",
       "             0.7287, 0.7294, 0.7302, 0.7310, 0.7317, 0.7325, 0.7333, 0.7341, 0.7364,\n",
       "             0.7371, 0.7387, 0.7402, 0.7410, 0.7417, 0.7440, 0.7448, 0.7463, 0.7471,\n",
       "             0.7479, 0.7487, 0.7494, 0.7510, 0.7517, 0.7525, 0.7533, 0.7540, 0.7548,\n",
       "             0.7556, 0.7563, 0.7571, 0.7579, 0.7594, 0.7602, 0.7610, 0.7617, 0.7625,\n",
       "             0.7633, 0.7640, 0.7648, 0.7671, 0.7679, 0.7686, 0.7702, 0.7709, 0.7717,\n",
       "             0.7725, 0.7733, 0.7740, 0.7748, 0.7756, 0.7763, 0.7771, 0.7771, 0.7779,\n",
       "             0.7786, 0.7794, 0.7802, 0.7809, 0.7817, 0.7825, 0.7832, 0.7840, 0.7848,\n",
       "             0.7855, 0.7871, 0.7879, 0.7886, 0.7894, 0.7909, 0.7917, 0.7925, 0.7932,\n",
       "             0.7940, 0.7948, 0.7955, 0.7963, 0.7971, 0.7978, 0.7986, 0.7994, 0.8002,\n",
       "             0.8009, 0.8017, 0.8025, 0.8040, 0.8048, 0.8055, 0.8063, 0.8071, 0.8078,\n",
       "             0.8086, 0.8094, 0.8101, 0.8109, 0.8117, 0.8125, 0.8132, 0.8140, 0.8148,\n",
       "             0.8155, 0.8163, 0.8171, 0.8178, 0.8186, 0.8194, 0.8201, 0.8209, 0.8217,\n",
       "             0.8224, 0.8240, 0.8248, 0.8255, 0.8263, 0.8271, 0.8278, 0.8286, 0.8294,\n",
       "             0.8301, 0.8309, 0.8317, 0.8324, 0.8332, 0.8340, 0.8347, 0.8355, 0.8370,\n",
       "             0.8378, 0.8386, 0.8394, 0.8401, 0.8409, 0.8417, 0.8424, 0.8432, 0.8440,\n",
       "             0.8447, 0.8455, 0.8463, 0.8470, 0.8478, 0.8486, 0.8493, 0.8501, 0.8509,\n",
       "             0.8517, 0.8524, 0.8532, 0.8540, 0.8547, 0.8555, 0.8555, 0.8563, 0.8570,\n",
       "             0.8578, 0.8586, 0.8593, 0.8601, 0.8609, 0.8616, 0.8624, 0.8632, 0.8640,\n",
       "             0.8647, 0.8655, 0.8663, 0.8670, 0.8678, 0.8686, 0.8693, 0.8701, 0.8709,\n",
       "             0.8716, 0.8724, 0.8732, 0.8739, 0.8747, 0.8755, 0.8762, 0.8770, 0.8778,\n",
       "             0.8786, 0.8793, 0.8801, 0.8809, 0.8816, 0.8824, 0.8832, 0.8839, 0.8847,\n",
       "             0.8855, 0.8862, 0.8870, 0.8878, 0.8885, 0.8893, 0.8901, 0.8909, 0.8916,\n",
       "             0.8924, 0.8932, 0.8939, 0.8947, 0.8955, 0.8962, 0.8962, 0.8970, 0.8978,\n",
       "             0.8985, 0.8993, 0.9001, 0.9008, 0.9016, 0.9024, 0.9032, 0.9039, 0.9047,\n",
       "             0.9055, 0.9062, 0.9070, 0.9078, 0.9085, 0.9093, 0.9101, 0.9108, 0.9116,\n",
       "             0.9124, 0.9131, 0.9139, 0.9147, 0.9154, 0.9162, 0.9170, 0.9178, 0.9185,\n",
       "             0.9193, 0.9201, 0.9208, 0.9216, 0.9224, 0.9231, 0.9239, 0.9247, 0.9254,\n",
       "             0.9262, 0.9270, 0.9277, 0.9285, 0.9293, 0.9301, 0.9308, 0.9316, 0.9324,\n",
       "             0.9331, 0.9339, 0.9347, 0.9354, 0.9354, 0.9362, 0.9362, 0.9370, 0.9377,\n",
       "             0.9385, 0.9393, 0.9400, 0.9408, 0.9408, 0.9416, 0.9424, 0.9431, 0.9431,\n",
       "             0.9439, 0.9447, 0.9454, 0.9462, 0.9470, 0.9477, 0.9485, 0.9493, 0.9493,\n",
       "             0.9500, 0.9508, 0.9516, 0.9516, 0.9523, 0.9523, 0.9523, 0.9531, 0.9539,\n",
       "             0.9547, 0.9554, 0.9562, 0.9570, 0.9570, 0.9577, 0.9585, 0.9593, 0.9600,\n",
       "             0.9600, 0.9608, 0.9608, 0.9608, 0.9616, 0.9616, 0.9616, 0.9623, 0.9631,\n",
       "             0.9639, 0.9646, 0.9654, 0.9654, 0.9662, 0.9662, 0.9669, 0.9677, 0.9677,\n",
       "             0.9685, 0.9693, 0.9700, 0.9700, 0.9708, 0.9716, 0.9723, 0.9731, 0.9731,\n",
       "             0.9739, 0.9746, 0.9754, 0.9762, 0.9762, 0.9762, 0.9769, 0.9777, 0.9785,\n",
       "             0.9792, 0.9800, 0.9808, 0.9816, 0.9816, 0.9816, 0.9816, 0.9823, 0.9823,\n",
       "             0.9823, 0.9823, 0.9831, 0.9831, 0.9831, 0.9839, 0.9839, 0.9846, 0.9854,\n",
       "             0.9854, 0.9854, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9869, 0.9869,\n",
       "             0.9869, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877,\n",
       "             0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877,\n",
       "             0.9877, 0.9885, 0.9892, 0.9892, 0.9892, 0.9892, 0.9900, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9908, 0.9908, 0.9908, 0.9908, 0.9915,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9915, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9939, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9962, 0.9962, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9970e-01, 9.9970e-01,\n",
       "             9.9970e-01, 9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9969e-01,\n",
       "             9.9967e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9964e-01, 9.9964e-01, 9.9963e-01, 9.9962e-01, 9.9960e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9957e-01, 9.9957e-01, 9.9955e-01, 9.9953e-01,\n",
       "             9.9953e-01, 9.9951e-01, 9.9949e-01, 9.9947e-01, 9.9947e-01, 9.9943e-01,\n",
       "             9.9943e-01, 9.9942e-01, 9.9942e-01, 9.9942e-01, 9.9941e-01, 9.9939e-01,\n",
       "             9.9939e-01, 9.9939e-01, 9.9937e-01, 9.9936e-01, 9.9930e-01, 9.9929e-01,\n",
       "             9.9928e-01, 9.9924e-01, 9.9923e-01, 9.9919e-01, 9.9919e-01, 9.9914e-01,\n",
       "             9.9912e-01, 9.9910e-01, 9.9902e-01, 9.9897e-01, 9.9895e-01, 9.9885e-01,\n",
       "             9.9884e-01, 9.9878e-01, 9.9876e-01, 9.9873e-01, 9.9863e-01, 9.9852e-01,\n",
       "             9.9849e-01, 9.9848e-01, 9.9841e-01, 9.9836e-01, 9.9824e-01, 9.9817e-01,\n",
       "             9.9817e-01, 9.9811e-01, 9.9797e-01, 9.9795e-01, 9.9790e-01, 9.9789e-01,\n",
       "             9.9783e-01, 9.9778e-01, 9.9775e-01, 9.9774e-01, 9.9771e-01, 9.9739e-01,\n",
       "             9.9733e-01, 9.9727e-01, 9.9715e-01, 9.9691e-01, 9.9651e-01, 9.9651e-01,\n",
       "             9.9633e-01, 9.9626e-01, 9.9593e-01, 9.9555e-01, 9.9516e-01, 9.9499e-01,\n",
       "             9.9456e-01, 9.9426e-01, 9.9417e-01, 9.9365e-01, 9.9337e-01, 9.9290e-01,\n",
       "             9.9240e-01, 9.9199e-01, 9.9142e-01, 9.9038e-01, 9.9028e-01, 9.8963e-01,\n",
       "             9.8785e-01, 9.8785e-01, 9.8780e-01, 9.8749e-01, 9.8585e-01, 9.8448e-01,\n",
       "             9.8421e-01, 9.8349e-01, 9.8289e-01, 9.8241e-01, 9.8151e-01, 9.8092e-01,\n",
       "             9.7937e-01, 9.7898e-01, 9.7777e-01, 9.7612e-01, 9.7600e-01, 9.7478e-01,\n",
       "             9.7378e-01, 9.7372e-01, 9.7248e-01, 9.7005e-01, 9.6906e-01, 9.6852e-01,\n",
       "             9.5611e-01, 9.5363e-01, 9.4636e-01, 9.4563e-01, 9.4508e-01, 9.4339e-01,\n",
       "             9.4321e-01, 9.3013e-01, 9.2986e-01, 9.2885e-01, 9.2240e-01, 9.2221e-01,\n",
       "             9.1125e-01, 9.1036e-01, 9.0616e-01, 9.0329e-01, 9.0307e-01, 8.9406e-01,\n",
       "             8.9325e-01, 8.9312e-01, 8.7486e-01, 8.6606e-01, 8.6416e-01, 8.5205e-01,\n",
       "             8.4591e-01, 8.4429e-01, 8.2319e-01, 8.0874e-01, 7.3609e-01, 6.5328e-01,\n",
       "             6.3771e-01, 6.3104e-01, 6.1731e-01, 6.1539e-01, 6.1505e-01, 5.6075e-01,\n",
       "             5.3933e-01, 5.3241e-01, 5.2830e-01, 4.3341e-01, 4.1026e-01, 3.8940e-01,\n",
       "             3.4718e-01, 3.4181e-01, 3.2601e-01, 3.2514e-01, 2.5155e-01, 2.3993e-01,\n",
       "             2.3917e-01, 2.3509e-01, 2.0013e-01, 1.9994e-01, 1.9121e-01, 1.5490e-01,\n",
       "             1.5255e-01, 1.3501e-01, 1.3453e-01, 1.3411e-01, 1.2470e-01, 1.0440e-01,\n",
       "             1.0374e-01, 1.0157e-01, 1.0157e-01, 8.3511e-02, 6.4075e-02, 5.0122e-02,\n",
       "             5.0080e-02, 4.0069e-02, 3.2817e-02, 3.1704e-02, 3.1460e-02, 3.0286e-02,\n",
       "             2.6820e-02, 2.2533e-02, 2.2076e-02, 1.4671e-02, 1.2137e-02, 1.1918e-02,\n",
       "             1.1607e-02, 1.1275e-02, 1.0887e-02, 1.0815e-02, 9.9111e-03, 8.6135e-03,\n",
       "             8.3029e-03, 8.2768e-03, 8.1502e-03, 6.9631e-03, 6.4463e-03, 6.1227e-03,\n",
       "             5.0998e-03, 4.8729e-03, 4.7896e-03, 4.6686e-03, 4.2609e-03, 3.8633e-03,\n",
       "             3.0533e-03, 2.9772e-03, 2.7361e-03, 2.3792e-03, 2.3245e-03, 1.9769e-03,\n",
       "             1.8023e-03, 1.8018e-03, 1.6750e-03, 1.3795e-03, 1.2805e-03, 1.2622e-03,\n",
       "             1.2271e-03, 1.2022e-03, 1.0907e-03, 1.0170e-03, 8.8908e-04, 7.7987e-04,\n",
       "             7.4363e-04, 7.2044e-04, 7.0611e-04, 6.7290e-04, 6.6097e-04, 6.3894e-04,\n",
       "             6.3102e-04, 5.1237e-04, 5.0534e-04, 5.0465e-04, 4.7498e-04, 4.6938e-04,\n",
       "             4.5006e-04, 4.2282e-04, 4.0832e-04, 3.8300e-04, 3.5259e-04, 3.4828e-04,\n",
       "             3.4140e-04, 3.3360e-04, 2.7642e-04, 2.5407e-04, 2.4362e-04, 2.3584e-04,\n",
       "             2.3546e-04, 2.3493e-04, 1.9856e-04, 1.9236e-04, 1.8371e-04, 1.6317e-04,\n",
       "             1.5600e-04, 1.4367e-04, 1.3725e-04, 1.3509e-04, 1.3271e-04, 1.3077e-04,\n",
       "             1.2785e-04, 1.2363e-04, 1.2191e-04, 1.1756e-04, 7.4788e-05, 7.0270e-05,\n",
       "             5.9701e-05, 5.8963e-05, 5.8864e-05, 5.5212e-05, 4.9993e-05, 4.9276e-05,\n",
       "             4.6918e-05, 4.5451e-05, 3.7971e-05, 2.9282e-05, 1.9569e-05, 1.8698e-05,\n",
       "             1.8274e-05, 1.7624e-05, 1.4509e-05, 1.3433e-05, 1.2390e-05, 1.2276e-05,\n",
       "             1.2223e-05, 1.1348e-05, 1.1320e-05, 1.1085e-05, 1.0483e-05, 9.9183e-06,\n",
       "             9.6682e-06, 8.7422e-06, 7.8468e-06, 7.4340e-06, 7.3526e-06, 6.9519e-06,\n",
       "             6.5741e-06, 6.4198e-06, 6.0358e-06, 5.6550e-06, 5.4420e-06, 5.4346e-06,\n",
       "             5.2962e-06, 5.0823e-06, 4.4539e-06, 4.3668e-06, 4.2456e-06, 3.4555e-06,\n",
       "             3.2961e-06, 3.2615e-06, 3.1887e-06, 3.1396e-06, 3.1122e-06, 2.9063e-06,\n",
       "             2.6886e-06, 2.6716e-06, 2.4872e-06, 2.4409e-06, 2.3714e-06, 2.3049e-06,\n",
       "             2.2809e-06, 2.1706e-06, 2.0978e-06, 2.0155e-06, 1.9041e-06, 1.8066e-06,\n",
       "             1.5832e-06, 1.4781e-06, 1.3324e-06, 1.2989e-06, 1.2603e-06, 1.2300e-06,\n",
       "             1.2185e-06, 1.1982e-06, 1.1601e-06, 1.1493e-06, 1.1196e-06, 1.0352e-06,\n",
       "             1.0286e-06, 1.0075e-06, 9.8726e-07, 9.5965e-07, 9.2269e-07, 9.1721e-07,\n",
       "             8.4767e-07, 8.3131e-07, 8.0708e-07, 7.9789e-07, 7.2565e-07, 7.2103e-07,\n",
       "             6.8564e-07, 6.6531e-07, 6.4594e-07, 6.4120e-07, 6.2106e-07, 5.9309e-07,\n",
       "             5.7525e-07, 4.4758e-07, 4.3489e-07, 4.2477e-07, 4.0794e-07, 3.6648e-07,\n",
       "             3.1905e-07, 2.9207e-07, 2.6651e-07, 2.6143e-07, 2.5765e-07, 2.5688e-07,\n",
       "             2.5512e-07, 2.3967e-07, 2.2963e-07, 2.2024e-07, 2.2018e-07, 2.1429e-07,\n",
       "             2.1101e-07, 2.0867e-07, 2.0034e-07, 1.8150e-07, 1.7258e-07, 1.6886e-07,\n",
       "             1.6560e-07, 1.6092e-07, 1.5802e-07, 1.4191e-07, 1.4100e-07, 1.2104e-07,\n",
       "             1.2003e-07, 1.1747e-07, 1.1327e-07, 1.0944e-07, 1.0552e-07, 1.0022e-07,\n",
       "             9.7014e-08, 9.6138e-08, 9.5252e-08, 9.1824e-08, 8.3551e-08, 8.3270e-08,\n",
       "             8.0003e-08, 7.1454e-08, 6.2509e-08, 6.2206e-08, 5.9735e-08, 5.4474e-08,\n",
       "             5.1806e-08, 5.1025e-08, 4.8851e-08, 4.8102e-08, 4.5071e-08, 4.3297e-08,\n",
       "             4.1127e-08, 3.2761e-08, 2.7085e-08, 2.6682e-08, 2.3707e-08, 2.3560e-08,\n",
       "             2.1778e-08, 2.0611e-08, 2.0008e-08, 1.9693e-08, 1.5557e-08, 1.4763e-08,\n",
       "             1.4628e-08, 1.3896e-08, 1.2739e-08, 1.2406e-08, 1.0979e-08, 8.9554e-09,\n",
       "             8.6633e-09, 8.2592e-09, 7.8117e-09, 7.7810e-09, 6.9564e-09, 6.8762e-09,\n",
       "             5.6570e-09, 5.6329e-09, 5.3692e-09, 4.9244e-09, 4.7806e-09, 4.7123e-09,\n",
       "             4.4305e-09, 3.8152e-09, 3.6244e-09, 3.6193e-09, 3.4273e-09, 3.3668e-09,\n",
       "             3.0015e-09, 2.9601e-09, 2.8040e-09, 2.7887e-09, 2.7103e-09, 2.6827e-09,\n",
       "             2.6359e-09, 2.5633e-09, 2.4592e-09, 2.4573e-09, 2.4373e-09, 2.3014e-09,\n",
       "             2.2939e-09, 2.1588e-09, 2.1214e-09, 2.0194e-09, 1.9759e-09, 1.9520e-09,\n",
       "             1.9515e-09, 1.8925e-09, 1.7989e-09, 1.6862e-09, 1.4735e-09, 1.4213e-09,\n",
       "             1.3693e-09, 1.2960e-09, 1.2877e-09, 1.2843e-09, 1.2099e-09, 1.0859e-09,\n",
       "             1.0063e-09, 8.4516e-10, 8.2944e-10, 8.2116e-10, 7.9693e-10, 7.3427e-10,\n",
       "             7.1048e-10, 7.0707e-10, 6.2898e-10, 6.2379e-10, 6.1581e-10, 5.7175e-10,\n",
       "             5.5356e-10, 4.6538e-10, 4.3593e-10, 4.3273e-10, 4.0225e-10, 3.6527e-10,\n",
       "             3.5678e-10, 3.1344e-10, 3.0585e-10, 2.5869e-10, 2.5788e-10, 2.4635e-10,\n",
       "             2.3783e-10, 1.9350e-10, 1.7680e-10, 1.6338e-10, 1.6245e-10, 1.4125e-10,\n",
       "             1.2938e-10, 1.2881e-10, 1.2547e-10, 1.2312e-10, 1.0252e-10, 8.1696e-11,\n",
       "             7.6084e-11, 6.7963e-11, 6.6431e-11, 4.6510e-11, 4.1653e-11, 3.5412e-11,\n",
       "             3.4031e-11, 3.3890e-11, 3.0036e-11, 2.1710e-11, 1.8499e-11, 1.7003e-11,\n",
       "             1.5892e-11, 1.5667e-11, 1.5104e-11, 1.3416e-11, 1.2856e-11, 1.2298e-11,\n",
       "             9.0231e-12, 8.9449e-12, 7.2529e-12, 6.9925e-12, 6.9202e-12, 6.7757e-12,\n",
       "             6.4486e-12, 5.8866e-12, 3.8956e-12, 2.9016e-12, 2.3303e-12, 1.1714e-12,\n",
       "             1.0189e-12, 9.5203e-13, 9.1044e-13, 6.0157e-13, 5.2949e-13, 4.7808e-13,\n",
       "             4.1183e-13, 2.7946e-13, 2.7735e-13, 1.1782e-13, 9.3152e-14, 8.1081e-14,\n",
       "             7.8042e-14, 4.8987e-14, 1.9630e-14, 1.2850e-14, 7.9758e-15, 5.3247e-15,\n",
       "             5.2576e-15, 2.9521e-15, 2.8425e-15, 2.1481e-15, 2.0794e-15, 1.0226e-15,\n",
       "             2.9976e-16, 2.1139e-16, 1.8301e-16, 9.7597e-17, 8.4954e-17, 2.5373e-18])}},\n",
       "   {'fpr': np.float64(0.0539906103286385),\n",
       "    'tpr': np.float64(0.9746348962336664),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023,\n",
       "             0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
       "             0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
       "             0.0117, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
       "             0.0141, 0.0141, 0.0141, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0188,\n",
       "             0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188,\n",
       "             0.0211, 0.0211, 0.0211, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235,\n",
       "             0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0258,\n",
       "             0.0258, 0.0258, 0.0258, 0.0282, 0.0282, 0.0305, 0.0305, 0.0329, 0.0329,\n",
       "             0.0329, 0.0329, 0.0329, 0.0352, 0.0352, 0.0376, 0.0399, 0.0423, 0.0423,\n",
       "             0.0423, 0.0423, 0.0423, 0.0446, 0.0446, 0.0446, 0.0469, 0.0469, 0.0469,\n",
       "             0.0469, 0.0493, 0.0516, 0.0540, 0.0540, 0.0540, 0.0540, 0.0563, 0.0563,\n",
       "             0.0563, 0.0587, 0.0610, 0.0634, 0.0657, 0.0681, 0.0704, 0.0704, 0.0728,\n",
       "             0.0751, 0.0775, 0.0775, 0.0798, 0.0798, 0.0822, 0.0845, 0.0869, 0.0869,\n",
       "             0.0869, 0.0892, 0.0892, 0.0892, 0.0915, 0.0939, 0.0962, 0.0986, 0.1009,\n",
       "             0.1009, 0.1009, 0.1009, 0.1033, 0.1056, 0.1056, 0.1080, 0.1080, 0.1080,\n",
       "             0.1103, 0.1103, 0.1103, 0.1127, 0.1150, 0.1174, 0.1197, 0.1221, 0.1244,\n",
       "             0.1268, 0.1268, 0.1268, 0.1268, 0.1291, 0.1315, 0.1338, 0.1362, 0.1385,\n",
       "             0.1408, 0.1432, 0.1455, 0.1479, 0.1502, 0.1502, 0.1526, 0.1526, 0.1549,\n",
       "             0.1573, 0.1596, 0.1620, 0.1643, 0.1667, 0.1690, 0.1714, 0.1737, 0.1761,\n",
       "             0.1761, 0.1784, 0.1808, 0.1831, 0.1831, 0.1854, 0.1878, 0.1901, 0.1925,\n",
       "             0.1948, 0.1972, 0.1995, 0.2019, 0.2042, 0.2066, 0.2066, 0.2089, 0.2113,\n",
       "             0.2136, 0.2160, 0.2160, 0.2183, 0.2207, 0.2230, 0.2254, 0.2277, 0.2300,\n",
       "             0.2324, 0.2347, 0.2371, 0.2394, 0.2394, 0.2418, 0.2418, 0.2441, 0.2465,\n",
       "             0.2488, 0.2512, 0.2535, 0.2559, 0.2582, 0.2582, 0.2606, 0.2629, 0.2653,\n",
       "             0.2676, 0.2700, 0.2723, 0.2746, 0.2770, 0.2793, 0.2817, 0.2840, 0.2864,\n",
       "             0.2864, 0.2887, 0.2911, 0.2934, 0.2958, 0.2981, 0.3005, 0.3028, 0.3052,\n",
       "             0.3075, 0.3099, 0.3122, 0.3146, 0.3169, 0.3192, 0.3216, 0.3239, 0.3263,\n",
       "             0.3286, 0.3310, 0.3333, 0.3357, 0.3380, 0.3404, 0.3427, 0.3451, 0.3474,\n",
       "             0.3498, 0.3521, 0.3545, 0.3568, 0.3592, 0.3615, 0.3638, 0.3662, 0.3685,\n",
       "             0.3709, 0.3732, 0.3756, 0.3779, 0.3803, 0.3826, 0.3850, 0.3873, 0.3897,\n",
       "             0.3920, 0.3944, 0.3967, 0.3991, 0.4014, 0.4038, 0.4061, 0.4085, 0.4108,\n",
       "             0.4131, 0.4155, 0.4178, 0.4202, 0.4225, 0.4249, 0.4272, 0.4296, 0.4319,\n",
       "             0.4343, 0.4366, 0.4390, 0.4413, 0.4437, 0.4460, 0.4484, 0.4507, 0.4531,\n",
       "             0.4554, 0.4577, 0.4601, 0.4624, 0.4648, 0.4671, 0.4695, 0.4718, 0.4742,\n",
       "             0.4765, 0.4789, 0.4812, 0.4836, 0.4859, 0.4883, 0.4906, 0.4930, 0.4953,\n",
       "             0.4977, 0.5000, 0.5023, 0.5047, 0.5070, 0.5094, 0.5117, 0.5141, 0.5164,\n",
       "             0.5188, 0.5211, 0.5235, 0.5258, 0.5282, 0.5305, 0.5305, 0.5329, 0.5352,\n",
       "             0.5376, 0.5399, 0.5423, 0.5446, 0.5469, 0.5493, 0.5516, 0.5540, 0.5563,\n",
       "             0.5587, 0.5610, 0.5634, 0.5657, 0.5681, 0.5704, 0.5728, 0.5751, 0.5775,\n",
       "             0.5798, 0.5822, 0.5845, 0.5869, 0.5892, 0.5915, 0.5939, 0.5962, 0.5986,\n",
       "             0.6009, 0.6033, 0.6056, 0.6080, 0.6103, 0.6127, 0.6150, 0.6174, 0.6197,\n",
       "             0.6221, 0.6244, 0.6268, 0.6291, 0.6315, 0.6338, 0.6362, 0.6385, 0.6408,\n",
       "             0.6432, 0.6455, 0.6479, 0.6502, 0.6526, 0.6549, 0.6573, 0.6596, 0.6620,\n",
       "             0.6643, 0.6667, 0.6690, 0.6714, 0.6737, 0.6761, 0.6784, 0.6808, 0.6831,\n",
       "             0.6854, 0.6878, 0.6901, 0.6925, 0.6948, 0.6972, 0.6995, 0.7019, 0.7019,\n",
       "             0.7042, 0.7066, 0.7089, 0.7113, 0.7136, 0.7160, 0.7183, 0.7207, 0.7230,\n",
       "             0.7254, 0.7277, 0.7300, 0.7324, 0.7347, 0.7371, 0.7394, 0.7418, 0.7441,\n",
       "             0.7465, 0.7488, 0.7512, 0.7535, 0.7559, 0.7582, 0.7606, 0.7629, 0.7653,\n",
       "             0.7676, 0.7700, 0.7723, 0.7746, 0.7770, 0.7793, 0.7817, 0.7840, 0.7864,\n",
       "             0.7887, 0.7911, 0.7934, 0.7958, 0.7981, 0.8005, 0.8028, 0.8052, 0.8075,\n",
       "             0.8099, 0.8122, 0.8146, 0.8169, 0.8192, 0.8216, 0.8239, 0.8263, 0.8286,\n",
       "             0.8310, 0.8333, 0.8357, 0.8380, 0.8404, 0.8427, 0.8451, 0.8474, 0.8498,\n",
       "             0.8521, 0.8545, 0.8568, 0.8592, 0.8615, 0.8638, 0.8662, 0.8685, 0.8709,\n",
       "             0.8732, 0.8756, 0.8779, 0.8803, 0.8826, 0.8850, 0.8873, 0.8897, 0.8920,\n",
       "             0.8944, 0.8967, 0.8991, 0.9014, 0.9038, 0.9061, 0.9085, 0.9108, 0.9131,\n",
       "             0.9155, 0.9178, 0.9202, 0.9225, 0.9249, 0.9272, 0.9296, 0.9319, 0.9343,\n",
       "             0.9366, 0.9390, 0.9413, 0.9437, 0.9460, 0.9484, 0.9507, 0.9531, 0.9554,\n",
       "             0.9577, 0.9601, 0.9624, 0.9648, 0.9671, 0.9695, 0.9718, 0.9742, 0.9765,\n",
       "             0.9789, 0.9812, 0.9836, 0.9859, 0.9883, 0.9906, 0.9930, 0.9953, 0.9977,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.5227, 0.6080, 0.6434, 0.6580, 0.6710, 0.6802, 0.6887, 0.6972,\n",
       "             0.7033, 0.7087, 0.7133, 0.7179, 0.7194, 0.7241, 0.7287, 0.7333, 0.7348,\n",
       "             0.7364, 0.7387, 0.7425, 0.7433, 0.7463, 0.7510, 0.7533, 0.7556, 0.7579,\n",
       "             0.7602, 0.7625, 0.7656, 0.7679, 0.7702, 0.7717, 0.7740, 0.7771, 0.7802,\n",
       "             0.7825, 0.7879, 0.7886, 0.7894, 0.7909, 0.7925, 0.7932, 0.7940, 0.7948,\n",
       "             0.7963, 0.7978, 0.8002, 0.8009, 0.8025, 0.8032, 0.8048, 0.8055, 0.8063,\n",
       "             0.8071, 0.8086, 0.8101, 0.8109, 0.8117, 0.8132, 0.8148, 0.8155, 0.8171,\n",
       "             0.8178, 0.8186, 0.8201, 0.8209, 0.8217, 0.8232, 0.8263, 0.8271, 0.8286,\n",
       "             0.8301, 0.8309, 0.8317, 0.8332, 0.8340, 0.8347, 0.8355, 0.8370, 0.8378,\n",
       "             0.8386, 0.8394, 0.8401, 0.8409, 0.8417, 0.8417, 0.8432, 0.8440, 0.8447,\n",
       "             0.8455, 0.8463, 0.8470, 0.8478, 0.8486, 0.8493, 0.8501, 0.8509, 0.8509,\n",
       "             0.8517, 0.8524, 0.8532, 0.8540, 0.8547, 0.8555, 0.8563, 0.8570, 0.8578,\n",
       "             0.8586, 0.8593, 0.8601, 0.8609, 0.8616, 0.8624, 0.8632, 0.8640, 0.8655,\n",
       "             0.8663, 0.8670, 0.8678, 0.8686, 0.8693, 0.8701, 0.8709, 0.8716, 0.8724,\n",
       "             0.8732, 0.8739, 0.8747, 0.8755, 0.8762, 0.8770, 0.8778, 0.8786, 0.8793,\n",
       "             0.8801, 0.8809, 0.8816, 0.8824, 0.8832, 0.8832, 0.8839, 0.8847, 0.8855,\n",
       "             0.8862, 0.8870, 0.8878, 0.8885, 0.8893, 0.8901, 0.8909, 0.8916, 0.8924,\n",
       "             0.8932, 0.8947, 0.8955, 0.8955, 0.8962, 0.8970, 0.8978, 0.8985, 0.8993,\n",
       "             0.9001, 0.9008, 0.9016, 0.9032, 0.9039, 0.9047, 0.9062, 0.9070, 0.9078,\n",
       "             0.9085, 0.9085, 0.9093, 0.9101, 0.9108, 0.9116, 0.9124, 0.9131, 0.9139,\n",
       "             0.9147, 0.9154, 0.9162, 0.9162, 0.9170, 0.9178, 0.9185, 0.9193, 0.9201,\n",
       "             0.9208, 0.9216, 0.9224, 0.9231, 0.9239, 0.9247, 0.9254, 0.9262, 0.9270,\n",
       "             0.9277, 0.9285, 0.9293, 0.9301, 0.9308, 0.9316, 0.9324, 0.9331, 0.9339,\n",
       "             0.9347, 0.9354, 0.9362, 0.9370, 0.9377, 0.9385, 0.9393, 0.9400, 0.9400,\n",
       "             0.9408, 0.9416, 0.9424, 0.9431, 0.9439, 0.9447, 0.9454, 0.9462, 0.9470,\n",
       "             0.9470, 0.9477, 0.9485, 0.9485, 0.9493, 0.9500, 0.9508, 0.9516, 0.9523,\n",
       "             0.9531, 0.9539, 0.9547, 0.9554, 0.9562, 0.9570, 0.9577, 0.9585, 0.9585,\n",
       "             0.9593, 0.9600, 0.9608, 0.9608, 0.9616, 0.9616, 0.9623, 0.9623, 0.9631,\n",
       "             0.9639, 0.9646, 0.9654, 0.9654, 0.9662, 0.9662, 0.9662, 0.9662, 0.9669,\n",
       "             0.9677, 0.9685, 0.9693, 0.9693, 0.9700, 0.9708, 0.9708, 0.9716, 0.9723,\n",
       "             0.9731, 0.9731, 0.9731, 0.9731, 0.9739, 0.9746, 0.9754, 0.9754, 0.9762,\n",
       "             0.9769, 0.9769, 0.9769, 0.9769, 0.9769, 0.9769, 0.9769, 0.9777, 0.9777,\n",
       "             0.9777, 0.9777, 0.9785, 0.9785, 0.9792, 0.9792, 0.9792, 0.9792, 0.9800,\n",
       "             0.9808, 0.9808, 0.9816, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823,\n",
       "             0.9831, 0.9839, 0.9846, 0.9846, 0.9846, 0.9854, 0.9854, 0.9862, 0.9869,\n",
       "             0.9869, 0.9877, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9892, 0.9900, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9915, 0.9915, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939,\n",
       "             0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9962, 0.9962, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01, 9.9978e-01, 9.9977e-01,\n",
       "             9.9977e-01, 9.9974e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9970e-01,\n",
       "             9.9968e-01, 9.9968e-01, 9.9968e-01, 9.9965e-01, 9.9965e-01, 9.9962e-01,\n",
       "             9.9961e-01, 9.9958e-01, 9.9957e-01, 9.9956e-01, 9.9951e-01, 9.9950e-01,\n",
       "             9.9948e-01, 9.9945e-01, 9.9945e-01, 9.9944e-01, 9.9941e-01, 9.9939e-01,\n",
       "             9.9932e-01, 9.9928e-01, 9.9926e-01, 9.9919e-01, 9.9912e-01, 9.9903e-01,\n",
       "             9.9902e-01, 9.9898e-01, 9.9884e-01, 9.9860e-01, 9.9857e-01, 9.9835e-01,\n",
       "             9.9814e-01, 9.9799e-01, 9.9780e-01, 9.9774e-01, 9.9769e-01, 9.9759e-01,\n",
       "             9.9757e-01, 9.9752e-01, 9.9728e-01, 9.9697e-01, 9.9693e-01, 9.9692e-01,\n",
       "             9.9674e-01, 9.9669e-01, 9.9665e-01, 9.9663e-01, 9.9644e-01, 9.9641e-01,\n",
       "             9.9590e-01, 9.9549e-01, 9.9526e-01, 9.9523e-01, 9.9493e-01, 9.9481e-01,\n",
       "             9.9426e-01, 9.9374e-01, 9.9276e-01, 9.9115e-01, 9.9024e-01, 9.8889e-01,\n",
       "             9.8815e-01, 9.8527e-01, 9.8284e-01, 9.7712e-01, 9.7619e-01, 9.7525e-01,\n",
       "             9.7005e-01, 9.6006e-01, 9.5785e-01, 9.5323e-01, 9.3954e-01, 9.3877e-01,\n",
       "             9.3859e-01, 9.3285e-01, 9.2635e-01, 9.2408e-01, 9.1104e-01, 9.0462e-01,\n",
       "             8.6814e-01, 8.4504e-01, 8.4222e-01, 8.3259e-01, 8.1285e-01, 8.0633e-01,\n",
       "             7.8164e-01, 7.5949e-01, 6.8368e-01, 6.7914e-01, 6.7630e-01, 6.0964e-01,\n",
       "             6.0416e-01, 5.9724e-01, 5.6309e-01, 5.3537e-01, 5.2339e-01, 5.1139e-01,\n",
       "             4.9822e-01, 4.9232e-01, 4.8833e-01, 4.6190e-01, 4.4554e-01, 4.1561e-01,\n",
       "             3.7729e-01, 3.7099e-01, 3.3581e-01, 3.2012e-01, 3.1411e-01, 2.9467e-01,\n",
       "             2.7063e-01, 2.0613e-01, 1.8208e-01, 1.7763e-01, 1.6953e-01, 1.6562e-01,\n",
       "             1.5321e-01, 1.5313e-01, 1.4626e-01, 1.3479e-01, 1.2582e-01, 1.0479e-01,\n",
       "             9.8057e-02, 7.0039e-02, 5.1392e-02, 5.1194e-02, 4.5367e-02, 4.0026e-02,\n",
       "             3.9246e-02, 3.3075e-02, 2.8755e-02, 2.7129e-02, 2.4237e-02, 2.3573e-02,\n",
       "             2.2358e-02, 1.3109e-02, 1.2087e-02, 1.1848e-02, 1.0795e-02, 9.8502e-03,\n",
       "             7.0289e-03, 6.7624e-03, 6.0350e-03, 5.7909e-03, 5.6652e-03, 4.9438e-03,\n",
       "             4.8922e-03, 4.3686e-03, 4.0029e-03, 3.9661e-03, 3.8830e-03, 3.7738e-03,\n",
       "             3.7092e-03, 3.3678e-03, 3.3303e-03, 3.2936e-03, 3.1217e-03, 2.8563e-03,\n",
       "             2.4513e-03, 2.3362e-03, 2.2980e-03, 1.9123e-03, 1.8283e-03, 1.7762e-03,\n",
       "             1.5240e-03, 1.3301e-03, 6.9907e-04, 4.9431e-04, 3.5803e-04, 2.8098e-04,\n",
       "             2.8053e-04, 2.2626e-04, 2.2024e-04, 1.8932e-04, 1.6152e-04, 1.5735e-04,\n",
       "             1.3792e-04, 1.2455e-04, 1.2376e-04, 1.0417e-04, 8.8089e-05, 7.8552e-05,\n",
       "             7.6990e-05, 6.7499e-05, 5.1167e-05, 3.1781e-05, 3.1290e-05, 2.5744e-05,\n",
       "             2.4886e-05, 2.1666e-05, 1.7621e-05, 1.6004e-05, 1.5892e-05, 1.5824e-05,\n",
       "             1.5177e-05, 1.0539e-05, 9.8777e-06, 9.7189e-06, 8.5689e-06, 8.5162e-06,\n",
       "             8.0988e-06, 7.5130e-06, 6.2002e-06, 5.6076e-06, 5.0205e-06, 4.9244e-06,\n",
       "             4.4979e-06, 3.9829e-06, 3.8803e-06, 3.0687e-06, 2.9648e-06, 2.5976e-06,\n",
       "             2.5954e-06, 2.3209e-06, 2.1600e-06, 1.7672e-06, 1.6394e-06, 1.3467e-06,\n",
       "             1.1012e-06, 9.9133e-07, 8.8794e-07, 7.7879e-07, 7.6687e-07, 5.3810e-07,\n",
       "             5.0804e-07, 4.3749e-07, 4.0095e-07, 3.8718e-07, 3.5079e-07, 3.2886e-07,\n",
       "             2.6984e-07, 2.6783e-07, 2.4140e-07, 2.4139e-07, 1.9990e-07, 1.5597e-07,\n",
       "             1.4335e-07, 1.2051e-07, 9.7845e-08, 9.5874e-08, 8.2371e-08, 7.9838e-08,\n",
       "             7.2571e-08, 7.1351e-08, 6.1973e-08, 5.5786e-08, 4.8596e-08, 4.5328e-08,\n",
       "             3.7206e-08, 3.0165e-08, 2.7871e-08, 2.5608e-08, 2.5154e-08, 2.4596e-08,\n",
       "             2.4459e-08, 2.0489e-08, 1.6688e-08, 1.4752e-08, 1.4483e-08, 1.4083e-08,\n",
       "             9.8206e-09, 9.6846e-09, 8.1065e-09, 8.0267e-09, 7.4201e-09, 5.6110e-09,\n",
       "             5.1763e-09, 4.6244e-09, 4.0015e-09, 3.8394e-09, 3.8392e-09, 3.6309e-09,\n",
       "             3.4867e-09, 3.3070e-09, 3.3022e-09, 3.2789e-09, 2.8267e-09, 2.7734e-09,\n",
       "             2.6023e-09, 2.3617e-09, 2.2082e-09, 2.1857e-09, 1.9633e-09, 1.9516e-09,\n",
       "             1.9445e-09, 1.8739e-09, 1.7040e-09, 1.5151e-09, 1.5090e-09, 1.4965e-09,\n",
       "             1.4766e-09, 1.3541e-09, 1.3515e-09, 1.3119e-09, 1.3087e-09, 1.2418e-09,\n",
       "             1.1955e-09, 1.0960e-09, 1.0648e-09, 1.0079e-09, 9.5454e-10, 9.3136e-10,\n",
       "             8.8087e-10, 8.4191e-10, 8.1233e-10, 8.1178e-10, 7.8680e-10, 5.5078e-10,\n",
       "             5.4019e-10, 4.8574e-10, 4.6418e-10, 4.5432e-10, 4.0883e-10, 3.9795e-10,\n",
       "             3.8073e-10, 3.7076e-10, 3.4175e-10, 3.1206e-10, 3.0327e-10, 2.9891e-10,\n",
       "             2.3378e-10, 2.2362e-10, 2.0051e-10, 1.9779e-10, 1.9635e-10, 1.6719e-10,\n",
       "             1.6489e-10, 1.6362e-10, 1.6048e-10, 1.5620e-10, 1.4869e-10, 1.3722e-10,\n",
       "             1.2805e-10, 1.1630e-10, 1.1428e-10, 1.0339e-10, 7.5676e-11, 7.5483e-11,\n",
       "             7.4802e-11, 6.7843e-11, 6.2813e-11, 5.1214e-11, 5.1104e-11, 5.1048e-11,\n",
       "             4.5397e-11, 4.5201e-11, 4.3939e-11, 4.2349e-11, 3.6959e-11, 3.6258e-11,\n",
       "             3.6162e-11, 3.4234e-11, 3.4114e-11, 3.2113e-11, 2.9887e-11, 2.4558e-11,\n",
       "             2.2928e-11, 2.1910e-11, 2.1440e-11, 1.9163e-11, 1.8654e-11, 1.7351e-11,\n",
       "             1.7169e-11, 1.5627e-11, 1.4691e-11, 1.4158e-11, 1.4109e-11, 1.3384e-11,\n",
       "             1.2980e-11, 1.2055e-11, 1.1663e-11, 1.1033e-11, 1.1022e-11, 1.0971e-11,\n",
       "             9.4805e-12, 8.4729e-12, 8.0761e-12, 6.1754e-12, 5.8204e-12, 5.3246e-12,\n",
       "             5.2051e-12, 4.9855e-12, 4.9368e-12, 4.2841e-12, 4.2100e-12, 3.7935e-12,\n",
       "             3.6319e-12, 3.0199e-12, 2.8849e-12, 2.8377e-12, 2.5029e-12, 2.3382e-12,\n",
       "             1.6508e-12, 1.6394e-12, 1.6184e-12, 1.4545e-12, 1.4230e-12, 1.2533e-12,\n",
       "             1.2371e-12, 1.2301e-12, 1.2182e-12, 1.1458e-12, 1.1092e-12, 9.2406e-13,\n",
       "             8.4048e-13, 7.9050e-13, 7.2551e-13, 6.2700e-13, 6.1858e-13, 6.1272e-13,\n",
       "             5.3012e-13, 5.2992e-13, 4.7987e-13, 3.9672e-13, 3.9556e-13, 3.8494e-13,\n",
       "             3.8159e-13, 3.0586e-13, 2.8048e-13, 2.6677e-13, 2.6351e-13, 2.5741e-13,\n",
       "             2.3877e-13, 2.3405e-13, 2.1479e-13, 1.9778e-13, 1.5823e-13, 1.2806e-13,\n",
       "             1.1087e-13, 1.0803e-13, 1.0096e-13, 9.6268e-14, 8.8269e-14, 8.4460e-14,\n",
       "             7.7471e-14, 7.5354e-14, 6.9402e-14, 6.3235e-14, 6.1144e-14, 4.3820e-14,\n",
       "             4.2923e-14, 3.3732e-14, 3.2836e-14, 3.2715e-14, 3.1103e-14, 2.7426e-14,\n",
       "             2.7255e-14, 2.6445e-14, 2.1802e-14, 1.8114e-14, 1.4625e-14, 1.1556e-14,\n",
       "             1.0065e-14, 9.2985e-15, 9.1641e-15, 6.3563e-15, 5.3018e-15, 5.1749e-15,\n",
       "             5.0648e-15, 4.6017e-15, 4.4608e-15, 4.2943e-15, 4.2631e-15, 3.1622e-15,\n",
       "             2.7005e-15, 2.5065e-15, 2.1822e-15, 1.8481e-15, 1.7728e-15, 1.7141e-15,\n",
       "             1.6164e-15, 1.5611e-15, 1.4830e-15, 1.3787e-15, 1.3270e-15, 1.2854e-15,\n",
       "             1.0541e-15, 1.0181e-15, 9.0823e-16, 9.0499e-16, 8.5042e-16, 7.7839e-16,\n",
       "             6.1456e-16, 6.0560e-16, 6.0153e-16, 4.7774e-16, 3.9851e-16, 3.0030e-16,\n",
       "             2.7749e-16, 2.5798e-16, 2.5789e-16, 2.3094e-16, 1.3243e-16, 1.3004e-16,\n",
       "             1.0949e-16, 1.0753e-16, 9.1400e-17, 7.8094e-17, 5.9639e-17, 5.2543e-17,\n",
       "             5.0308e-17, 4.7506e-17, 4.3440e-17, 4.2705e-17, 3.9388e-17, 3.8837e-17,\n",
       "             3.8452e-17, 3.6967e-17, 2.9834e-17, 2.1144e-17, 1.8944e-17, 8.8350e-18,\n",
       "             6.2086e-18, 3.9150e-18, 2.6966e-18, 1.8283e-18, 1.6641e-18, 1.2620e-18,\n",
       "             4.4070e-19, 2.3359e-19, 2.7025e-20, 2.3335e-20, 2.2165e-20, 9.9643e-21,\n",
       "             8.6043e-21, 4.0143e-21, 1.0260e-21, 1.7907e-22, 9.0031e-23, 1.7970e-23,\n",
       "             1.4746e-23, 6.8864e-24, 5.3601e-25, 1.8402e-25])}},\n",
       "   {'fpr': np.float64(0.07276995305164319),\n",
       "    'tpr': np.float64(0.9823212913143735),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
       "             0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188,\n",
       "             0.0188, 0.0211, 0.0211, 0.0211, 0.0211, 0.0211, 0.0235, 0.0235, 0.0235,\n",
       "             0.0235, 0.0235, 0.0258, 0.0258, 0.0258, 0.0282, 0.0282, 0.0282, 0.0282,\n",
       "             0.0282, 0.0282, 0.0305, 0.0305, 0.0329, 0.0329, 0.0329, 0.0329, 0.0352,\n",
       "             0.0352, 0.0352, 0.0352, 0.0352, 0.0376, 0.0376, 0.0399, 0.0399, 0.0399,\n",
       "             0.0423, 0.0423, 0.0423, 0.0446, 0.0446, 0.0446, 0.0446, 0.0446, 0.0446,\n",
       "             0.0446, 0.0469, 0.0469, 0.0469, 0.0493, 0.0516, 0.0516, 0.0540, 0.0563,\n",
       "             0.0587, 0.0587, 0.0610, 0.0634, 0.0634, 0.0634, 0.0657, 0.0657, 0.0681,\n",
       "             0.0681, 0.0681, 0.0681, 0.0681, 0.0704, 0.0704, 0.0728, 0.0751, 0.0775,\n",
       "             0.0798, 0.0798, 0.0798, 0.0798, 0.0822, 0.0845, 0.0845, 0.0869, 0.0892,\n",
       "             0.0915, 0.0939, 0.0962, 0.0962, 0.0986, 0.1009, 0.1033, 0.1056, 0.1080,\n",
       "             0.1103, 0.1103, 0.1127, 0.1127, 0.1150, 0.1174, 0.1197, 0.1221, 0.1244,\n",
       "             0.1268, 0.1291, 0.1315, 0.1315, 0.1338, 0.1362, 0.1385, 0.1408, 0.1432,\n",
       "             0.1455, 0.1455, 0.1455, 0.1455, 0.1479, 0.1502, 0.1526, 0.1549, 0.1573,\n",
       "             0.1596, 0.1620, 0.1643, 0.1667, 0.1690, 0.1714, 0.1737, 0.1737, 0.1761,\n",
       "             0.1784, 0.1808, 0.1831, 0.1854, 0.1878, 0.1901, 0.1901, 0.1925, 0.1948,\n",
       "             0.1948, 0.1948, 0.1972, 0.1972, 0.1995, 0.2019, 0.2042, 0.2066, 0.2089,\n",
       "             0.2113, 0.2136, 0.2160, 0.2183, 0.2207, 0.2230, 0.2254, 0.2277, 0.2300,\n",
       "             0.2324, 0.2347, 0.2371, 0.2371, 0.2394, 0.2418, 0.2441, 0.2465, 0.2465,\n",
       "             0.2488, 0.2512, 0.2535, 0.2559, 0.2582, 0.2606, 0.2629, 0.2653, 0.2676,\n",
       "             0.2700, 0.2723, 0.2746, 0.2770, 0.2793, 0.2817, 0.2840, 0.2864, 0.2887,\n",
       "             0.2911, 0.2934, 0.2958, 0.2981, 0.3005, 0.3028, 0.3052, 0.3075, 0.3099,\n",
       "             0.3122, 0.3146, 0.3169, 0.3169, 0.3192, 0.3216, 0.3239, 0.3239, 0.3263,\n",
       "             0.3286, 0.3310, 0.3333, 0.3357, 0.3380, 0.3404, 0.3427, 0.3451, 0.3474,\n",
       "             0.3498, 0.3521, 0.3545, 0.3568, 0.3592, 0.3615, 0.3638, 0.3662, 0.3685,\n",
       "             0.3709, 0.3709, 0.3732, 0.3756, 0.3779, 0.3803, 0.3826, 0.3826, 0.3850,\n",
       "             0.3873, 0.3897, 0.3920, 0.3944, 0.3967, 0.3991, 0.4014, 0.4038, 0.4061,\n",
       "             0.4085, 0.4108, 0.4131, 0.4155, 0.4178, 0.4202, 0.4225, 0.4249, 0.4272,\n",
       "             0.4296, 0.4319, 0.4343, 0.4366, 0.4390, 0.4413, 0.4437, 0.4460, 0.4484,\n",
       "             0.4507, 0.4531, 0.4554, 0.4577, 0.4601, 0.4624, 0.4648, 0.4671, 0.4695,\n",
       "             0.4718, 0.4742, 0.4765, 0.4789, 0.4812, 0.4836, 0.4859, 0.4883, 0.4906,\n",
       "             0.4930, 0.4953, 0.4977, 0.5000, 0.5023, 0.5047, 0.5070, 0.5094, 0.5117,\n",
       "             0.5141, 0.5164, 0.5188, 0.5211, 0.5235, 0.5258, 0.5282, 0.5305, 0.5329,\n",
       "             0.5352, 0.5376, 0.5399, 0.5423, 0.5446, 0.5469, 0.5493, 0.5516, 0.5540,\n",
       "             0.5563, 0.5587, 0.5610, 0.5634, 0.5657, 0.5681, 0.5704, 0.5728, 0.5751,\n",
       "             0.5775, 0.5798, 0.5822, 0.5845, 0.5869, 0.5892, 0.5915, 0.5939, 0.5962,\n",
       "             0.5986, 0.6009, 0.6033, 0.6056, 0.6080, 0.6103, 0.6127, 0.6150, 0.6174,\n",
       "             0.6197, 0.6221, 0.6244, 0.6268, 0.6291, 0.6315, 0.6338, 0.6362, 0.6385,\n",
       "             0.6408, 0.6432, 0.6455, 0.6479, 0.6502, 0.6526, 0.6549, 0.6573, 0.6596,\n",
       "             0.6620, 0.6643, 0.6667, 0.6690, 0.6714, 0.6737, 0.6761, 0.6784, 0.6808,\n",
       "             0.6831, 0.6854, 0.6878, 0.6901, 0.6925, 0.6948, 0.6972, 0.6995, 0.7019,\n",
       "             0.7042, 0.7066, 0.7089, 0.7113, 0.7136, 0.7160, 0.7183, 0.7207, 0.7230,\n",
       "             0.7254, 0.7277, 0.7300, 0.7324, 0.7347, 0.7371, 0.7394, 0.7418, 0.7441,\n",
       "             0.7465, 0.7488, 0.7512, 0.7535, 0.7559, 0.7582, 0.7606, 0.7629, 0.7653,\n",
       "             0.7676, 0.7700, 0.7723, 0.7746, 0.7770, 0.7793, 0.7817, 0.7840, 0.7864,\n",
       "             0.7887, 0.7911, 0.7934, 0.7934, 0.7958, 0.7981, 0.8005, 0.8028, 0.8052,\n",
       "             0.8075, 0.8099, 0.8122, 0.8146, 0.8169, 0.8192, 0.8216, 0.8239, 0.8263,\n",
       "             0.8286, 0.8310, 0.8333, 0.8357, 0.8380, 0.8404, 0.8427, 0.8451, 0.8474,\n",
       "             0.8498, 0.8521, 0.8545, 0.8568, 0.8592, 0.8615, 0.8638, 0.8662, 0.8685,\n",
       "             0.8709, 0.8732, 0.8756, 0.8779, 0.8803, 0.8826, 0.8850, 0.8873, 0.8897,\n",
       "             0.8920, 0.8944, 0.8967, 0.8991, 0.9014, 0.9038, 0.9061, 0.9085, 0.9108,\n",
       "             0.9131, 0.9155, 0.9178, 0.9202, 0.9225, 0.9249, 0.9272, 0.9296, 0.9319,\n",
       "             0.9343, 0.9366, 0.9390, 0.9413, 0.9437, 0.9460, 0.9484, 0.9507, 0.9531,\n",
       "             0.9554, 0.9577, 0.9601, 0.9624, 0.9648, 0.9671, 0.9695, 0.9718, 0.9742,\n",
       "             0.9765, 0.9789, 0.9812, 0.9836, 0.9859, 0.9883, 0.9906, 0.9930, 0.9953,\n",
       "             0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7556, 0.8055, 0.8240, 0.8347, 0.8401, 0.8447, 0.8478, 0.8517,\n",
       "             0.8540, 0.8547, 0.8578, 0.8586, 0.8609, 0.8647, 0.8670, 0.8678, 0.8693,\n",
       "             0.8701, 0.8724, 0.8732, 0.8755, 0.8762, 0.8770, 0.8786, 0.8801, 0.8824,\n",
       "             0.8832, 0.8839, 0.8847, 0.8862, 0.8870, 0.8878, 0.8893, 0.8901, 0.8909,\n",
       "             0.8909, 0.8924, 0.8932, 0.8939, 0.8955, 0.8970, 0.8978, 0.8985, 0.8993,\n",
       "             0.9001, 0.9008, 0.9016, 0.9024, 0.9032, 0.9039, 0.9047, 0.9055, 0.9062,\n",
       "             0.9070, 0.9085, 0.9093, 0.9101, 0.9116, 0.9124, 0.9131, 0.9139, 0.9147,\n",
       "             0.9154, 0.9162, 0.9170, 0.9178, 0.9178, 0.9185, 0.9193, 0.9201, 0.9208,\n",
       "             0.9208, 0.9216, 0.9224, 0.9231, 0.9239, 0.9239, 0.9247, 0.9254, 0.9262,\n",
       "             0.9270, 0.9277, 0.9285, 0.9293, 0.9301, 0.9308, 0.9316, 0.9324, 0.9331,\n",
       "             0.9339, 0.9347, 0.9354, 0.9362, 0.9370, 0.9377, 0.9385, 0.9393, 0.9400,\n",
       "             0.9408, 0.9416, 0.9416, 0.9424, 0.9431, 0.9439, 0.9447, 0.9454, 0.9462,\n",
       "             0.9470, 0.9470, 0.9477, 0.9485, 0.9493, 0.9500, 0.9500, 0.9508, 0.9516,\n",
       "             0.9523, 0.9531, 0.9531, 0.9539, 0.9547, 0.9547, 0.9554, 0.9562, 0.9570,\n",
       "             0.9577, 0.9585, 0.9585, 0.9593, 0.9593, 0.9600, 0.9608, 0.9616, 0.9616,\n",
       "             0.9623, 0.9631, 0.9639, 0.9646, 0.9646, 0.9654, 0.9654, 0.9662, 0.9669,\n",
       "             0.9669, 0.9677, 0.9685, 0.9685, 0.9693, 0.9700, 0.9708, 0.9716, 0.9723,\n",
       "             0.9731, 0.9731, 0.9739, 0.9746, 0.9746, 0.9746, 0.9754, 0.9754, 0.9754,\n",
       "             0.9754, 0.9762, 0.9762, 0.9762, 0.9769, 0.9777, 0.9777, 0.9785, 0.9785,\n",
       "             0.9792, 0.9800, 0.9808, 0.9816, 0.9816, 0.9823, 0.9823, 0.9823, 0.9823,\n",
       "             0.9823, 0.9831, 0.9839, 0.9846, 0.9846, 0.9846, 0.9854, 0.9854, 0.9854,\n",
       "             0.9854, 0.9854, 0.9854, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862,\n",
       "             0.9862, 0.9869, 0.9869, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877,\n",
       "             0.9877, 0.9877, 0.9877, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9892, 0.9900, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9915, 0.9915,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923,\n",
       "             0.9931, 0.9939, 0.9939, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9989e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9983e-01,\n",
       "             9.9981e-01, 9.9976e-01, 9.9974e-01, 9.9970e-01, 9.9968e-01, 9.9965e-01,\n",
       "             9.9963e-01, 9.9962e-01, 9.9961e-01, 9.9958e-01, 9.9953e-01, 9.9950e-01,\n",
       "             9.9943e-01, 9.9939e-01, 9.9938e-01, 9.9933e-01, 9.9932e-01, 9.9930e-01,\n",
       "             9.9927e-01, 9.9919e-01, 9.9918e-01, 9.9915e-01, 9.9909e-01, 9.9906e-01,\n",
       "             9.9899e-01, 9.9892e-01, 9.9892e-01, 9.9888e-01, 9.9875e-01, 9.9867e-01,\n",
       "             9.9864e-01, 9.9861e-01, 9.9816e-01, 9.9816e-01, 9.9798e-01, 9.9745e-01,\n",
       "             9.9745e-01, 9.9728e-01, 9.9720e-01, 9.9705e-01, 9.9660e-01, 9.9601e-01,\n",
       "             9.9453e-01, 9.9347e-01, 9.9290e-01, 9.9242e-01, 9.9209e-01, 9.9078e-01,\n",
       "             9.9040e-01, 9.8695e-01, 9.8365e-01, 9.8287e-01, 9.7907e-01, 9.7865e-01,\n",
       "             9.7843e-01, 9.7141e-01, 9.7038e-01, 9.6515e-01, 9.5311e-01, 9.4989e-01,\n",
       "             9.4403e-01, 9.3950e-01, 9.3030e-01, 9.1783e-01, 9.0627e-01, 8.7071e-01,\n",
       "             8.6379e-01, 8.6059e-01, 8.5576e-01, 8.4451e-01, 8.4326e-01, 8.4159e-01,\n",
       "             8.3559e-01, 8.3550e-01, 7.3458e-01, 6.8833e-01, 6.5719e-01, 6.2040e-01,\n",
       "             5.8847e-01, 5.8194e-01, 5.7390e-01, 5.5836e-01, 4.9029e-01, 4.5598e-01,\n",
       "             3.7044e-01, 3.5187e-01, 3.2641e-01, 3.1617e-01, 3.1029e-01, 2.7531e-01,\n",
       "             2.0451e-01, 1.7847e-01, 1.6212e-01, 1.3980e-01, 1.1290e-01, 9.2846e-02,\n",
       "             9.0643e-02, 8.3822e-02, 7.5482e-02, 7.0792e-02, 6.1191e-02, 5.2734e-02,\n",
       "             4.9034e-02, 4.7793e-02, 4.4936e-02, 3.9071e-02, 3.5111e-02, 3.2459e-02,\n",
       "             3.0531e-02, 3.0332e-02, 2.7479e-02, 2.6390e-02, 2.3090e-02, 2.0590e-02,\n",
       "             1.8891e-02, 1.5338e-02, 1.2661e-02, 6.6569e-03, 4.8609e-03, 4.0492e-03,\n",
       "             2.9429e-03, 2.9143e-03, 2.6586e-03, 2.6511e-03, 2.6491e-03, 2.5629e-03,\n",
       "             2.5014e-03, 2.0908e-03, 1.5146e-03, 1.4180e-03, 1.3348e-03, 1.2945e-03,\n",
       "             1.0760e-03, 1.0310e-03, 8.0668e-04, 6.4489e-04, 5.6838e-04, 4.8802e-04,\n",
       "             4.2573e-04, 4.1908e-04, 4.1106e-04, 2.9585e-04, 2.4832e-04, 1.9789e-04,\n",
       "             1.8469e-04, 1.6116e-04, 1.5792e-04, 1.3405e-04, 1.2919e-04, 1.1275e-04,\n",
       "             1.1237e-04, 1.0918e-04, 9.2033e-05, 8.9075e-05, 7.5595e-05, 7.1491e-05,\n",
       "             7.0385e-05, 6.4964e-05, 6.2817e-05, 6.1421e-05, 5.9901e-05, 4.0445e-05,\n",
       "             4.0365e-05, 3.1555e-05, 2.4660e-05, 2.0745e-05, 1.7358e-05, 1.6776e-05,\n",
       "             1.6702e-05, 1.5001e-05, 1.3499e-05, 1.2024e-05, 1.0276e-05, 9.4198e-06,\n",
       "             8.6394e-06, 8.1459e-06, 6.1028e-06, 6.0570e-06, 4.3905e-06, 3.5816e-06,\n",
       "             3.5311e-06, 2.9661e-06, 2.8509e-06, 2.8317e-06, 2.4125e-06, 1.9716e-06,\n",
       "             1.7758e-06, 1.5607e-06, 1.5440e-06, 1.4353e-06, 1.2528e-06, 1.0947e-06,\n",
       "             9.8331e-07, 9.7147e-07, 8.6580e-07, 8.3628e-07, 8.0251e-07, 6.8118e-07,\n",
       "             5.9163e-07, 5.7388e-07, 5.3883e-07, 5.3177e-07, 5.0697e-07, 4.9411e-07,\n",
       "             4.4515e-07, 4.2844e-07, 4.2356e-07, 3.8122e-07, 3.1555e-07, 3.1334e-07,\n",
       "             2.9058e-07, 2.7808e-07, 2.7519e-07, 2.6216e-07, 2.5910e-07, 2.2743e-07,\n",
       "             1.7913e-07, 1.7833e-07, 1.6001e-07, 1.5999e-07, 1.4523e-07, 1.2064e-07,\n",
       "             1.2012e-07, 1.1327e-07, 1.0823e-07, 1.0543e-07, 1.0142e-07, 9.2010e-08,\n",
       "             8.2034e-08, 7.5278e-08, 6.8134e-08, 6.0300e-08, 5.9143e-08, 5.4615e-08,\n",
       "             5.1338e-08, 4.6101e-08, 3.9594e-08, 3.8661e-08, 3.5806e-08, 3.4424e-08,\n",
       "             3.4169e-08, 3.0903e-08, 3.0420e-08, 2.9334e-08, 2.6176e-08, 2.5602e-08,\n",
       "             2.4238e-08, 2.3980e-08, 2.0623e-08, 1.9064e-08, 1.8962e-08, 1.7446e-08,\n",
       "             1.7116e-08, 1.5563e-08, 1.5102e-08, 1.3737e-08, 1.2633e-08, 1.0445e-08,\n",
       "             1.0123e-08, 9.9777e-09, 9.2810e-09, 8.3141e-09, 6.8662e-09, 6.7499e-09,\n",
       "             6.6611e-09, 5.5765e-09, 5.5338e-09, 5.4997e-09, 5.3435e-09, 4.6747e-09,\n",
       "             4.2696e-09, 3.8850e-09, 3.8251e-09, 3.7888e-09, 3.5025e-09, 3.3788e-09,\n",
       "             3.1577e-09, 2.7225e-09, 2.6590e-09, 2.5442e-09, 2.3029e-09, 2.1207e-09,\n",
       "             2.0874e-09, 2.0128e-09, 1.5643e-09, 1.4735e-09, 1.4365e-09, 1.3833e-09,\n",
       "             1.2862e-09, 1.2861e-09, 1.2771e-09, 1.2570e-09, 1.2269e-09, 1.2115e-09,\n",
       "             1.0187e-09, 1.0112e-09, 9.8234e-10, 9.2226e-10, 8.8960e-10, 8.8324e-10,\n",
       "             8.6302e-10, 8.4678e-10, 8.1601e-10, 7.3356e-10, 7.2206e-10, 4.9580e-10,\n",
       "             4.8646e-10, 4.5795e-10, 4.1517e-10, 3.9428e-10, 3.1972e-10, 3.1799e-10,\n",
       "             3.0652e-10, 2.6577e-10, 2.4364e-10, 2.3682e-10, 2.3597e-10, 2.2005e-10,\n",
       "             2.1687e-10, 2.0603e-10, 2.0569e-10, 1.8815e-10, 1.8402e-10, 1.8250e-10,\n",
       "             1.7139e-10, 1.6848e-10, 1.6421e-10, 1.3752e-10, 1.1989e-10, 1.0493e-10,\n",
       "             1.0017e-10, 9.7632e-11, 9.2567e-11, 8.1274e-11, 7.9682e-11, 7.9061e-11,\n",
       "             6.8826e-11, 6.3938e-11, 5.6802e-11, 5.6417e-11, 5.5401e-11, 5.4867e-11,\n",
       "             4.4886e-11, 4.2442e-11, 4.1919e-11, 3.9072e-11, 3.7907e-11, 3.7622e-11,\n",
       "             3.1873e-11, 2.9801e-11, 2.9137e-11, 2.8346e-11, 2.6681e-11, 2.5988e-11,\n",
       "             1.5250e-11, 1.4104e-11, 1.3306e-11, 1.3114e-11, 1.2846e-11, 1.2673e-11,\n",
       "             1.2029e-11, 1.1391e-11, 8.5048e-12, 6.3132e-12, 5.7852e-12, 5.5301e-12,\n",
       "             4.6431e-12, 4.5613e-12, 4.1466e-12, 3.6682e-12, 3.5379e-12, 3.4554e-12,\n",
       "             3.4406e-12, 3.3867e-12, 3.2283e-12, 2.9753e-12, 2.8986e-12, 2.8586e-12,\n",
       "             2.7600e-12, 2.3488e-12, 2.3207e-12, 2.2630e-12, 1.7894e-12, 1.7832e-12,\n",
       "             1.6878e-12, 1.6820e-12, 1.6390e-12, 1.5618e-12, 1.5567e-12, 1.4010e-12,\n",
       "             1.0124e-12, 9.7934e-13, 7.8803e-13, 7.5625e-13, 7.1336e-13, 6.5575e-13,\n",
       "             6.0061e-13, 5.4467e-13, 3.5118e-13, 3.4919e-13, 3.3357e-13, 3.1529e-13,\n",
       "             3.0896e-13, 2.9037e-13, 2.8682e-13, 2.8579e-13, 2.5405e-13, 2.4760e-13,\n",
       "             2.2750e-13, 2.2174e-13, 2.2036e-13, 1.8892e-13, 1.6062e-13, 1.5507e-13,\n",
       "             1.5019e-13, 1.4776e-13, 1.4650e-13, 1.3574e-13, 1.2399e-13, 1.1970e-13,\n",
       "             1.1394e-13, 1.0287e-13, 9.3199e-14, 9.2700e-14, 8.2105e-14, 7.5841e-14,\n",
       "             6.9187e-14, 5.8463e-14, 5.0554e-14, 4.9740e-14, 4.8179e-14, 4.5845e-14,\n",
       "             4.1401e-14, 2.6706e-14, 2.6626e-14, 2.6560e-14, 2.1178e-14, 2.0393e-14,\n",
       "             1.9881e-14, 1.4135e-14, 1.3405e-14, 8.3588e-15, 7.2372e-15, 7.0147e-15,\n",
       "             6.4787e-15, 6.3342e-15, 4.1687e-15, 3.4106e-15, 3.3571e-15, 2.3769e-15,\n",
       "             2.2937e-15, 1.7040e-15, 1.2867e-15, 1.1717e-15, 1.1483e-15, 1.1440e-15,\n",
       "             1.0956e-15, 7.7088e-16, 6.5243e-16, 6.3595e-16, 6.2232e-16, 3.8912e-16,\n",
       "             3.7998e-16, 3.6493e-16, 2.7447e-16, 2.6998e-16, 2.6587e-16, 2.5660e-16,\n",
       "             1.5591e-16, 1.2568e-16, 1.2414e-16, 9.5023e-17, 8.6641e-17, 7.2410e-17,\n",
       "             5.9782e-17, 4.8501e-17, 4.5671e-17, 3.0278e-17, 2.8273e-17, 2.0449e-17,\n",
       "             1.8119e-17, 1.6919e-17, 1.1448e-17, 8.8737e-18, 7.5035e-18, 7.2014e-18,\n",
       "             3.5084e-18, 9.8032e-19, 3.5695e-19, 2.3840e-19, 1.5501e-19, 1.3411e-19,\n",
       "             6.4407e-20, 1.6589e-20, 8.8748e-21, 1.3795e-21, 1.0930e-21, 9.8639e-22,\n",
       "             1.9099e-22, 7.0529e-23])}},\n",
       "   {'fpr': np.float64(0.06807511737089202),\n",
       "    'tpr': np.float64(0.9769408147578785),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0047, 0.0047, 0.0070, 0.0094, 0.0094, 0.0094, 0.0094, 0.0117,\n",
       "             0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
       "             0.0117, 0.0117, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
       "             0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188,\n",
       "             0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0211, 0.0211,\n",
       "             0.0211, 0.0235, 0.0235, 0.0258, 0.0258, 0.0282, 0.0282, 0.0282, 0.0282,\n",
       "             0.0282, 0.0282, 0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0305,\n",
       "             0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0329, 0.0329, 0.0329,\n",
       "             0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0352, 0.0352, 0.0376,\n",
       "             0.0376, 0.0376, 0.0376, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0423,\n",
       "             0.0423, 0.0423, 0.0446, 0.0446, 0.0469, 0.0469, 0.0493, 0.0493, 0.0516,\n",
       "             0.0516, 0.0516, 0.0540, 0.0540, 0.0540, 0.0540, 0.0540, 0.0540, 0.0540,\n",
       "             0.0563, 0.0587, 0.0610, 0.0610, 0.0634, 0.0634, 0.0657, 0.0681, 0.0681,\n",
       "             0.0681, 0.0681, 0.0681, 0.0681, 0.0704, 0.0728, 0.0728, 0.0728, 0.0751,\n",
       "             0.0751, 0.0775, 0.0798, 0.0822, 0.0822, 0.0822, 0.0845, 0.0869, 0.0892,\n",
       "             0.0892, 0.0892, 0.0915, 0.0915, 0.0939, 0.0962, 0.0962, 0.0962, 0.0986,\n",
       "             0.1009, 0.1009, 0.1033, 0.1056, 0.1056, 0.1080, 0.1103, 0.1127, 0.1150,\n",
       "             0.1150, 0.1174, 0.1174, 0.1174, 0.1197, 0.1197, 0.1221, 0.1221, 0.1244,\n",
       "             0.1244, 0.1268, 0.1268, 0.1291, 0.1315, 0.1315, 0.1338, 0.1362, 0.1385,\n",
       "             0.1385, 0.1408, 0.1432, 0.1455, 0.1455, 0.1479, 0.1502, 0.1526, 0.1549,\n",
       "             0.1573, 0.1596, 0.1620, 0.1643, 0.1667, 0.1690, 0.1714, 0.1714, 0.1737,\n",
       "             0.1761, 0.1784, 0.1808, 0.1831, 0.1854, 0.1854, 0.1878, 0.1901, 0.1901,\n",
       "             0.1925, 0.1948, 0.1972, 0.1995, 0.2019, 0.2042, 0.2066, 0.2089, 0.2113,\n",
       "             0.2136, 0.2160, 0.2183, 0.2207, 0.2230, 0.2254, 0.2277, 0.2300, 0.2324,\n",
       "             0.2347, 0.2371, 0.2394, 0.2418, 0.2441, 0.2465, 0.2488, 0.2512, 0.2535,\n",
       "             0.2559, 0.2582, 0.2606, 0.2629, 0.2653, 0.2676, 0.2700, 0.2723, 0.2746,\n",
       "             0.2770, 0.2770, 0.2793, 0.2817, 0.2840, 0.2864, 0.2887, 0.2911, 0.2934,\n",
       "             0.2958, 0.2981, 0.3005, 0.3028, 0.3052, 0.3075, 0.3099, 0.3122, 0.3146,\n",
       "             0.3169, 0.3192, 0.3216, 0.3239, 0.3263, 0.3286, 0.3310, 0.3333, 0.3357,\n",
       "             0.3380, 0.3404, 0.3427, 0.3451, 0.3474, 0.3498, 0.3521, 0.3545, 0.3568,\n",
       "             0.3592, 0.3615, 0.3638, 0.3662, 0.3685, 0.3709, 0.3732, 0.3756, 0.3779,\n",
       "             0.3803, 0.3826, 0.3850, 0.3850, 0.3873, 0.3897, 0.3920, 0.3944, 0.3967,\n",
       "             0.3991, 0.4014, 0.4038, 0.4061, 0.4085, 0.4108, 0.4131, 0.4155, 0.4178,\n",
       "             0.4202, 0.4225, 0.4249, 0.4272, 0.4296, 0.4319, 0.4343, 0.4366, 0.4390,\n",
       "             0.4413, 0.4437, 0.4460, 0.4484, 0.4507, 0.4531, 0.4554, 0.4577, 0.4601,\n",
       "             0.4601, 0.4624, 0.4648, 0.4671, 0.4695, 0.4718, 0.4742, 0.4765, 0.4789,\n",
       "             0.4812, 0.4836, 0.4859, 0.4883, 0.4906, 0.4930, 0.4953, 0.4977, 0.5000,\n",
       "             0.5023, 0.5047, 0.5070, 0.5094, 0.5117, 0.5141, 0.5164, 0.5188, 0.5211,\n",
       "             0.5235, 0.5258, 0.5282, 0.5305, 0.5329, 0.5352, 0.5376, 0.5399, 0.5423,\n",
       "             0.5446, 0.5469, 0.5493, 0.5516, 0.5540, 0.5563, 0.5587, 0.5610, 0.5634,\n",
       "             0.5657, 0.5681, 0.5704, 0.5728, 0.5751, 0.5775, 0.5798, 0.5822, 0.5845,\n",
       "             0.5869, 0.5892, 0.5915, 0.5939, 0.5962, 0.5986, 0.6009, 0.6033, 0.6056,\n",
       "             0.6080, 0.6103, 0.6127, 0.6150, 0.6174, 0.6197, 0.6221, 0.6244, 0.6268,\n",
       "             0.6291, 0.6315, 0.6338, 0.6362, 0.6385, 0.6408, 0.6432, 0.6455, 0.6479,\n",
       "             0.6502, 0.6526, 0.6549, 0.6573, 0.6596, 0.6620, 0.6643, 0.6667, 0.6690,\n",
       "             0.6714, 0.6737, 0.6761, 0.6784, 0.6808, 0.6831, 0.6854, 0.6878, 0.6901,\n",
       "             0.6925, 0.6948, 0.6972, 0.6995, 0.7019, 0.7042, 0.7066, 0.7089, 0.7113,\n",
       "             0.7136, 0.7160, 0.7183, 0.7207, 0.7230, 0.7254, 0.7277, 0.7300, 0.7324,\n",
       "             0.7347, 0.7371, 0.7394, 0.7418, 0.7441, 0.7465, 0.7488, 0.7512, 0.7535,\n",
       "             0.7559, 0.7582, 0.7606, 0.7629, 0.7653, 0.7676, 0.7700, 0.7723, 0.7746,\n",
       "             0.7770, 0.7793, 0.7817, 0.7840, 0.7864, 0.7887, 0.7911, 0.7934, 0.7958,\n",
       "             0.7981, 0.8005, 0.8028, 0.8052, 0.8075, 0.8099, 0.8122, 0.8146, 0.8169,\n",
       "             0.8192, 0.8216, 0.8239, 0.8263, 0.8286, 0.8310, 0.8333, 0.8357, 0.8380,\n",
       "             0.8404, 0.8427, 0.8451, 0.8474, 0.8498, 0.8521, 0.8545, 0.8568, 0.8592,\n",
       "             0.8615, 0.8638, 0.8638, 0.8662, 0.8685, 0.8709, 0.8732, 0.8756, 0.8779,\n",
       "             0.8803, 0.8826, 0.8850, 0.8873, 0.8897, 0.8920, 0.8944, 0.8967, 0.8991,\n",
       "             0.9014, 0.9038, 0.9061, 0.9085, 0.9108, 0.9131, 0.9155, 0.9178, 0.9202,\n",
       "             0.9225, 0.9249, 0.9272, 0.9296, 0.9319, 0.9343, 0.9366, 0.9390, 0.9413,\n",
       "             0.9437, 0.9460, 0.9484, 0.9507, 0.9531, 0.9554, 0.9577, 0.9601, 0.9624,\n",
       "             0.9648, 0.9671, 0.9695, 0.9718, 0.9742, 0.9765, 0.9789, 0.9812, 0.9836,\n",
       "             0.9859, 0.9883, 0.9906, 0.9930, 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8186, 0.8401, 0.8532, 0.8586, 0.8624, 0.8670, 0.8716, 0.8716,\n",
       "             0.8739, 0.8762, 0.8786, 0.8793, 0.8832, 0.8862, 0.8870, 0.8878, 0.8885,\n",
       "             0.8909, 0.8916, 0.8932, 0.8939, 0.8947, 0.8955, 0.8978, 0.8985, 0.9001,\n",
       "             0.9008, 0.9016, 0.9024, 0.9032, 0.9039, 0.9047, 0.9055, 0.9062, 0.9070,\n",
       "             0.9078, 0.9085, 0.9093, 0.9108, 0.9116, 0.9124, 0.9131, 0.9139, 0.9147,\n",
       "             0.9154, 0.9162, 0.9170, 0.9178, 0.9185, 0.9193, 0.9201, 0.9208, 0.9216,\n",
       "             0.9224, 0.9231, 0.9239, 0.9247, 0.9247, 0.9254, 0.9262, 0.9270, 0.9277,\n",
       "             0.9285, 0.9293, 0.9301, 0.9308, 0.9316, 0.9324, 0.9331, 0.9331, 0.9339,\n",
       "             0.9347, 0.9347, 0.9354, 0.9354, 0.9362, 0.9370, 0.9377, 0.9385, 0.9393,\n",
       "             0.9400, 0.9408, 0.9408, 0.9416, 0.9424, 0.9431, 0.9439, 0.9447, 0.9454,\n",
       "             0.9462, 0.9470, 0.9477, 0.9485, 0.9493, 0.9500, 0.9500, 0.9508, 0.9516,\n",
       "             0.9523, 0.9531, 0.9539, 0.9547, 0.9554, 0.9562, 0.9562, 0.9570, 0.9570,\n",
       "             0.9577, 0.9585, 0.9593, 0.9593, 0.9600, 0.9608, 0.9616, 0.9623, 0.9623,\n",
       "             0.9631, 0.9639, 0.9639, 0.9646, 0.9646, 0.9654, 0.9654, 0.9662, 0.9662,\n",
       "             0.9669, 0.9677, 0.9677, 0.9685, 0.9693, 0.9700, 0.9708, 0.9716, 0.9723,\n",
       "             0.9723, 0.9723, 0.9723, 0.9731, 0.9731, 0.9739, 0.9739, 0.9739, 0.9746,\n",
       "             0.9754, 0.9762, 0.9769, 0.9777, 0.9777, 0.9777, 0.9785, 0.9792, 0.9792,\n",
       "             0.9800, 0.9800, 0.9800, 0.9800, 0.9808, 0.9816, 0.9816, 0.9816, 0.9816,\n",
       "             0.9823, 0.9831, 0.9831, 0.9839, 0.9839, 0.9839, 0.9846, 0.9854, 0.9854,\n",
       "             0.9854, 0.9862, 0.9862, 0.9862, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869,\n",
       "             0.9877, 0.9877, 0.9885, 0.9892, 0.9892, 0.9900, 0.9900, 0.9908, 0.9908,\n",
       "             0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9939, 0.9939, 0.9939, 0.9939, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9962, 0.9962, 0.9962, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9983e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9976e-01, 9.9974e-01,\n",
       "             9.9964e-01, 9.9963e-01, 9.9951e-01, 9.9945e-01, 9.9945e-01, 9.9935e-01,\n",
       "             9.9923e-01, 9.9919e-01, 9.9902e-01, 9.9871e-01, 9.9868e-01, 9.9836e-01,\n",
       "             9.9823e-01, 9.9813e-01, 9.9770e-01, 9.9756e-01, 9.9755e-01, 9.9745e-01,\n",
       "             9.9740e-01, 9.9723e-01, 9.9718e-01, 9.9571e-01, 9.9552e-01, 9.9525e-01,\n",
       "             9.9465e-01, 9.9414e-01, 9.9341e-01, 9.9312e-01, 9.9161e-01, 9.8989e-01,\n",
       "             9.8500e-01, 9.7933e-01, 9.7807e-01, 9.7780e-01, 9.7704e-01, 9.7687e-01,\n",
       "             9.7671e-01, 9.7489e-01, 9.7411e-01, 9.7014e-01, 9.5652e-01, 9.3381e-01,\n",
       "             9.2058e-01, 9.0211e-01, 9.0108e-01, 8.7081e-01, 8.1725e-01, 7.9698e-01,\n",
       "             7.7869e-01, 7.6121e-01, 7.5931e-01, 6.3183e-01, 6.2763e-01, 6.0226e-01,\n",
       "             5.9922e-01, 5.7625e-01, 5.4893e-01, 4.6870e-01, 3.8682e-01, 3.3430e-01,\n",
       "             2.6587e-01, 2.3184e-01, 2.2388e-01, 1.6525e-01, 1.5787e-01, 1.5490e-01,\n",
       "             1.4820e-01, 1.2276e-01, 9.6694e-02, 9.5059e-02, 9.2263e-02, 8.7023e-02,\n",
       "             8.3755e-02, 6.0597e-02, 5.5775e-02, 3.8864e-02, 3.1785e-02, 1.9801e-02,\n",
       "             1.5059e-02, 1.4587e-02, 1.3220e-02, 1.1064e-02, 9.1715e-03, 7.7426e-03,\n",
       "             6.6900e-03, 6.2740e-03, 5.9530e-03, 5.5283e-03, 4.4097e-03, 3.4117e-03,\n",
       "             2.8341e-03, 2.4723e-03, 2.1957e-03, 1.9126e-03, 1.4183e-03, 9.8915e-04,\n",
       "             9.4702e-04, 9.1659e-04, 7.1095e-04, 2.1565e-04, 1.9031e-04, 1.6361e-04,\n",
       "             1.6177e-04, 1.4522e-04, 8.7286e-05, 5.6438e-05, 3.0924e-05, 2.5593e-05,\n",
       "             1.5698e-05, 1.4155e-05, 1.1606e-05, 6.1676e-06, 4.5227e-06, 4.1515e-06,\n",
       "             3.0939e-06, 2.1084e-06, 2.0421e-06, 1.8623e-06, 1.4121e-06, 1.1870e-06,\n",
       "             7.6046e-07, 6.8865e-07, 6.3116e-07, 5.6541e-07, 5.1168e-07, 4.8938e-07,\n",
       "             4.3781e-07, 4.0455e-07, 3.2783e-07, 2.4400e-07, 2.3083e-07, 2.0907e-07,\n",
       "             1.9966e-07, 1.9737e-07, 1.8579e-07, 1.8156e-07, 1.4742e-07, 1.1687e-07,\n",
       "             4.1685e-08, 3.1734e-08, 3.1540e-08, 2.6676e-08, 2.5552e-08, 2.4750e-08,\n",
       "             2.2902e-08, 2.1493e-08, 1.9379e-08, 1.5286e-08, 1.4828e-08, 1.4338e-08,\n",
       "             1.2800e-08, 1.2437e-08, 1.1690e-08, 1.0627e-08, 1.0047e-08, 9.1258e-09,\n",
       "             8.4848e-09, 5.7627e-09, 5.1933e-09, 4.7925e-09, 3.1738e-09, 2.5335e-09,\n",
       "             2.3371e-09, 2.1251e-09, 1.9232e-09, 1.4874e-09, 1.4216e-09, 6.4406e-10,\n",
       "             6.3825e-10, 5.1651e-10, 4.4130e-10, 4.0154e-10, 2.6509e-10, 2.3836e-10,\n",
       "             1.9801e-10, 1.5922e-10, 1.2916e-10, 9.5864e-11, 7.2888e-11, 6.6003e-11,\n",
       "             4.9420e-11, 4.1216e-11, 3.4602e-11, 3.2743e-11, 2.3804e-11, 1.9427e-11,\n",
       "             1.8343e-11, 1.6911e-11, 1.5268e-11, 1.3827e-11, 1.0468e-11, 1.0075e-11,\n",
       "             9.6440e-12, 9.1679e-12, 8.8099e-12, 7.8208e-12, 6.2875e-12, 5.6992e-12,\n",
       "             5.2797e-12, 4.7906e-12, 4.5502e-12, 3.7896e-12, 3.7008e-12, 3.5768e-12,\n",
       "             3.3270e-12, 3.1581e-12, 2.7875e-12, 2.1376e-12, 2.0208e-12, 1.8128e-12,\n",
       "             1.7714e-12, 1.6897e-12, 1.3912e-12, 1.3550e-12, 1.0730e-12, 9.5366e-13,\n",
       "             8.4297e-13, 7.6734e-13, 7.1843e-13, 6.3308e-13, 5.2086e-13, 4.9181e-13,\n",
       "             4.8505e-13, 4.7719e-13, 4.6429e-13, 4.4985e-13, 4.3860e-13, 3.3400e-13,\n",
       "             3.2868e-13, 2.9545e-13, 2.8703e-13, 2.8246e-13, 1.9890e-13, 1.8460e-13,\n",
       "             1.6681e-13, 1.4148e-13, 1.1152e-13, 1.1098e-13, 1.0596e-13, 1.0452e-13,\n",
       "             8.2347e-14, 8.0416e-14, 7.7453e-14, 7.7029e-14, 7.1060e-14, 6.5644e-14,\n",
       "             4.2892e-14, 4.2087e-14, 3.6471e-14, 3.5912e-14, 3.0492e-14, 2.8846e-14,\n",
       "             1.9858e-14, 1.9621e-14, 1.9014e-14, 1.7793e-14, 1.6674e-14, 1.6246e-14,\n",
       "             1.5572e-14, 1.5357e-14, 1.4743e-14, 1.4386e-14, 1.2976e-14, 1.0954e-14,\n",
       "             1.0066e-14, 9.7916e-15, 9.6459e-15, 8.8657e-15, 7.7899e-15, 7.0457e-15,\n",
       "             5.7420e-15, 5.7150e-15, 5.1418e-15, 4.5681e-15, 3.9522e-15, 3.4629e-15,\n",
       "             3.0154e-15, 2.9569e-15, 2.8742e-15, 2.6627e-15, 2.3051e-15, 2.2116e-15,\n",
       "             2.0599e-15, 1.2656e-15, 1.2496e-15, 1.0807e-15, 1.0604e-15, 9.6877e-16,\n",
       "             9.6392e-16, 9.4964e-16, 9.4933e-16, 7.8621e-16, 4.3035e-16, 3.9284e-16,\n",
       "             3.6946e-16, 3.6926e-16, 3.5031e-16, 3.3584e-16, 3.2111e-16, 2.9884e-16,\n",
       "             2.7661e-16, 2.5454e-16, 2.3959e-16, 2.3078e-16, 2.2251e-16, 1.9490e-16,\n",
       "             1.8575e-16, 1.3731e-16, 1.3008e-16, 1.2842e-16, 1.2761e-16, 1.2558e-16,\n",
       "             1.2389e-16, 1.2233e-16, 1.1632e-16, 1.0896e-16, 1.0470e-16, 7.7551e-17,\n",
       "             6.9078e-17, 6.6597e-17, 5.7093e-17, 5.5656e-17, 5.4758e-17, 5.4668e-17,\n",
       "             4.8540e-17, 4.2877e-17, 4.1811e-17, 3.8922e-17, 2.8962e-17, 2.7671e-17,\n",
       "             2.5573e-17, 2.0899e-17, 2.0376e-17, 1.9077e-17, 1.9036e-17, 1.6082e-17,\n",
       "             1.4531e-17, 1.4358e-17, 1.3781e-17, 1.1066e-17, 9.8734e-18, 8.4517e-18,\n",
       "             7.8584e-18, 7.5280e-18, 6.2036e-18, 5.3584e-18, 5.2132e-18, 3.9178e-18,\n",
       "             3.8511e-18, 3.5073e-18, 3.3367e-18, 2.1820e-18, 1.6691e-18, 1.6634e-18,\n",
       "             1.6220e-18, 1.5090e-18, 1.4746e-18, 1.4079e-18, 1.3581e-18, 1.2166e-18,\n",
       "             1.1298e-18, 9.3600e-19, 8.4068e-19, 7.8086e-19, 7.5268e-19, 7.2171e-19,\n",
       "             4.9552e-19, 4.9504e-19, 4.8240e-19, 3.9881e-19, 3.5316e-19, 3.2979e-19,\n",
       "             3.0834e-19, 2.9671e-19, 2.5069e-19, 2.5025e-19, 2.1952e-19, 2.1730e-19,\n",
       "             2.1678e-19, 2.1312e-19, 2.0980e-19, 1.4753e-19, 1.4502e-19, 1.3246e-19,\n",
       "             1.3030e-19, 1.2738e-19, 1.2270e-19, 1.2116e-19, 1.0836e-19, 8.4622e-20,\n",
       "             6.7875e-20, 5.2717e-20, 4.6636e-20, 4.4379e-20, 4.2828e-20, 3.7838e-20,\n",
       "             2.6422e-20, 2.6278e-20, 2.5909e-20, 2.4370e-20, 1.6374e-20, 1.4715e-20,\n",
       "             1.4305e-20, 1.3538e-20, 1.2841e-20, 1.1437e-20, 9.9201e-21, 6.2303e-21,\n",
       "             5.9166e-21, 3.9974e-21, 3.0149e-21, 3.0004e-21, 2.7756e-21, 1.9820e-21,\n",
       "             1.9188e-21, 1.6848e-21, 1.6707e-21, 1.4803e-21, 1.1644e-21, 1.1491e-21,\n",
       "             1.0117e-21, 8.7080e-22, 8.4062e-22, 7.0499e-22, 6.8007e-22, 5.3013e-22,\n",
       "             4.0429e-22, 3.7061e-22, 3.6858e-22, 2.4738e-22, 2.0361e-22, 1.4081e-22,\n",
       "             1.2064e-22, 1.0967e-22, 8.8057e-23, 8.5541e-23, 5.6572e-23, 5.2924e-23,\n",
       "             4.9315e-23, 4.8629e-23, 4.5165e-23, 3.7112e-23, 3.2969e-23, 2.3918e-23,\n",
       "             1.6240e-23, 1.4177e-23, 1.2116e-23, 1.1576e-23, 8.4302e-24, 6.2491e-24,\n",
       "             4.9173e-24, 3.6494e-24, 3.3714e-24, 2.0862e-24, 1.9641e-24, 1.7407e-24,\n",
       "             1.5824e-24, 1.4275e-24, 4.6528e-25, 2.6746e-25, 2.1592e-25, 9.5673e-26,\n",
       "             8.7704e-26, 8.2062e-26, 6.2267e-26, 5.2338e-26, 4.2904e-26, 2.9957e-26,\n",
       "             1.0767e-26, 1.0560e-26, 9.8954e-27, 9.9251e-28, 9.4722e-28, 4.2497e-28,\n",
       "             3.6874e-28, 1.3693e-28, 9.3732e-29, 1.4583e-29, 9.5994e-30, 2.4634e-31,\n",
       "             1.4646e-32, 6.4459e-33, 5.5664e-35, 9.3537e-36])}},\n",
       "   {'fpr': np.float64(0.07276995305164319),\n",
       "    'tpr': np.float64(0.9846272098385856),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0047, 0.0047, 0.0070, 0.0094, 0.0094, 0.0094, 0.0117, 0.0141,\n",
       "             0.0141, 0.0141, 0.0164, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0235,\n",
       "             0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235,\n",
       "             0.0235, 0.0235, 0.0235, 0.0235, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258,\n",
       "             0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258,\n",
       "             0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258,\n",
       "             0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0282, 0.0282, 0.0282,\n",
       "             0.0282, 0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0329, 0.0329, 0.0352,\n",
       "             0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352,\n",
       "             0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0399,\n",
       "             0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399,\n",
       "             0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399,\n",
       "             0.0399, 0.0399, 0.0399, 0.0399, 0.0423, 0.0423, 0.0446, 0.0446, 0.0446,\n",
       "             0.0446, 0.0446, 0.0446, 0.0446, 0.0446, 0.0469, 0.0469, 0.0493, 0.0493,\n",
       "             0.0493, 0.0516, 0.0540, 0.0540, 0.0540, 0.0540, 0.0540, 0.0563, 0.0563,\n",
       "             0.0563, 0.0563, 0.0563, 0.0587, 0.0610, 0.0634, 0.0657, 0.0681, 0.0681,\n",
       "             0.0681, 0.0704, 0.0704, 0.0728, 0.0728, 0.0728, 0.0728, 0.0751, 0.0775,\n",
       "             0.0798, 0.0822, 0.0845, 0.0869, 0.0892, 0.0892, 0.0915, 0.0915, 0.0939,\n",
       "             0.0962, 0.0986, 0.1009, 0.1033, 0.1033, 0.1056, 0.1080, 0.1103, 0.1127,\n",
       "             0.1150, 0.1174, 0.1197, 0.1221, 0.1244, 0.1268, 0.1268, 0.1291, 0.1315,\n",
       "             0.1338, 0.1362, 0.1385, 0.1385, 0.1408, 0.1432, 0.1455, 0.1479, 0.1502,\n",
       "             0.1526, 0.1549, 0.1573, 0.1596, 0.1620, 0.1643, 0.1643, 0.1667, 0.1690,\n",
       "             0.1690, 0.1714, 0.1737, 0.1761, 0.1784, 0.1784, 0.1808, 0.1831, 0.1854,\n",
       "             0.1878, 0.1901, 0.1925, 0.1925, 0.1948, 0.1972, 0.1972, 0.1995, 0.2019,\n",
       "             0.2042, 0.2066, 0.2089, 0.2113, 0.2136, 0.2160, 0.2183, 0.2183, 0.2207,\n",
       "             0.2230, 0.2254, 0.2277, 0.2300, 0.2324, 0.2347, 0.2347, 0.2371, 0.2394,\n",
       "             0.2418, 0.2441, 0.2465, 0.2488, 0.2512, 0.2535, 0.2559, 0.2582, 0.2582,\n",
       "             0.2606, 0.2629, 0.2653, 0.2676, 0.2700, 0.2723, 0.2746, 0.2770, 0.2793,\n",
       "             0.2793, 0.2817, 0.2817, 0.2840, 0.2864, 0.2887, 0.2911, 0.2934, 0.2958,\n",
       "             0.2981, 0.3005, 0.3028, 0.3052, 0.3075, 0.3099, 0.3122, 0.3146, 0.3169,\n",
       "             0.3192, 0.3216, 0.3239, 0.3263, 0.3286, 0.3310, 0.3333, 0.3357, 0.3380,\n",
       "             0.3404, 0.3427, 0.3451, 0.3474, 0.3498, 0.3521, 0.3545, 0.3568, 0.3568,\n",
       "             0.3592, 0.3615, 0.3638, 0.3662, 0.3685, 0.3709, 0.3732, 0.3756, 0.3779,\n",
       "             0.3803, 0.3826, 0.3850, 0.3873, 0.3897, 0.3920, 0.3944, 0.3967, 0.3991,\n",
       "             0.4014, 0.4038, 0.4061, 0.4085, 0.4108, 0.4131, 0.4155, 0.4178, 0.4202,\n",
       "             0.4225, 0.4249, 0.4272, 0.4296, 0.4319, 0.4343, 0.4366, 0.4390, 0.4413,\n",
       "             0.4437, 0.4460, 0.4484, 0.4507, 0.4531, 0.4554, 0.4554, 0.4577, 0.4601,\n",
       "             0.4624, 0.4648, 0.4671, 0.4695, 0.4718, 0.4742, 0.4765, 0.4789, 0.4812,\n",
       "             0.4836, 0.4859, 0.4883, 0.4906, 0.4930, 0.4953, 0.4977, 0.5000, 0.5023,\n",
       "             0.5047, 0.5070, 0.5094, 0.5117, 0.5141, 0.5164, 0.5188, 0.5211, 0.5235,\n",
       "             0.5258, 0.5282, 0.5305, 0.5329, 0.5352, 0.5376, 0.5399, 0.5423, 0.5446,\n",
       "             0.5469, 0.5493, 0.5516, 0.5540, 0.5563, 0.5587, 0.5610, 0.5634, 0.5657,\n",
       "             0.5681, 0.5704, 0.5728, 0.5751, 0.5775, 0.5798, 0.5822, 0.5845, 0.5869,\n",
       "             0.5892, 0.5915, 0.5939, 0.5962, 0.5986, 0.6009, 0.6033, 0.6056, 0.6080,\n",
       "             0.6103, 0.6127, 0.6150, 0.6174, 0.6197, 0.6221, 0.6244, 0.6268, 0.6291,\n",
       "             0.6315, 0.6338, 0.6362, 0.6385, 0.6408, 0.6432, 0.6455, 0.6479, 0.6502,\n",
       "             0.6526, 0.6549, 0.6573, 0.6596, 0.6620, 0.6643, 0.6667, 0.6690, 0.6714,\n",
       "             0.6737, 0.6761, 0.6784, 0.6808, 0.6831, 0.6854, 0.6878, 0.6901, 0.6925,\n",
       "             0.6948, 0.6972, 0.6995, 0.7019, 0.7042, 0.7066, 0.7089, 0.7113, 0.7136,\n",
       "             0.7160, 0.7183, 0.7207, 0.7230, 0.7254, 0.7277, 0.7300, 0.7324, 0.7347,\n",
       "             0.7371, 0.7394, 0.7418, 0.7441, 0.7465, 0.7488, 0.7512, 0.7535, 0.7559,\n",
       "             0.7582, 0.7606, 0.7629, 0.7653, 0.7676, 0.7700, 0.7723, 0.7746, 0.7770,\n",
       "             0.7793, 0.7817, 0.7840, 0.7864, 0.7887, 0.7911, 0.7934, 0.7958, 0.7981,\n",
       "             0.8005, 0.8028, 0.8052, 0.8075, 0.8099, 0.8122, 0.8146, 0.8169, 0.8192,\n",
       "             0.8216, 0.8239, 0.8263, 0.8286, 0.8310, 0.8333, 0.8357, 0.8380, 0.8404,\n",
       "             0.8427, 0.8451, 0.8474, 0.8498, 0.8521, 0.8545, 0.8568, 0.8592, 0.8615,\n",
       "             0.8638, 0.8662, 0.8685, 0.8709, 0.8732, 0.8756, 0.8779, 0.8803, 0.8826,\n",
       "             0.8850, 0.8873, 0.8897, 0.8920, 0.8944, 0.8967, 0.8991, 0.9014, 0.9038,\n",
       "             0.9061, 0.9085, 0.9108, 0.9131, 0.9131, 0.9155, 0.9178, 0.9202, 0.9225,\n",
       "             0.9249, 0.9272, 0.9296, 0.9319, 0.9343, 0.9366, 0.9390, 0.9413, 0.9437,\n",
       "             0.9460, 0.9484, 0.9507, 0.9531, 0.9554, 0.9577, 0.9601, 0.9624, 0.9648,\n",
       "             0.9671, 0.9695, 0.9718, 0.9742, 0.9765, 0.9789, 0.9812, 0.9836, 0.9859,\n",
       "             0.9883, 0.9906, 0.9930, 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8271, 0.8440, 0.8555, 0.8601, 0.8632, 0.8693, 0.8724, 0.8747,\n",
       "             0.8793, 0.8816, 0.8847, 0.8870, 0.8885, 0.8893, 0.8909, 0.8924, 0.8939,\n",
       "             0.8947, 0.8955, 0.8962, 0.8978, 0.8985, 0.9001, 0.9008, 0.9032, 0.9039,\n",
       "             0.9055, 0.9062, 0.9070, 0.9078, 0.9085, 0.9093, 0.9101, 0.9108, 0.9116,\n",
       "             0.9124, 0.9131, 0.9139, 0.9147, 0.9154, 0.9162, 0.9170, 0.9178, 0.9185,\n",
       "             0.9193, 0.9201, 0.9208, 0.9216, 0.9224, 0.9239, 0.9247, 0.9262, 0.9270,\n",
       "             0.9277, 0.9285, 0.9293, 0.9301, 0.9308, 0.9316, 0.9316, 0.9324, 0.9331,\n",
       "             0.9339, 0.9339, 0.9347, 0.9354, 0.9362, 0.9370, 0.9370, 0.9377, 0.9377,\n",
       "             0.9385, 0.9393, 0.9400, 0.9408, 0.9416, 0.9424, 0.9431, 0.9439, 0.9447,\n",
       "             0.9447, 0.9454, 0.9462, 0.9470, 0.9477, 0.9485, 0.9493, 0.9500, 0.9500,\n",
       "             0.9508, 0.9516, 0.9523, 0.9531, 0.9539, 0.9547, 0.9554, 0.9562, 0.9570,\n",
       "             0.9577, 0.9585, 0.9593, 0.9600, 0.9608, 0.9616, 0.9623, 0.9631, 0.9639,\n",
       "             0.9646, 0.9654, 0.9662, 0.9669, 0.9669, 0.9677, 0.9677, 0.9685, 0.9693,\n",
       "             0.9700, 0.9708, 0.9716, 0.9723, 0.9731, 0.9731, 0.9739, 0.9739, 0.9746,\n",
       "             0.9754, 0.9754, 0.9754, 0.9762, 0.9769, 0.9777, 0.9785, 0.9785, 0.9792,\n",
       "             0.9800, 0.9808, 0.9816, 0.9816, 0.9816, 0.9816, 0.9816, 0.9816, 0.9823,\n",
       "             0.9831, 0.9831, 0.9839, 0.9839, 0.9846, 0.9854, 0.9862, 0.9862, 0.9862,\n",
       "             0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9869, 0.9869, 0.9877, 0.9877,\n",
       "             0.9877, 0.9877, 0.9877, 0.9877, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9892, 0.9892, 0.9892,\n",
       "             0.9892, 0.9892, 0.9892, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9908, 0.9908, 0.9908,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9939, 0.9939, 0.9939,\n",
       "             0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9978e-01, 9.9977e-01, 9.9973e-01, 9.9968e-01,\n",
       "             9.9967e-01, 9.9967e-01, 9.9966e-01, 9.9965e-01, 9.9959e-01, 9.9955e-01,\n",
       "             9.9950e-01, 9.9949e-01, 9.9940e-01, 9.9931e-01, 9.9918e-01, 9.9912e-01,\n",
       "             9.9896e-01, 9.9896e-01, 9.9867e-01, 9.9865e-01, 9.9809e-01, 9.9807e-01,\n",
       "             9.9806e-01, 9.9795e-01, 9.9788e-01, 9.9744e-01, 9.9660e-01, 9.9645e-01,\n",
       "             9.9621e-01, 9.9595e-01, 9.9553e-01, 9.9386e-01, 9.9173e-01, 9.8523e-01,\n",
       "             9.8202e-01, 9.8149e-01, 9.7821e-01, 9.7775e-01, 9.7117e-01, 9.6363e-01,\n",
       "             9.4835e-01, 9.4699e-01, 9.4654e-01, 9.3774e-01, 9.1976e-01, 9.1354e-01,\n",
       "             9.0646e-01, 8.9836e-01, 8.9496e-01, 8.9411e-01, 8.7785e-01, 8.4630e-01,\n",
       "             7.7996e-01, 7.7659e-01, 7.7157e-01, 7.6965e-01, 7.4021e-01, 6.9626e-01,\n",
       "             6.2342e-01, 6.1771e-01, 6.0917e-01, 5.6823e-01, 5.2106e-01, 3.7771e-01,\n",
       "             3.4802e-01, 3.3300e-01, 3.3015e-01, 3.1998e-01, 3.1528e-01, 3.0502e-01,\n",
       "             2.9738e-01, 2.4551e-01, 2.3807e-01, 2.2233e-01, 1.7427e-01, 1.2272e-01,\n",
       "             1.1038e-01, 5.6342e-02, 5.4858e-02, 4.3449e-02, 3.5636e-02, 3.4691e-02,\n",
       "             3.0382e-02, 2.8826e-02, 2.6094e-02, 2.1803e-02, 1.7840e-02, 1.6738e-02,\n",
       "             1.5134e-02, 1.5108e-02, 8.2219e-03, 7.0320e-03, 5.6971e-03, 5.5416e-03,\n",
       "             5.2436e-03, 4.6382e-03, 4.1264e-03, 3.9101e-03, 3.8122e-03, 3.7699e-03,\n",
       "             2.6305e-03, 2.4220e-03, 2.3514e-03, 2.2699e-03, 1.9616e-03, 1.6691e-03,\n",
       "             1.5898e-03, 8.3010e-04, 5.7416e-04, 4.6452e-04, 3.5216e-04, 3.0084e-04,\n",
       "             2.8553e-04, 2.4409e-04, 2.2655e-04, 1.9293e-04, 1.5088e-04, 1.5064e-04,\n",
       "             1.3285e-04, 1.1202e-04, 1.0463e-04, 9.3698e-05, 8.3542e-05, 7.9031e-05,\n",
       "             7.8751e-05, 7.7575e-05, 6.1541e-05, 5.6870e-05, 4.7973e-05, 3.7918e-05,\n",
       "             3.4880e-05, 3.2218e-05, 3.0557e-05, 2.9803e-05, 2.9194e-05, 2.8241e-05,\n",
       "             1.8353e-05, 1.6150e-05, 1.3756e-05, 1.0431e-05, 9.6026e-06, 8.8087e-06,\n",
       "             8.7168e-06, 5.7492e-06, 5.4997e-06, 4.2970e-06, 3.9747e-06, 3.6149e-06,\n",
       "             3.3008e-06, 3.1624e-06, 3.0765e-06, 2.9619e-06, 2.8338e-06, 2.7014e-06,\n",
       "             2.5844e-06, 2.0725e-06, 1.9742e-06, 1.9423e-06, 1.3932e-06, 9.9105e-07,\n",
       "             9.8601e-07, 9.4206e-07, 9.0906e-07, 8.3731e-07, 8.2949e-07, 7.9896e-07,\n",
       "             7.7252e-07, 6.8555e-07, 6.3819e-07, 4.5079e-07, 3.5628e-07, 3.4546e-07,\n",
       "             2.9436e-07, 2.5800e-07, 2.4343e-07, 2.0308e-07, 1.5827e-07, 1.4539e-07,\n",
       "             1.2745e-07, 1.0206e-07, 7.6093e-08, 6.0339e-08, 5.0444e-08, 4.9791e-08,\n",
       "             4.8653e-08, 4.1191e-08, 4.0231e-08, 3.6554e-08, 3.6331e-08, 3.2234e-08,\n",
       "             2.4750e-08, 2.0250e-08, 1.6233e-08, 1.6131e-08, 1.5279e-08, 1.4424e-08,\n",
       "             1.0390e-08, 1.0070e-08, 8.3191e-09, 8.2807e-09, 7.9156e-09, 6.1113e-09,\n",
       "             5.5527e-09, 5.3700e-09, 5.1810e-09, 4.3169e-09, 3.7063e-09, 3.6410e-09,\n",
       "             3.3529e-09, 3.0657e-09, 3.0217e-09, 2.3208e-09, 2.2831e-09, 1.8774e-09,\n",
       "             1.7600e-09, 1.2599e-09, 1.1080e-09, 1.0601e-09, 8.2774e-10, 6.8430e-10,\n",
       "             6.3901e-10, 5.7245e-10, 4.5292e-10, 3.7874e-10, 3.6458e-10, 3.6299e-10,\n",
       "             3.4419e-10, 3.2116e-10, 3.1632e-10, 3.0925e-10, 2.9171e-10, 2.3922e-10,\n",
       "             2.2793e-10, 2.1996e-10, 2.1409e-10, 1.8244e-10, 1.6728e-10, 1.4449e-10,\n",
       "             1.4196e-10, 1.3710e-10, 1.3686e-10, 1.3356e-10, 1.1528e-10, 1.0625e-10,\n",
       "             8.4071e-11, 7.9670e-11, 6.9968e-11, 6.4483e-11, 6.1787e-11, 5.2897e-11,\n",
       "             5.0304e-11, 4.4428e-11, 3.9974e-11, 3.5801e-11, 3.4274e-11, 2.9951e-11,\n",
       "             2.9443e-11, 2.8951e-11, 2.8778e-11, 2.4899e-11, 2.2679e-11, 2.0848e-11,\n",
       "             1.9368e-11, 1.8725e-11, 1.8180e-11, 1.5987e-11, 1.4382e-11, 1.3358e-11,\n",
       "             7.8459e-12, 7.3664e-12, 6.2400e-12, 5.7170e-12, 5.0982e-12, 4.3015e-12,\n",
       "             4.0116e-12, 3.9289e-12, 2.9161e-12, 2.5863e-12, 2.3551e-12, 1.9478e-12,\n",
       "             1.7573e-12, 1.7161e-12, 1.4480e-12, 1.3022e-12, 1.2431e-12, 1.2123e-12,\n",
       "             1.1723e-12, 1.0168e-12, 9.7705e-13, 7.0537e-13, 6.2491e-13, 5.7399e-13,\n",
       "             5.5545e-13, 4.5801e-13, 4.5279e-13, 4.4027e-13, 4.2228e-13, 3.9409e-13,\n",
       "             3.6924e-13, 3.6197e-13, 3.5494e-13, 3.3709e-13, 3.3560e-13, 3.0008e-13,\n",
       "             2.9353e-13, 2.8808e-13, 2.5147e-13, 2.2381e-13, 2.1059e-13, 2.0492e-13,\n",
       "             1.8283e-13, 1.7022e-13, 1.5149e-13, 1.4166e-13, 1.3540e-13, 1.1301e-13,\n",
       "             1.1143e-13, 9.9684e-14, 9.3515e-14, 9.2000e-14, 8.9735e-14, 7.2050e-14,\n",
       "             5.4722e-14, 4.6934e-14, 4.5957e-14, 4.5482e-14, 4.2360e-14, 3.6831e-14,\n",
       "             3.6818e-14, 3.1397e-14, 2.6555e-14, 2.4767e-14, 2.3210e-14, 1.9565e-14,\n",
       "             1.7751e-14, 1.6139e-14, 1.5291e-14, 1.4073e-14, 1.3155e-14, 1.2897e-14,\n",
       "             1.1975e-14, 1.0149e-14, 6.9785e-15, 6.2584e-15, 6.2051e-15, 5.4423e-15,\n",
       "             5.1725e-15, 5.0636e-15, 5.0358e-15, 4.3183e-15, 3.9698e-15, 3.2827e-15,\n",
       "             1.8369e-15, 1.4387e-15, 1.3437e-15, 9.8989e-16, 9.1238e-16, 8.1504e-16,\n",
       "             7.8785e-16, 7.7801e-16, 6.9509e-16, 6.3579e-16, 5.9686e-16, 5.0198e-16,\n",
       "             4.6433e-16, 4.4833e-16, 4.4058e-16, 4.0728e-16, 3.8004e-16, 3.6624e-16,\n",
       "             3.5910e-16, 3.5032e-16, 3.1470e-16, 3.1418e-16, 3.0656e-16, 3.0305e-16,\n",
       "             2.5430e-16, 2.1126e-16, 2.1058e-16, 1.9521e-16, 1.6962e-16, 1.6269e-16,\n",
       "             1.5891e-16, 1.4997e-16, 9.5263e-17, 8.6338e-17, 7.6783e-17, 4.6721e-17,\n",
       "             4.3921e-17, 3.8185e-17, 3.1955e-17, 3.0267e-17, 2.8424e-17, 2.7940e-17,\n",
       "             1.7651e-17, 1.7638e-17, 1.6366e-17, 1.4122e-17, 1.3667e-17, 1.3339e-17,\n",
       "             1.3210e-17, 1.0001e-17, 9.1072e-18, 9.1069e-18, 8.6226e-18, 7.0510e-18,\n",
       "             6.7924e-18, 6.5046e-18, 5.6466e-18, 3.8908e-18, 3.4630e-18, 3.3410e-18,\n",
       "             3.0747e-18, 1.7047e-18, 1.7011e-18, 1.5724e-18, 1.2554e-18, 1.1286e-18,\n",
       "             9.2386e-19, 8.0025e-19, 7.6256e-19, 7.2180e-19, 6.5545e-19, 5.6321e-19,\n",
       "             5.5490e-19, 5.0328e-19, 3.3406e-19, 3.1696e-19, 3.1646e-19, 2.6306e-19,\n",
       "             2.5647e-19, 2.3474e-19, 2.3469e-19, 1.6894e-19, 1.6376e-19, 1.4602e-19,\n",
       "             1.4245e-19, 1.1705e-19, 1.1431e-19, 1.1103e-19, 4.9519e-20, 4.6637e-20,\n",
       "             3.2611e-20, 3.2356e-20, 2.8307e-20, 1.0407e-20, 8.3917e-21, 2.4695e-21,\n",
       "             2.4321e-21, 2.3566e-21, 1.6219e-21, 1.3443e-21, 1.2630e-21, 1.0921e-21,\n",
       "             6.9458e-22, 6.3241e-22, 3.6317e-22, 1.4180e-22, 1.3524e-22, 1.3353e-22,\n",
       "             5.9034e-23, 5.5762e-23, 4.9788e-23, 4.5706e-23, 3.7641e-23, 2.0436e-23,\n",
       "             8.2239e-24, 2.6920e-24, 2.4897e-24, 2.0904e-24, 1.0973e-25, 2.2479e-26,\n",
       "             3.4082e-27, 3.4056e-27, 2.5821e-27, 2.0643e-30, 7.0655e-31, 3.2118e-31])}},\n",
       "   {'fpr': np.float64(0.0539906103286385),\n",
       "    'tpr': np.float64(0.9777094542659492),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047,\n",
       "             0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0094, 0.0094, 0.0117, 0.0117, 0.0117, 0.0117,\n",
       "             0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
       "             0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
       "             0.0141, 0.0141, 0.0141, 0.0141, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0188, 0.0188,\n",
       "             0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188,\n",
       "             0.0188, 0.0188, 0.0211, 0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0258,\n",
       "             0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258,\n",
       "             0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258, 0.0258,\n",
       "             0.0282, 0.0282, 0.0282, 0.0282, 0.0282, 0.0305, 0.0305, 0.0305, 0.0305,\n",
       "             0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0329, 0.0352,\n",
       "             0.0352, 0.0352, 0.0352, 0.0376, 0.0376, 0.0399, 0.0399, 0.0399, 0.0399,\n",
       "             0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0399, 0.0423, 0.0423, 0.0423,\n",
       "             0.0423, 0.0423, 0.0423, 0.0423, 0.0446, 0.0469, 0.0469, 0.0469, 0.0469,\n",
       "             0.0469, 0.0469, 0.0469, 0.0469, 0.0493, 0.0493, 0.0516, 0.0516, 0.0516,\n",
       "             0.0540, 0.0540, 0.0563, 0.0563, 0.0587, 0.0587, 0.0610, 0.0634, 0.0657,\n",
       "             0.0657, 0.0681, 0.0681, 0.0681, 0.0704, 0.0728, 0.0751, 0.0751, 0.0751,\n",
       "             0.0775, 0.0798, 0.0822, 0.0845, 0.0869, 0.0869, 0.0892, 0.0892, 0.0892,\n",
       "             0.0892, 0.0892, 0.0915, 0.0915, 0.0939, 0.0939, 0.0962, 0.0962, 0.0962,\n",
       "             0.0962, 0.0986, 0.0986, 0.0986, 0.1009, 0.1033, 0.1056, 0.1080, 0.1103,\n",
       "             0.1127, 0.1150, 0.1174, 0.1197, 0.1221, 0.1244, 0.1268, 0.1291, 0.1291,\n",
       "             0.1291, 0.1315, 0.1338, 0.1362, 0.1385, 0.1408, 0.1432, 0.1455, 0.1455,\n",
       "             0.1479, 0.1502, 0.1526, 0.1549, 0.1549, 0.1549, 0.1573, 0.1596, 0.1620,\n",
       "             0.1620, 0.1643, 0.1667, 0.1690, 0.1714, 0.1737, 0.1761, 0.1784, 0.1808,\n",
       "             0.1831, 0.1854, 0.1878, 0.1901, 0.1925, 0.1948, 0.1972, 0.1995, 0.2019,\n",
       "             0.2019, 0.2042, 0.2066, 0.2089, 0.2113, 0.2136, 0.2160, 0.2183, 0.2207,\n",
       "             0.2230, 0.2254, 0.2277, 0.2300, 0.2324, 0.2347, 0.2371, 0.2394, 0.2418,\n",
       "             0.2441, 0.2465, 0.2488, 0.2512, 0.2535, 0.2559, 0.2582, 0.2606, 0.2629,\n",
       "             0.2653, 0.2676, 0.2700, 0.2723, 0.2746, 0.2770, 0.2793, 0.2817, 0.2840,\n",
       "             0.2864, 0.2887, 0.2911, 0.2934, 0.2958, 0.2981, 0.3005, 0.3028, 0.3052,\n",
       "             0.3075, 0.3099, 0.3122, 0.3146, 0.3169, 0.3192, 0.3216, 0.3239, 0.3263,\n",
       "             0.3286, 0.3310, 0.3333, 0.3357, 0.3380, 0.3404, 0.3427, 0.3451, 0.3474,\n",
       "             0.3498, 0.3521, 0.3545, 0.3568, 0.3592, 0.3615, 0.3638, 0.3662, 0.3685,\n",
       "             0.3709, 0.3732, 0.3756, 0.3779, 0.3803, 0.3826, 0.3850, 0.3873, 0.3897,\n",
       "             0.3920, 0.3944, 0.3967, 0.3991, 0.4014, 0.4038, 0.4061, 0.4085, 0.4108,\n",
       "             0.4131, 0.4155, 0.4178, 0.4202, 0.4225, 0.4249, 0.4272, 0.4296, 0.4319,\n",
       "             0.4343, 0.4366, 0.4390, 0.4413, 0.4437, 0.4460, 0.4484, 0.4507, 0.4531,\n",
       "             0.4554, 0.4577, 0.4577, 0.4601, 0.4624, 0.4648, 0.4671, 0.4695, 0.4695,\n",
       "             0.4718, 0.4742, 0.4765, 0.4789, 0.4812, 0.4836, 0.4859, 0.4883, 0.4906,\n",
       "             0.4930, 0.4953, 0.4977, 0.5000, 0.5023, 0.5047, 0.5070, 0.5094, 0.5117,\n",
       "             0.5141, 0.5164, 0.5188, 0.5211, 0.5235, 0.5258, 0.5282, 0.5305, 0.5329,\n",
       "             0.5352, 0.5376, 0.5399, 0.5423, 0.5446, 0.5469, 0.5493, 0.5516, 0.5540,\n",
       "             0.5563, 0.5587, 0.5610, 0.5634, 0.5657, 0.5681, 0.5704, 0.5728, 0.5751,\n",
       "             0.5775, 0.5798, 0.5822, 0.5845, 0.5869, 0.5892, 0.5915, 0.5939, 0.5962,\n",
       "             0.5986, 0.6009, 0.6033, 0.6056, 0.6080, 0.6103, 0.6127, 0.6150, 0.6174,\n",
       "             0.6197, 0.6221, 0.6244, 0.6268, 0.6291, 0.6315, 0.6338, 0.6362, 0.6385,\n",
       "             0.6408, 0.6432, 0.6455, 0.6479, 0.6502, 0.6526, 0.6549, 0.6573, 0.6596,\n",
       "             0.6620, 0.6643, 0.6667, 0.6690, 0.6714, 0.6737, 0.6761, 0.6784, 0.6808,\n",
       "             0.6831, 0.6854, 0.6878, 0.6901, 0.6925, 0.6948, 0.6972, 0.6995, 0.7019,\n",
       "             0.7042, 0.7066, 0.7089, 0.7113, 0.7136, 0.7160, 0.7183, 0.7207, 0.7230,\n",
       "             0.7254, 0.7277, 0.7300, 0.7324, 0.7347, 0.7371, 0.7394, 0.7418, 0.7441,\n",
       "             0.7465, 0.7488, 0.7512, 0.7535, 0.7559, 0.7582, 0.7606, 0.7629, 0.7653,\n",
       "             0.7676, 0.7700, 0.7723, 0.7746, 0.7770, 0.7793, 0.7817, 0.7840, 0.7864,\n",
       "             0.7887, 0.7911, 0.7934, 0.7958, 0.7981, 0.8005, 0.8028, 0.8052, 0.8075,\n",
       "             0.8099, 0.8122, 0.8146, 0.8169, 0.8192, 0.8216, 0.8239, 0.8263, 0.8286,\n",
       "             0.8310, 0.8333, 0.8357, 0.8380, 0.8404, 0.8427, 0.8451, 0.8474, 0.8498,\n",
       "             0.8521, 0.8545, 0.8568, 0.8592, 0.8615, 0.8638, 0.8662, 0.8685, 0.8709,\n",
       "             0.8732, 0.8756, 0.8779, 0.8803, 0.8803, 0.8826, 0.8850, 0.8873, 0.8897,\n",
       "             0.8920, 0.8944, 0.8967, 0.8991, 0.9014, 0.9038, 0.9061, 0.9085, 0.9108,\n",
       "             0.9131, 0.9155, 0.9178, 0.9202, 0.9225, 0.9249, 0.9272, 0.9296, 0.9319,\n",
       "             0.9343, 0.9366, 0.9390, 0.9413, 0.9437, 0.9460, 0.9484, 0.9507, 0.9531,\n",
       "             0.9554, 0.9577, 0.9601, 0.9624, 0.9648, 0.9671, 0.9695, 0.9718, 0.9742,\n",
       "             0.9765, 0.9789, 0.9812, 0.9836, 0.9859, 0.9883, 0.9906, 0.9930, 0.9953,\n",
       "             0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8071, 0.8332, 0.8447, 0.8478, 0.8532, 0.8593, 0.8624, 0.8678,\n",
       "             0.8701, 0.8709, 0.8724, 0.8732, 0.8747, 0.8755, 0.8778, 0.8786, 0.8793,\n",
       "             0.8809, 0.8816, 0.8824, 0.8847, 0.8855, 0.8870, 0.8878, 0.8885, 0.8893,\n",
       "             0.8901, 0.8924, 0.8932, 0.8939, 0.8955, 0.8962, 0.8970, 0.8978, 0.8985,\n",
       "             0.8993, 0.9001, 0.9016, 0.9024, 0.9032, 0.9039, 0.9047, 0.9055, 0.9062,\n",
       "             0.9062, 0.9070, 0.9078, 0.9085, 0.9085, 0.9093, 0.9101, 0.9108, 0.9116,\n",
       "             0.9124, 0.9131, 0.9139, 0.9147, 0.9154, 0.9162, 0.9170, 0.9170, 0.9178,\n",
       "             0.9185, 0.9193, 0.9201, 0.9208, 0.9216, 0.9224, 0.9231, 0.9239, 0.9247,\n",
       "             0.9254, 0.9262, 0.9270, 0.9270, 0.9277, 0.9285, 0.9293, 0.9301, 0.9301,\n",
       "             0.9308, 0.9316, 0.9324, 0.9331, 0.9339, 0.9347, 0.9354, 0.9362, 0.9370,\n",
       "             0.9377, 0.9385, 0.9393, 0.9400, 0.9408, 0.9416, 0.9424, 0.9431, 0.9439,\n",
       "             0.9439, 0.9447, 0.9454, 0.9462, 0.9470, 0.9470, 0.9477, 0.9485, 0.9493,\n",
       "             0.9500, 0.9508, 0.9516, 0.9523, 0.9531, 0.9539, 0.9547, 0.9547, 0.9547,\n",
       "             0.9554, 0.9562, 0.9570, 0.9570, 0.9577, 0.9577, 0.9585, 0.9593, 0.9600,\n",
       "             0.9608, 0.9616, 0.9623, 0.9631, 0.9639, 0.9646, 0.9646, 0.9654, 0.9662,\n",
       "             0.9669, 0.9677, 0.9685, 0.9693, 0.9693, 0.9693, 0.9700, 0.9708, 0.9716,\n",
       "             0.9723, 0.9731, 0.9739, 0.9746, 0.9746, 0.9754, 0.9754, 0.9762, 0.9769,\n",
       "             0.9769, 0.9777, 0.9777, 0.9785, 0.9785, 0.9792, 0.9792, 0.9792, 0.9792,\n",
       "             0.9800, 0.9800, 0.9808, 0.9816, 0.9816, 0.9816, 0.9816, 0.9823, 0.9831,\n",
       "             0.9831, 0.9831, 0.9831, 0.9831, 0.9831, 0.9839, 0.9839, 0.9846, 0.9854,\n",
       "             0.9862, 0.9869, 0.9869, 0.9877, 0.9877, 0.9885, 0.9885, 0.9892, 0.9900,\n",
       "             0.9908, 0.9908, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931,\n",
       "             0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9982e-01, 9.9980e-01, 9.9979e-01, 9.9970e-01,\n",
       "             9.9966e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9964e-01, 9.9963e-01,\n",
       "             9.9960e-01, 9.9960e-01, 9.9950e-01, 9.9944e-01, 9.9939e-01, 9.9930e-01,\n",
       "             9.9893e-01, 9.9887e-01, 9.9883e-01, 9.9876e-01, 9.9874e-01, 9.9814e-01,\n",
       "             9.9804e-01, 9.9800e-01, 9.9706e-01, 9.9704e-01, 9.9678e-01, 9.9651e-01,\n",
       "             9.9650e-01, 9.9647e-01, 9.9647e-01, 9.9636e-01, 9.9551e-01, 9.9380e-01,\n",
       "             9.9231e-01, 9.9196e-01, 9.9138e-01, 9.9091e-01, 9.8987e-01, 9.8557e-01,\n",
       "             9.8447e-01, 9.8257e-01, 9.8211e-01, 9.7767e-01, 9.7394e-01, 9.7174e-01,\n",
       "             9.6942e-01, 9.6845e-01, 9.6526e-01, 9.5936e-01, 9.5271e-01, 9.4391e-01,\n",
       "             9.4174e-01, 9.3958e-01, 9.3720e-01, 9.2710e-01, 9.2638e-01, 9.2477e-01,\n",
       "             9.2329e-01, 8.7633e-01, 8.6723e-01, 8.6358e-01, 8.2858e-01, 8.2697e-01,\n",
       "             7.3198e-01, 6.9609e-01, 6.4418e-01, 5.7532e-01, 5.2047e-01, 4.8049e-01,\n",
       "             4.5164e-01, 4.2210e-01, 4.0993e-01, 3.7845e-01, 2.7766e-01, 2.6375e-01,\n",
       "             1.8115e-01, 1.2301e-01, 1.1615e-01, 9.4067e-02, 8.7113e-02, 8.0159e-02,\n",
       "             7.0866e-02, 6.2951e-02, 3.9356e-02, 3.5106e-02, 2.5092e-02, 2.4815e-02,\n",
       "             1.5807e-02, 1.4632e-02, 1.1266e-02, 8.8513e-03, 7.0387e-03, 6.6975e-03,\n",
       "             5.9628e-03, 5.6851e-03, 3.1687e-03, 3.0087e-03, 2.5331e-03, 2.5244e-03,\n",
       "             2.1231e-03, 1.1992e-03, 1.0855e-03, 8.2371e-04, 7.6759e-04, 5.3282e-04,\n",
       "             5.0159e-04, 4.1323e-04, 3.0768e-04, 2.5907e-04, 1.4388e-04, 1.1237e-04,\n",
       "             9.5341e-05, 8.7241e-05, 5.5757e-05, 4.9368e-05, 4.8167e-05, 4.2379e-05,\n",
       "             4.2344e-05, 3.3678e-05, 2.8419e-05, 1.9741e-05, 1.7061e-05, 1.6623e-05,\n",
       "             1.1084e-05, 1.0788e-05, 1.0110e-05, 8.6917e-06, 5.8445e-06, 5.4866e-06,\n",
       "             5.2554e-06, 4.9289e-06, 4.4415e-06, 4.1270e-06, 2.9997e-06, 1.2739e-06,\n",
       "             1.2191e-06, 1.1835e-06, 8.5681e-07, 7.3577e-07, 6.1682e-07, 5.5635e-07,\n",
       "             5.5205e-07, 4.8147e-07, 3.8072e-07, 3.2373e-07, 3.0009e-07, 2.4832e-07,\n",
       "             1.6218e-07, 1.4238e-07, 1.4084e-07, 1.2045e-07, 1.1178e-07, 1.0938e-07,\n",
       "             6.3773e-08, 4.4471e-08, 4.1263e-08, 3.9468e-08, 3.6048e-08, 3.0202e-08,\n",
       "             2.2424e-08, 1.9037e-08, 1.7579e-08, 1.7028e-08, 1.6715e-08, 1.2643e-08,\n",
       "             1.2521e-08, 9.7019e-09, 9.1840e-09, 7.0241e-09, 4.9576e-09, 4.4957e-09,\n",
       "             3.9362e-09, 3.8064e-09, 3.2964e-09, 2.3341e-09, 2.1645e-09, 2.0090e-09,\n",
       "             1.7770e-09, 1.5077e-09, 1.5026e-09, 1.4465e-09, 1.4312e-09, 1.3308e-09,\n",
       "             1.1028e-09, 1.0250e-09, 9.1411e-10, 7.0494e-10, 6.0697e-10, 5.7062e-10,\n",
       "             5.3038e-10, 4.3411e-10, 4.2657e-10, 4.0628e-10, 3.9962e-10, 3.9400e-10,\n",
       "             3.8159e-10, 3.4578e-10, 2.7936e-10, 2.7617e-10, 2.6883e-10, 2.0261e-10,\n",
       "             1.8012e-10, 1.2557e-10, 1.2533e-10, 7.3927e-11, 6.5500e-11, 5.9397e-11,\n",
       "             5.9160e-11, 5.1788e-11, 5.1475e-11, 3.8930e-11, 3.8210e-11, 3.1465e-11,\n",
       "             3.1229e-11, 2.5655e-11, 2.5628e-11, 2.2578e-11, 1.9228e-11, 1.8593e-11,\n",
       "             1.6529e-11, 1.4821e-11, 1.2043e-11, 1.1746e-11, 1.1731e-11, 9.9474e-12,\n",
       "             9.2424e-12, 8.9244e-12, 8.6132e-12, 8.4144e-12, 7.9045e-12, 7.5963e-12,\n",
       "             6.5582e-12, 6.4643e-12, 5.1249e-12, 4.9597e-12, 4.8825e-12, 3.1445e-12,\n",
       "             2.6690e-12, 2.6529e-12, 2.0549e-12, 1.4284e-12, 1.2669e-12, 1.2625e-12,\n",
       "             1.2047e-12, 9.1987e-13, 9.0419e-13, 8.6549e-13, 8.5451e-13, 8.0110e-13,\n",
       "             7.7534e-13, 7.0434e-13, 6.7513e-13, 4.8787e-13, 3.8314e-13, 3.6025e-13,\n",
       "             3.3384e-13, 2.9853e-13, 2.7037e-13, 2.6951e-13, 2.6690e-13, 2.3173e-13,\n",
       "             2.0103e-13, 1.7929e-13, 1.6792e-13, 1.4157e-13, 1.2349e-13, 1.2077e-13,\n",
       "             1.2022e-13, 9.0112e-14, 8.8930e-14, 8.5277e-14, 8.5125e-14, 7.8862e-14,\n",
       "             6.7156e-14, 5.7784e-14, 5.7244e-14, 4.3783e-14, 3.8302e-14, 3.2809e-14,\n",
       "             3.2410e-14, 3.0861e-14, 2.9872e-14, 2.8805e-14, 2.3773e-14, 2.1202e-14,\n",
       "             1.8174e-14, 1.7979e-14, 1.7338e-14, 1.6832e-14, 1.6537e-14, 1.6138e-14,\n",
       "             1.3787e-14, 1.3781e-14, 1.3366e-14, 1.3030e-14, 1.1324e-14, 1.1292e-14,\n",
       "             1.0913e-14, 1.0129e-14, 9.7190e-15, 8.8585e-15, 8.4892e-15, 7.8544e-15,\n",
       "             7.4010e-15, 7.1422e-15, 7.0910e-15, 6.3611e-15, 5.8569e-15, 5.8513e-15,\n",
       "             5.7606e-15, 5.7107e-15, 5.6879e-15, 5.5840e-15, 4.2156e-15, 4.1766e-15,\n",
       "             3.9548e-15, 3.5405e-15, 3.4110e-15, 3.3348e-15, 3.1820e-15, 3.0826e-15,\n",
       "             2.9442e-15, 2.8620e-15, 2.7417e-15, 2.2418e-15, 2.1326e-15, 1.9163e-15,\n",
       "             1.6889e-15, 1.3262e-15, 1.2637e-15, 1.1304e-15, 8.4861e-16, 8.3471e-16,\n",
       "             6.1580e-16, 5.5789e-16, 5.0830e-16, 4.4413e-16, 4.2130e-16, 3.8457e-16,\n",
       "             3.1427e-16, 2.7502e-16, 2.4478e-16, 2.3492e-16, 2.2018e-16, 1.9719e-16,\n",
       "             1.5981e-16, 1.5650e-16, 1.4414e-16, 1.2394e-16, 1.1948e-16, 1.0364e-16,\n",
       "             9.9244e-17, 9.5945e-17, 8.2161e-17, 7.8870e-17, 7.8363e-17, 7.3932e-17,\n",
       "             6.9753e-17, 6.2346e-17, 5.1404e-17, 4.5494e-17, 4.5270e-17, 3.9104e-17,\n",
       "             3.5444e-17, 3.3307e-17, 3.3106e-17, 2.8381e-17, 2.8176e-17, 2.7244e-17,\n",
       "             2.0693e-17, 1.6964e-17, 1.4996e-17, 1.3922e-17, 1.1944e-17, 1.0849e-17,\n",
       "             1.0618e-17, 8.6682e-18, 8.1218e-18, 7.8195e-18, 7.8038e-18, 7.4757e-18,\n",
       "             7.2212e-18, 6.5690e-18, 6.5070e-18, 5.8574e-18, 5.4149e-18, 4.6311e-18,\n",
       "             4.4232e-18, 3.2255e-18, 2.7387e-18, 2.6088e-18, 2.3748e-18, 2.2598e-18,\n",
       "             2.2270e-18, 2.1375e-18, 2.0028e-18, 1.6927e-18, 1.1633e-18, 9.9048e-19,\n",
       "             9.2010e-19, 8.0989e-19, 7.9125e-19, 7.8636e-19, 7.6593e-19, 7.5494e-19,\n",
       "             6.9693e-19, 6.4054e-19, 5.5425e-19, 5.0987e-19, 4.8123e-19, 4.7981e-19,\n",
       "             4.5171e-19, 4.2140e-19, 2.5680e-19, 2.0464e-19, 1.9379e-19, 1.9100e-19,\n",
       "             1.7666e-19, 1.3942e-19, 1.0316e-19, 8.6409e-20, 8.5480e-20, 8.0382e-20,\n",
       "             7.3154e-20, 6.2398e-20, 5.3390e-20, 4.5015e-20, 4.4381e-20, 4.3598e-20,\n",
       "             3.5215e-20, 2.6407e-20, 2.0919e-20, 2.0159e-20, 1.7555e-20, 1.7400e-20,\n",
       "             1.6856e-20, 1.6787e-20, 1.4949e-20, 1.1774e-20, 1.1746e-20, 9.5499e-21,\n",
       "             8.5211e-21, 8.1900e-21, 5.8338e-21, 5.7431e-21, 5.5700e-21, 5.2724e-21,\n",
       "             4.6618e-21, 4.1569e-21, 3.7532e-21, 2.6796e-21, 1.5699e-21, 1.4454e-21,\n",
       "             1.0687e-21, 9.6155e-22, 7.1479e-22, 4.1651e-22, 3.1450e-22, 1.7018e-22,\n",
       "             1.4631e-22, 8.8332e-23, 6.7691e-23, 5.3608e-23, 4.9134e-23, 3.5431e-23,\n",
       "             2.4137e-23, 2.3134e-23, 2.1334e-23, 1.0074e-23, 3.3593e-24, 2.6409e-24,\n",
       "             1.3791e-24, 1.2443e-24, 1.2290e-24, 6.3655e-25, 6.1039e-25, 5.5275e-25,\n",
       "             1.8635e-25, 1.7021e-25, 1.2413e-25, 9.2144e-26, 6.6360e-26, 5.2343e-26,\n",
       "             4.6644e-26, 1.7723e-26, 4.2515e-27, 1.7795e-27, 1.1232e-27, 6.1601e-28,\n",
       "             5.7012e-28, 4.8941e-28, 1.7415e-28, 1.2169e-28, 1.7908e-31, 1.3360e-31,\n",
       "             7.8801e-32, 6.4531e-32, 2.2588e-32, 3.5656e-36, 8.9891e-38])}},\n",
       "   {'fpr': np.float64(0.1431924882629108),\n",
       "    'tpr': np.float64(0.9861644888547272),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0047, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
       "             0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0117, 0.0117, 0.0117,\n",
       "             0.0117, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188,\n",
       "             0.0188, 0.0188, 0.0188, 0.0211, 0.0211, 0.0211, 0.0211, 0.0211, 0.0235,\n",
       "             0.0235, 0.0235, 0.0235, 0.0235, 0.0235, 0.0258, 0.0258, 0.0282, 0.0282,\n",
       "             0.0282, 0.0282, 0.0282, 0.0282, 0.0282, 0.0305, 0.0305, 0.0305, 0.0305,\n",
       "             0.0305, 0.0305, 0.0305, 0.0305, 0.0305, 0.0329, 0.0329, 0.0329, 0.0329,\n",
       "             0.0329, 0.0329, 0.0329, 0.0329, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352,\n",
       "             0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0399, 0.0399, 0.0399,\n",
       "             0.0399, 0.0423, 0.0423, 0.0446, 0.0446, 0.0469, 0.0493, 0.0516, 0.0516,\n",
       "             0.0516, 0.0540, 0.0540, 0.0540, 0.0563, 0.0563, 0.0563, 0.0587, 0.0610,\n",
       "             0.0610, 0.0634, 0.0657, 0.0681, 0.0681, 0.0681, 0.0681, 0.0704, 0.0704,\n",
       "             0.0704, 0.0704, 0.0728, 0.0751, 0.0775, 0.0798, 0.0798, 0.0822, 0.0845,\n",
       "             0.0845, 0.0869, 0.0892, 0.0915, 0.0939, 0.0939, 0.0962, 0.0986, 0.0986,\n",
       "             0.0986, 0.0986, 0.1009, 0.1033, 0.1033, 0.1033, 0.1056, 0.1080, 0.1103,\n",
       "             0.1127, 0.1150, 0.1174, 0.1197, 0.1221, 0.1221, 0.1244, 0.1268, 0.1291,\n",
       "             0.1315, 0.1338, 0.1362, 0.1362, 0.1385, 0.1408, 0.1408, 0.1432, 0.1455,\n",
       "             0.1479, 0.1502, 0.1526, 0.1549, 0.1573, 0.1596, 0.1620, 0.1643, 0.1667,\n",
       "             0.1690, 0.1714, 0.1737, 0.1761, 0.1761, 0.1784, 0.1808, 0.1831, 0.1854,\n",
       "             0.1878, 0.1878, 0.1901, 0.1925, 0.1948, 0.1972, 0.1995, 0.2019, 0.2042,\n",
       "             0.2066, 0.2089, 0.2113, 0.2113, 0.2136, 0.2160, 0.2183, 0.2207, 0.2207,\n",
       "             0.2207, 0.2207, 0.2207, 0.2230, 0.2254, 0.2277, 0.2300, 0.2324, 0.2347,\n",
       "             0.2371, 0.2371, 0.2394, 0.2418, 0.2441, 0.2441, 0.2465, 0.2488, 0.2488,\n",
       "             0.2512, 0.2535, 0.2559, 0.2582, 0.2606, 0.2629, 0.2653, 0.2676, 0.2700,\n",
       "             0.2723, 0.2746, 0.2770, 0.2793, 0.2817, 0.2817, 0.2817, 0.2840, 0.2864,\n",
       "             0.2887, 0.2911, 0.2934, 0.2958, 0.2981, 0.3005, 0.3028, 0.3052, 0.3075,\n",
       "             0.3099, 0.3122, 0.3122, 0.3146, 0.3169, 0.3192, 0.3216, 0.3239, 0.3263,\n",
       "             0.3286, 0.3310, 0.3333, 0.3333, 0.3357, 0.3380, 0.3404, 0.3427, 0.3451,\n",
       "             0.3474, 0.3498, 0.3521, 0.3545, 0.3568, 0.3592, 0.3615, 0.3638, 0.3662,\n",
       "             0.3685, 0.3709, 0.3732, 0.3756, 0.3779, 0.3803, 0.3826, 0.3850, 0.3873,\n",
       "             0.3897, 0.3920, 0.3944, 0.3967, 0.3991, 0.4014, 0.4038, 0.4061, 0.4085,\n",
       "             0.4108, 0.4108, 0.4131, 0.4155, 0.4178, 0.4202, 0.4225, 0.4249, 0.4272,\n",
       "             0.4296, 0.4319, 0.4343, 0.4366, 0.4390, 0.4413, 0.4437, 0.4460, 0.4484,\n",
       "             0.4507, 0.4507, 0.4531, 0.4554, 0.4577, 0.4601, 0.4624, 0.4648, 0.4671,\n",
       "             0.4695, 0.4718, 0.4742, 0.4765, 0.4789, 0.4812, 0.4836, 0.4859, 0.4883,\n",
       "             0.4906, 0.4930, 0.4953, 0.4977, 0.5000, 0.5023, 0.5047, 0.5070, 0.5094,\n",
       "             0.5117, 0.5141, 0.5164, 0.5188, 0.5211, 0.5235, 0.5258, 0.5282, 0.5305,\n",
       "             0.5329, 0.5352, 0.5376, 0.5399, 0.5423, 0.5446, 0.5469, 0.5493, 0.5516,\n",
       "             0.5540, 0.5563, 0.5587, 0.5610, 0.5634, 0.5657, 0.5681, 0.5704, 0.5728,\n",
       "             0.5751, 0.5775, 0.5798, 0.5822, 0.5845, 0.5869, 0.5892, 0.5915, 0.5939,\n",
       "             0.5962, 0.5986, 0.6009, 0.6033, 0.6056, 0.6080, 0.6103, 0.6127, 0.6150,\n",
       "             0.6174, 0.6197, 0.6221, 0.6244, 0.6268, 0.6291, 0.6315, 0.6338, 0.6362,\n",
       "             0.6385, 0.6408, 0.6432, 0.6455, 0.6479, 0.6502, 0.6526, 0.6549, 0.6573,\n",
       "             0.6596, 0.6620, 0.6643, 0.6667, 0.6690, 0.6714, 0.6737, 0.6761, 0.6784,\n",
       "             0.6808, 0.6831, 0.6854, 0.6878, 0.6901, 0.6925, 0.6948, 0.6972, 0.6995,\n",
       "             0.7019, 0.7042, 0.7066, 0.7089, 0.7113, 0.7136, 0.7160, 0.7183, 0.7207,\n",
       "             0.7230, 0.7254, 0.7277, 0.7277, 0.7300, 0.7324, 0.7347, 0.7371, 0.7394,\n",
       "             0.7418, 0.7441, 0.7465, 0.7488, 0.7512, 0.7535, 0.7559, 0.7582, 0.7606,\n",
       "             0.7629, 0.7653, 0.7676, 0.7700, 0.7723, 0.7746, 0.7770, 0.7793, 0.7817,\n",
       "             0.7840, 0.7864, 0.7887, 0.7911, 0.7934, 0.7958, 0.7981, 0.8005, 0.8028,\n",
       "             0.8052, 0.8075, 0.8099, 0.8122, 0.8146, 0.8169, 0.8192, 0.8216, 0.8239,\n",
       "             0.8263, 0.8286, 0.8310, 0.8333, 0.8357, 0.8380, 0.8404, 0.8427, 0.8451,\n",
       "             0.8474, 0.8498, 0.8521, 0.8545, 0.8568, 0.8592, 0.8615, 0.8638, 0.8662,\n",
       "             0.8685, 0.8709, 0.8732, 0.8756, 0.8779, 0.8803, 0.8826, 0.8850, 0.8873,\n",
       "             0.8897, 0.8920, 0.8944, 0.8967, 0.8991, 0.9014, 0.9038, 0.9061, 0.9085,\n",
       "             0.9108, 0.9131, 0.9155, 0.9178, 0.9202, 0.9225, 0.9249, 0.9272, 0.9296,\n",
       "             0.9319, 0.9343, 0.9366, 0.9390, 0.9413, 0.9413, 0.9437, 0.9460, 0.9484,\n",
       "             0.9507, 0.9531, 0.9554, 0.9577, 0.9601, 0.9624, 0.9648, 0.9671, 0.9695,\n",
       "             0.9718, 0.9742, 0.9765, 0.9789, 0.9812, 0.9836, 0.9859, 0.9883, 0.9906,\n",
       "             0.9930, 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8347, 0.8632, 0.8716, 0.8770, 0.8801, 0.8847, 0.8893, 0.8916,\n",
       "             0.8947, 0.8993, 0.9001, 0.9008, 0.9039, 0.9047, 0.9085, 0.9093, 0.9101,\n",
       "             0.9108, 0.9108, 0.9116, 0.9124, 0.9131, 0.9139, 0.9154, 0.9162, 0.9170,\n",
       "             0.9178, 0.9185, 0.9193, 0.9201, 0.9208, 0.9216, 0.9224, 0.9231, 0.9239,\n",
       "             0.9247, 0.9254, 0.9254, 0.9270, 0.9277, 0.9285, 0.9293, 0.9301, 0.9308,\n",
       "             0.9316, 0.9324, 0.9331, 0.9331, 0.9339, 0.9347, 0.9354, 0.9362, 0.9362,\n",
       "             0.9370, 0.9377, 0.9385, 0.9393, 0.9400, 0.9400, 0.9408, 0.9408, 0.9416,\n",
       "             0.9424, 0.9431, 0.9439, 0.9447, 0.9454, 0.9454, 0.9462, 0.9470, 0.9477,\n",
       "             0.9485, 0.9493, 0.9500, 0.9508, 0.9516, 0.9516, 0.9523, 0.9531, 0.9539,\n",
       "             0.9547, 0.9554, 0.9562, 0.9570, 0.9570, 0.9577, 0.9585, 0.9593, 0.9600,\n",
       "             0.9600, 0.9608, 0.9616, 0.9623, 0.9631, 0.9639, 0.9639, 0.9646, 0.9654,\n",
       "             0.9662, 0.9662, 0.9669, 0.9669, 0.9677, 0.9677, 0.9677, 0.9677, 0.9685,\n",
       "             0.9693, 0.9693, 0.9700, 0.9708, 0.9708, 0.9716, 0.9723, 0.9723, 0.9723,\n",
       "             0.9731, 0.9731, 0.9731, 0.9731, 0.9739, 0.9746, 0.9754, 0.9754, 0.9762,\n",
       "             0.9769, 0.9777, 0.9777, 0.9777, 0.9777, 0.9777, 0.9785, 0.9785, 0.9785,\n",
       "             0.9792, 0.9792, 0.9792, 0.9792, 0.9792, 0.9800, 0.9800, 0.9800, 0.9808,\n",
       "             0.9816, 0.9823, 0.9823, 0.9823, 0.9831, 0.9839, 0.9839, 0.9839, 0.9839,\n",
       "             0.9839, 0.9839, 0.9839, 0.9839, 0.9839, 0.9846, 0.9846, 0.9846, 0.9846,\n",
       "             0.9846, 0.9846, 0.9846, 0.9854, 0.9854, 0.9854, 0.9862, 0.9862, 0.9862,\n",
       "             0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862,\n",
       "             0.9862, 0.9862, 0.9862, 0.9862, 0.9869, 0.9869, 0.9869, 0.9869, 0.9869,\n",
       "             0.9869, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877,\n",
       "             0.9877, 0.9877, 0.9877, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9892,\n",
       "             0.9900, 0.9908, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9939,\n",
       "             0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939,\n",
       "             0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9946, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9976e-01,\n",
       "             9.9975e-01, 9.9968e-01, 9.9966e-01, 9.9962e-01, 9.9961e-01, 9.9960e-01,\n",
       "             9.9957e-01, 9.9947e-01, 9.9937e-01, 9.9937e-01, 9.9936e-01, 9.9930e-01,\n",
       "             9.9927e-01, 9.9917e-01, 9.9917e-01, 9.9914e-01, 9.9904e-01, 9.9896e-01,\n",
       "             9.9872e-01, 9.9870e-01, 9.9855e-01, 9.9818e-01, 9.9806e-01, 9.9805e-01,\n",
       "             9.9804e-01, 9.9781e-01, 9.9769e-01, 9.9764e-01, 9.9752e-01, 9.9676e-01,\n",
       "             9.9633e-01, 9.9604e-01, 9.9535e-01, 9.9490e-01, 9.9443e-01, 9.9283e-01,\n",
       "             9.9188e-01, 9.9046e-01, 9.9042e-01, 9.8996e-01, 9.8880e-01, 9.8646e-01,\n",
       "             9.8305e-01, 9.8236e-01, 9.8109e-01, 9.8102e-01, 9.7988e-01, 9.7955e-01,\n",
       "             9.6949e-01, 9.6779e-01, 9.6302e-01, 9.6152e-01, 9.5749e-01, 9.5664e-01,\n",
       "             9.5476e-01, 9.5020e-01, 9.4708e-01, 9.3023e-01, 9.2721e-01, 9.2360e-01,\n",
       "             9.2108e-01, 8.9632e-01, 8.7903e-01, 7.8621e-01, 7.5360e-01, 7.2829e-01,\n",
       "             6.9849e-01, 6.8852e-01, 6.6425e-01, 6.5161e-01, 6.4098e-01, 6.3894e-01,\n",
       "             6.3462e-01, 6.2185e-01, 6.0414e-01, 5.9945e-01, 5.8092e-01, 5.5075e-01,\n",
       "             5.1224e-01, 5.0617e-01, 4.9922e-01, 4.8128e-01, 4.4098e-01, 4.3878e-01,\n",
       "             4.2124e-01, 4.1667e-01, 4.1140e-01, 4.0241e-01, 3.6414e-01, 3.5866e-01,\n",
       "             3.2543e-01, 3.1872e-01, 3.1746e-01, 3.0974e-01, 2.9643e-01, 2.6229e-01,\n",
       "             2.5564e-01, 2.5332e-01, 2.2784e-01, 2.2607e-01, 2.1926e-01, 1.8151e-01,\n",
       "             1.6383e-01, 1.6075e-01, 1.5165e-01, 1.4324e-01, 1.3186e-01, 1.3093e-01,\n",
       "             1.1822e-01, 9.9726e-02, 9.8074e-02, 9.3488e-02, 9.1745e-02, 8.2598e-02,\n",
       "             6.5749e-02, 6.1529e-02, 6.0132e-02, 5.8815e-02, 5.8085e-02, 5.7411e-02,\n",
       "             5.5951e-02, 5.5645e-02, 4.8730e-02, 4.7729e-02, 4.7501e-02, 4.2749e-02,\n",
       "             4.2723e-02, 3.8978e-02, 3.8664e-02, 3.2524e-02, 2.9286e-02, 2.8425e-02,\n",
       "             2.7149e-02, 2.3172e-02, 1.9206e-02, 1.5611e-02, 1.4680e-02, 1.3840e-02,\n",
       "             1.2283e-02, 1.2003e-02, 1.1854e-02, 1.0222e-02, 1.0138e-02, 9.1347e-03,\n",
       "             8.8849e-03, 7.9476e-03, 7.5900e-03, 6.9882e-03, 6.8075e-03, 6.1455e-03,\n",
       "             6.1390e-03, 5.5941e-03, 5.3429e-03, 3.9734e-03, 3.9197e-03, 3.1954e-03,\n",
       "             3.1440e-03, 3.1086e-03, 2.9953e-03, 2.6442e-03, 2.4758e-03, 2.0324e-03,\n",
       "             2.0037e-03, 1.9752e-03, 1.9149e-03, 1.8663e-03, 1.5238e-03, 1.3836e-03,\n",
       "             1.0667e-03, 8.5879e-04, 8.0762e-04, 7.7160e-04, 7.4906e-04, 6.9624e-04,\n",
       "             4.6246e-04, 4.5676e-04, 4.0696e-04, 3.9355e-04, 3.6776e-04, 3.5759e-04,\n",
       "             3.4559e-04, 3.4073e-04, 3.3232e-04, 2.7889e-04, 2.2976e-04, 1.8757e-04,\n",
       "             1.7882e-04, 1.7613e-04, 1.7297e-04, 1.6861e-04, 1.6727e-04, 1.6553e-04,\n",
       "             1.5650e-04, 1.4186e-04, 1.3258e-04, 1.2776e-04, 1.2661e-04, 1.1771e-04,\n",
       "             1.0367e-04, 9.8278e-05, 9.7876e-05, 9.5829e-05, 9.1890e-05, 8.6131e-05,\n",
       "             7.7817e-05, 7.4353e-05, 7.0215e-05, 6.0078e-05, 5.9150e-05, 5.5290e-05,\n",
       "             5.0200e-05, 4.7703e-05, 4.6898e-05, 4.6287e-05, 4.5582e-05, 4.4188e-05,\n",
       "             4.2256e-05, 3.9220e-05, 3.4553e-05, 3.1110e-05, 2.7717e-05, 2.7607e-05,\n",
       "             2.5022e-05, 2.2626e-05, 2.2220e-05, 1.7318e-05, 1.6863e-05, 1.4304e-05,\n",
       "             1.1409e-05, 1.0687e-05, 1.0655e-05, 1.0144e-05, 8.8685e-06, 8.7420e-06,\n",
       "             6.2953e-06, 5.8095e-06, 5.5552e-06, 5.3752e-06, 4.6937e-06, 4.6761e-06,\n",
       "             4.2897e-06, 4.0160e-06, 3.8189e-06, 3.6435e-06, 3.5518e-06, 2.9291e-06,\n",
       "             2.6651e-06, 2.6067e-06, 2.5579e-06, 2.5192e-06, 2.3501e-06, 2.2024e-06,\n",
       "             1.8462e-06, 1.8201e-06, 1.7366e-06, 1.6193e-06, 1.5777e-06, 1.5289e-06,\n",
       "             1.5017e-06, 1.4061e-06, 1.2543e-06, 1.0473e-06, 9.8377e-07, 8.9544e-07,\n",
       "             8.6310e-07, 7.6148e-07, 7.3857e-07, 7.2199e-07, 6.8082e-07, 6.5153e-07,\n",
       "             5.9524e-07, 5.6561e-07, 5.2394e-07, 4.3713e-07, 4.2857e-07, 4.0573e-07,\n",
       "             3.6921e-07, 3.6903e-07, 3.4816e-07, 3.1516e-07, 2.8763e-07, 2.6126e-07,\n",
       "             2.3078e-07, 2.2583e-07, 2.2455e-07, 2.0462e-07, 1.8586e-07, 1.8102e-07,\n",
       "             1.6540e-07, 1.6187e-07, 1.6175e-07, 1.6174e-07, 1.6112e-07, 1.5239e-07,\n",
       "             1.3263e-07, 1.3138e-07, 1.2603e-07, 1.2527e-07, 1.0416e-07, 1.0187e-07,\n",
       "             9.6135e-08, 9.5996e-08, 8.4852e-08, 8.1120e-08, 7.0544e-08, 6.8196e-08,\n",
       "             6.0238e-08, 5.6080e-08, 5.3704e-08, 5.1012e-08, 4.1909e-08, 4.1757e-08,\n",
       "             3.9772e-08, 3.3840e-08, 3.0428e-08, 3.0328e-08, 2.7804e-08, 2.7039e-08,\n",
       "             2.3597e-08, 2.0472e-08, 1.8403e-08, 1.8294e-08, 1.5533e-08, 1.5375e-08,\n",
       "             1.4641e-08, 1.3595e-08, 1.3058e-08, 1.2103e-08, 1.1757e-08, 1.1012e-08,\n",
       "             1.0610e-08, 9.9520e-09, 8.6563e-09, 8.6351e-09, 8.2928e-09, 7.6647e-09,\n",
       "             6.5856e-09, 5.9201e-09, 5.6080e-09, 5.3209e-09, 3.5350e-09, 3.1223e-09,\n",
       "             2.6852e-09, 2.6494e-09, 2.5801e-09, 2.5272e-09, 1.8839e-09, 1.8149e-09,\n",
       "             1.6240e-09, 1.6139e-09, 1.5599e-09, 1.3814e-09, 1.3578e-09, 1.2716e-09,\n",
       "             1.2131e-09, 1.1887e-09, 1.1724e-09, 1.1478e-09, 1.1391e-09, 1.0625e-09,\n",
       "             1.0045e-09, 9.0388e-10, 8.7335e-10, 7.6797e-10, 6.6112e-10, 5.4250e-10,\n",
       "             5.2690e-10, 5.0984e-10, 5.0868e-10, 4.7670e-10, 4.1678e-10, 4.0765e-10,\n",
       "             3.6782e-10, 3.6585e-10, 3.6090e-10, 2.7973e-10, 2.7116e-10, 2.6802e-10,\n",
       "             2.4488e-10, 2.4103e-10, 2.2297e-10, 2.2221e-10, 2.2119e-10, 2.0946e-10,\n",
       "             1.8648e-10, 1.7940e-10, 1.6375e-10, 1.5966e-10, 1.5200e-10, 1.4919e-10,\n",
       "             1.3917e-10, 1.3538e-10, 1.3405e-10, 1.2850e-10, 1.2573e-10, 1.2072e-10,\n",
       "             1.1469e-10, 1.0949e-10, 1.0686e-10, 1.0585e-10, 9.6116e-11, 9.1580e-11,\n",
       "             7.7349e-11, 6.7029e-11, 6.2816e-11, 6.1612e-11, 5.8881e-11, 4.4531e-11,\n",
       "             3.9645e-11, 3.7333e-11, 3.7001e-11, 3.5062e-11, 3.0463e-11, 3.0190e-11,\n",
       "             2.8892e-11, 2.4472e-11, 2.3125e-11, 1.8944e-11, 1.7256e-11, 1.5026e-11,\n",
       "             1.5010e-11, 1.4099e-11, 1.3621e-11, 1.3204e-11, 9.6079e-12, 8.9334e-12,\n",
       "             7.8751e-12, 7.5899e-12, 3.1986e-12, 2.9614e-12, 2.7925e-12, 2.6151e-12,\n",
       "             1.5993e-12, 1.5948e-12, 7.3570e-13, 6.7708e-13, 5.6545e-13, 1.7292e-13,\n",
       "             1.3602e-13, 9.4890e-14, 9.0045e-14, 7.2002e-14, 3.9598e-14, 2.9704e-14,\n",
       "             1.3633e-14, 1.3537e-14, 5.1317e-15, 1.1043e-15, 1.0570e-15, 4.1291e-16,\n",
       "             2.8698e-16, 2.7887e-16, 2.6432e-16, 1.1262e-16, 8.5370e-17, 2.6261e-17,\n",
       "             2.1379e-17, 1.2720e-17, 3.0318e-18, 2.8227e-18, 5.8601e-19, 9.2137e-20,\n",
       "             1.9733e-20])}},\n",
       "   {'fpr': np.float64(0.06807511737089202),\n",
       "    'tpr': np.float64(0.9853958493466565),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
       "             0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0094,\n",
       "             0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0141, 0.0141, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0188, 0.0188, 0.0188, 0.0211, 0.0211, 0.0211, 0.0211, 0.0211, 0.0211,\n",
       "             0.0211, 0.0211, 0.0235, 0.0258, 0.0258, 0.0282, 0.0282, 0.0305, 0.0329,\n",
       "             0.0329, 0.0329, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352,\n",
       "             0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0376, 0.0399,\n",
       "             0.0399, 0.0399, 0.0423, 0.0423, 0.0423, 0.0423, 0.0423, 0.0423, 0.0446,\n",
       "             0.0469, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0516, 0.0540, 0.0563,\n",
       "             0.0587, 0.0610, 0.0634, 0.0634, 0.0657, 0.0681, 0.0681, 0.0681, 0.0704,\n",
       "             0.0728, 0.0751, 0.0751, 0.0775, 0.0798, 0.0798, 0.0822, 0.0845, 0.0869,\n",
       "             0.0892, 0.0915, 0.0939, 0.0962, 0.0986, 0.0986, 0.0986, 0.1009, 0.1033,\n",
       "             0.1056, 0.1080, 0.1103, 0.1127, 0.1150, 0.1150, 0.1150, 0.1174, 0.1174,\n",
       "             0.1197, 0.1221, 0.1221, 0.1244, 0.1268, 0.1291, 0.1315, 0.1315, 0.1338,\n",
       "             0.1362, 0.1385, 0.1408, 0.1432, 0.1455, 0.1455, 0.1479, 0.1502, 0.1526,\n",
       "             0.1549, 0.1573, 0.1596, 0.1620, 0.1643, 0.1667, 0.1690, 0.1714, 0.1737,\n",
       "             0.1761, 0.1761, 0.1761, 0.1784, 0.1808, 0.1831, 0.1854, 0.1878, 0.1901,\n",
       "             0.1925, 0.1925, 0.1948, 0.1972, 0.1995, 0.2019, 0.2042, 0.2066, 0.2089,\n",
       "             0.2113, 0.2136, 0.2160, 0.2183, 0.2207, 0.2230, 0.2254, 0.2277, 0.2300,\n",
       "             0.2324, 0.2347, 0.2371, 0.2394, 0.2418, 0.2441, 0.2465, 0.2488, 0.2512,\n",
       "             0.2535, 0.2535, 0.2559, 0.2582, 0.2606, 0.2629, 0.2653, 0.2676, 0.2700,\n",
       "             0.2723, 0.2746, 0.2770, 0.2793, 0.2817, 0.2840, 0.2864, 0.2887, 0.2911,\n",
       "             0.2934, 0.2958, 0.2958, 0.2981, 0.3005, 0.3028, 0.3052, 0.3075, 0.3099,\n",
       "             0.3122, 0.3146, 0.3169, 0.3192, 0.3192, 0.3216, 0.3239, 0.3263, 0.3286,\n",
       "             0.3310, 0.3333, 0.3357, 0.3380, 0.3404, 0.3427, 0.3451, 0.3474, 0.3498,\n",
       "             0.3521, 0.3545, 0.3568, 0.3592, 0.3615, 0.3638, 0.3662, 0.3685, 0.3709,\n",
       "             0.3732, 0.3756, 0.3779, 0.3803, 0.3826, 0.3850, 0.3873, 0.3897, 0.3920,\n",
       "             0.3944, 0.3967, 0.3991, 0.4014, 0.4038, 0.4061, 0.4085, 0.4108, 0.4131,\n",
       "             0.4155, 0.4178, 0.4202, 0.4225, 0.4249, 0.4272, 0.4296, 0.4319, 0.4343,\n",
       "             0.4366, 0.4390, 0.4413, 0.4437, 0.4460, 0.4484, 0.4507, 0.4531, 0.4554,\n",
       "             0.4577, 0.4601, 0.4624, 0.4648, 0.4671, 0.4695, 0.4718, 0.4742, 0.4765,\n",
       "             0.4789, 0.4812, 0.4836, 0.4859, 0.4883, 0.4906, 0.4930, 0.4953, 0.4977,\n",
       "             0.5000, 0.5023, 0.5047, 0.5070, 0.5094, 0.5117, 0.5141, 0.5164, 0.5188,\n",
       "             0.5211, 0.5235, 0.5258, 0.5282, 0.5305, 0.5329, 0.5352, 0.5376, 0.5399,\n",
       "             0.5423, 0.5446, 0.5469, 0.5493, 0.5516, 0.5540, 0.5563, 0.5587, 0.5610,\n",
       "             0.5634, 0.5657, 0.5681, 0.5704, 0.5728, 0.5751, 0.5775, 0.5798, 0.5822,\n",
       "             0.5845, 0.5869, 0.5892, 0.5915, 0.5939, 0.5962, 0.5986, 0.6009, 0.6033,\n",
       "             0.6056, 0.6080, 0.6103, 0.6127, 0.6150, 0.6174, 0.6197, 0.6221, 0.6244,\n",
       "             0.6268, 0.6291, 0.6315, 0.6338, 0.6362, 0.6385, 0.6408, 0.6432, 0.6455,\n",
       "             0.6479, 0.6502, 0.6526, 0.6549, 0.6573, 0.6596, 0.6620, 0.6643, 0.6667,\n",
       "             0.6690, 0.6690, 0.6714, 0.6737, 0.6761, 0.6784, 0.6808, 0.6831, 0.6854,\n",
       "             0.6878, 0.6901, 0.6925, 0.6948, 0.6972, 0.6995, 0.7019, 0.7042, 0.7066,\n",
       "             0.7089, 0.7113, 0.7136, 0.7160, 0.7183, 0.7207, 0.7230, 0.7254, 0.7277,\n",
       "             0.7300, 0.7324, 0.7347, 0.7371, 0.7394, 0.7418, 0.7441, 0.7465, 0.7488,\n",
       "             0.7512, 0.7535, 0.7559, 0.7582, 0.7606, 0.7629, 0.7653, 0.7676, 0.7700,\n",
       "             0.7723, 0.7746, 0.7770, 0.7793, 0.7817, 0.7840, 0.7864, 0.7887, 0.7911,\n",
       "             0.7934, 0.7958, 0.7981, 0.8005, 0.8028, 0.8052, 0.8075, 0.8099, 0.8122,\n",
       "             0.8146, 0.8169, 0.8192, 0.8216, 0.8239, 0.8239, 0.8263, 0.8286, 0.8310,\n",
       "             0.8333, 0.8357, 0.8380, 0.8404, 0.8427, 0.8451, 0.8474, 0.8498, 0.8521,\n",
       "             0.8545, 0.8568, 0.8592, 0.8615, 0.8638, 0.8662, 0.8685, 0.8709, 0.8732,\n",
       "             0.8756, 0.8779, 0.8803, 0.8826, 0.8850, 0.8873, 0.8897, 0.8920, 0.8944,\n",
       "             0.8967, 0.8991, 0.9014, 0.9038, 0.9061, 0.9085, 0.9108, 0.9131, 0.9155,\n",
       "             0.9178, 0.9202, 0.9225, 0.9249, 0.9272, 0.9296, 0.9319, 0.9343, 0.9366,\n",
       "             0.9390, 0.9413, 0.9437, 0.9460, 0.9484, 0.9507, 0.9531, 0.9554, 0.9577,\n",
       "             0.9601, 0.9624, 0.9648, 0.9671, 0.9695, 0.9718, 0.9742, 0.9765, 0.9789,\n",
       "             0.9812, 0.9836, 0.9859, 0.9883, 0.9906, 0.9930, 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8839, 0.8970, 0.9039, 0.9085, 0.9108, 0.9131, 0.9154, 0.9178,\n",
       "             0.9193, 0.9208, 0.9216, 0.9224, 0.9247, 0.9262, 0.9277, 0.9293, 0.9293,\n",
       "             0.9293, 0.9301, 0.9308, 0.9316, 0.9324, 0.9324, 0.9331, 0.9331, 0.9339,\n",
       "             0.9347, 0.9354, 0.9362, 0.9370, 0.9377, 0.9385, 0.9393, 0.9400, 0.9408,\n",
       "             0.9424, 0.9431, 0.9439, 0.9447, 0.9454, 0.9462, 0.9470, 0.9477, 0.9485,\n",
       "             0.9493, 0.9500, 0.9508, 0.9516, 0.9523, 0.9531, 0.9539, 0.9547, 0.9554,\n",
       "             0.9554, 0.9562, 0.9570, 0.9570, 0.9577, 0.9585, 0.9593, 0.9600, 0.9608,\n",
       "             0.9616, 0.9623, 0.9623, 0.9623, 0.9631, 0.9631, 0.9639, 0.9639, 0.9639,\n",
       "             0.9646, 0.9654, 0.9654, 0.9662, 0.9669, 0.9677, 0.9685, 0.9693, 0.9700,\n",
       "             0.9700, 0.9708, 0.9716, 0.9723, 0.9731, 0.9739, 0.9746, 0.9754, 0.9754,\n",
       "             0.9762, 0.9769, 0.9769, 0.9777, 0.9785, 0.9792, 0.9800, 0.9808, 0.9808,\n",
       "             0.9808, 0.9808, 0.9816, 0.9823, 0.9831, 0.9839, 0.9839, 0.9839, 0.9839,\n",
       "             0.9839, 0.9839, 0.9839, 0.9846, 0.9846, 0.9846, 0.9854, 0.9862, 0.9862,\n",
       "             0.9862, 0.9862, 0.9869, 0.9869, 0.9869, 0.9877, 0.9877, 0.9877, 0.9877,\n",
       "             0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9885, 0.9892, 0.9892, 0.9892,\n",
       "             0.9892, 0.9892, 0.9892, 0.9892, 0.9892, 0.9900, 0.9908, 0.9908, 0.9915,\n",
       "             0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9939, 0.9939, 0.9939, 0.9939,\n",
       "             0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939, 0.9939,\n",
       "             0.9939, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962, 0.9962,\n",
       "             0.9962, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9989e-01, 9.9987e-01, 9.9981e-01, 9.9976e-01, 9.9975e-01, 9.9968e-01,\n",
       "             9.9964e-01, 9.9959e-01, 9.9949e-01, 9.9940e-01, 9.9936e-01, 9.9932e-01,\n",
       "             9.9928e-01, 9.9927e-01, 9.9924e-01, 9.9903e-01, 9.9887e-01, 9.9880e-01,\n",
       "             9.9870e-01, 9.9869e-01, 9.9862e-01, 9.9819e-01, 9.9812e-01, 9.9744e-01,\n",
       "             9.9715e-01, 9.9699e-01, 9.9691e-01, 9.9446e-01, 9.9319e-01, 9.9315e-01,\n",
       "             9.9260e-01, 9.9233e-01, 9.9125e-01, 9.8690e-01, 9.8508e-01, 9.8104e-01,\n",
       "             9.7987e-01, 9.7966e-01, 9.7753e-01, 9.7598e-01, 9.6882e-01, 9.5933e-01,\n",
       "             9.5320e-01, 9.3191e-01, 9.2670e-01, 9.2553e-01, 9.2149e-01, 9.1734e-01,\n",
       "             9.1025e-01, 9.0480e-01, 8.8628e-01, 8.0646e-01, 7.9511e-01, 7.8626e-01,\n",
       "             7.7278e-01, 7.5689e-01, 7.1409e-01, 7.0542e-01, 6.8539e-01, 6.0728e-01,\n",
       "             6.0178e-01, 4.9653e-01, 4.4999e-01, 3.2694e-01, 2.8625e-01, 2.2236e-01,\n",
       "             1.8678e-01, 1.5624e-01, 1.4737e-01, 1.4535e-01, 1.2603e-01, 9.6948e-02,\n",
       "             7.8778e-02, 4.3823e-02, 4.2641e-02, 4.0247e-02, 3.7649e-02, 3.5330e-02,\n",
       "             3.3277e-02, 3.1725e-02, 2.2026e-02, 2.0316e-02, 1.7651e-02, 1.6013e-02,\n",
       "             1.5490e-02, 1.3550e-02, 1.3331e-02, 1.3194e-02, 1.2263e-02, 1.0559e-02,\n",
       "             8.7300e-03, 8.4354e-03, 4.8285e-03, 4.6538e-03, 4.5993e-03, 3.4116e-03,\n",
       "             2.4829e-03, 2.4094e-03, 1.8671e-03, 1.7125e-03, 1.2475e-03, 9.6153e-04,\n",
       "             9.2610e-04, 8.7601e-04, 8.3110e-04, 6.3662e-04, 5.2549e-04, 4.8126e-04,\n",
       "             4.0281e-04, 4.0121e-04, 3.4256e-04, 2.7960e-04, 2.5220e-04, 2.4372e-04,\n",
       "             1.9451e-04, 1.8122e-04, 1.5933e-04, 1.4694e-04, 1.4634e-04, 1.4432e-04,\n",
       "             1.3622e-04, 1.1530e-04, 1.1332e-04, 1.0519e-04, 8.9515e-05, 8.1732e-05,\n",
       "             6.6440e-05, 6.4303e-05, 3.8955e-05, 3.8175e-05, 3.7739e-05, 3.7442e-05,\n",
       "             3.4585e-05, 3.2445e-05, 2.9113e-05, 1.7296e-05, 1.1006e-05, 1.0275e-05,\n",
       "             9.6094e-06, 8.8639e-06, 8.2840e-06, 8.0114e-06, 7.6162e-06, 6.1154e-06,\n",
       "             5.9965e-06, 5.6743e-06, 5.2950e-06, 3.9249e-06, 3.7593e-06, 3.3693e-06,\n",
       "             3.0920e-06, 2.5895e-06, 2.3694e-06, 2.1699e-06, 2.1184e-06, 1.5729e-06,\n",
       "             1.4672e-06, 1.3312e-06, 1.2261e-06, 1.0475e-06, 1.0385e-06, 1.0135e-06,\n",
       "             7.6568e-07, 7.1985e-07, 7.1953e-07, 6.6145e-07, 5.3927e-07, 4.9498e-07,\n",
       "             4.7541e-07, 4.6434e-07, 4.4982e-07, 4.4348e-07, 3.5033e-07, 3.1571e-07,\n",
       "             3.0863e-07, 3.0238e-07, 2.9428e-07, 2.6053e-07, 1.5263e-07, 1.0726e-07,\n",
       "             1.0171e-07, 9.4620e-08, 9.0523e-08, 8.5130e-08, 5.5754e-08, 5.4903e-08,\n",
       "             5.1630e-08, 5.1034e-08, 4.7160e-08, 4.3909e-08, 4.2787e-08, 4.1405e-08,\n",
       "             4.0088e-08, 3.9438e-08, 3.8510e-08, 3.5783e-08, 3.2549e-08, 2.5676e-08,\n",
       "             2.1691e-08, 1.8980e-08, 1.6763e-08, 1.4618e-08, 1.2150e-08, 1.1589e-08,\n",
       "             1.0687e-08, 9.6768e-09, 9.1120e-09, 8.2459e-09, 7.5477e-09, 5.7820e-09,\n",
       "             4.9058e-09, 4.9031e-09, 4.5275e-09, 3.8417e-09, 3.6277e-09, 3.5729e-09,\n",
       "             3.0494e-09, 3.0481e-09, 2.9567e-09, 2.6413e-09, 2.4929e-09, 2.4089e-09,\n",
       "             1.9061e-09, 1.7327e-09, 1.7159e-09, 1.6976e-09, 1.6762e-09, 1.6496e-09,\n",
       "             1.5389e-09, 1.3577e-09, 1.3401e-09, 1.1978e-09, 1.1274e-09, 1.1156e-09,\n",
       "             9.9790e-10, 9.7071e-10, 9.5735e-10, 6.0959e-10, 5.9494e-10, 5.2021e-10,\n",
       "             5.1683e-10, 4.8438e-10, 4.3523e-10, 3.6623e-10, 3.1466e-10, 2.7434e-10,\n",
       "             2.7322e-10, 2.5636e-10, 2.5533e-10, 2.5504e-10, 2.4188e-10, 2.3193e-10,\n",
       "             2.2134e-10, 2.0835e-10, 2.0771e-10, 1.9378e-10, 1.8132e-10, 1.3238e-10,\n",
       "             9.3725e-11, 8.8273e-11, 7.1196e-11, 6.7784e-11, 6.5883e-11, 5.5598e-11,\n",
       "             5.0259e-11, 4.6943e-11, 4.4344e-11, 3.5581e-11, 3.3596e-11, 3.0988e-11,\n",
       "             3.0921e-11, 2.9204e-11, 2.8136e-11, 2.7953e-11, 2.6553e-11, 2.5013e-11,\n",
       "             2.4994e-11, 2.3493e-11, 2.2344e-11, 2.0188e-11, 1.7678e-11, 1.6369e-11,\n",
       "             1.6206e-11, 1.5944e-11, 1.4341e-11, 1.3791e-11, 1.3067e-11, 1.2757e-11,\n",
       "             1.2150e-11, 1.1853e-11, 1.1125e-11, 1.0407e-11, 1.0350e-11, 1.0186e-11,\n",
       "             9.0316e-12, 8.6520e-12, 8.0966e-12, 6.6686e-12, 5.5019e-12, 4.8993e-12,\n",
       "             4.5115e-12, 4.2522e-12, 4.1530e-12, 4.1007e-12, 3.8188e-12, 3.5282e-12,\n",
       "             3.5126e-12, 2.9333e-12, 2.6047e-12, 2.5782e-12, 2.2317e-12, 2.1285e-12,\n",
       "             2.0778e-12, 1.6783e-12, 1.6077e-12, 1.5854e-12, 1.4216e-12, 1.3952e-12,\n",
       "             1.1912e-12, 9.5501e-13, 9.5388e-13, 9.0933e-13, 8.9868e-13, 8.8993e-13,\n",
       "             8.2180e-13, 7.8317e-13, 6.4842e-13, 3.7784e-13, 3.6941e-13, 3.5000e-13,\n",
       "             3.3732e-13, 3.0054e-13, 2.9529e-13, 2.7471e-13, 2.4078e-13, 2.0251e-13,\n",
       "             1.9865e-13, 1.8719e-13, 1.8174e-13, 1.7954e-13, 1.7483e-13, 1.6805e-13,\n",
       "             1.5490e-13, 1.3639e-13, 1.3075e-13, 1.2892e-13, 1.0830e-13, 1.0675e-13,\n",
       "             9.7596e-14, 8.4908e-14, 7.3951e-14, 6.1969e-14, 5.5521e-14, 5.2928e-14,\n",
       "             3.5321e-14, 3.2359e-14, 3.0997e-14, 2.7442e-14, 2.6570e-14, 2.5418e-14,\n",
       "             2.4708e-14, 2.2277e-14, 2.2137e-14, 1.9612e-14, 1.9294e-14, 1.8992e-14,\n",
       "             1.7733e-14, 1.5393e-14, 1.5051e-14, 1.4587e-14, 1.4063e-14, 1.2878e-14,\n",
       "             1.1949e-14, 1.1619e-14, 1.0700e-14, 1.0611e-14, 1.0472e-14, 9.8870e-15,\n",
       "             9.6769e-15, 8.8583e-15, 8.4479e-15, 7.4181e-15, 6.5290e-15, 6.2976e-15,\n",
       "             5.3498e-15, 5.2778e-15, 5.1241e-15, 4.5545e-15, 3.9363e-15, 3.6628e-15,\n",
       "             2.9732e-15, 2.6323e-15, 2.1518e-15, 2.0491e-15, 1.7681e-15, 1.6323e-15,\n",
       "             1.5764e-15, 1.4410e-15, 1.3913e-15, 1.1584e-15, 1.0657e-15, 8.3205e-16,\n",
       "             7.7763e-16, 7.7687e-16, 7.0512e-16, 6.6368e-16, 5.8539e-16, 5.7229e-16,\n",
       "             5.4647e-16, 5.3789e-16, 4.8418e-16, 4.7362e-16, 4.4246e-16, 4.0057e-16,\n",
       "             3.8018e-16, 3.7228e-16, 3.6726e-16, 3.0228e-16, 2.1211e-16, 1.9881e-16,\n",
       "             1.8690e-16, 1.8681e-16, 1.5137e-16, 1.5079e-16, 1.2463e-16, 1.1197e-16,\n",
       "             9.7216e-17, 7.7554e-17, 7.4811e-17, 6.9903e-17, 6.2664e-17, 4.1625e-17,\n",
       "             3.4651e-17, 3.3450e-17, 3.2811e-17, 2.4260e-17, 2.1195e-17, 2.1148e-17,\n",
       "             1.6647e-17, 1.4872e-17, 1.2761e-17, 1.2631e-17, 8.9769e-18, 8.2389e-18,\n",
       "             5.4234e-18, 1.4191e-18, 6.4529e-19, 5.6874e-19, 3.5456e-19, 2.9446e-19,\n",
       "             2.4785e-19, 2.4197e-19, 1.9401e-19, 1.5428e-19, 1.0624e-19, 9.1921e-20,\n",
       "             5.9446e-20, 4.8582e-20, 4.3551e-20, 3.0726e-20, 2.8491e-20, 2.4891e-20,\n",
       "             1.8126e-20, 1.6408e-20, 1.5490e-20, 8.6093e-22, 4.6962e-22, 3.0212e-22,\n",
       "             1.5281e-22, 8.8378e-23, 8.4455e-23, 5.3763e-24, 2.6060e-24, 1.4444e-25,\n",
       "             1.9696e-26, 7.1609e-27, 3.5386e-29])}},\n",
       "   {'fpr': np.float64(0.9976525821596244),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.9695, 0.9742, 0.9765, 0.9789, 0.9812, 0.9836, 0.9859, 0.9883,\n",
       "             0.9906, 0.9930, 0.9953, 0.9977, 1.0000]),\n",
       "     'tpr': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "     'thresholds': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,\n",
       "             0.9998, 0.9998, 0.9996, 0.9958, 0.4634])}}],\n",
       "  [{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7101e-04, 1.5420e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 4.3426e-01, 4.3160e-01,  ..., 2.3658e-09, 1.0728e-09,\n",
       "             4.2973e-10])}},\n",
       "   {'fpr': np.float64(0.01312910284463895),\n",
       "    'tpr': np.float64(0.9321511179645335),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7101e-04, 1.5420e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9921e-01, 9.9913e-01,  ..., 5.8322e-09, 4.5633e-09,\n",
       "             1.9638e-09])}},\n",
       "   {'fpr': np.float64(0.087527352297593),\n",
       "    'tpr': np.float64(0.9745566692367),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7101e-04, 1.5420e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9945e-01, 9.9916e-01,  ..., 4.6731e-07, 3.5156e-07,\n",
       "             2.9421e-07])}},\n",
       "   {'fpr': np.float64(0.05908096280087528),\n",
       "    'tpr': np.float64(0.9799537393986122),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7101e-04, 1.5420e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9996e-01,  ..., 7.2539e-08, 2.9460e-08,\n",
       "             2.7930e-08])}},\n",
       "   {'fpr': np.float64(0.024070021881838075),\n",
       "    'tpr': np.float64(0.9629915188897455),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7101e-04, 1.5420e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9966e-01, 9.9957e-01,  ..., 9.2622e-07, 8.8257e-07,\n",
       "             2.9759e-07])}},\n",
       "   {'fpr': np.float64(0.07439824945295405),\n",
       "    'tpr': np.float64(0.9915188897455667),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7101e-04, 1.5420e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 9.9999e-01,  ..., 6.1711e-08, 2.5759e-08,\n",
       "             5.4070e-09])}},\n",
       "   {'fpr': np.float64(0.05908096280087528),\n",
       "    'tpr': np.float64(0.9884348496530455),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7101e-04, 3.0840e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0886e-09, 7.1501e-10,\n",
       "             6.5295e-10])}},\n",
       "   {'fpr': np.float64(0.061269146608315096),\n",
       "    'tpr': np.float64(0.9868928296067849),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000e+00, 7.7101e-04, 1.5420e-03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 7.6109e-08, 4.7606e-08,\n",
       "             3.0253e-08])}},\n",
       "   {'fpr': np.float64(0.04814004376367615),\n",
       "    'tpr': np.float64(0.9776407093292213),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0278, 0.0524,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 4.9178e-12, 6.3769e-13,\n",
       "             3.1712e-13])}},\n",
       "   {'fpr': np.float64(0.037199124726477024),\n",
       "    'tpr': np.float64(0.9760986892829607),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0054, 0.0123,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.4406e-14, 2.3694e-16,\n",
       "             1.9674e-17])}},\n",
       "   {'fpr': np.float64(0.04157549234135667),\n",
       "    'tpr': np.float64(0.9791827293754819),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0208, 0.0439,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 6.3530e-14, 2.8451e-16,\n",
       "             4.5172e-19])}},\n",
       "   {'fpr': np.float64(0.0787746170678337),\n",
       "    'tpr': np.float64(0.9868928296067849),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0175, 0.0175, 0.0175,\n",
       "             0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175,\n",
       "             0.0175, 0.0197, 0.0197, 0.0197, 0.0219, 0.0219, 0.0219, 0.0219, 0.0219,\n",
       "             0.0219, 0.0219, 0.0219, 0.0219, 0.0219, 0.0241, 0.0241, 0.0241, 0.0241,\n",
       "             0.0241, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0284,\n",
       "             0.0284, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306,\n",
       "             0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306,\n",
       "             0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0328, 0.0328, 0.0328,\n",
       "             0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0350,\n",
       "             0.0350, 0.0350, 0.0350, 0.0350, 0.0350, 0.0350, 0.0372, 0.0372, 0.0372,\n",
       "             0.0394, 0.0394, 0.0416, 0.0416, 0.0438, 0.0438, 0.0438, 0.0438, 0.0438,\n",
       "             0.0460, 0.0460, 0.0481, 0.0503, 0.0503, 0.0525, 0.0525, 0.0547, 0.0569,\n",
       "             0.0591, 0.0613, 0.0635, 0.0656, 0.0678, 0.0678, 0.0700, 0.0722, 0.0744,\n",
       "             0.0766, 0.0788, 0.0788, 0.0788, 0.0788, 0.0810, 0.0832, 0.0832, 0.0853,\n",
       "             0.0875, 0.0875, 0.0875, 0.0897, 0.0919, 0.0941, 0.0941, 0.0941, 0.0963,\n",
       "             0.0985, 0.1007, 0.1028, 0.1050, 0.1072, 0.1072, 0.1094, 0.1116, 0.1116,\n",
       "             0.1138, 0.1160, 0.1182, 0.1204, 0.1225, 0.1247, 0.1269, 0.1291, 0.1313,\n",
       "             0.1335, 0.1357, 0.1357, 0.1379, 0.1400, 0.1400, 0.1422, 0.1444, 0.1466,\n",
       "             0.1488, 0.1510, 0.1510, 0.1532, 0.1554, 0.1575, 0.1597, 0.1619, 0.1641,\n",
       "             0.1663, 0.1685, 0.1707, 0.1729, 0.1751, 0.1772, 0.1772, 0.1794, 0.1816,\n",
       "             0.1838, 0.1860, 0.1882, 0.1904, 0.1926, 0.1947, 0.1969, 0.1991, 0.2013,\n",
       "             0.2035, 0.2057, 0.2057, 0.2079, 0.2101, 0.2123, 0.2144, 0.2166, 0.2188,\n",
       "             0.2210, 0.2232, 0.2254, 0.2276, 0.2298, 0.2319, 0.2341, 0.2363, 0.2385,\n",
       "             0.2407, 0.2429, 0.2451, 0.2473, 0.2495, 0.2516, 0.2538, 0.2560, 0.2560,\n",
       "             0.2582, 0.2604, 0.2626, 0.2648, 0.2670, 0.2691, 0.2713, 0.2735, 0.2757,\n",
       "             0.2779, 0.2801, 0.2823, 0.2845, 0.2867, 0.2888, 0.2910, 0.2932, 0.2954,\n",
       "             0.2976, 0.2998, 0.3020, 0.3042, 0.3063, 0.3085, 0.3107, 0.3129, 0.3151,\n",
       "             0.3173, 0.3173, 0.3195, 0.3217, 0.3239, 0.3260, 0.3282, 0.3304, 0.3326,\n",
       "             0.3348, 0.3370, 0.3392, 0.3414, 0.3435, 0.3457, 0.3479, 0.3501, 0.3523,\n",
       "             0.3545, 0.3567, 0.3589, 0.3611, 0.3632, 0.3654, 0.3676, 0.3698, 0.3720,\n",
       "             0.3742, 0.3764, 0.3786, 0.3807, 0.3829, 0.3851, 0.3873, 0.3895, 0.3917,\n",
       "             0.3939, 0.3961, 0.3982, 0.4004, 0.4026, 0.4048, 0.4070, 0.4092, 0.4114,\n",
       "             0.4136, 0.4158, 0.4179, 0.4201, 0.4223, 0.4245, 0.4267, 0.4289, 0.4311,\n",
       "             0.4333, 0.4354, 0.4376, 0.4398, 0.4420, 0.4442, 0.4464, 0.4486, 0.4508,\n",
       "             0.4530, 0.4551, 0.4573, 0.4595, 0.4617, 0.4639, 0.4661, 0.4683, 0.4705,\n",
       "             0.4705, 0.4726, 0.4748, 0.4770, 0.4792, 0.4814, 0.4836, 0.4858, 0.4880,\n",
       "             0.4902, 0.4923, 0.4945, 0.4967, 0.4989, 0.5011, 0.5033, 0.5055, 0.5077,\n",
       "             0.5098, 0.5120, 0.5142, 0.5164, 0.5186, 0.5208, 0.5230, 0.5252, 0.5274,\n",
       "             0.5295, 0.5317, 0.5339, 0.5361, 0.5383, 0.5405, 0.5427, 0.5449, 0.5470,\n",
       "             0.5492, 0.5514, 0.5536, 0.5558, 0.5580, 0.5602, 0.5624, 0.5646, 0.5667,\n",
       "             0.5689, 0.5711, 0.5733, 0.5755, 0.5777, 0.5799, 0.5821, 0.5842, 0.5864,\n",
       "             0.5886, 0.5908, 0.5930, 0.5952, 0.5974, 0.5996, 0.6018, 0.6039, 0.6061,\n",
       "             0.6083, 0.6105, 0.6127, 0.6149, 0.6171, 0.6193, 0.6214, 0.6236, 0.6258,\n",
       "             0.6280, 0.6302, 0.6324, 0.6346, 0.6368, 0.6389, 0.6411, 0.6433, 0.6455,\n",
       "             0.6477, 0.6499, 0.6521, 0.6543, 0.6565, 0.6586, 0.6608, 0.6630, 0.6652,\n",
       "             0.6674, 0.6696, 0.6718, 0.6740, 0.6761, 0.6783, 0.6805, 0.6827, 0.6849,\n",
       "             0.6871, 0.6893, 0.6915, 0.6937, 0.6958, 0.6980, 0.7002, 0.7024, 0.7046,\n",
       "             0.7068, 0.7090, 0.7112, 0.7133, 0.7155, 0.7177, 0.7199, 0.7221, 0.7243,\n",
       "             0.7265, 0.7287, 0.7309, 0.7330, 0.7352, 0.7374, 0.7396, 0.7418, 0.7440,\n",
       "             0.7462, 0.7484, 0.7505, 0.7527, 0.7549, 0.7571, 0.7593, 0.7615, 0.7637,\n",
       "             0.7659, 0.7681, 0.7702, 0.7724, 0.7746, 0.7768, 0.7790, 0.7812, 0.7834,\n",
       "             0.7856, 0.7877, 0.7899, 0.7921, 0.7943, 0.7965, 0.7987, 0.8009, 0.8031,\n",
       "             0.8053, 0.8074, 0.8096, 0.8118, 0.8140, 0.8162, 0.8184, 0.8206, 0.8228,\n",
       "             0.8249, 0.8271, 0.8293, 0.8315, 0.8337, 0.8359, 0.8381, 0.8403, 0.8425,\n",
       "             0.8446, 0.8468, 0.8490, 0.8512, 0.8534, 0.8556, 0.8578, 0.8600, 0.8621,\n",
       "             0.8643, 0.8665, 0.8687, 0.8709, 0.8731, 0.8753, 0.8775, 0.8796, 0.8818,\n",
       "             0.8840, 0.8862, 0.8884, 0.8906, 0.8928, 0.8950, 0.8972, 0.8993, 0.9015,\n",
       "             0.9037, 0.9059, 0.9081, 0.9103, 0.9125, 0.9147, 0.9168, 0.9190, 0.9212,\n",
       "             0.9234, 0.9256, 0.9278, 0.9300, 0.9322, 0.9344, 0.9365, 0.9387, 0.9409,\n",
       "             0.9431, 0.9453, 0.9475, 0.9497, 0.9519, 0.9540, 0.9562, 0.9584, 0.9606,\n",
       "             0.9628, 0.9650, 0.9672, 0.9694, 0.9716, 0.9737, 0.9759, 0.9781, 0.9803,\n",
       "             0.9825, 0.9847, 0.9869, 0.9891, 0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.1565, 0.2575, 0.3061, 0.3323, 0.3624, 0.3863, 0.4025, 0.4148,\n",
       "             0.4256, 0.4387, 0.4426, 0.4464, 0.4549, 0.4634, 0.4711, 0.4780, 0.4827,\n",
       "             0.4896, 0.4942, 0.4958, 0.4996, 0.5019, 0.5066, 0.5096, 0.5166, 0.5220,\n",
       "             0.5227, 0.5274, 0.5297, 0.5312, 0.5366, 0.5389, 0.5420, 0.5459, 0.5466,\n",
       "             0.5520, 0.5528, 0.5536, 0.5598, 0.5621, 0.5636, 0.5667, 0.5698, 0.5721,\n",
       "             0.5752, 0.5798, 0.5813, 0.5837, 0.5844, 0.5860, 0.5875, 0.5898, 0.5906,\n",
       "             0.5914, 0.5937, 0.5952, 0.5991, 0.6014, 0.6022, 0.6060, 0.6076, 0.6083,\n",
       "             0.6106, 0.6122, 0.6130, 0.6137, 0.6145, 0.6168, 0.6191, 0.6222, 0.6237,\n",
       "             0.6261, 0.6276, 0.6299, 0.6322, 0.6338, 0.6345, 0.6353, 0.6361, 0.6384,\n",
       "             0.6399, 0.6415, 0.6423, 0.6438, 0.6446, 0.6453, 0.6469, 0.6484, 0.6500,\n",
       "             0.6507, 0.6523, 0.6530, 0.6546, 0.6561, 0.6577, 0.6584, 0.6600, 0.6623,\n",
       "             0.6631, 0.6638, 0.6654, 0.6669, 0.6677, 0.6685, 0.6692, 0.6715, 0.6723,\n",
       "             0.6739, 0.6746, 0.6762, 0.6785, 0.6793, 0.6800, 0.6808, 0.6823, 0.6831,\n",
       "             0.6839, 0.6854, 0.6862, 0.6870, 0.6877, 0.6885, 0.6893, 0.6908, 0.6924,\n",
       "             0.6931, 0.6939, 0.6955, 0.6962, 0.6970, 0.6978, 0.6985, 0.6993, 0.7008,\n",
       "             0.7016, 0.7024, 0.7032, 0.7039, 0.7055, 0.7062, 0.7070, 0.7078, 0.7086,\n",
       "             0.7093, 0.7109, 0.7116, 0.7132, 0.7147, 0.7155, 0.7163, 0.7170, 0.7178,\n",
       "             0.7186, 0.7194, 0.7201, 0.7209, 0.7217, 0.7224, 0.7232, 0.7240, 0.7247,\n",
       "             0.7255, 0.7271, 0.7278, 0.7294, 0.7301, 0.7309, 0.7317, 0.7325, 0.7332,\n",
       "             0.7340, 0.7348, 0.7355, 0.7371, 0.7379, 0.7386, 0.7394, 0.7402, 0.7409,\n",
       "             0.7417, 0.7433, 0.7440, 0.7448, 0.7456, 0.7463, 0.7471, 0.7479, 0.7487,\n",
       "             0.7494, 0.7502, 0.7510, 0.7517, 0.7525, 0.7533, 0.7540, 0.7548, 0.7556,\n",
       "             0.7564, 0.7571, 0.7579, 0.7587, 0.7594, 0.7602, 0.7610, 0.7618, 0.7633,\n",
       "             0.7641, 0.7648, 0.7656, 0.7664, 0.7672, 0.7687, 0.7695, 0.7702, 0.7710,\n",
       "             0.7718, 0.7726, 0.7733, 0.7741, 0.7749, 0.7756, 0.7764, 0.7772, 0.7779,\n",
       "             0.7787, 0.7795, 0.7803, 0.7810, 0.7818, 0.7826, 0.7833, 0.7841, 0.7849,\n",
       "             0.7857, 0.7864, 0.7872, 0.7880, 0.7887, 0.7895, 0.7903, 0.7911, 0.7918,\n",
       "             0.7926, 0.7934, 0.7941, 0.7949, 0.7957, 0.7965, 0.7980, 0.7988, 0.7995,\n",
       "             0.8003, 0.8011, 0.8019, 0.8026, 0.8034, 0.8042, 0.8049, 0.8057, 0.8065,\n",
       "             0.8072, 0.8080, 0.8088, 0.8096, 0.8103, 0.8111, 0.8119, 0.8126, 0.8134,\n",
       "             0.8142, 0.8150, 0.8157, 0.8165, 0.8173, 0.8180, 0.8204, 0.8211, 0.8219,\n",
       "             0.8227, 0.8234, 0.8242, 0.8250, 0.8258, 0.8265, 0.8265, 0.8273, 0.8281,\n",
       "             0.8288, 0.8296, 0.8304, 0.8311, 0.8319, 0.8327, 0.8335, 0.8342, 0.8350,\n",
       "             0.8358, 0.8365, 0.8373, 0.8381, 0.8389, 0.8396, 0.8404, 0.8412, 0.8419,\n",
       "             0.8427, 0.8435, 0.8450, 0.8458, 0.8466, 0.8473, 0.8481, 0.8489, 0.8497,\n",
       "             0.8504, 0.8512, 0.8520, 0.8527, 0.8535, 0.8543, 0.8551, 0.8558, 0.8566,\n",
       "             0.8574, 0.8581, 0.8589, 0.8597, 0.8604, 0.8612, 0.8620, 0.8628, 0.8635,\n",
       "             0.8643, 0.8651, 0.8658, 0.8666, 0.8674, 0.8682, 0.8689, 0.8697, 0.8705,\n",
       "             0.8712, 0.8712, 0.8720, 0.8728, 0.8736, 0.8743, 0.8751, 0.8759, 0.8774,\n",
       "             0.8774, 0.8782, 0.8790, 0.8797, 0.8797, 0.8805, 0.8813, 0.8820, 0.8828,\n",
       "             0.8836, 0.8843, 0.8851, 0.8851, 0.8859, 0.8867, 0.8874, 0.8882, 0.8890,\n",
       "             0.8897, 0.8905, 0.8913, 0.8921, 0.8928, 0.8936, 0.8944, 0.8951, 0.8959,\n",
       "             0.8967, 0.8975, 0.8982, 0.8990, 0.8998, 0.9005, 0.9013, 0.9021, 0.9029,\n",
       "             0.9036, 0.9044, 0.9052, 0.9059, 0.9067, 0.9075, 0.9082, 0.9090, 0.9098,\n",
       "             0.9106, 0.9113, 0.9121, 0.9129, 0.9136, 0.9144, 0.9152, 0.9160, 0.9167,\n",
       "             0.9175, 0.9183, 0.9190, 0.9198, 0.9206, 0.9214, 0.9214, 0.9221, 0.9229,\n",
       "             0.9237, 0.9244, 0.9252, 0.9260, 0.9268, 0.9275, 0.9283, 0.9291, 0.9298,\n",
       "             0.9306, 0.9306, 0.9314, 0.9322, 0.9322, 0.9329, 0.9337, 0.9345, 0.9352,\n",
       "             0.9360, 0.9368, 0.9375, 0.9383, 0.9391, 0.9391, 0.9399, 0.9406, 0.9414,\n",
       "             0.9422, 0.9422, 0.9429, 0.9437, 0.9445, 0.9453, 0.9460, 0.9468, 0.9468,\n",
       "             0.9476, 0.9476, 0.9483, 0.9491, 0.9499, 0.9507, 0.9514, 0.9522, 0.9530,\n",
       "             0.9537, 0.9545, 0.9553, 0.9561, 0.9568, 0.9576, 0.9584, 0.9591, 0.9599,\n",
       "             0.9607, 0.9614, 0.9622, 0.9630, 0.9638, 0.9645, 0.9645, 0.9653, 0.9661,\n",
       "             0.9668, 0.9676, 0.9684, 0.9692, 0.9699, 0.9707, 0.9715, 0.9722, 0.9722,\n",
       "             0.9730, 0.9738, 0.9746, 0.9753, 0.9761, 0.9769, 0.9769, 0.9776, 0.9784,\n",
       "             0.9784, 0.9792, 0.9792, 0.9800, 0.9800, 0.9807, 0.9815, 0.9823, 0.9830,\n",
       "             0.9830, 0.9838, 0.9838, 0.9838, 0.9846, 0.9846, 0.9854, 0.9854, 0.9854,\n",
       "             0.9854, 0.9854, 0.9854, 0.9854, 0.9854, 0.9861, 0.9861, 0.9861, 0.9861,\n",
       "             0.9861, 0.9861, 0.9869, 0.9877, 0.9884, 0.9884, 0.9884, 0.9892, 0.9892,\n",
       "             0.9892, 0.9900, 0.9907, 0.9907, 0.9907, 0.9907, 0.9915, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9938,\n",
       "             0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9938, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01,\n",
       "             9.9968e-01, 9.9964e-01, 9.9964e-01, 9.9964e-01, 9.9963e-01, 9.9962e-01,\n",
       "             9.9960e-01, 9.9958e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9955e-01, 9.9954e-01, 9.9954e-01,\n",
       "             9.9954e-01, 9.9953e-01, 9.9953e-01, 9.9952e-01, 9.9952e-01, 9.9948e-01,\n",
       "             9.9948e-01, 9.9945e-01, 9.9944e-01, 9.9944e-01, 9.9942e-01, 9.9941e-01,\n",
       "             9.9940e-01, 9.9939e-01, 9.9939e-01, 9.9938e-01, 9.9937e-01, 9.9936e-01,\n",
       "             9.9936e-01, 9.9935e-01, 9.9935e-01, 9.9933e-01, 9.9933e-01, 9.9933e-01,\n",
       "             9.9931e-01, 9.9930e-01, 9.9929e-01, 9.9929e-01, 9.9927e-01, 9.9925e-01,\n",
       "             9.9924e-01, 9.9923e-01, 9.9923e-01, 9.9922e-01, 9.9921e-01, 9.9919e-01,\n",
       "             9.9919e-01, 9.9918e-01, 9.9918e-01, 9.9916e-01, 9.9916e-01, 9.9914e-01,\n",
       "             9.9913e-01, 9.9913e-01, 9.9913e-01, 9.9912e-01, 9.9910e-01, 9.9909e-01,\n",
       "             9.9908e-01, 9.9904e-01, 9.9903e-01, 9.9901e-01, 9.9900e-01, 9.9899e-01,\n",
       "             9.9899e-01, 9.9899e-01, 9.9899e-01, 9.9896e-01, 9.9895e-01, 9.9893e-01,\n",
       "             9.9891e-01, 9.9890e-01, 9.9875e-01, 9.9873e-01, 9.9870e-01, 9.9867e-01,\n",
       "             9.9867e-01, 9.9866e-01, 9.9865e-01, 9.9864e-01, 9.9863e-01, 9.9858e-01,\n",
       "             9.9857e-01, 9.9855e-01, 9.9852e-01, 9.9848e-01, 9.9847e-01, 9.9842e-01,\n",
       "             9.9841e-01, 9.9839e-01, 9.9835e-01, 9.9834e-01, 9.9815e-01, 9.9806e-01,\n",
       "             9.9802e-01, 9.9800e-01, 9.9799e-01, 9.9793e-01, 9.9793e-01, 9.9792e-01,\n",
       "             9.9777e-01, 9.9774e-01, 9.9769e-01, 9.9752e-01, 9.9752e-01, 9.9751e-01,\n",
       "             9.9743e-01, 9.9742e-01, 9.9741e-01, 9.9738e-01, 9.9727e-01, 9.9726e-01,\n",
       "             9.9722e-01, 9.9714e-01, 9.9707e-01, 9.9704e-01, 9.9686e-01, 9.9680e-01,\n",
       "             9.9659e-01, 9.9657e-01, 9.9654e-01, 9.9653e-01, 9.9648e-01, 9.9638e-01,\n",
       "             9.9633e-01, 9.9590e-01, 9.9575e-01, 9.9574e-01, 9.9564e-01, 9.9506e-01,\n",
       "             9.9490e-01, 9.9478e-01, 9.9470e-01, 9.9400e-01, 9.9353e-01, 9.9073e-01,\n",
       "             9.9043e-01, 9.9040e-01, 9.9031e-01, 9.9021e-01, 9.8968e-01, 9.8869e-01,\n",
       "             9.8797e-01, 9.8702e-01, 9.8508e-01, 9.8103e-01, 9.8091e-01, 9.7834e-01,\n",
       "             9.7764e-01, 9.7673e-01, 9.7592e-01, 9.7281e-01, 9.7218e-01, 9.6304e-01,\n",
       "             9.6288e-01, 9.6016e-01, 9.5796e-01, 9.5508e-01, 9.5089e-01, 9.4173e-01,\n",
       "             9.3989e-01, 9.3947e-01, 9.3806e-01, 9.3127e-01, 9.1868e-01, 9.1104e-01,\n",
       "             9.0382e-01, 8.9369e-01, 8.6216e-01, 8.5882e-01, 8.4353e-01, 8.2254e-01,\n",
       "             8.1075e-01, 8.0842e-01, 8.0208e-01, 7.9807e-01, 7.5749e-01, 7.5164e-01,\n",
       "             7.3689e-01, 7.2116e-01, 7.1679e-01, 7.1452e-01, 6.8047e-01, 6.7393e-01,\n",
       "             6.5068e-01, 6.3130e-01, 6.1440e-01, 5.5700e-01, 5.5403e-01, 5.5153e-01,\n",
       "             5.4719e-01, 5.4617e-01, 5.0830e-01, 4.9487e-01, 4.9355e-01, 4.4310e-01,\n",
       "             3.8954e-01, 3.8740e-01, 3.7894e-01, 3.3377e-01, 3.0459e-01, 3.0151e-01,\n",
       "             2.6907e-01, 2.4039e-01, 2.3634e-01, 2.3255e-01, 1.9219e-01, 1.7027e-01,\n",
       "             1.6048e-01, 1.5892e-01, 1.4967e-01, 1.1939e-01, 1.0466e-01, 9.8545e-02,\n",
       "             8.9527e-02, 8.7965e-02, 6.6682e-02, 6.5800e-02, 6.4291e-02, 6.1667e-02,\n",
       "             5.7906e-02, 5.5252e-02, 5.4187e-02, 4.7540e-02, 4.7161e-02, 4.5183e-02,\n",
       "             4.3524e-02, 4.3041e-02, 4.1065e-02, 4.0706e-02, 3.4804e-02, 2.9038e-02,\n",
       "             2.8324e-02, 2.7547e-02, 2.7421e-02, 2.5161e-02, 2.1261e-02, 2.0180e-02,\n",
       "             1.9486e-02, 1.9212e-02, 1.9121e-02, 1.4468e-02, 1.4275e-02, 1.4242e-02,\n",
       "             1.3371e-02, 1.2736e-02, 1.1042e-02, 1.0904e-02, 1.0847e-02, 1.0801e-02,\n",
       "             8.6377e-03, 8.3873e-03, 8.2937e-03, 7.8483e-03, 7.5009e-03, 7.0271e-03,\n",
       "             6.9183e-03, 6.6612e-03, 5.7628e-03, 5.1364e-03, 4.9541e-03, 4.9073e-03,\n",
       "             4.6987e-03, 4.6763e-03, 4.4929e-03, 4.4364e-03, 4.2573e-03, 4.2360e-03,\n",
       "             3.0262e-03, 2.9490e-03, 2.6503e-03, 2.5626e-03, 2.5403e-03, 2.5177e-03,\n",
       "             2.0464e-03, 1.9989e-03, 1.8461e-03, 1.8346e-03, 1.7395e-03, 1.7196e-03,\n",
       "             1.5550e-03, 1.5010e-03, 1.3415e-03, 1.3178e-03, 1.3170e-03, 1.3165e-03,\n",
       "             1.2753e-03, 1.2336e-03, 1.1103e-03, 1.0581e-03, 9.7471e-04, 9.4889e-04,\n",
       "             8.8552e-04, 8.6986e-04, 8.6531e-04, 8.5359e-04, 8.1772e-04, 8.1762e-04,\n",
       "             7.9079e-04, 7.5163e-04, 6.4770e-04, 5.6255e-04, 4.5968e-04, 4.5968e-04,\n",
       "             4.4323e-04, 4.4216e-04, 4.3402e-04, 3.8566e-04, 3.7594e-04, 3.6529e-04,\n",
       "             3.1939e-04, 3.0957e-04, 3.0553e-04, 3.0395e-04, 2.9178e-04, 2.8810e-04,\n",
       "             2.4928e-04, 2.4129e-04, 2.4047e-04, 2.3778e-04, 2.3297e-04, 2.3113e-04,\n",
       "             2.2901e-04, 2.2733e-04, 2.1712e-04, 2.1509e-04, 2.0184e-04, 1.8276e-04,\n",
       "             1.7916e-04, 1.7223e-04, 1.7052e-04, 1.5695e-04, 1.5655e-04, 1.5205e-04,\n",
       "             1.5186e-04, 1.4710e-04, 1.3407e-04, 1.2464e-04, 1.2323e-04, 1.2268e-04,\n",
       "             1.1703e-04, 1.1649e-04, 1.1526e-04, 1.0485e-04, 1.0037e-04, 9.9093e-05,\n",
       "             9.0708e-05, 9.0024e-05, 8.3043e-05, 8.0713e-05, 7.7312e-05, 7.4607e-05,\n",
       "             7.3965e-05, 7.3543e-05, 7.2134e-05, 7.0729e-05, 6.4531e-05, 6.4357e-05,\n",
       "             6.3363e-05, 6.1551e-05, 5.6890e-05, 5.6804e-05, 5.2195e-05, 5.0786e-05,\n",
       "             5.0254e-05, 4.8732e-05, 4.7001e-05, 4.3935e-05, 4.0607e-05, 4.0120e-05,\n",
       "             3.9314e-05, 3.8210e-05, 3.8158e-05, 3.4928e-05, 3.3359e-05, 3.3036e-05,\n",
       "             3.1963e-05, 3.1752e-05, 3.1725e-05, 3.0891e-05, 3.0021e-05, 3.0007e-05,\n",
       "             2.9191e-05, 2.6817e-05, 2.4864e-05, 2.4085e-05, 2.1781e-05, 2.1695e-05,\n",
       "             2.0911e-05, 2.0859e-05, 2.0790e-05, 2.0741e-05, 1.9599e-05, 1.9208e-05,\n",
       "             1.8989e-05, 1.8776e-05, 1.7438e-05, 1.6967e-05, 1.6605e-05, 1.5898e-05,\n",
       "             1.5610e-05, 1.5438e-05, 1.5033e-05, 1.4862e-05, 1.4434e-05, 1.3708e-05,\n",
       "             1.3313e-05, 1.2568e-05, 1.2338e-05, 1.1677e-05, 1.0817e-05, 1.0251e-05,\n",
       "             8.9288e-06, 8.4095e-06, 7.7712e-06, 7.7462e-06, 7.1915e-06, 7.0728e-06,\n",
       "             6.7731e-06, 6.0755e-06, 5.9014e-06, 5.6101e-06, 5.5686e-06, 5.1650e-06,\n",
       "             4.8673e-06, 4.7523e-06, 4.4565e-06, 4.3831e-06, 3.9437e-06, 3.6042e-06,\n",
       "             3.4880e-06, 3.4100e-06, 3.0134e-06, 2.4085e-06, 2.1930e-06, 2.1633e-06,\n",
       "             2.1553e-06, 2.1039e-06, 2.0232e-06, 1.9610e-06, 1.8623e-06, 1.7132e-06,\n",
       "             1.5803e-06, 1.5105e-06, 1.4950e-06, 1.3520e-06, 1.2525e-06, 1.2467e-06,\n",
       "             1.2154e-06, 1.2071e-06, 1.1724e-06, 1.1244e-06, 1.0868e-06, 9.4037e-07,\n",
       "             8.5800e-07, 8.1883e-07, 7.2063e-07, 7.0506e-07, 7.0235e-07, 6.6337e-07,\n",
       "             6.4321e-07, 6.3490e-07, 6.0772e-07, 5.6644e-07, 5.5312e-07, 5.1085e-07,\n",
       "             4.9975e-07, 4.9113e-07, 4.3338e-07, 4.1936e-07, 4.1598e-07, 4.0709e-07,\n",
       "             3.6574e-07, 3.5402e-07, 3.4420e-07, 3.1825e-07, 3.0736e-07, 2.8932e-07,\n",
       "             2.7688e-07, 2.6793e-07, 2.6671e-07, 2.5946e-07, 2.5363e-07, 2.3208e-07,\n",
       "             2.3081e-07, 2.1414e-07, 2.1122e-07, 1.9100e-07, 1.8541e-07, 1.6769e-07,\n",
       "             1.6315e-07, 1.6239e-07, 1.6097e-07, 1.6016e-07, 1.4570e-07, 1.3716e-07,\n",
       "             1.3264e-07, 1.3212e-07, 1.2820e-07, 1.1491e-07, 1.1410e-07, 1.0823e-07,\n",
       "             9.8575e-08, 9.2440e-08, 8.9769e-08, 8.6186e-08, 8.4792e-08, 8.1365e-08,\n",
       "             7.3302e-08, 7.3205e-08, 7.1370e-08, 6.9333e-08, 6.8680e-08, 6.5739e-08,\n",
       "             6.1902e-08, 5.9005e-08, 4.6097e-08, 4.5118e-08, 3.9033e-08, 3.8809e-08,\n",
       "             3.7886e-08, 3.7593e-08, 3.7069e-08, 3.5237e-08, 3.5225e-08, 3.4193e-08,\n",
       "             3.4156e-08, 3.4018e-08, 3.2158e-08, 3.1812e-08, 3.1532e-08, 3.1013e-08,\n",
       "             3.0138e-08, 2.8550e-08, 2.8231e-08, 2.5890e-08, 2.3063e-08, 1.9920e-08,\n",
       "             1.9748e-08, 1.8512e-08, 1.7243e-08, 1.6767e-08, 1.6537e-08, 1.6214e-08,\n",
       "             1.6065e-08, 1.5411e-08, 1.3985e-08, 1.2568e-08, 1.2527e-08, 1.2434e-08,\n",
       "             1.1892e-08, 1.0912e-08, 1.0262e-08, 9.7345e-09, 8.7034e-09, 7.6884e-09,\n",
       "             7.4795e-09, 7.4332e-09, 7.1321e-09, 6.3313e-09, 6.0783e-09, 6.0661e-09,\n",
       "             5.9340e-09, 5.0153e-09, 4.5910e-09, 4.4825e-09, 4.2443e-09, 3.9009e-09,\n",
       "             3.8257e-09, 3.6185e-09, 2.6629e-09, 2.6531e-09, 2.6459e-09, 2.4056e-09,\n",
       "             2.1242e-09, 2.1166e-09, 2.0810e-09, 1.6884e-09, 1.5453e-09, 1.3686e-09,\n",
       "             1.1127e-09, 1.0734e-09, 9.8133e-10, 9.6832e-10, 8.5083e-10, 8.1168e-10,\n",
       "             7.8130e-10, 7.7146e-10, 6.2143e-10, 4.6311e-10, 3.9951e-10, 3.4466e-10,\n",
       "             3.3735e-10, 2.8738e-10, 2.4794e-10, 2.4205e-10, 2.1488e-10, 2.0775e-10,\n",
       "             1.6433e-10, 1.2150e-10, 1.2048e-10, 1.2015e-10, 1.0626e-10, 1.0013e-10,\n",
       "             7.5343e-11, 6.4479e-11, 4.5724e-11, 3.4830e-11, 2.8371e-11, 2.2000e-11,\n",
       "             2.1385e-11, 2.0987e-11, 2.0962e-11, 1.9802e-11, 1.3455e-11, 1.2900e-11,\n",
       "             1.1169e-11, 1.0073e-11, 9.7355e-12, 6.3222e-12, 6.2225e-12, 4.9040e-12,\n",
       "             2.9463e-12, 2.7860e-12, 2.7681e-12, 1.4677e-12, 1.1027e-12, 5.6515e-13,\n",
       "             3.9833e-14, 6.9896e-16, 1.7924e-16])}},\n",
       "   {'fpr': np.float64(0.019693654266958426),\n",
       "    'tpr': np.float64(0.9699306090979183),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.1118, 0.1827,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.3370e-21, 1.6020e-23,\n",
       "             2.8696e-27])}},\n",
       "   {'fpr': np.float64(0.0437636761487965),\n",
       "    'tpr': np.float64(0.9853508095605242),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0109, 0.0131, 0.0131, 0.0131, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0175, 0.0175, 0.0175, 0.0175, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0219, 0.0241, 0.0263, 0.0284,\n",
       "             0.0306, 0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0350,\n",
       "             0.0350, 0.0350, 0.0372, 0.0372, 0.0394, 0.0394, 0.0416, 0.0416, 0.0438,\n",
       "             0.0438, 0.0438, 0.0438, 0.0438, 0.0460, 0.0460, 0.0481, 0.0481, 0.0481,\n",
       "             0.0481, 0.0503, 0.0525, 0.0525, 0.0525, 0.0547, 0.0569, 0.0591, 0.0613,\n",
       "             0.0635, 0.0656, 0.0656, 0.0678, 0.0700, 0.0722, 0.0766, 0.0788, 0.0810,\n",
       "             0.0832, 0.0832, 0.0832, 0.0853, 0.0875, 0.0897, 0.0919, 0.0941, 0.0941,\n",
       "             0.0963, 0.0985, 0.1007, 0.1028, 0.1050, 0.1072, 0.1094, 0.1116, 0.1138,\n",
       "             0.1160, 0.1182, 0.1204, 0.1225, 0.1247, 0.1269, 0.1291, 0.1313, 0.1313,\n",
       "             0.1335, 0.1357, 0.1379, 0.1379, 0.1400, 0.1400, 0.1422, 0.1444, 0.1466,\n",
       "             0.1488, 0.1510, 0.1532, 0.1554, 0.1575, 0.1597, 0.1619, 0.1619, 0.1641,\n",
       "             0.1663, 0.1685, 0.1707, 0.1729, 0.1751, 0.1772, 0.1794, 0.1816, 0.1838,\n",
       "             0.1860, 0.1882, 0.1904, 0.1926, 0.1947, 0.1947, 0.1969, 0.1991, 0.2013,\n",
       "             0.2035, 0.2057, 0.2079, 0.2101, 0.2123, 0.2144, 0.2166, 0.2188, 0.2210,\n",
       "             0.2232, 0.2254, 0.2276, 0.2298, 0.2319, 0.2341, 0.2363, 0.2385, 0.2407,\n",
       "             0.2429, 0.2451, 0.2473, 0.2495, 0.2516, 0.2538, 0.2560, 0.2582, 0.2604,\n",
       "             0.2626, 0.2648, 0.2670, 0.2691, 0.2713, 0.2735, 0.2757, 0.2779, 0.2801,\n",
       "             0.2823, 0.2845, 0.2867, 0.2888, 0.2910, 0.2932, 0.2954, 0.2976, 0.2998,\n",
       "             0.3020, 0.3042, 0.3063, 0.3085, 0.3107, 0.3129, 0.3151, 0.3173, 0.3195,\n",
       "             0.3217, 0.3239, 0.3260, 0.3282, 0.3304, 0.3304, 0.3326, 0.3348, 0.3370,\n",
       "             0.3392, 0.3414, 0.3435, 0.3457, 0.3479, 0.3501, 0.3523, 0.3545, 0.3567,\n",
       "             0.3589, 0.3611, 0.3632, 0.3654, 0.3676, 0.3698, 0.3720, 0.3742, 0.3764,\n",
       "             0.3786, 0.3807, 0.3829, 0.3851, 0.3873, 0.3895, 0.3917, 0.3939, 0.3961,\n",
       "             0.3982, 0.4004, 0.4026, 0.4048, 0.4070, 0.4092, 0.4114, 0.4136, 0.4158,\n",
       "             0.4179, 0.4201, 0.4223, 0.4245, 0.4267, 0.4289, 0.4311, 0.4333, 0.4354,\n",
       "             0.4376, 0.4398, 0.4420, 0.4442, 0.4464, 0.4486, 0.4508, 0.4530, 0.4551,\n",
       "             0.4573, 0.4595, 0.4617, 0.4639, 0.4661, 0.4683, 0.4705, 0.4726, 0.4748,\n",
       "             0.4770, 0.4792, 0.4814, 0.4836, 0.4858, 0.4880, 0.4902, 0.4923, 0.4945,\n",
       "             0.4967, 0.4989, 0.5011, 0.5033, 0.5055, 0.5077, 0.5098, 0.5120, 0.5142,\n",
       "             0.5164, 0.5186, 0.5208, 0.5230, 0.5252, 0.5274, 0.5295, 0.5317, 0.5339,\n",
       "             0.5339, 0.5361, 0.5383, 0.5405, 0.5427, 0.5449, 0.5470, 0.5492, 0.5514,\n",
       "             0.5536, 0.5558, 0.5580, 0.5602, 0.5624, 0.5646, 0.5667, 0.5689, 0.5711,\n",
       "             0.5733, 0.5755, 0.5777, 0.5799, 0.5821, 0.5842, 0.5864, 0.5886, 0.5908,\n",
       "             0.5930, 0.5952, 0.5974, 0.5996, 0.6018, 0.6039, 0.6061, 0.6083, 0.6105,\n",
       "             0.6127, 0.6149, 0.6171, 0.6193, 0.6214, 0.6236, 0.6258, 0.6280, 0.6302,\n",
       "             0.6324, 0.6346, 0.6368, 0.6389, 0.6411, 0.6433, 0.6455, 0.6477, 0.6499,\n",
       "             0.6521, 0.6543, 0.6565, 0.6586, 0.6608, 0.6630, 0.6652, 0.6674, 0.6696,\n",
       "             0.6718, 0.6740, 0.6761, 0.6783, 0.6805, 0.6827, 0.6849, 0.6871, 0.6893,\n",
       "             0.6915, 0.6937, 0.6958, 0.6980, 0.7002, 0.7024, 0.7046, 0.7068, 0.7090,\n",
       "             0.7112, 0.7133, 0.7155, 0.7177, 0.7199, 0.7221, 0.7243, 0.7265, 0.7287,\n",
       "             0.7309, 0.7330, 0.7352, 0.7374, 0.7396, 0.7418, 0.7440, 0.7462, 0.7484,\n",
       "             0.7505, 0.7527, 0.7549, 0.7571, 0.7593, 0.7615, 0.7637, 0.7659, 0.7681,\n",
       "             0.7702, 0.7724, 0.7746, 0.7768, 0.7790, 0.7812, 0.7834, 0.7856, 0.7877,\n",
       "             0.7899, 0.7921, 0.7943, 0.7965, 0.7987, 0.8009, 0.8031, 0.8053, 0.8074,\n",
       "             0.8096, 0.8118, 0.8140, 0.8162, 0.8184, 0.8206, 0.8228, 0.8249, 0.8271,\n",
       "             0.8293, 0.8315, 0.8337, 0.8359, 0.8381, 0.8403, 0.8425, 0.8446, 0.8468,\n",
       "             0.8490, 0.8512, 0.8534, 0.8556, 0.8578, 0.8600, 0.8621, 0.8643, 0.8665,\n",
       "             0.8687, 0.8709, 0.8731, 0.8753, 0.8775, 0.8796, 0.8818, 0.8840, 0.8862,\n",
       "             0.8884, 0.8906, 0.8928, 0.8950, 0.8972, 0.8993, 0.9015, 0.9037, 0.9059,\n",
       "             0.9081, 0.9103, 0.9125, 0.9147, 0.9168, 0.9190, 0.9212, 0.9234, 0.9256,\n",
       "             0.9278, 0.9300, 0.9322, 0.9344, 0.9365, 0.9387, 0.9409, 0.9431, 0.9453,\n",
       "             0.9475, 0.9497, 0.9519, 0.9540, 0.9562, 0.9584, 0.9606, 0.9628, 0.9650,\n",
       "             0.9672, 0.9694, 0.9716, 0.9737, 0.9759, 0.9781, 0.9803, 0.9825, 0.9847,\n",
       "             0.9869, 0.9891, 0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.4749, 0.5628, 0.5960, 0.6153, 0.6291, 0.6415, 0.6530, 0.6646,\n",
       "             0.6739, 0.6816, 0.6870, 0.6924, 0.6955, 0.6978, 0.7016, 0.7062, 0.7086,\n",
       "             0.7109, 0.7163, 0.7178, 0.7217, 0.7232, 0.7271, 0.7301, 0.7332, 0.7348,\n",
       "             0.7363, 0.7386, 0.7402, 0.7433, 0.7463, 0.7487, 0.7494, 0.7517, 0.7548,\n",
       "             0.7564, 0.7579, 0.7602, 0.7625, 0.7641, 0.7648, 0.7679, 0.7687, 0.7695,\n",
       "             0.7718, 0.7726, 0.7741, 0.7749, 0.7756, 0.7764, 0.7779, 0.7787, 0.7803,\n",
       "             0.7810, 0.7826, 0.7833, 0.7841, 0.7849, 0.7880, 0.7895, 0.7903, 0.7911,\n",
       "             0.7941, 0.7949, 0.7972, 0.7980, 0.7988, 0.7995, 0.8011, 0.8019, 0.8026,\n",
       "             0.8034, 0.8049, 0.8057, 0.8065, 0.8072, 0.8088, 0.8096, 0.8111, 0.8119,\n",
       "             0.8126, 0.8142, 0.8150, 0.8157, 0.8165, 0.8173, 0.8180, 0.8188, 0.8196,\n",
       "             0.8204, 0.8211, 0.8219, 0.8227, 0.8234, 0.8242, 0.8250, 0.8258, 0.8265,\n",
       "             0.8273, 0.8281, 0.8296, 0.8311, 0.8319, 0.8319, 0.8327, 0.8335, 0.8342,\n",
       "             0.8350, 0.8358, 0.8365, 0.8373, 0.8381, 0.8396, 0.8412, 0.8419, 0.8435,\n",
       "             0.8443, 0.8450, 0.8458, 0.8466, 0.8473, 0.8481, 0.8489, 0.8497, 0.8504,\n",
       "             0.8512, 0.8520, 0.8527, 0.8535, 0.8543, 0.8551, 0.8558, 0.8566, 0.8574,\n",
       "             0.8581, 0.8589, 0.8597, 0.8604, 0.8612, 0.8620, 0.8628, 0.8643, 0.8658,\n",
       "             0.8666, 0.8674, 0.8682, 0.8689, 0.8697, 0.8705, 0.8712, 0.8720, 0.8728,\n",
       "             0.8736, 0.8743, 0.8751, 0.8759, 0.8766, 0.8774, 0.8782, 0.8790, 0.8797,\n",
       "             0.8805, 0.8813, 0.8820, 0.8828, 0.8836, 0.8843, 0.8843, 0.8851, 0.8859,\n",
       "             0.8867, 0.8874, 0.8882, 0.8890, 0.8897, 0.8905, 0.8913, 0.8921, 0.8928,\n",
       "             0.8936, 0.8944, 0.8951, 0.8959, 0.8967, 0.8975, 0.8982, 0.8990, 0.8998,\n",
       "             0.9005, 0.9013, 0.9021, 0.9029, 0.9036, 0.9044, 0.9052, 0.9059, 0.9067,\n",
       "             0.9075, 0.9082, 0.9090, 0.9098, 0.9106, 0.9113, 0.9121, 0.9129, 0.9136,\n",
       "             0.9144, 0.9152, 0.9160, 0.9167, 0.9175, 0.9183, 0.9190, 0.9198, 0.9206,\n",
       "             0.9214, 0.9221, 0.9229, 0.9237, 0.9244, 0.9252, 0.9260, 0.9268, 0.9275,\n",
       "             0.9283, 0.9291, 0.9298, 0.9306, 0.9314, 0.9322, 0.9329, 0.9337, 0.9345,\n",
       "             0.9352, 0.9360, 0.9368, 0.9375, 0.9383, 0.9391, 0.9399, 0.9406, 0.9414,\n",
       "             0.9422, 0.9429, 0.9437, 0.9445, 0.9453, 0.9460, 0.9468, 0.9476, 0.9483,\n",
       "             0.9491, 0.9499, 0.9499, 0.9507, 0.9514, 0.9522, 0.9530, 0.9537, 0.9545,\n",
       "             0.9553, 0.9561, 0.9568, 0.9576, 0.9584, 0.9591, 0.9599, 0.9607, 0.9614,\n",
       "             0.9622, 0.9622, 0.9622, 0.9630, 0.9638, 0.9638, 0.9645, 0.9653, 0.9661,\n",
       "             0.9668, 0.9668, 0.9676, 0.9684, 0.9692, 0.9692, 0.9699, 0.9707, 0.9715,\n",
       "             0.9722, 0.9730, 0.9738, 0.9746, 0.9753, 0.9753, 0.9753, 0.9753, 0.9753,\n",
       "             0.9753, 0.9753, 0.9761, 0.9769, 0.9776, 0.9784, 0.9792, 0.9800, 0.9800,\n",
       "             0.9807, 0.9815, 0.9815, 0.9823, 0.9823, 0.9830, 0.9830, 0.9838, 0.9838,\n",
       "             0.9846, 0.9854, 0.9861, 0.9869, 0.9869, 0.9877, 0.9877, 0.9884, 0.9892,\n",
       "             0.9900, 0.9900, 0.9900, 0.9907, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915,\n",
       "             0.9915, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9979e-01, 9.9977e-01, 9.9976e-01, 9.9975e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9970e-01,\n",
       "             9.9967e-01, 9.9964e-01, 9.9964e-01, 9.9963e-01, 9.9962e-01, 9.9959e-01,\n",
       "             9.9957e-01, 9.9955e-01, 9.9955e-01, 9.9954e-01, 9.9951e-01, 9.9951e-01,\n",
       "             9.9950e-01, 9.9950e-01, 9.9948e-01, 9.9945e-01, 9.9942e-01, 9.9941e-01,\n",
       "             9.9940e-01, 9.9939e-01, 9.9938e-01, 9.9936e-01, 9.9924e-01, 9.9916e-01,\n",
       "             9.9915e-01, 9.9902e-01, 9.9891e-01, 9.9889e-01, 9.9880e-01, 9.9880e-01,\n",
       "             9.9879e-01, 9.9877e-01, 9.9873e-01, 9.9867e-01, 9.9859e-01, 9.9851e-01,\n",
       "             9.9846e-01, 9.9845e-01, 9.9813e-01, 9.9784e-01, 9.9780e-01, 9.9775e-01,\n",
       "             9.9766e-01, 9.9739e-01, 9.9726e-01, 9.9706e-01, 9.9633e-01, 9.9630e-01,\n",
       "             9.9623e-01, 9.9592e-01, 9.9592e-01, 9.9509e-01, 9.9455e-01, 9.9444e-01,\n",
       "             9.9383e-01, 9.9241e-01, 9.9157e-01, 9.9156e-01, 9.9147e-01, 9.9124e-01,\n",
       "             9.9112e-01, 9.9029e-01, 9.9021e-01, 9.8923e-01, 9.8905e-01, 9.8858e-01,\n",
       "             9.8730e-01, 9.8661e-01, 9.8611e-01, 9.7841e-01, 9.7622e-01, 9.7379e-01,\n",
       "             9.6824e-01, 9.6803e-01, 9.6584e-01, 9.6257e-01, 9.5283e-01, 9.5162e-01,\n",
       "             9.3922e-01, 9.3741e-01, 9.3289e-01, 9.2756e-01, 9.1598e-01, 8.5915e-01,\n",
       "             8.4789e-01, 8.4666e-01, 8.1020e-01, 7.1435e-01, 7.1238e-01, 6.9250e-01,\n",
       "             6.9148e-01, 6.8029e-01, 6.7768e-01, 6.1628e-01, 6.0557e-01, 5.8615e-01,\n",
       "             5.4123e-01, 5.4023e-01, 5.1977e-01, 5.1739e-01, 5.0375e-01, 4.8655e-01,\n",
       "             4.8316e-01, 4.8177e-01, 4.6648e-01, 4.2662e-01, 4.0400e-01, 3.8043e-01,\n",
       "             3.5587e-01, 3.5564e-01, 3.4974e-01, 2.8497e-01, 2.5015e-01, 2.4886e-01,\n",
       "             1.8952e-01, 1.3680e-01, 1.3211e-01, 1.1926e-01, 9.4474e-02, 8.8762e-02,\n",
       "             7.0877e-02, 6.0689e-02, 5.7462e-02, 5.7163e-02, 5.0143e-02, 4.6807e-02,\n",
       "             4.6689e-02, 4.6090e-02, 4.5450e-02, 4.1641e-02, 4.1135e-02, 3.5370e-02,\n",
       "             3.0412e-02, 3.0021e-02, 2.9630e-02, 2.9516e-02, 2.9292e-02, 2.6984e-02,\n",
       "             2.6679e-02, 2.4607e-02, 1.8911e-02, 1.6951e-02, 1.4683e-02, 1.0462e-02,\n",
       "             9.5604e-03, 8.8795e-03, 8.3726e-03, 8.0371e-03, 7.7551e-03, 6.2003e-03,\n",
       "             5.9117e-03, 5.7913e-03, 5.6112e-03, 5.4488e-03, 5.1395e-03, 4.9320e-03,\n",
       "             4.8428e-03, 4.6764e-03, 4.2896e-03, 4.2872e-03, 4.2235e-03, 4.1139e-03,\n",
       "             3.8140e-03, 3.7255e-03, 3.2989e-03, 3.0626e-03, 2.7384e-03, 2.7022e-03,\n",
       "             2.4496e-03, 2.2698e-03, 1.9713e-03, 1.7988e-03, 1.4727e-03, 1.2006e-03,\n",
       "             1.1920e-03, 1.1408e-03, 1.0555e-03, 9.8703e-04, 9.2571e-04, 8.8106e-04,\n",
       "             7.0155e-04, 5.7772e-04, 5.1124e-04, 5.0283e-04, 4.7775e-04, 4.5330e-04,\n",
       "             3.9951e-04, 3.3750e-04, 3.2922e-04, 3.2799e-04, 3.1735e-04, 2.8179e-04,\n",
       "             2.7572e-04, 2.7491e-04, 2.7394e-04, 2.6996e-04, 2.6408e-04, 2.3524e-04,\n",
       "             2.2370e-04, 2.1913e-04, 2.1412e-04, 2.1051e-04, 1.9700e-04, 1.8966e-04,\n",
       "             1.6482e-04, 1.3851e-04, 1.3747e-04, 1.3611e-04, 1.2582e-04, 1.2187e-04,\n",
       "             1.1024e-04, 1.0383e-04, 1.0152e-04, 9.7750e-05, 9.1153e-05, 8.9837e-05,\n",
       "             8.9372e-05, 8.7260e-05, 7.7692e-05, 7.3183e-05, 6.7640e-05, 6.6486e-05,\n",
       "             6.2103e-05, 5.9332e-05, 5.7704e-05, 5.3866e-05, 5.1528e-05, 4.7662e-05,\n",
       "             4.7108e-05, 4.4711e-05, 4.2962e-05, 3.6744e-05, 3.4053e-05, 3.1745e-05,\n",
       "             3.0365e-05, 2.8556e-05, 2.8159e-05, 2.6176e-05, 2.5962e-05, 2.3783e-05,\n",
       "             2.3679e-05, 2.3245e-05, 2.2987e-05, 2.2535e-05, 2.1893e-05, 2.1157e-05,\n",
       "             1.8766e-05, 1.8456e-05, 1.6853e-05, 1.6378e-05, 1.5984e-05, 1.2557e-05,\n",
       "             1.1040e-05, 1.0933e-05, 1.0929e-05, 1.0920e-05, 1.0050e-05, 9.9456e-06,\n",
       "             9.8526e-06, 9.7651e-06, 8.7853e-06, 8.7706e-06, 7.3115e-06, 7.1779e-06,\n",
       "             7.1049e-06, 6.5802e-06, 6.3812e-06, 6.2931e-06, 6.2209e-06, 6.1864e-06,\n",
       "             5.7164e-06, 5.6494e-06, 5.2056e-06, 5.0830e-06, 4.9675e-06, 4.8284e-06,\n",
       "             4.6116e-06, 4.3319e-06, 4.1877e-06, 4.0218e-06, 3.7851e-06, 3.5163e-06,\n",
       "             3.2111e-06, 3.1653e-06, 3.1225e-06, 2.9869e-06, 2.9232e-06, 2.4606e-06,\n",
       "             2.4096e-06, 1.8582e-06, 1.7393e-06, 1.6695e-06, 1.6278e-06, 1.6008e-06,\n",
       "             1.5629e-06, 1.5420e-06, 1.4391e-06, 1.4320e-06, 1.4210e-06, 1.3361e-06,\n",
       "             1.1991e-06, 1.1808e-06, 1.1612e-06, 1.1490e-06, 1.1297e-06, 1.0997e-06,\n",
       "             1.0700e-06, 1.0235e-06, 8.8749e-07, 8.8340e-07, 8.4426e-07, 8.2038e-07,\n",
       "             7.6395e-07, 7.4827e-07, 6.6028e-07, 6.1283e-07, 5.6534e-07, 5.4103e-07,\n",
       "             5.3192e-07, 4.6307e-07, 4.5780e-07, 4.4407e-07, 4.4034e-07, 4.2798e-07,\n",
       "             4.2052e-07, 4.1567e-07, 3.9790e-07, 3.9697e-07, 3.8183e-07, 3.7081e-07,\n",
       "             3.6875e-07, 3.2161e-07, 2.9808e-07, 2.9394e-07, 2.7998e-07, 2.2559e-07,\n",
       "             2.2366e-07, 2.1315e-07, 2.0800e-07, 2.0298e-07, 2.0051e-07, 1.9879e-07,\n",
       "             1.8259e-07, 1.7619e-07, 1.6510e-07, 1.6383e-07, 1.6103e-07, 1.6082e-07,\n",
       "             1.5955e-07, 1.5266e-07, 1.4322e-07, 1.4147e-07, 1.2283e-07, 1.2198e-07,\n",
       "             1.2189e-07, 1.2123e-07, 1.1997e-07, 1.1507e-07, 1.0651e-07, 1.0192e-07,\n",
       "             1.0026e-07, 9.4975e-08, 8.7971e-08, 8.7322e-08, 8.2878e-08, 7.7452e-08,\n",
       "             7.1893e-08, 6.8481e-08, 6.5919e-08, 6.5188e-08, 6.4867e-08, 6.4143e-08,\n",
       "             5.8854e-08, 5.2989e-08, 5.1169e-08, 4.8960e-08, 4.0864e-08, 3.8320e-08,\n",
       "             3.5732e-08, 3.4085e-08, 3.4045e-08, 3.1710e-08, 3.0991e-08, 3.0829e-08,\n",
       "             2.9848e-08, 2.8951e-08, 2.8581e-08, 2.4958e-08, 2.2262e-08, 2.0436e-08,\n",
       "             1.7327e-08, 1.7264e-08, 1.5862e-08, 1.4012e-08, 1.2514e-08, 1.0788e-08,\n",
       "             9.8813e-09, 9.7441e-09, 8.5438e-09, 7.9050e-09, 7.8810e-09, 7.1842e-09,\n",
       "             6.7902e-09, 6.7294e-09, 5.9501e-09, 5.6488e-09, 4.8528e-09, 4.6690e-09,\n",
       "             4.4321e-09, 4.4226e-09, 4.2695e-09, 4.2329e-09, 4.1308e-09, 4.0583e-09,\n",
       "             3.8165e-09, 3.6993e-09, 3.4194e-09, 3.3094e-09, 3.2098e-09, 3.2060e-09,\n",
       "             3.0498e-09, 2.9522e-09, 2.8670e-09, 2.7620e-09, 2.6010e-09, 2.5424e-09,\n",
       "             2.3855e-09, 2.3528e-09, 2.0529e-09, 1.8626e-09, 1.7623e-09, 1.7483e-09,\n",
       "             1.6916e-09, 1.5442e-09, 1.3789e-09, 1.3013e-09, 1.2979e-09, 1.2315e-09,\n",
       "             1.2092e-09, 1.1808e-09, 1.0040e-09, 9.9581e-10, 9.7125e-10, 9.5266e-10,\n",
       "             9.1020e-10, 8.9826e-10, 8.6588e-10, 8.1061e-10, 7.7499e-10, 7.6361e-10,\n",
       "             7.1241e-10, 7.0745e-10, 6.9161e-10, 6.2379e-10, 6.0354e-10, 5.7709e-10,\n",
       "             5.2745e-10, 4.6606e-10, 4.3415e-10, 4.2888e-10, 4.0542e-10, 3.4697e-10,\n",
       "             3.2236e-10, 2.1322e-10, 1.9295e-10, 1.9261e-10, 1.7299e-10, 1.6957e-10,\n",
       "             1.6308e-10, 1.5649e-10, 1.4609e-10, 1.3653e-10, 1.3521e-10, 1.2890e-10,\n",
       "             1.2450e-10, 1.1556e-10, 9.7231e-11, 8.9357e-11, 7.2821e-11, 6.9333e-11,\n",
       "             6.5598e-11, 6.4455e-11, 5.8866e-11, 5.7063e-11, 5.0888e-11, 4.3706e-11,\n",
       "             4.2009e-11, 3.7461e-11, 3.3034e-11, 3.1259e-11, 3.0509e-11, 2.9628e-11,\n",
       "             2.6873e-11, 2.3784e-11, 2.3454e-11, 2.1668e-11, 1.9106e-11, 1.9040e-11,\n",
       "             1.4593e-11, 1.3651e-11, 1.3572e-11, 9.3949e-12, 9.0457e-12, 8.9248e-12,\n",
       "             8.8918e-12, 8.6695e-12, 8.1645e-12, 7.8867e-12, 5.8901e-12, 5.1889e-12,\n",
       "             4.9458e-12, 4.7853e-12, 3.2419e-12, 3.1335e-12, 2.5781e-12, 2.4903e-12,\n",
       "             1.8676e-12, 1.7492e-12, 1.7364e-12, 1.7003e-12, 1.4044e-12, 1.2113e-12,\n",
       "             1.1923e-12, 1.1159e-12, 1.0875e-12, 1.0556e-12, 8.0493e-13, 6.9757e-13,\n",
       "             6.2571e-13, 5.5784e-13, 3.6738e-13, 3.1586e-13, 3.0169e-13, 2.4903e-13,\n",
       "             2.0236e-13, 1.8664e-13, 1.8591e-13, 1.5544e-13, 5.3106e-14, 3.9513e-14,\n",
       "             2.4970e-14, 2.0539e-14, 1.4755e-14, 1.4400e-14, 1.3517e-14, 1.2252e-14,\n",
       "             9.0514e-15, 6.0506e-15, 5.8796e-15, 3.3416e-15, 2.2211e-15, 1.4048e-16,\n",
       "             9.1349e-17, 3.5909e-17, 9.7419e-18, 1.1028e-19])}},\n",
       "   {'fpr': np.float64(0.037199124726477024),\n",
       "    'tpr': np.float64(0.9799537393986122),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0109,\n",
       "             0.0109, 0.0131, 0.0131, 0.0131, 0.0131, 0.0131, 0.0131, 0.0131, 0.0131,\n",
       "             0.0131, 0.0131, 0.0131, 0.0131, 0.0131, 0.0131, 0.0131, 0.0131, 0.0131,\n",
       "             0.0131, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0175, 0.0175, 0.0175, 0.0175,\n",
       "             0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0197, 0.0197, 0.0197,\n",
       "             0.0219, 0.0241, 0.0241, 0.0241, 0.0241, 0.0263, 0.0263, 0.0263, 0.0263,\n",
       "             0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0284, 0.0284, 0.0306,\n",
       "             0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306,\n",
       "             0.0306, 0.0328, 0.0328, 0.0328, 0.0350, 0.0350, 0.0350, 0.0350, 0.0350,\n",
       "             0.0372, 0.0372, 0.0394, 0.0394, 0.0416, 0.0416, 0.0416, 0.0438, 0.0438,\n",
       "             0.0460, 0.0481, 0.0481, 0.0503, 0.0525, 0.0525, 0.0525, 0.0547, 0.0547,\n",
       "             0.0569, 0.0591, 0.0613, 0.0656, 0.0656, 0.0656, 0.0678, 0.0700, 0.0722,\n",
       "             0.0744, 0.0744, 0.0766, 0.0788, 0.0788, 0.0810, 0.0832, 0.0853, 0.0853,\n",
       "             0.0853, 0.0875, 0.0897, 0.0919, 0.0941, 0.0941, 0.0963, 0.0963, 0.0985,\n",
       "             0.1007, 0.1028, 0.1050, 0.1072, 0.1072, 0.1094, 0.1116, 0.1138, 0.1160,\n",
       "             0.1182, 0.1182, 0.1204, 0.1225, 0.1247, 0.1269, 0.1291, 0.1313, 0.1335,\n",
       "             0.1357, 0.1379, 0.1400, 0.1422, 0.1444, 0.1466, 0.1488, 0.1510, 0.1532,\n",
       "             0.1554, 0.1575, 0.1597, 0.1619, 0.1641, 0.1663, 0.1685, 0.1707, 0.1707,\n",
       "             0.1729, 0.1729, 0.1751, 0.1772, 0.1772, 0.1794, 0.1816, 0.1838, 0.1860,\n",
       "             0.1882, 0.1904, 0.1926, 0.1926, 0.1947, 0.1969, 0.1991, 0.2013, 0.2035,\n",
       "             0.2057, 0.2079, 0.2101, 0.2101, 0.2123, 0.2144, 0.2166, 0.2188, 0.2210,\n",
       "             0.2232, 0.2254, 0.2276, 0.2298, 0.2319, 0.2341, 0.2363, 0.2385, 0.2407,\n",
       "             0.2429, 0.2451, 0.2473, 0.2495, 0.2516, 0.2538, 0.2560, 0.2582, 0.2604,\n",
       "             0.2626, 0.2648, 0.2670, 0.2691, 0.2713, 0.2735, 0.2757, 0.2779, 0.2801,\n",
       "             0.2823, 0.2845, 0.2867, 0.2888, 0.2910, 0.2932, 0.2954, 0.2976, 0.2998,\n",
       "             0.3020, 0.3042, 0.3063, 0.3085, 0.3107, 0.3129, 0.3151, 0.3173, 0.3195,\n",
       "             0.3217, 0.3239, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370, 0.3392,\n",
       "             0.3414, 0.3435, 0.3457, 0.3479, 0.3501, 0.3523, 0.3545, 0.3567, 0.3589,\n",
       "             0.3611, 0.3632, 0.3654, 0.3676, 0.3698, 0.3720, 0.3742, 0.3764, 0.3786,\n",
       "             0.3807, 0.3829, 0.3851, 0.3873, 0.3895, 0.3917, 0.3939, 0.3961, 0.3982,\n",
       "             0.4004, 0.4004, 0.4026, 0.4048, 0.4070, 0.4070, 0.4092, 0.4114, 0.4136,\n",
       "             0.4158, 0.4179, 0.4201, 0.4223, 0.4245, 0.4267, 0.4289, 0.4311, 0.4333,\n",
       "             0.4354, 0.4376, 0.4398, 0.4420, 0.4442, 0.4464, 0.4486, 0.4508, 0.4530,\n",
       "             0.4551, 0.4573, 0.4595, 0.4617, 0.4639, 0.4661, 0.4683, 0.4705, 0.4726,\n",
       "             0.4748, 0.4770, 0.4792, 0.4814, 0.4836, 0.4858, 0.4880, 0.4902, 0.4923,\n",
       "             0.4945, 0.4967, 0.4989, 0.5011, 0.5033, 0.5055, 0.5077, 0.5098, 0.5120,\n",
       "             0.5142, 0.5164, 0.5186, 0.5208, 0.5230, 0.5252, 0.5274, 0.5295, 0.5317,\n",
       "             0.5339, 0.5361, 0.5383, 0.5405, 0.5427, 0.5449, 0.5470, 0.5492, 0.5514,\n",
       "             0.5536, 0.5558, 0.5580, 0.5602, 0.5624, 0.5646, 0.5667, 0.5689, 0.5711,\n",
       "             0.5733, 0.5755, 0.5777, 0.5799, 0.5821, 0.5842, 0.5864, 0.5886, 0.5908,\n",
       "             0.5930, 0.5952, 0.5974, 0.5996, 0.6018, 0.6039, 0.6061, 0.6083, 0.6105,\n",
       "             0.6127, 0.6149, 0.6171, 0.6171, 0.6193, 0.6214, 0.6236, 0.6258, 0.6280,\n",
       "             0.6302, 0.6324, 0.6346, 0.6368, 0.6389, 0.6411, 0.6433, 0.6455, 0.6477,\n",
       "             0.6499, 0.6521, 0.6543, 0.6565, 0.6586, 0.6608, 0.6630, 0.6652, 0.6674,\n",
       "             0.6696, 0.6718, 0.6740, 0.6761, 0.6783, 0.6805, 0.6827, 0.6849, 0.6871,\n",
       "             0.6893, 0.6915, 0.6937, 0.6958, 0.6980, 0.7002, 0.7024, 0.7046, 0.7068,\n",
       "             0.7090, 0.7112, 0.7133, 0.7155, 0.7177, 0.7199, 0.7221, 0.7243, 0.7265,\n",
       "             0.7287, 0.7309, 0.7330, 0.7352, 0.7374, 0.7396, 0.7418, 0.7440, 0.7462,\n",
       "             0.7484, 0.7505, 0.7527, 0.7549, 0.7571, 0.7593, 0.7615, 0.7637, 0.7659,\n",
       "             0.7681, 0.7702, 0.7724, 0.7746, 0.7768, 0.7790, 0.7812, 0.7834, 0.7856,\n",
       "             0.7877, 0.7899, 0.7921, 0.7943, 0.7965, 0.7987, 0.8009, 0.8031, 0.8053,\n",
       "             0.8074, 0.8096, 0.8118, 0.8140, 0.8162, 0.8184, 0.8206, 0.8228, 0.8249,\n",
       "             0.8271, 0.8293, 0.8315, 0.8337, 0.8359, 0.8381, 0.8403, 0.8425, 0.8446,\n",
       "             0.8468, 0.8490, 0.8512, 0.8534, 0.8556, 0.8578, 0.8600, 0.8621, 0.8643,\n",
       "             0.8665, 0.8687, 0.8709, 0.8731, 0.8753, 0.8775, 0.8796, 0.8818, 0.8840,\n",
       "             0.8862, 0.8884, 0.8906, 0.8928, 0.8950, 0.8972, 0.8993, 0.9015, 0.9037,\n",
       "             0.9059, 0.9081, 0.9103, 0.9125, 0.9147, 0.9168, 0.9190, 0.9212, 0.9234,\n",
       "             0.9256, 0.9278, 0.9300, 0.9322, 0.9344, 0.9365, 0.9387, 0.9409, 0.9431,\n",
       "             0.9453, 0.9475, 0.9497, 0.9519, 0.9540, 0.9562, 0.9584, 0.9606, 0.9628,\n",
       "             0.9650, 0.9672, 0.9694, 0.9716, 0.9737, 0.9759, 0.9781, 0.9803, 0.9825,\n",
       "             0.9847, 0.9869, 0.9891, 0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.6399, 0.6993, 0.7263, 0.7394, 0.7533, 0.7594, 0.7641, 0.7702,\n",
       "             0.7749, 0.7787, 0.7841, 0.7857, 0.7895, 0.7903, 0.7934, 0.7941, 0.7965,\n",
       "             0.7988, 0.7995, 0.8019, 0.8026, 0.8034, 0.8042, 0.8065, 0.8080, 0.8088,\n",
       "             0.8103, 0.8119, 0.8126, 0.8150, 0.8157, 0.8188, 0.8196, 0.8204, 0.8227,\n",
       "             0.8234, 0.8242, 0.8258, 0.8273, 0.8296, 0.8304, 0.8327, 0.8335, 0.8342,\n",
       "             0.8350, 0.8358, 0.8365, 0.8373, 0.8381, 0.8389, 0.8404, 0.8419, 0.8427,\n",
       "             0.8435, 0.8443, 0.8450, 0.8458, 0.8466, 0.8473, 0.8481, 0.8497, 0.8504,\n",
       "             0.8512, 0.8520, 0.8527, 0.8535, 0.8543, 0.8551, 0.8558, 0.8566, 0.8574,\n",
       "             0.8581, 0.8589, 0.8597, 0.8604, 0.8612, 0.8620, 0.8628, 0.8635, 0.8643,\n",
       "             0.8651, 0.8658, 0.8666, 0.8674, 0.8689, 0.8697, 0.8705, 0.8712, 0.8720,\n",
       "             0.8728, 0.8736, 0.8743, 0.8751, 0.8751, 0.8759, 0.8766, 0.8774, 0.8782,\n",
       "             0.8797, 0.8805, 0.8813, 0.8820, 0.8828, 0.8836, 0.8843, 0.8851, 0.8859,\n",
       "             0.8867, 0.8874, 0.8882, 0.8890, 0.8897, 0.8905, 0.8913, 0.8921, 0.8928,\n",
       "             0.8936, 0.8944, 0.8951, 0.8959, 0.8967, 0.8975, 0.8982, 0.8990, 0.8998,\n",
       "             0.9005, 0.9013, 0.9021, 0.9029, 0.9036, 0.9036, 0.9044, 0.9052, 0.9059,\n",
       "             0.9067, 0.9075, 0.9082, 0.9090, 0.9098, 0.9106, 0.9106, 0.9113, 0.9121,\n",
       "             0.9129, 0.9136, 0.9144, 0.9152, 0.9160, 0.9167, 0.9175, 0.9183, 0.9183,\n",
       "             0.9190, 0.9190, 0.9198, 0.9206, 0.9214, 0.9221, 0.9229, 0.9237, 0.9244,\n",
       "             0.9252, 0.9260, 0.9268, 0.9275, 0.9283, 0.9291, 0.9298, 0.9306, 0.9314,\n",
       "             0.9322, 0.9322, 0.9329, 0.9337, 0.9345, 0.9352, 0.9360, 0.9368, 0.9375,\n",
       "             0.9383, 0.9391, 0.9399, 0.9406, 0.9414, 0.9422, 0.9429, 0.9437, 0.9445,\n",
       "             0.9453, 0.9460, 0.9468, 0.9476, 0.9483, 0.9483, 0.9491, 0.9499, 0.9507,\n",
       "             0.9514, 0.9522, 0.9530, 0.9537, 0.9545, 0.9553, 0.9553, 0.9561, 0.9568,\n",
       "             0.9568, 0.9568, 0.9576, 0.9584, 0.9591, 0.9591, 0.9599, 0.9607, 0.9614,\n",
       "             0.9622, 0.9630, 0.9638, 0.9645, 0.9653, 0.9661, 0.9661, 0.9668, 0.9668,\n",
       "             0.9676, 0.9684, 0.9692, 0.9699, 0.9707, 0.9715, 0.9722, 0.9730, 0.9738,\n",
       "             0.9746, 0.9746, 0.9753, 0.9761, 0.9761, 0.9769, 0.9776, 0.9784, 0.9792,\n",
       "             0.9792, 0.9800, 0.9800, 0.9807, 0.9807, 0.9815, 0.9823, 0.9823, 0.9830,\n",
       "             0.9830, 0.9830, 0.9838, 0.9838, 0.9838, 0.9846, 0.9854, 0.9854, 0.9861,\n",
       "             0.9861, 0.9861, 0.9861, 0.9861, 0.9869, 0.9877, 0.9877, 0.9877, 0.9877,\n",
       "             0.9877, 0.9884, 0.9884, 0.9884, 0.9892, 0.9892, 0.9892, 0.9892, 0.9900,\n",
       "             0.9907, 0.9907, 0.9907, 0.9907, 0.9907, 0.9915, 0.9915, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9946,\n",
       "             0.9946, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9983e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9978e-01, 9.9977e-01, 9.9974e-01, 9.9970e-01, 9.9970e-01,\n",
       "             9.9970e-01, 9.9970e-01, 9.9969e-01, 9.9968e-01, 9.9966e-01, 9.9965e-01,\n",
       "             9.9956e-01, 9.9956e-01, 9.9955e-01, 9.9954e-01, 9.9951e-01, 9.9950e-01,\n",
       "             9.9948e-01, 9.9945e-01, 9.9944e-01, 9.9944e-01, 9.9938e-01, 9.9938e-01,\n",
       "             9.9935e-01, 9.9932e-01, 9.9930e-01, 9.9926e-01, 9.9925e-01, 9.9917e-01,\n",
       "             9.9913e-01, 9.9913e-01, 9.9912e-01, 9.9910e-01, 9.9907e-01, 9.9900e-01,\n",
       "             9.9898e-01, 9.9893e-01, 9.9880e-01, 9.9876e-01, 9.9872e-01, 9.9870e-01,\n",
       "             9.9848e-01, 9.9839e-01, 9.9825e-01, 9.9805e-01, 9.9801e-01, 9.9778e-01,\n",
       "             9.9774e-01, 9.9767e-01, 9.9742e-01, 9.9684e-01, 9.9682e-01, 9.9625e-01,\n",
       "             9.9620e-01, 9.9615e-01, 9.9554e-01, 9.9552e-01, 9.9550e-01, 9.9533e-01,\n",
       "             9.9521e-01, 9.9513e-01, 9.9302e-01, 9.9300e-01, 9.9257e-01, 9.9202e-01,\n",
       "             9.9089e-01, 9.8961e-01, 9.8845e-01, 9.8836e-01, 9.8679e-01, 9.8656e-01,\n",
       "             9.8559e-01, 9.8447e-01, 9.8192e-01, 9.7570e-01, 9.7283e-01, 9.7146e-01,\n",
       "             9.6798e-01, 9.6602e-01, 9.6464e-01, 9.6031e-01, 9.5273e-01, 9.4947e-01,\n",
       "             9.3756e-01, 9.3625e-01, 9.1701e-01, 9.0989e-01, 8.8930e-01, 8.8789e-01,\n",
       "             8.8143e-01, 8.8098e-01, 8.6887e-01, 8.4868e-01, 8.2560e-01, 7.9499e-01,\n",
       "             7.8787e-01, 7.8445e-01, 7.4816e-01, 7.4108e-01, 6.9641e-01, 6.9326e-01,\n",
       "             6.2751e-01, 6.2698e-01, 5.4328e-01, 5.2664e-01, 5.0592e-01, 4.9281e-01,\n",
       "             4.2870e-01, 4.1583e-01, 3.9302e-01, 3.8338e-01, 3.6972e-01, 3.4130e-01,\n",
       "             2.7062e-01, 2.6872e-01, 2.5268e-01, 2.3387e-01, 1.9190e-01, 1.7482e-01,\n",
       "             1.7260e-01, 1.5634e-01, 1.2367e-01, 1.2242e-01, 1.0817e-01, 1.0088e-01,\n",
       "             9.3642e-02, 8.8453e-02, 6.7297e-02, 6.5828e-02, 3.3517e-02, 3.0897e-02,\n",
       "             2.8513e-02, 2.5925e-02, 1.9265e-02, 1.8934e-02, 1.7480e-02, 1.3429e-02,\n",
       "             1.2762e-02, 1.1868e-02, 1.1610e-02, 7.5411e-03, 7.4770e-03, 6.0688e-03,\n",
       "             5.9230e-03, 3.6226e-03, 2.4167e-03, 2.3177e-03, 2.1302e-03, 1.6426e-03,\n",
       "             1.4827e-03, 1.3507e-03, 1.1429e-03, 9.8532e-04, 6.2112e-04, 5.5079e-04,\n",
       "             4.7378e-04, 4.1974e-04, 3.4881e-04, 3.2762e-04, 2.4958e-04, 2.2685e-04,\n",
       "             1.8449e-04, 1.5225e-04, 1.3060e-04, 1.2001e-04, 1.0033e-04, 9.4196e-05,\n",
       "             8.8026e-05, 8.4968e-05, 8.2259e-05, 7.3090e-05, 6.3197e-05, 5.8134e-05,\n",
       "             4.6040e-05, 4.4982e-05, 4.3338e-05, 3.9929e-05, 3.4190e-05, 2.5431e-05,\n",
       "             2.3436e-05, 2.3089e-05, 2.0080e-05, 1.9223e-05, 1.8621e-05, 1.6124e-05,\n",
       "             1.5769e-05, 1.3925e-05, 1.1389e-05, 1.0617e-05, 9.1162e-06, 8.3474e-06,\n",
       "             7.9944e-06, 7.3129e-06, 5.4109e-06, 5.0818e-06, 4.8265e-06, 4.0124e-06,\n",
       "             3.9231e-06, 3.4259e-06, 3.0708e-06, 2.8959e-06, 2.6909e-06, 2.5753e-06,\n",
       "             2.3933e-06, 1.8441e-06, 1.7479e-06, 1.6982e-06, 1.6962e-06, 1.6715e-06,\n",
       "             1.0683e-06, 9.9442e-07, 9.5740e-07, 9.2540e-07, 8.7966e-07, 8.2993e-07,\n",
       "             7.8562e-07, 7.3557e-07, 7.1305e-07, 7.0771e-07, 6.9961e-07, 6.9092e-07,\n",
       "             6.2662e-07, 6.2230e-07, 5.5690e-07, 5.3523e-07, 5.2004e-07, 5.0003e-07,\n",
       "             4.3959e-07, 4.3710e-07, 4.3540e-07, 4.2564e-07, 4.0174e-07, 3.9894e-07,\n",
       "             2.7961e-07, 2.4676e-07, 2.4303e-07, 2.2330e-07, 2.2180e-07, 2.1628e-07,\n",
       "             1.9022e-07, 1.8313e-07, 1.4844e-07, 1.3967e-07, 1.2649e-07, 1.1417e-07,\n",
       "             1.0448e-07, 9.9531e-08, 9.2689e-08, 8.1188e-08, 7.4256e-08, 6.9331e-08,\n",
       "             6.5879e-08, 6.1553e-08, 5.3380e-08, 5.2032e-08, 4.8876e-08, 4.6306e-08,\n",
       "             4.5625e-08, 4.1203e-08, 4.0202e-08, 3.8496e-08, 3.5206e-08, 3.2286e-08,\n",
       "             3.2036e-08, 3.2013e-08, 3.1441e-08, 2.9880e-08, 2.4769e-08, 2.3758e-08,\n",
       "             2.1718e-08, 2.1663e-08, 1.9700e-08, 1.9640e-08, 1.9129e-08, 1.5279e-08,\n",
       "             1.4542e-08, 1.2382e-08, 1.2156e-08, 1.0833e-08, 9.4497e-09, 9.3235e-09,\n",
       "             8.4308e-09, 8.0082e-09, 7.8231e-09, 7.7446e-09, 6.3516e-09, 5.8132e-09,\n",
       "             5.7231e-09, 5.6507e-09, 5.1590e-09, 5.1422e-09, 4.9524e-09, 4.6442e-09,\n",
       "             4.6009e-09, 3.0408e-09, 2.9691e-09, 2.8863e-09, 2.7099e-09, 2.6238e-09,\n",
       "             2.3306e-09, 2.3071e-09, 2.2270e-09, 1.9925e-09, 1.7963e-09, 1.7261e-09,\n",
       "             1.6915e-09, 1.5622e-09, 1.3964e-09, 1.3003e-09, 1.1326e-09, 1.0539e-09,\n",
       "             1.0017e-09, 1.0008e-09, 8.7122e-10, 8.4382e-10, 7.3663e-10, 6.4190e-10,\n",
       "             5.9501e-10, 4.5880e-10, 4.4731e-10, 4.1420e-10, 3.4573e-10, 2.9798e-10,\n",
       "             2.9731e-10, 2.8345e-10, 2.6559e-10, 2.3533e-10, 2.1250e-10, 1.7911e-10,\n",
       "             1.4975e-10, 1.4278e-10, 1.4050e-10, 1.3906e-10, 1.2830e-10, 1.2367e-10,\n",
       "             1.2013e-10, 1.1177e-10, 1.0429e-10, 8.6846e-11, 8.1244e-11, 7.1988e-11,\n",
       "             7.1026e-11, 6.9176e-11, 5.3668e-11, 5.2216e-11, 5.2110e-11, 4.7176e-11,\n",
       "             4.0573e-11, 2.8294e-11, 1.7966e-11, 1.7812e-11, 1.7735e-11, 1.6286e-11,\n",
       "             1.5991e-11, 1.5200e-11, 1.2974e-11, 1.2162e-11, 1.1181e-11, 1.0286e-11,\n",
       "             9.2691e-12, 8.8454e-12, 8.1572e-12, 8.0109e-12, 7.4418e-12, 5.5819e-12,\n",
       "             4.8355e-12, 4.7456e-12, 4.4087e-12, 3.4864e-12, 3.4056e-12, 3.3480e-12,\n",
       "             2.7767e-12, 2.4120e-12, 2.3167e-12, 2.1682e-12, 1.8630e-12, 1.3954e-12,\n",
       "             1.3937e-12, 1.3240e-12, 1.2872e-12, 1.2753e-12, 9.9672e-13, 9.8911e-13,\n",
       "             8.5089e-13, 8.0237e-13, 5.7663e-13, 5.5605e-13, 5.3625e-13, 4.3259e-13,\n",
       "             4.2250e-13, 3.8344e-13, 3.8023e-13, 3.6221e-13, 3.6030e-13, 3.5324e-13,\n",
       "             3.0270e-13, 2.7557e-13, 2.7330e-13, 2.6236e-13, 2.5323e-13, 2.5214e-13,\n",
       "             2.3583e-13, 2.1393e-13, 1.7164e-13, 1.4691e-13, 1.2522e-13, 1.2437e-13,\n",
       "             1.1939e-13, 1.1862e-13, 1.1578e-13, 9.8930e-14, 9.7951e-14, 7.5449e-14,\n",
       "             6.9022e-14, 6.8525e-14, 6.7218e-14, 5.9657e-14, 5.9656e-14, 5.6663e-14,\n",
       "             5.5751e-14, 4.5021e-14, 4.0506e-14, 4.0112e-14, 3.7268e-14, 3.7136e-14,\n",
       "             3.7135e-14, 3.4694e-14, 3.1928e-14, 3.1738e-14, 2.9037e-14, 2.6909e-14,\n",
       "             2.2569e-14, 2.2307e-14, 2.1710e-14, 1.9654e-14, 1.8222e-14, 1.6250e-14,\n",
       "             1.2441e-14, 1.2068e-14, 1.1652e-14, 1.0579e-14, 1.0306e-14, 1.0084e-14,\n",
       "             1.0068e-14, 1.0023e-14, 9.6041e-15, 9.2632e-15, 9.2627e-15, 9.1572e-15,\n",
       "             8.7138e-15, 7.9138e-15, 7.4582e-15, 6.6908e-15, 6.2154e-15, 6.2031e-15,\n",
       "             5.9459e-15, 5.5256e-15, 5.1586e-15, 4.5962e-15, 4.2546e-15, 3.5567e-15,\n",
       "             2.0686e-15, 1.6488e-15, 1.4422e-15, 1.2917e-15, 1.0649e-15, 9.7545e-16,\n",
       "             9.0588e-16, 8.3919e-16, 6.8969e-16, 6.7189e-16, 6.3240e-16, 6.0164e-16,\n",
       "             5.1604e-16, 4.7012e-16, 2.9872e-16, 2.8951e-16, 2.5580e-16, 2.3727e-16,\n",
       "             1.7510e-16, 1.7118e-16, 1.3166e-16, 1.0411e-16, 8.7037e-17, 6.1282e-17,\n",
       "             5.3653e-17, 5.1527e-17, 4.8221e-17, 3.9889e-17, 3.5242e-17, 3.4206e-17,\n",
       "             3.2101e-17, 3.1012e-17, 3.0335e-17, 2.8767e-17, 2.8535e-17, 2.2580e-17,\n",
       "             2.2370e-17, 1.9604e-17, 1.8724e-17, 1.1711e-17, 1.0766e-17, 1.0657e-17,\n",
       "             8.2667e-18, 6.5744e-18, 5.5084e-18, 4.7812e-18, 4.0157e-18, 2.6786e-18,\n",
       "             2.5875e-18, 1.5044e-18, 1.1649e-18, 1.0730e-18, 8.3116e-19, 5.0923e-19,\n",
       "             3.9749e-19, 3.2570e-19, 2.7310e-19, 2.3715e-19, 2.0993e-19, 1.6465e-19,\n",
       "             1.4710e-19, 9.2204e-20, 9.0355e-20, 7.3204e-20, 6.8125e-20, 6.5396e-20,\n",
       "             6.3989e-20, 5.8892e-20, 5.0209e-20, 3.3937e-20, 3.1637e-20, 1.4287e-20,\n",
       "             1.3287e-20, 1.2989e-20, 1.2342e-20, 1.2293e-20, 8.4095e-21, 7.6506e-21,\n",
       "             5.0675e-21, 3.5608e-21, 3.0291e-21, 2.5610e-21, 2.4813e-21, 2.1914e-21,\n",
       "             2.0064e-21, 2.0049e-21, 1.7606e-21, 9.7156e-22, 6.5626e-22, 5.0318e-22,\n",
       "             4.6551e-22, 4.3548e-22, 3.1394e-22, 5.4769e-23, 3.2063e-23, 1.8378e-23,\n",
       "             1.8316e-23, 1.1620e-23, 1.8244e-24, 8.7877e-25, 1.4962e-25, 4.0136e-26,\n",
       "             2.4367e-26, 1.7517e-26, 4.6943e-27, 2.4627e-27, 2.1358e-27, 3.8827e-28,\n",
       "             1.6604e-32, 0.0000e+00])}},\n",
       "   {'fpr': np.float64(0.061269146608315096),\n",
       "    'tpr': np.float64(0.9907478797224364),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "             0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044, 0.0044,\n",
       "             0.0044, 0.0044, 0.0044, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088,\n",
       "             0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0109, 0.0109,\n",
       "             0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0131,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0175, 0.0175,\n",
       "             0.0175, 0.0197, 0.0219, 0.0219, 0.0219, 0.0241, 0.0241, 0.0241, 0.0241,\n",
       "             0.0241, 0.0241, 0.0263, 0.0263, 0.0263, 0.0263, 0.0284, 0.0284, 0.0284,\n",
       "             0.0306, 0.0306, 0.0306, 0.0306, 0.0350, 0.0350, 0.0372, 0.0394, 0.0394,\n",
       "             0.0394, 0.0394, 0.0416, 0.0438, 0.0438, 0.0438, 0.0438, 0.0460, 0.0460,\n",
       "             0.0481, 0.0503, 0.0525, 0.0525, 0.0525, 0.0525, 0.0525, 0.0547, 0.0569,\n",
       "             0.0591, 0.0613, 0.0635, 0.0635, 0.0635, 0.0656, 0.0678, 0.0700, 0.0722,\n",
       "             0.0744, 0.0766, 0.0788, 0.0788, 0.0788, 0.0810, 0.0810, 0.0832, 0.0832,\n",
       "             0.0853, 0.0875, 0.0897, 0.0919, 0.0919, 0.0941, 0.0963, 0.0985, 0.1007,\n",
       "             0.1028, 0.1028, 0.1050, 0.1072, 0.1094, 0.1116, 0.1138, 0.1160, 0.1160,\n",
       "             0.1182, 0.1204, 0.1225, 0.1247, 0.1269, 0.1291, 0.1313, 0.1335, 0.1357,\n",
       "             0.1379, 0.1400, 0.1400, 0.1422, 0.1444, 0.1466, 0.1488, 0.1510, 0.1532,\n",
       "             0.1554, 0.1575, 0.1597, 0.1619, 0.1641, 0.1663, 0.1685, 0.1707, 0.1729,\n",
       "             0.1751, 0.1772, 0.1794, 0.1816, 0.1838, 0.1860, 0.1882, 0.1904, 0.1926,\n",
       "             0.1947, 0.1969, 0.1991, 0.2013, 0.2035, 0.2057, 0.2079, 0.2101, 0.2123,\n",
       "             0.2144, 0.2166, 0.2188, 0.2210, 0.2232, 0.2254, 0.2276, 0.2298, 0.2319,\n",
       "             0.2341, 0.2363, 0.2385, 0.2385, 0.2407, 0.2429, 0.2451, 0.2473, 0.2495,\n",
       "             0.2516, 0.2538, 0.2560, 0.2582, 0.2604, 0.2626, 0.2648, 0.2670, 0.2691,\n",
       "             0.2713, 0.2735, 0.2757, 0.2779, 0.2801, 0.2823, 0.2845, 0.2867, 0.2888,\n",
       "             0.2888, 0.2910, 0.2932, 0.2954, 0.2976, 0.2998, 0.3020, 0.3042, 0.3063,\n",
       "             0.3085, 0.3107, 0.3129, 0.3151, 0.3173, 0.3195, 0.3217, 0.3239, 0.3260,\n",
       "             0.3282, 0.3304, 0.3326, 0.3348, 0.3370, 0.3392, 0.3414, 0.3435, 0.3457,\n",
       "             0.3479, 0.3501, 0.3523, 0.3545, 0.3567, 0.3589, 0.3611, 0.3632, 0.3654,\n",
       "             0.3676, 0.3698, 0.3720, 0.3742, 0.3764, 0.3786, 0.3807, 0.3829, 0.3851,\n",
       "             0.3873, 0.3895, 0.3917, 0.3939, 0.3961, 0.3982, 0.4004, 0.4026, 0.4048,\n",
       "             0.4070, 0.4092, 0.4114, 0.4136, 0.4158, 0.4179, 0.4201, 0.4223, 0.4245,\n",
       "             0.4267, 0.4289, 0.4311, 0.4333, 0.4354, 0.4376, 0.4398, 0.4420, 0.4442,\n",
       "             0.4464, 0.4486, 0.4508, 0.4530, 0.4551, 0.4573, 0.4595, 0.4617, 0.4639,\n",
       "             0.4661, 0.4683, 0.4705, 0.4726, 0.4748, 0.4770, 0.4792, 0.4814, 0.4836,\n",
       "             0.4858, 0.4880, 0.4902, 0.4923, 0.4945, 0.4967, 0.4989, 0.5011, 0.5033,\n",
       "             0.5055, 0.5077, 0.5098, 0.5120, 0.5142, 0.5164, 0.5186, 0.5208, 0.5230,\n",
       "             0.5252, 0.5274, 0.5295, 0.5317, 0.5339, 0.5361, 0.5383, 0.5405, 0.5427,\n",
       "             0.5449, 0.5470, 0.5492, 0.5514, 0.5536, 0.5558, 0.5580, 0.5602, 0.5624,\n",
       "             0.5646, 0.5667, 0.5689, 0.5711, 0.5733, 0.5755, 0.5777, 0.5799, 0.5821,\n",
       "             0.5842, 0.5864, 0.5886, 0.5908, 0.5930, 0.5952, 0.5974, 0.5996, 0.6018,\n",
       "             0.6039, 0.6061, 0.6083, 0.6105, 0.6127, 0.6149, 0.6171, 0.6193, 0.6214,\n",
       "             0.6236, 0.6258, 0.6280, 0.6302, 0.6324, 0.6346, 0.6368, 0.6389, 0.6411,\n",
       "             0.6433, 0.6455, 0.6477, 0.6499, 0.6521, 0.6543, 0.6565, 0.6586, 0.6608,\n",
       "             0.6630, 0.6652, 0.6674, 0.6696, 0.6718, 0.6740, 0.6761, 0.6783, 0.6805,\n",
       "             0.6827, 0.6849, 0.6871, 0.6893, 0.6915, 0.6937, 0.6958, 0.6980, 0.7002,\n",
       "             0.7024, 0.7046, 0.7068, 0.7090, 0.7112, 0.7133, 0.7155, 0.7177, 0.7199,\n",
       "             0.7221, 0.7243, 0.7265, 0.7287, 0.7309, 0.7330, 0.7352, 0.7374, 0.7396,\n",
       "             0.7418, 0.7440, 0.7462, 0.7484, 0.7505, 0.7527, 0.7549, 0.7571, 0.7593,\n",
       "             0.7615, 0.7637, 0.7659, 0.7681, 0.7702, 0.7724, 0.7746, 0.7768, 0.7790,\n",
       "             0.7812, 0.7834, 0.7856, 0.7877, 0.7899, 0.7921, 0.7943, 0.7965, 0.7987,\n",
       "             0.8009, 0.8031, 0.8053, 0.8074, 0.8096, 0.8118, 0.8140, 0.8162, 0.8184,\n",
       "             0.8206, 0.8228, 0.8249, 0.8271, 0.8293, 0.8315, 0.8337, 0.8359, 0.8381,\n",
       "             0.8403, 0.8425, 0.8446, 0.8468, 0.8490, 0.8512, 0.8534, 0.8556, 0.8578,\n",
       "             0.8600, 0.8621, 0.8643, 0.8665, 0.8687, 0.8709, 0.8731, 0.8753, 0.8775,\n",
       "             0.8796, 0.8818, 0.8840, 0.8862, 0.8884, 0.8906, 0.8928, 0.8950, 0.8972,\n",
       "             0.8993, 0.9015, 0.9037, 0.9059, 0.9081, 0.9103, 0.9125, 0.9147, 0.9168,\n",
       "             0.9190, 0.9212, 0.9234, 0.9256, 0.9278, 0.9300, 0.9322, 0.9344, 0.9365,\n",
       "             0.9387, 0.9409, 0.9431, 0.9453, 0.9475, 0.9497, 0.9519, 0.9540, 0.9562,\n",
       "             0.9584, 0.9606, 0.9628, 0.9650, 0.9672, 0.9694, 0.9716, 0.9737, 0.9759,\n",
       "             0.9781, 0.9803, 0.9825, 0.9847, 0.9869, 0.9891, 0.9912, 0.9934, 0.9956,\n",
       "             0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.6045, 0.6584, 0.6962, 0.7194, 0.7325, 0.7417, 0.7533, 0.7602,\n",
       "             0.7679, 0.7741, 0.7779, 0.7818, 0.7849, 0.7872, 0.7887, 0.7934, 0.7949,\n",
       "             0.7972, 0.7988, 0.8011, 0.8042, 0.8065, 0.8103, 0.8126, 0.8150, 0.8157,\n",
       "             0.8173, 0.8180, 0.8196, 0.8219, 0.8227, 0.8234, 0.8242, 0.8250, 0.8265,\n",
       "             0.8273, 0.8304, 0.8319, 0.8335, 0.8358, 0.8389, 0.8396, 0.8419, 0.8435,\n",
       "             0.8443, 0.8458, 0.8466, 0.8473, 0.8481, 0.8497, 0.8504, 0.8520, 0.8551,\n",
       "             0.8558, 0.8566, 0.8574, 0.8604, 0.8620, 0.8628, 0.8635, 0.8643, 0.8651,\n",
       "             0.8658, 0.8666, 0.8674, 0.8689, 0.8697, 0.8705, 0.8712, 0.8720, 0.8728,\n",
       "             0.8743, 0.8751, 0.8759, 0.8766, 0.8774, 0.8782, 0.8790, 0.8797, 0.8805,\n",
       "             0.8820, 0.8828, 0.8836, 0.8843, 0.8851, 0.8859, 0.8867, 0.8874, 0.8882,\n",
       "             0.8897, 0.8905, 0.8913, 0.8921, 0.8936, 0.8936, 0.8944, 0.8951, 0.8959,\n",
       "             0.8975, 0.8982, 0.8990, 0.8998, 0.9013, 0.9021, 0.9029, 0.9036, 0.9044,\n",
       "             0.9052, 0.9059, 0.9067, 0.9075, 0.9082, 0.9090, 0.9098, 0.9106, 0.9113,\n",
       "             0.9121, 0.9129, 0.9136, 0.9144, 0.9152, 0.9160, 0.9167, 0.9175, 0.9183,\n",
       "             0.9190, 0.9198, 0.9206, 0.9214, 0.9221, 0.9229, 0.9237, 0.9244, 0.9252,\n",
       "             0.9260, 0.9268, 0.9275, 0.9283, 0.9291, 0.9298, 0.9306, 0.9314, 0.9322,\n",
       "             0.9329, 0.9337, 0.9345, 0.9345, 0.9352, 0.9360, 0.9368, 0.9375, 0.9383,\n",
       "             0.9391, 0.9399, 0.9406, 0.9414, 0.9422, 0.9429, 0.9437, 0.9445, 0.9453,\n",
       "             0.9460, 0.9468, 0.9476, 0.9483, 0.9483, 0.9491, 0.9499, 0.9507, 0.9514,\n",
       "             0.9522, 0.9530, 0.9537, 0.9545, 0.9553, 0.9561, 0.9568, 0.9568, 0.9576,\n",
       "             0.9584, 0.9591, 0.9599, 0.9607, 0.9614, 0.9622, 0.9630, 0.9638, 0.9638,\n",
       "             0.9638, 0.9645, 0.9653, 0.9661, 0.9668, 0.9676, 0.9684, 0.9684, 0.9692,\n",
       "             0.9699, 0.9699, 0.9699, 0.9707, 0.9715, 0.9715, 0.9722, 0.9730, 0.9738,\n",
       "             0.9746, 0.9753, 0.9753, 0.9761, 0.9769, 0.9776, 0.9776, 0.9784, 0.9792,\n",
       "             0.9792, 0.9800, 0.9807, 0.9815, 0.9815, 0.9823, 0.9823, 0.9823, 0.9830,\n",
       "             0.9838, 0.9846, 0.9846, 0.9846, 0.9854, 0.9861, 0.9869, 0.9869, 0.9877,\n",
       "             0.9877, 0.9877, 0.9877, 0.9884, 0.9892, 0.9900, 0.9907, 0.9907, 0.9907,\n",
       "             0.9907, 0.9907, 0.9907, 0.9915, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9931, 0.9938, 0.9938, 0.9946, 0.9946, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9986e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9978e-01, 9.9975e-01, 9.9975e-01, 9.9973e-01,\n",
       "             9.9962e-01, 9.9961e-01, 9.9960e-01, 9.9960e-01, 9.9959e-01, 9.9957e-01,\n",
       "             9.9956e-01, 9.9956e-01, 9.9956e-01, 9.9956e-01, 9.9951e-01, 9.9943e-01,\n",
       "             9.9941e-01, 9.9941e-01, 9.9934e-01, 9.9929e-01, 9.9922e-01, 9.9915e-01,\n",
       "             9.9911e-01, 9.9910e-01, 9.9904e-01, 9.9902e-01, 9.9899e-01, 9.9897e-01,\n",
       "             9.9895e-01, 9.9876e-01, 9.9868e-01, 9.9865e-01, 9.9863e-01, 9.9863e-01,\n",
       "             9.9849e-01, 9.9810e-01, 9.9806e-01, 9.9798e-01, 9.9789e-01, 9.9788e-01,\n",
       "             9.9784e-01, 9.9766e-01, 9.9710e-01, 9.9709e-01, 9.9678e-01, 9.9659e-01,\n",
       "             9.9649e-01, 9.9629e-01, 9.9607e-01, 9.9595e-01, 9.9583e-01, 9.9571e-01,\n",
       "             9.9557e-01, 9.9479e-01, 9.9353e-01, 9.9284e-01, 9.9253e-01, 9.9011e-01,\n",
       "             9.8809e-01, 9.8799e-01, 9.8673e-01, 9.8669e-01, 9.8191e-01, 9.7897e-01,\n",
       "             9.7214e-01, 9.6386e-01, 9.6108e-01, 9.6080e-01, 9.4906e-01, 9.4736e-01,\n",
       "             9.3852e-01, 9.3664e-01, 9.3593e-01, 9.2427e-01, 9.2231e-01, 9.1761e-01,\n",
       "             9.1641e-01, 9.1354e-01, 9.0368e-01, 8.5398e-01, 8.5232e-01, 8.5023e-01,\n",
       "             8.1754e-01, 7.9930e-01, 7.6838e-01, 7.4965e-01, 7.1277e-01, 6.5931e-01,\n",
       "             6.5810e-01, 6.4432e-01, 6.0154e-01, 5.8503e-01, 5.3069e-01, 4.9408e-01,\n",
       "             4.5530e-01, 4.3634e-01, 4.0460e-01, 3.5683e-01, 3.2381e-01, 3.1393e-01,\n",
       "             2.9835e-01, 2.8763e-01, 2.6149e-01, 2.6076e-01, 2.4290e-01, 2.3642e-01,\n",
       "             2.2661e-01, 1.8182e-01, 1.5832e-01, 1.5625e-01, 1.5080e-01, 1.4561e-01,\n",
       "             9.4372e-02, 9.3388e-02, 8.2014e-02, 7.8675e-02, 6.2715e-02, 6.1108e-02,\n",
       "             4.9559e-02, 4.6741e-02, 4.0190e-02, 2.8935e-02, 2.5137e-02, 2.1523e-02,\n",
       "             1.5646e-02, 1.5533e-02, 1.2382e-02, 1.1824e-02, 1.1233e-02, 1.0367e-02,\n",
       "             8.6849e-03, 7.8290e-03, 7.5506e-03, 7.2312e-03, 6.0080e-03, 5.9739e-03,\n",
       "             5.9584e-03, 4.7954e-03, 4.7637e-03, 3.9076e-03, 3.8788e-03, 3.0528e-03,\n",
       "             2.9713e-03, 2.6388e-03, 1.9619e-03, 1.6498e-03, 1.5774e-03, 1.4187e-03,\n",
       "             1.2231e-03, 1.0331e-03, 9.9344e-04, 9.8440e-04, 8.4685e-04, 8.3958e-04,\n",
       "             7.9871e-04, 7.0326e-04, 6.5693e-04, 6.2444e-04, 6.1839e-04, 5.8599e-04,\n",
       "             5.7070e-04, 5.1048e-04, 5.0567e-04, 4.7373e-04, 4.4405e-04, 3.7423e-04,\n",
       "             3.6873e-04, 3.4350e-04, 3.2400e-04, 2.9988e-04, 2.8661e-04, 2.8256e-04,\n",
       "             2.3673e-04, 2.1687e-04, 2.0465e-04, 2.0450e-04, 1.9761e-04, 1.9518e-04,\n",
       "             1.9510e-04, 1.8978e-04, 1.8879e-04, 1.8565e-04, 1.7995e-04, 1.7877e-04,\n",
       "             1.6291e-04, 1.5827e-04, 1.5123e-04, 1.1740e-04, 9.8256e-05, 9.6953e-05,\n",
       "             8.6550e-05, 7.2971e-05, 7.1916e-05, 6.8389e-05, 6.5342e-05, 6.3007e-05,\n",
       "             5.4755e-05, 4.8175e-05, 4.7589e-05, 4.7209e-05, 4.6585e-05, 4.6053e-05,\n",
       "             4.5496e-05, 4.3109e-05, 3.6102e-05, 2.9135e-05, 2.7930e-05, 2.7437e-05,\n",
       "             2.5846e-05, 2.4231e-05, 2.3626e-05, 2.2021e-05, 2.1638e-05, 2.0478e-05,\n",
       "             2.0200e-05, 1.6635e-05, 1.6545e-05, 1.5392e-05, 1.5121e-05, 1.5088e-05,\n",
       "             1.4372e-05, 1.4326e-05, 1.4231e-05, 1.3428e-05, 1.3353e-05, 1.2359e-05,\n",
       "             1.1977e-05, 1.1900e-05, 1.1831e-05, 1.0230e-05, 7.9245e-06, 7.8797e-06,\n",
       "             6.6243e-06, 6.5143e-06, 5.8185e-06, 5.5805e-06, 5.2237e-06, 4.9237e-06,\n",
       "             4.5276e-06, 4.3803e-06, 4.3441e-06, 4.1130e-06, 3.3661e-06, 3.3485e-06,\n",
       "             3.2851e-06, 2.9543e-06, 2.8388e-06, 2.7937e-06, 2.7090e-06, 2.6538e-06,\n",
       "             2.6133e-06, 2.5734e-06, 2.5629e-06, 2.1443e-06, 1.9745e-06, 1.9035e-06,\n",
       "             1.8460e-06, 1.6046e-06, 1.5702e-06, 1.4265e-06, 1.2678e-06, 1.2239e-06,\n",
       "             1.1767e-06, 1.0134e-06, 9.5483e-07, 9.3893e-07, 9.3610e-07, 9.3244e-07,\n",
       "             6.3498e-07, 5.3713e-07, 5.2178e-07, 5.1064e-07, 4.9908e-07, 4.8558e-07,\n",
       "             4.0513e-07, 3.4788e-07, 3.4157e-07, 3.3081e-07, 2.8969e-07, 2.7541e-07,\n",
       "             2.7331e-07, 2.6758e-07, 2.1983e-07, 2.1108e-07, 2.0874e-07, 1.9332e-07,\n",
       "             1.8045e-07, 1.6340e-07, 1.5043e-07, 1.3988e-07, 1.2511e-07, 1.1745e-07,\n",
       "             1.0779e-07, 1.0758e-07, 1.0757e-07, 1.0632e-07, 1.0027e-07, 9.6456e-08,\n",
       "             9.5392e-08, 8.9805e-08, 8.6619e-08, 8.4675e-08, 7.9657e-08, 7.8105e-08,\n",
       "             7.5774e-08, 7.0271e-08, 6.8664e-08, 6.7219e-08, 6.4274e-08, 6.1177e-08,\n",
       "             5.5998e-08, 5.5199e-08, 5.4543e-08, 5.4542e-08, 5.3401e-08, 5.2622e-08,\n",
       "             4.9253e-08, 4.0120e-08, 3.9574e-08, 3.8945e-08, 3.8240e-08, 3.6375e-08,\n",
       "             3.2695e-08, 2.7293e-08, 2.4170e-08, 2.2954e-08, 2.2069e-08, 2.0949e-08,\n",
       "             2.0369e-08, 1.9137e-08, 1.9030e-08, 1.8548e-08, 1.7752e-08, 1.7714e-08,\n",
       "             1.7518e-08, 1.5070e-08, 1.4038e-08, 1.1204e-08, 9.5632e-09, 9.3426e-09,\n",
       "             9.2668e-09, 9.0894e-09, 8.4477e-09, 7.8533e-09, 6.6770e-09, 5.9933e-09,\n",
       "             5.5640e-09, 5.4454e-09, 5.2951e-09, 5.0230e-09, 4.9526e-09, 4.6162e-09,\n",
       "             4.2429e-09, 3.9514e-09, 3.8426e-09, 3.7576e-09, 3.7544e-09, 3.5100e-09,\n",
       "             3.4662e-09, 3.2956e-09, 3.2441e-09, 3.1237e-09, 3.0033e-09, 2.9679e-09,\n",
       "             2.6669e-09, 2.5233e-09, 2.4292e-09, 2.2560e-09, 2.1137e-09, 2.0657e-09,\n",
       "             1.9939e-09, 1.8719e-09, 1.6499e-09, 1.6410e-09, 1.5156e-09, 1.3811e-09,\n",
       "             1.3779e-09, 1.3729e-09, 8.1632e-10, 6.7268e-10, 6.3187e-10, 6.3008e-10,\n",
       "             5.5522e-10, 5.3885e-10, 5.3513e-10, 5.1621e-10, 4.9918e-10, 4.1046e-10,\n",
       "             4.0951e-10, 3.5607e-10, 3.0442e-10, 2.9611e-10, 2.8701e-10, 2.7852e-10,\n",
       "             2.7819e-10, 2.6920e-10, 2.6825e-10, 2.1891e-10, 2.0259e-10, 2.0140e-10,\n",
       "             1.9715e-10, 1.8832e-10, 1.7728e-10, 1.2683e-10, 1.2614e-10, 1.0490e-10,\n",
       "             1.0485e-10, 1.0374e-10, 8.0667e-11, 7.4372e-11, 7.4027e-11, 6.2871e-11,\n",
       "             6.1000e-11, 5.6195e-11, 5.3822e-11, 4.9766e-11, 4.8138e-11, 4.4681e-11,\n",
       "             4.4352e-11, 3.5720e-11, 3.4382e-11, 3.4311e-11, 3.3274e-11, 2.9519e-11,\n",
       "             2.8399e-11, 2.8245e-11, 2.4770e-11, 2.4509e-11, 2.1379e-11, 2.0710e-11,\n",
       "             1.6902e-11, 1.6085e-11, 1.6015e-11, 1.2907e-11, 1.1931e-11, 1.1168e-11,\n",
       "             1.0687e-11, 8.5436e-12, 6.3324e-12, 6.1415e-12, 5.8269e-12, 5.3994e-12,\n",
       "             4.5857e-12, 4.4333e-12, 4.1933e-12, 3.3020e-12, 3.1496e-12, 2.9698e-12,\n",
       "             2.7556e-12, 2.6785e-12, 2.4329e-12, 2.4303e-12, 2.3635e-12, 2.3390e-12,\n",
       "             2.0426e-12, 1.9567e-12, 1.7049e-12, 1.5935e-12, 1.2299e-12, 1.1800e-12,\n",
       "             1.1716e-12, 1.0614e-12, 1.0464e-12, 1.0426e-12, 9.2262e-13, 8.6084e-13,\n",
       "             7.9155e-13, 6.5539e-13, 6.0710e-13, 5.9436e-13, 5.7558e-13, 5.7091e-13,\n",
       "             5.0429e-13, 3.8492e-13, 3.6948e-13, 3.0178e-13, 2.3608e-13, 1.9039e-13,\n",
       "             1.7035e-13, 1.5368e-13, 1.4615e-13, 1.4535e-13, 1.3743e-13, 1.3142e-13,\n",
       "             1.2443e-13, 1.1172e-13, 9.9869e-14, 8.6908e-14, 8.2265e-14, 7.5600e-14,\n",
       "             6.3284e-14, 6.1480e-14, 6.0972e-14, 4.9364e-14, 4.4484e-14, 4.2917e-14,\n",
       "             3.8630e-14, 3.2976e-14, 3.0688e-14, 2.7688e-14, 2.3333e-14, 1.7328e-14,\n",
       "             1.4776e-14, 9.8161e-15, 9.3212e-15, 7.0671e-15, 6.4536e-15, 5.5081e-15,\n",
       "             3.2416e-15, 1.9213e-15, 1.5455e-15, 1.3455e-15, 1.1868e-15, 1.1525e-15,\n",
       "             1.0947e-15, 7.0872e-16, 4.7988e-16, 2.3385e-16, 2.0016e-16, 1.6938e-16,\n",
       "             1.0228e-16, 1.1242e-17, 8.5507e-18, 4.1455e-18, 3.7197e-18, 2.2839e-18,\n",
       "             1.0994e-18, 5.0826e-19, 3.2045e-19, 2.6517e-19, 1.9113e-20, 5.1875e-21,\n",
       "             4.6044e-23, 4.8530e-29])}},\n",
       "   {'fpr': np.float64(0.13347921225382933),\n",
       "    'tpr': np.float64(0.9946029298380878),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0109, 0.0131, 0.0175, 0.0175, 0.0175, 0.0241, 0.0284, 0.0284,\n",
       "             0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0328, 0.0350, 0.0350,\n",
       "             0.0350, 0.0350, 0.0350, 0.0350, 0.0350, 0.0394, 0.0394, 0.0394, 0.0394,\n",
       "             0.0416, 0.0416, 0.0438, 0.0438, 0.0438, 0.0438, 0.0438, 0.0460, 0.0460,\n",
       "             0.0481, 0.0481, 0.0503, 0.0503, 0.0503, 0.0525, 0.0525, 0.0525, 0.0525,\n",
       "             0.0547, 0.0547, 0.0569, 0.0591, 0.0591, 0.0613, 0.0613, 0.0635, 0.0635,\n",
       "             0.0635, 0.0635, 0.0656, 0.0678, 0.0700, 0.0700, 0.0722, 0.0722, 0.0744,\n",
       "             0.0744, 0.0766, 0.0788, 0.0788, 0.0810, 0.0810, 0.0810, 0.0810, 0.0832,\n",
       "             0.0832, 0.0853, 0.0853, 0.0853, 0.0875, 0.0897, 0.0919, 0.0919, 0.0941,\n",
       "             0.0941, 0.0963, 0.0963, 0.0963, 0.0985, 0.1007, 0.1007, 0.1028, 0.1028,\n",
       "             0.1028, 0.1050, 0.1072, 0.1094, 0.1116, 0.1138, 0.1160, 0.1182, 0.1204,\n",
       "             0.1225, 0.1247, 0.1269, 0.1291, 0.1313, 0.1335, 0.1357, 0.1379, 0.1400,\n",
       "             0.1400, 0.1422, 0.1444, 0.1444, 0.1444, 0.1466, 0.1488, 0.1510, 0.1532,\n",
       "             0.1554, 0.1575, 0.1597, 0.1619, 0.1641, 0.1663, 0.1685, 0.1707, 0.1729,\n",
       "             0.1751, 0.1772, 0.1794, 0.1816, 0.1816, 0.1838, 0.1860, 0.1882, 0.1904,\n",
       "             0.1926, 0.1947, 0.1969, 0.1991, 0.2013, 0.2035, 0.2057, 0.2079, 0.2101,\n",
       "             0.2123, 0.2144, 0.2166, 0.2188, 0.2210, 0.2232, 0.2254, 0.2276, 0.2298,\n",
       "             0.2319, 0.2341, 0.2363, 0.2385, 0.2407, 0.2429, 0.2451, 0.2473, 0.2495,\n",
       "             0.2516, 0.2538, 0.2560, 0.2582, 0.2604, 0.2626, 0.2648, 0.2670, 0.2691,\n",
       "             0.2713, 0.2735, 0.2757, 0.2779, 0.2801, 0.2823, 0.2845, 0.2867, 0.2888,\n",
       "             0.2910, 0.2932, 0.2954, 0.2976, 0.2998, 0.3020, 0.3042, 0.3063, 0.3085,\n",
       "             0.3107, 0.3129, 0.3151, 0.3173, 0.3195, 0.3217, 0.3239, 0.3260, 0.3282,\n",
       "             0.3304, 0.3326, 0.3326, 0.3348, 0.3348, 0.3370, 0.3392, 0.3414, 0.3435,\n",
       "             0.3457, 0.3479, 0.3501, 0.3523, 0.3545, 0.3567, 0.3589, 0.3611, 0.3632,\n",
       "             0.3654, 0.3676, 0.3698, 0.3720, 0.3742, 0.3764, 0.3786, 0.3807, 0.3829,\n",
       "             0.3851, 0.3873, 0.3895, 0.3917, 0.3939, 0.3961, 0.3982, 0.4004, 0.4026,\n",
       "             0.4048, 0.4070, 0.4092, 0.4114, 0.4136, 0.4158, 0.4179, 0.4201, 0.4223,\n",
       "             0.4245, 0.4267, 0.4289, 0.4311, 0.4333, 0.4354, 0.4376, 0.4398, 0.4420,\n",
       "             0.4442, 0.4464, 0.4486, 0.4508, 0.4530, 0.4551, 0.4573, 0.4595, 0.4617,\n",
       "             0.4639, 0.4661, 0.4683, 0.4705, 0.4726, 0.4748, 0.4770, 0.4792, 0.4814,\n",
       "             0.4836, 0.4858, 0.4880, 0.4902, 0.4923, 0.4945, 0.4967, 0.4989, 0.5011,\n",
       "             0.5033, 0.5055, 0.5077, 0.5098, 0.5120, 0.5142, 0.5164, 0.5186, 0.5208,\n",
       "             0.5230, 0.5252, 0.5274, 0.5295, 0.5295, 0.5317, 0.5339, 0.5361, 0.5383,\n",
       "             0.5405, 0.5427, 0.5449, 0.5470, 0.5492, 0.5514, 0.5536, 0.5558, 0.5580,\n",
       "             0.5602, 0.5624, 0.5646, 0.5667, 0.5689, 0.5711, 0.5733, 0.5755, 0.5777,\n",
       "             0.5799, 0.5821, 0.5842, 0.5864, 0.5886, 0.5908, 0.5930, 0.5952, 0.5974,\n",
       "             0.5996, 0.6018, 0.6039, 0.6061, 0.6083, 0.6105, 0.6127, 0.6149, 0.6171,\n",
       "             0.6193, 0.6214, 0.6236, 0.6258, 0.6280, 0.6302, 0.6324, 0.6346, 0.6368,\n",
       "             0.6389, 0.6411, 0.6433, 0.6455, 0.6477, 0.6499, 0.6521, 0.6543, 0.6565,\n",
       "             0.6586, 0.6608, 0.6630, 0.6652, 0.6674, 0.6696, 0.6718, 0.6740, 0.6761,\n",
       "             0.6783, 0.6805, 0.6827, 0.6849, 0.6871, 0.6893, 0.6915, 0.6937, 0.6958,\n",
       "             0.6980, 0.7002, 0.7024, 0.7046, 0.7068, 0.7090, 0.7112, 0.7133, 0.7155,\n",
       "             0.7177, 0.7199, 0.7221, 0.7243, 0.7265, 0.7287, 0.7309, 0.7330, 0.7352,\n",
       "             0.7374, 0.7396, 0.7418, 0.7440, 0.7462, 0.7484, 0.7505, 0.7527, 0.7549,\n",
       "             0.7571, 0.7593, 0.7615, 0.7637, 0.7659, 0.7681, 0.7702, 0.7724, 0.7746,\n",
       "             0.7768, 0.7790, 0.7812, 0.7834, 0.7856, 0.7877, 0.7899, 0.7921, 0.7943,\n",
       "             0.7965, 0.7987, 0.8009, 0.8031, 0.8053, 0.8074, 0.8096, 0.8118, 0.8140,\n",
       "             0.8162, 0.8184, 0.8206, 0.8228, 0.8249, 0.8271, 0.8293, 0.8315, 0.8337,\n",
       "             0.8359, 0.8381, 0.8403, 0.8425, 0.8446, 0.8468, 0.8490, 0.8512, 0.8534,\n",
       "             0.8556, 0.8578, 0.8600, 0.8621, 0.8643, 0.8665, 0.8687, 0.8709, 0.8731,\n",
       "             0.8753, 0.8775, 0.8796, 0.8818, 0.8840, 0.8862, 0.8884, 0.8906, 0.8928,\n",
       "             0.8950, 0.8972, 0.8993, 0.9015, 0.9037, 0.9059, 0.9081, 0.9103, 0.9125,\n",
       "             0.9147, 0.9168, 0.9190, 0.9212, 0.9234, 0.9256, 0.9278, 0.9300, 0.9322,\n",
       "             0.9344, 0.9365, 0.9387, 0.9409, 0.9431, 0.9453, 0.9475, 0.9497, 0.9519,\n",
       "             0.9540, 0.9562, 0.9584, 0.9606, 0.9628, 0.9650, 0.9672, 0.9694, 0.9716,\n",
       "             0.9737, 0.9759, 0.9781, 0.9803, 0.9825, 0.9847, 0.9869, 0.9891, 0.9912,\n",
       "             0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9306, 0.9414, 0.9468, 0.9476, 0.9514, 0.9514, 0.9530, 0.9545,\n",
       "             0.9545, 0.9568, 0.9576, 0.9584, 0.9591, 0.9599, 0.9607, 0.9607, 0.9614,\n",
       "             0.9622, 0.9630, 0.9638, 0.9645, 0.9653, 0.9653, 0.9661, 0.9668, 0.9676,\n",
       "             0.9676, 0.9684, 0.9684, 0.9692, 0.9699, 0.9707, 0.9715, 0.9715, 0.9722,\n",
       "             0.9722, 0.9730, 0.9730, 0.9738, 0.9746, 0.9746, 0.9753, 0.9761, 0.9769,\n",
       "             0.9769, 0.9776, 0.9776, 0.9776, 0.9784, 0.9784, 0.9792, 0.9792, 0.9800,\n",
       "             0.9807, 0.9815, 0.9815, 0.9815, 0.9815, 0.9823, 0.9823, 0.9830, 0.9830,\n",
       "             0.9838, 0.9838, 0.9838, 0.9846, 0.9846, 0.9854, 0.9861, 0.9869, 0.9869,\n",
       "             0.9877, 0.9877, 0.9884, 0.9892, 0.9892, 0.9892, 0.9892, 0.9900, 0.9900,\n",
       "             0.9907, 0.9907, 0.9915, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9938,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9954, 0.9954, 0.9954, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01, 9.9991e-01, 9.9986e-01,\n",
       "             9.9983e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9975e-01, 9.9971e-01,\n",
       "             9.9967e-01, 9.9965e-01, 9.9956e-01, 9.9956e-01, 9.9951e-01, 9.9937e-01,\n",
       "             9.9928e-01, 9.9923e-01, 9.9920e-01, 9.9904e-01, 9.9904e-01, 9.9902e-01,\n",
       "             9.9897e-01, 9.9880e-01, 9.9878e-01, 9.9798e-01, 9.9793e-01, 9.9654e-01,\n",
       "             9.9438e-01, 9.9421e-01, 9.9390e-01, 9.9366e-01, 9.9299e-01, 9.9265e-01,\n",
       "             9.9069e-01, 9.9040e-01, 9.8860e-01, 9.8767e-01, 9.8566e-01, 9.8396e-01,\n",
       "             9.8391e-01, 9.8188e-01, 9.7817e-01, 9.7730e-01, 9.5981e-01, 9.5930e-01,\n",
       "             9.5113e-01, 9.4359e-01, 9.3933e-01, 8.8390e-01, 8.6123e-01, 8.5835e-01,\n",
       "             8.5309e-01, 7.8926e-01, 7.8651e-01, 7.7733e-01, 6.7423e-01, 6.2856e-01,\n",
       "             6.2825e-01, 5.9585e-01, 5.0013e-01, 4.8219e-01, 4.7828e-01, 4.6282e-01,\n",
       "             4.5408e-01, 4.4342e-01, 4.0708e-01, 3.9296e-01, 3.9155e-01, 3.7619e-01,\n",
       "             3.6674e-01, 2.9747e-01, 2.5272e-01, 1.8601e-01, 1.8297e-01, 1.7105e-01,\n",
       "             1.6876e-01, 1.6547e-01, 1.5842e-01, 1.3284e-01, 1.3135e-01, 1.1405e-01,\n",
       "             9.2870e-02, 9.2610e-02, 8.8084e-02, 7.7994e-02, 6.4311e-02, 4.9049e-02,\n",
       "             4.4276e-02, 4.3268e-02, 3.7077e-02, 3.0392e-02, 1.5310e-02, 1.4799e-02,\n",
       "             1.0714e-02, 1.0292e-02, 9.9383e-03, 9.7374e-03, 7.3984e-03, 6.7206e-03,\n",
       "             6.7011e-03, 5.5925e-03, 5.4838e-03, 2.9533e-03, 2.2926e-03, 2.2726e-03,\n",
       "             2.2360e-03, 2.0247e-03, 1.4348e-03, 1.0753e-03, 1.0418e-03, 1.0098e-03,\n",
       "             9.9274e-04, 9.4776e-04, 8.3139e-04, 7.9645e-04, 7.4965e-04, 7.1152e-04,\n",
       "             7.0417e-04, 6.7797e-04, 3.2683e-04, 2.9202e-04, 2.5006e-04, 2.4426e-04,\n",
       "             2.0957e-04, 1.9432e-04, 1.9369e-04, 1.9226e-04, 1.1077e-04, 1.0906e-04,\n",
       "             1.0358e-04, 1.0161e-04, 1.0127e-04, 9.9009e-05, 9.3896e-05, 9.0948e-05,\n",
       "             7.5265e-05, 7.5163e-05, 6.5186e-05, 6.1177e-05, 4.7563e-05, 4.6248e-05,\n",
       "             4.4053e-05, 4.0524e-05, 4.0328e-05, 3.9438e-05, 3.8138e-05, 3.5196e-05,\n",
       "             3.3088e-05, 2.8496e-05, 2.4047e-05, 2.1761e-05, 2.0980e-05, 1.9184e-05,\n",
       "             1.8444e-05, 1.7423e-05, 1.3995e-05, 1.1944e-05, 1.1547e-05, 9.2512e-06,\n",
       "             9.0792e-06, 9.0744e-06, 8.0204e-06, 7.8538e-06, 7.5076e-06, 6.6673e-06,\n",
       "             6.4304e-06, 6.3833e-06, 6.1623e-06, 5.9144e-06, 5.7222e-06, 5.3245e-06,\n",
       "             5.1670e-06, 4.5792e-06, 4.5676e-06, 4.2960e-06, 4.1486e-06, 3.9277e-06,\n",
       "             2.5182e-06, 2.2693e-06, 2.2324e-06, 1.8381e-06, 1.6651e-06, 1.6126e-06,\n",
       "             1.4487e-06, 1.3747e-06, 1.3540e-06, 1.1721e-06, 9.2365e-07, 9.1871e-07,\n",
       "             7.1770e-07, 7.0869e-07, 6.7577e-07, 6.4898e-07, 5.9064e-07, 5.8898e-07,\n",
       "             5.7503e-07, 5.6696e-07, 5.5463e-07, 5.2616e-07, 5.0475e-07, 3.2281e-07,\n",
       "             2.7567e-07, 2.7260e-07, 2.7071e-07, 1.9725e-07, 1.8867e-07, 1.6025e-07,\n",
       "             1.5919e-07, 1.3950e-07, 1.2278e-07, 8.9611e-08, 8.4437e-08, 8.3612e-08,\n",
       "             7.6106e-08, 7.5117e-08, 5.8737e-08, 5.3134e-08, 4.5766e-08, 4.3317e-08,\n",
       "             3.9046e-08, 3.8757e-08, 3.8380e-08, 3.8229e-08, 3.7733e-08, 3.7594e-08,\n",
       "             3.6867e-08, 3.2322e-08, 3.1671e-08, 2.6371e-08, 2.6295e-08, 2.2086e-08,\n",
       "             1.8889e-08, 1.8223e-08, 1.8036e-08, 1.7407e-08, 1.0444e-08, 8.6108e-09,\n",
       "             6.1187e-09, 5.4504e-09, 5.3038e-09, 5.2184e-09, 4.2707e-09, 4.0798e-09,\n",
       "             3.9854e-09, 3.1532e-09, 2.9354e-09, 2.1758e-09, 2.1432e-09, 2.1242e-09,\n",
       "             1.5520e-09, 1.3954e-09, 1.3554e-09, 1.3133e-09, 1.2389e-09, 1.1728e-09,\n",
       "             1.0553e-09, 9.8819e-10, 9.7795e-10, 9.6530e-10, 9.3925e-10, 8.4161e-10,\n",
       "             7.1159e-10, 5.5032e-10, 4.1593e-10, 3.9213e-10, 3.6474e-10, 3.3344e-10,\n",
       "             3.2444e-10, 3.0745e-10, 3.0368e-10, 2.7304e-10, 2.5307e-10, 2.3312e-10,\n",
       "             2.1370e-10, 1.8362e-10, 1.4597e-10, 1.3396e-10, 1.3246e-10, 1.3020e-10,\n",
       "             1.2645e-10, 1.1982e-10, 9.5339e-11, 8.1582e-11, 8.1464e-11, 7.0622e-11,\n",
       "             6.7162e-11, 5.4795e-11, 4.7984e-11, 4.7121e-11, 4.5930e-11, 4.3958e-11,\n",
       "             4.1624e-11, 4.0170e-11, 3.9821e-11, 3.8885e-11, 3.1998e-11, 3.0724e-11,\n",
       "             2.4840e-11, 1.6649e-11, 1.4504e-11, 1.3506e-11, 1.1852e-11, 9.2339e-12,\n",
       "             8.4450e-12, 8.2344e-12, 7.6006e-12, 6.8339e-12, 5.5784e-12, 4.1027e-12,\n",
       "             3.8063e-12, 3.7698e-12, 3.5238e-12, 3.2874e-12, 2.8510e-12, 2.7877e-12,\n",
       "             2.7679e-12, 2.2869e-12, 2.2535e-12, 2.1628e-12, 2.1337e-12, 2.0253e-12,\n",
       "             1.9385e-12, 1.8378e-12, 1.7013e-12, 1.6724e-12, 1.6659e-12, 1.5249e-12,\n",
       "             1.3011e-12, 1.1685e-12, 1.0941e-12, 1.0048e-12, 8.8675e-13, 7.2238e-13,\n",
       "             6.3071e-13, 6.2762e-13, 5.3348e-13, 5.1224e-13, 4.3595e-13, 4.1285e-13,\n",
       "             3.3986e-13, 3.3938e-13, 2.8534e-13, 2.8307e-13, 2.4246e-13, 2.0171e-13,\n",
       "             1.9466e-13, 1.7993e-13, 1.6712e-13, 1.4202e-13, 1.1782e-13, 1.0830e-13,\n",
       "             1.0215e-13, 8.7570e-14, 8.3149e-14, 7.0073e-14, 6.5915e-14, 6.4134e-14,\n",
       "             4.4159e-14, 4.3586e-14, 4.2616e-14, 4.0462e-14, 3.7965e-14, 3.0441e-14,\n",
       "             3.0190e-14, 2.9155e-14, 2.6076e-14, 2.3290e-14, 2.2564e-14, 1.9576e-14,\n",
       "             1.9278e-14, 1.9173e-14, 1.7801e-14, 8.5820e-15, 5.8398e-15, 5.6131e-15,\n",
       "             4.5675e-15, 4.5562e-15, 4.5051e-15, 3.5267e-15, 2.6254e-15, 1.9261e-15,\n",
       "             1.2884e-15, 1.0394e-15, 9.6220e-16, 7.4382e-16, 6.2267e-16, 5.9713e-16,\n",
       "             5.5310e-16, 5.3562e-16, 5.0141e-16, 4.0924e-16, 3.4513e-16, 3.1984e-16,\n",
       "             2.5339e-16, 2.4728e-16, 2.0205e-16, 1.7916e-16, 1.6686e-16, 1.5990e-16,\n",
       "             1.4842e-16, 8.1409e-17, 7.3237e-17, 6.9799e-17, 3.4559e-17, 2.2144e-17,\n",
       "             2.1060e-17, 2.0690e-17, 1.8509e-17, 1.3445e-17, 1.1284e-17, 1.1121e-17,\n",
       "             5.6765e-18, 5.3660e-18, 4.5420e-18, 4.5191e-18, 4.2241e-18, 4.2102e-18,\n",
       "             2.2718e-18, 2.0510e-18, 1.8555e-18, 1.8393e-18, 1.1139e-18, 7.8867e-19,\n",
       "             6.8511e-19, 6.1751e-19, 4.2743e-19, 3.7019e-19, 3.3900e-19, 2.6247e-19,\n",
       "             1.2894e-19, 8.4193e-20, 7.8985e-20, 7.6366e-20, 5.3702e-20, 4.8430e-20,\n",
       "             3.9783e-20, 1.5517e-20, 1.5471e-20, 1.0285e-20, 9.3910e-21, 8.0081e-21,\n",
       "             3.6722e-21, 1.7652e-21, 1.1411e-21, 8.7692e-22, 8.5687e-22, 3.3384e-22,\n",
       "             1.8832e-22, 8.1514e-23, 4.4896e-23, 3.7727e-23, 2.1489e-23, 1.2306e-23,\n",
       "             1.1893e-24, 9.0709e-25, 7.5553e-25, 6.6959e-25, 1.4109e-25, 9.5813e-26,\n",
       "             1.0885e-26, 9.3266e-29, 1.2805e-29, 3.7959e-31])}},\n",
       "   {'fpr': np.float64(0.0962800875273523),\n",
       "    'tpr': np.float64(0.992289899768697),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0088, 0.0109, 0.0109, 0.0131, 0.0153, 0.0153, 0.0153, 0.0175,\n",
       "             0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0219, 0.0219, 0.0219, 0.0219, 0.0219, 0.0263,\n",
       "             0.0284, 0.0284, 0.0284, 0.0284, 0.0284, 0.0284, 0.0284, 0.0284, 0.0284,\n",
       "             0.0284, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0306, 0.0328,\n",
       "             0.0350, 0.0350, 0.0372, 0.0372, 0.0372, 0.0372, 0.0372, 0.0372, 0.0372,\n",
       "             0.0372, 0.0372, 0.0372, 0.0372, 0.0372, 0.0394, 0.0394, 0.0416, 0.0416,\n",
       "             0.0416, 0.0438, 0.0438, 0.0460, 0.0460, 0.0481, 0.0503, 0.0525, 0.0547,\n",
       "             0.0547, 0.0569, 0.0591, 0.0591, 0.0591, 0.0591, 0.0613, 0.0635, 0.0635,\n",
       "             0.0656, 0.0656, 0.0656, 0.0678, 0.0700, 0.0722, 0.0744, 0.0766, 0.0766,\n",
       "             0.0788, 0.0810, 0.0832, 0.0853, 0.0875, 0.0875, 0.0897, 0.0897, 0.0897,\n",
       "             0.0919, 0.0941, 0.0963, 0.0985, 0.1007, 0.1028, 0.1050, 0.1072, 0.1094,\n",
       "             0.1116, 0.1116, 0.1116, 0.1138, 0.1138, 0.1160, 0.1182, 0.1204, 0.1225,\n",
       "             0.1247, 0.1269, 0.1291, 0.1313, 0.1335, 0.1335, 0.1357, 0.1379, 0.1400,\n",
       "             0.1422, 0.1444, 0.1466, 0.1488, 0.1510, 0.1532, 0.1554, 0.1575, 0.1597,\n",
       "             0.1619, 0.1641, 0.1663, 0.1685, 0.1707, 0.1729, 0.1751, 0.1772, 0.1794,\n",
       "             0.1816, 0.1816, 0.1838, 0.1860, 0.1882, 0.1904, 0.1926, 0.1926, 0.1947,\n",
       "             0.1969, 0.1991, 0.2013, 0.2035, 0.2057, 0.2079, 0.2101, 0.2123, 0.2144,\n",
       "             0.2166, 0.2188, 0.2210, 0.2232, 0.2254, 0.2276, 0.2298, 0.2319, 0.2341,\n",
       "             0.2341, 0.2363, 0.2385, 0.2407, 0.2429, 0.2451, 0.2473, 0.2495, 0.2516,\n",
       "             0.2538, 0.2560, 0.2582, 0.2582, 0.2604, 0.2626, 0.2648, 0.2670, 0.2691,\n",
       "             0.2713, 0.2735, 0.2757, 0.2779, 0.2801, 0.2823, 0.2845, 0.2867, 0.2888,\n",
       "             0.2910, 0.2932, 0.2954, 0.2976, 0.2998, 0.3020, 0.3042, 0.3063, 0.3085,\n",
       "             0.3107, 0.3129, 0.3129, 0.3151, 0.3151, 0.3173, 0.3195, 0.3217, 0.3239,\n",
       "             0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370, 0.3392, 0.3414, 0.3435,\n",
       "             0.3457, 0.3479, 0.3501, 0.3523, 0.3545, 0.3567, 0.3589, 0.3611, 0.3632,\n",
       "             0.3654, 0.3676, 0.3698, 0.3720, 0.3742, 0.3764, 0.3786, 0.3807, 0.3829,\n",
       "             0.3851, 0.3873, 0.3895, 0.3917, 0.3939, 0.3961, 0.3982, 0.4004, 0.4026,\n",
       "             0.4048, 0.4070, 0.4092, 0.4114, 0.4136, 0.4158, 0.4179, 0.4201, 0.4223,\n",
       "             0.4245, 0.4267, 0.4289, 0.4311, 0.4333, 0.4354, 0.4376, 0.4398, 0.4420,\n",
       "             0.4442, 0.4464, 0.4486, 0.4508, 0.4530, 0.4551, 0.4573, 0.4595, 0.4617,\n",
       "             0.4639, 0.4661, 0.4683, 0.4705, 0.4726, 0.4748, 0.4770, 0.4792, 0.4814,\n",
       "             0.4836, 0.4858, 0.4880, 0.4902, 0.4923, 0.4945, 0.4967, 0.4989, 0.5011,\n",
       "             0.5033, 0.5055, 0.5077, 0.5098, 0.5120, 0.5142, 0.5164, 0.5186, 0.5208,\n",
       "             0.5230, 0.5252, 0.5274, 0.5295, 0.5317, 0.5339, 0.5361, 0.5383, 0.5405,\n",
       "             0.5427, 0.5449, 0.5470, 0.5492, 0.5514, 0.5536, 0.5558, 0.5580, 0.5602,\n",
       "             0.5624, 0.5646, 0.5667, 0.5689, 0.5711, 0.5733, 0.5755, 0.5777, 0.5799,\n",
       "             0.5821, 0.5842, 0.5864, 0.5886, 0.5908, 0.5930, 0.5952, 0.5974, 0.5996,\n",
       "             0.6018, 0.6039, 0.6061, 0.6083, 0.6105, 0.6127, 0.6149, 0.6171, 0.6193,\n",
       "             0.6214, 0.6236, 0.6258, 0.6280, 0.6302, 0.6324, 0.6346, 0.6368, 0.6389,\n",
       "             0.6411, 0.6433, 0.6455, 0.6477, 0.6499, 0.6521, 0.6543, 0.6565, 0.6586,\n",
       "             0.6608, 0.6630, 0.6652, 0.6674, 0.6696, 0.6718, 0.6740, 0.6761, 0.6783,\n",
       "             0.6805, 0.6827, 0.6849, 0.6871, 0.6893, 0.6915, 0.6937, 0.6958, 0.6980,\n",
       "             0.7002, 0.7024, 0.7046, 0.7068, 0.7090, 0.7112, 0.7133, 0.7155, 0.7177,\n",
       "             0.7199, 0.7221, 0.7243, 0.7265, 0.7287, 0.7309, 0.7330, 0.7352, 0.7374,\n",
       "             0.7396, 0.7418, 0.7440, 0.7462, 0.7484, 0.7505, 0.7527, 0.7549, 0.7571,\n",
       "             0.7593, 0.7615, 0.7637, 0.7659, 0.7681, 0.7702, 0.7724, 0.7746, 0.7768,\n",
       "             0.7790, 0.7812, 0.7834, 0.7856, 0.7877, 0.7899, 0.7921, 0.7943, 0.7965,\n",
       "             0.7987, 0.8009, 0.8031, 0.8053, 0.8074, 0.8096, 0.8118, 0.8140, 0.8162,\n",
       "             0.8184, 0.8206, 0.8228, 0.8249, 0.8271, 0.8293, 0.8315, 0.8337, 0.8359,\n",
       "             0.8381, 0.8403, 0.8425, 0.8446, 0.8468, 0.8490, 0.8512, 0.8534, 0.8556,\n",
       "             0.8578, 0.8600, 0.8621, 0.8643, 0.8665, 0.8687, 0.8709, 0.8731, 0.8753,\n",
       "             0.8775, 0.8796, 0.8818, 0.8840, 0.8862, 0.8884, 0.8906, 0.8928, 0.8950,\n",
       "             0.8972, 0.8993, 0.9015, 0.9037, 0.9059, 0.9081, 0.9103, 0.9125, 0.9147,\n",
       "             0.9168, 0.9190, 0.9212, 0.9234, 0.9256, 0.9278, 0.9300, 0.9322, 0.9344,\n",
       "             0.9365, 0.9387, 0.9409, 0.9431, 0.9453, 0.9475, 0.9497, 0.9519, 0.9540,\n",
       "             0.9562, 0.9584, 0.9606, 0.9628, 0.9650, 0.9672, 0.9694, 0.9716, 0.9737,\n",
       "             0.9759, 0.9781, 0.9803, 0.9825, 0.9847, 0.9869, 0.9891, 0.9912, 0.9934,\n",
       "             0.9956, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9098, 0.9198, 0.9291, 0.9314, 0.9337, 0.9391, 0.9406, 0.9406,\n",
       "             0.9414, 0.9437, 0.9453, 0.9468, 0.9483, 0.9491, 0.9499, 0.9514, 0.9530,\n",
       "             0.9537, 0.9545, 0.9553, 0.9553, 0.9561, 0.9568, 0.9576, 0.9584, 0.9584,\n",
       "             0.9584, 0.9591, 0.9607, 0.9614, 0.9622, 0.9630, 0.9638, 0.9645, 0.9653,\n",
       "             0.9661, 0.9661, 0.9668, 0.9676, 0.9684, 0.9692, 0.9699, 0.9707, 0.9707,\n",
       "             0.9707, 0.9715, 0.9715, 0.9722, 0.9730, 0.9738, 0.9746, 0.9753, 0.9761,\n",
       "             0.9769, 0.9776, 0.9784, 0.9792, 0.9800, 0.9800, 0.9807, 0.9807, 0.9815,\n",
       "             0.9823, 0.9823, 0.9830, 0.9830, 0.9838, 0.9838, 0.9838, 0.9838, 0.9838,\n",
       "             0.9846, 0.9846, 0.9846, 0.9854, 0.9861, 0.9869, 0.9869, 0.9869, 0.9877,\n",
       "             0.9877, 0.9884, 0.9892, 0.9892, 0.9892, 0.9892, 0.9892, 0.9892, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9907, 0.9907, 0.9915, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "             0.9923, 0.9931, 0.9938, 0.9938, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9990e-01, 9.9989e-01, 9.9988e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9979e-01, 9.9978e-01, 9.9960e-01,\n",
       "             9.9958e-01, 9.9952e-01, 9.9951e-01, 9.9927e-01, 9.9921e-01, 9.9915e-01,\n",
       "             9.9909e-01, 9.9904e-01, 9.9898e-01, 9.9856e-01, 9.9850e-01, 9.9828e-01,\n",
       "             9.9812e-01, 9.9729e-01, 9.9555e-01, 9.9525e-01, 9.9389e-01, 9.9188e-01,\n",
       "             9.9088e-01, 9.9062e-01, 9.9048e-01, 9.8968e-01, 9.8742e-01, 9.8508e-01,\n",
       "             9.7924e-01, 9.6815e-01, 9.6014e-01, 9.4687e-01, 9.4347e-01, 9.3190e-01,\n",
       "             9.1052e-01, 8.9243e-01, 8.8186e-01, 8.6848e-01, 8.6763e-01, 7.9019e-01,\n",
       "             7.7343e-01, 7.5282e-01, 7.4787e-01, 7.0976e-01, 7.0580e-01, 5.7385e-01,\n",
       "             4.8964e-01, 4.1557e-01, 3.9590e-01, 2.9412e-01, 2.6363e-01, 2.4747e-01,\n",
       "             1.5377e-01, 1.3128e-01, 1.1753e-01, 9.3464e-02, 8.7281e-02, 3.7868e-02,\n",
       "             2.9018e-02, 2.8400e-02, 2.3773e-02, 2.1457e-02, 1.9791e-02, 1.7470e-02,\n",
       "             1.6314e-02, 1.1245e-02, 1.0612e-02, 9.8763e-03, 9.4864e-03, 7.8777e-03,\n",
       "             6.0913e-03, 5.7319e-03, 5.0334e-03, 2.8502e-03, 2.3478e-03, 2.0604e-03,\n",
       "             1.0425e-03, 8.4198e-04, 8.4085e-04, 7.4824e-04, 7.4731e-04, 5.8422e-04,\n",
       "             5.6907e-04, 4.0587e-04, 3.9378e-04, 2.8596e-04, 2.5006e-04, 2.1849e-04,\n",
       "             1.3852e-04, 1.2785e-04, 1.2730e-04, 7.6434e-05, 6.5602e-05, 5.1067e-05,\n",
       "             4.4669e-05, 4.4190e-05, 3.5506e-05, 3.4949e-05, 3.1403e-05, 2.8806e-05,\n",
       "             2.4301e-05, 2.2657e-05, 2.2444e-05, 2.2330e-05, 1.9390e-05, 1.9157e-05,\n",
       "             1.7971e-05, 1.7343e-05, 1.4372e-05, 1.4231e-05, 1.4214e-05, 1.3909e-05,\n",
       "             1.2793e-05, 1.2204e-05, 1.0834e-05, 8.6300e-06, 8.4414e-06, 7.7779e-06,\n",
       "             6.7104e-06, 4.6190e-06, 4.5649e-06, 4.3489e-06, 4.2641e-06, 3.9722e-06,\n",
       "             3.7087e-06, 3.5907e-06, 3.2905e-06, 3.2212e-06, 3.0941e-06, 3.0797e-06,\n",
       "             2.5382e-06, 2.4674e-06, 2.3211e-06, 2.0605e-06, 2.0046e-06, 1.5043e-06,\n",
       "             1.4912e-06, 1.3925e-06, 1.3332e-06, 1.2178e-06, 1.1060e-06, 8.7764e-07,\n",
       "             7.8810e-07, 6.1037e-07, 5.6838e-07, 5.6446e-07, 5.2075e-07, 5.0966e-07,\n",
       "             3.6695e-07, 3.6623e-07, 3.5753e-07, 1.7147e-07, 1.5231e-07, 1.3183e-07,\n",
       "             1.1341e-07, 1.1233e-07, 8.4772e-08, 7.5067e-08, 7.0067e-08, 6.4405e-08,\n",
       "             5.4099e-08, 5.1568e-08, 4.6117e-08, 4.4422e-08, 3.8908e-08, 3.4767e-08,\n",
       "             3.4535e-08, 3.2791e-08, 2.4877e-08, 2.4651e-08, 2.3087e-08, 1.9229e-08,\n",
       "             1.7534e-08, 1.1690e-08, 1.1335e-08, 1.0596e-08, 1.0099e-08, 8.6885e-09,\n",
       "             7.7485e-09, 7.4617e-09, 6.4266e-09, 6.1827e-09, 5.8281e-09, 4.3017e-09,\n",
       "             3.8612e-09, 3.7982e-09, 3.3262e-09, 3.2418e-09, 2.9656e-09, 2.5406e-09,\n",
       "             2.5161e-09, 2.3758e-09, 2.2641e-09, 2.1100e-09, 2.0551e-09, 1.9058e-09,\n",
       "             1.4012e-09, 1.1500e-09, 1.0110e-09, 1.0107e-09, 1.0080e-09, 1.0022e-09,\n",
       "             7.8009e-10, 7.6967e-10, 6.3599e-10, 6.3153e-10, 5.7276e-10, 5.2750e-10,\n",
       "             4.8152e-10, 4.1618e-10, 3.8470e-10, 3.7995e-10, 2.3892e-10, 1.8747e-10,\n",
       "             1.6876e-10, 1.4038e-10, 1.0400e-10, 1.0212e-10, 9.6027e-11, 9.3967e-11,\n",
       "             9.3592e-11, 8.8507e-11, 8.6430e-11, 8.4013e-11, 6.9683e-11, 6.8959e-11,\n",
       "             4.0540e-11, 4.0187e-11, 3.9034e-11, 3.4876e-11, 3.0770e-11, 2.5157e-11,\n",
       "             2.0423e-11, 1.8456e-11, 1.6080e-11, 1.5663e-11, 1.4114e-11, 1.1061e-11,\n",
       "             9.0770e-12, 9.0713e-12, 8.5852e-12, 7.7249e-12, 7.3051e-12, 6.7228e-12,\n",
       "             6.7081e-12, 6.6166e-12, 5.9981e-12, 4.6629e-12, 4.4492e-12, 3.0177e-12,\n",
       "             2.5662e-12, 1.7198e-12, 1.6392e-12, 1.6334e-12, 1.4444e-12, 1.3353e-12,\n",
       "             1.1772e-12, 1.1255e-12, 1.0615e-12, 9.9980e-13, 9.6265e-13, 7.8587e-13,\n",
       "             7.4809e-13, 5.4843e-13, 5.3211e-13, 5.0890e-13, 4.9662e-13, 3.3439e-13,\n",
       "             2.8051e-13, 2.4600e-13, 1.9435e-13, 1.5228e-13, 1.1082e-13, 9.9184e-14,\n",
       "             9.4046e-14, 9.0352e-14, 8.4276e-14, 7.7831e-14, 6.5218e-14, 5.9032e-14,\n",
       "             5.5427e-14, 4.3261e-14, 3.9508e-14, 3.7802e-14, 3.0185e-14, 2.4078e-14,\n",
       "             2.0754e-14, 1.9700e-14, 1.9417e-14, 1.9210e-14, 1.8569e-14, 1.8001e-14,\n",
       "             1.3855e-14, 1.3240e-14, 1.2590e-14, 1.0490e-14, 1.0219e-14, 8.8334e-15,\n",
       "             8.6643e-15, 8.1638e-15, 5.8098e-15, 4.9401e-15, 4.6907e-15, 4.6444e-15,\n",
       "             4.5226e-15, 4.1697e-15, 3.9862e-15, 3.7145e-15, 3.6369e-15, 2.0909e-15,\n",
       "             1.5158e-15, 1.3741e-15, 1.3488e-15, 1.1831e-15, 1.1626e-15, 9.1174e-16,\n",
       "             8.5028e-16, 7.8918e-16, 6.1614e-16, 6.0426e-16, 3.0910e-16, 2.7585e-16,\n",
       "             2.6080e-16, 2.1924e-16, 1.9515e-16, 1.8641e-16, 1.6553e-16, 1.6018e-16,\n",
       "             1.4005e-16, 1.3951e-16, 1.2728e-16, 1.0620e-16, 9.2572e-17, 8.0727e-17,\n",
       "             6.2368e-17, 6.0254e-17, 5.9903e-17, 5.5051e-17, 4.8802e-17, 4.7446e-17,\n",
       "             4.4204e-17, 4.0346e-17, 3.9052e-17, 3.2323e-17, 2.9991e-17, 2.1819e-17,\n",
       "             1.5744e-17, 1.4751e-17, 9.2469e-18, 6.9777e-18, 5.9580e-18, 4.5193e-18,\n",
       "             2.5133e-18, 2.3583e-18, 1.7428e-18, 1.7109e-18, 4.4795e-19, 4.1046e-19,\n",
       "             3.6970e-19, 3.6884e-19, 3.5486e-19, 2.4241e-19, 2.2819e-19, 2.2793e-19,\n",
       "             2.1334e-19, 1.7657e-19, 1.4843e-19, 1.2802e-19, 1.0886e-19, 8.8854e-20,\n",
       "             7.9996e-20, 7.9554e-20, 6.7453e-20, 4.8931e-20, 3.0025e-20, 2.7877e-20,\n",
       "             1.9834e-20, 1.7566e-20, 1.4013e-20, 1.0880e-20, 1.0424e-20, 6.6562e-21,\n",
       "             6.0769e-21, 5.3681e-21, 5.3356e-21, 4.5116e-21, 3.3252e-21, 3.1883e-21,\n",
       "             3.1001e-21, 2.9637e-21, 2.7433e-21, 2.1814e-21, 1.5737e-21, 1.3762e-21,\n",
       "             1.3716e-21, 1.2322e-21, 1.1453e-21, 1.1203e-21, 9.0598e-22, 8.2742e-22,\n",
       "             5.6853e-22, 1.6558e-22, 1.4925e-22, 1.4671e-22, 1.1260e-22, 1.1149e-22,\n",
       "             1.0951e-22, 9.9734e-23, 9.5431e-23, 7.4505e-23, 4.3749e-23, 3.3606e-23,\n",
       "             3.0782e-23, 2.0766e-23, 2.0351e-23, 1.7346e-23, 1.6189e-23, 1.3955e-23,\n",
       "             7.4000e-24, 5.1329e-24, 3.0592e-24, 2.5394e-24, 2.0929e-24, 1.7409e-24,\n",
       "             2.1141e-25, 1.9989e-25, 1.5636e-25, 1.0027e-25, 8.3043e-26, 7.4896e-26,\n",
       "             5.1303e-26, 2.2467e-26, 1.6826e-26, 1.1858e-26, 9.0604e-27, 4.0121e-27,\n",
       "             3.6426e-27, 3.1013e-27, 2.4213e-27, 1.1591e-27, 1.0814e-27, 1.0243e-27,\n",
       "             5.8543e-28, 4.7241e-28, 3.7334e-28, 3.0967e-28, 2.3920e-28, 2.8944e-29,\n",
       "             2.1737e-29, 4.9846e-30, 1.4783e-30, 1.3476e-30, 4.3658e-31, 2.4159e-31,\n",
       "             2.2896e-31, 7.8640e-32, 5.2341e-32, 4.5623e-32, 3.0903e-32, 2.1968e-32,\n",
       "             5.8288e-33, 2.5140e-34, 3.8372e-35, 3.8360e-36, 1.1058e-36, 4.4098e-37,\n",
       "             6.2299e-39, 0.0000e+00])}},\n",
       "   {'fpr': np.float64(0.07658643326039387),\n",
       "    'tpr': np.float64(0.9899768696993061),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0044, 0.0109, 0.0109, 0.0109, 0.0153, 0.0153, 0.0153, 0.0153,\n",
       "             0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0175, 0.0175, 0.0175,\n",
       "             0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175,\n",
       "             0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0219, 0.0219, 0.0219,\n",
       "             0.0219, 0.0219, 0.0219, 0.0241, 0.0241, 0.0241, 0.0241, 0.0263, 0.0263,\n",
       "             0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0284, 0.0284, 0.0284, 0.0284,\n",
       "             0.0284, 0.0306, 0.0328, 0.0350, 0.0372, 0.0372, 0.0372, 0.0372, 0.0372,\n",
       "             0.0372, 0.0372, 0.0372, 0.0372, 0.0394, 0.0416, 0.0416, 0.0438, 0.0438,\n",
       "             0.0438, 0.0438, 0.0438, 0.0460, 0.0481, 0.0481, 0.0503, 0.0503, 0.0503,\n",
       "             0.0503, 0.0503, 0.0503, 0.0503, 0.0525, 0.0525, 0.0525, 0.0525, 0.0525,\n",
       "             0.0525, 0.0525, 0.0525, 0.0547, 0.0547, 0.0547, 0.0569, 0.0569, 0.0569,\n",
       "             0.0569, 0.0591, 0.0613, 0.0635, 0.0656, 0.0678, 0.0678, 0.0678, 0.0678,\n",
       "             0.0700, 0.0700, 0.0722, 0.0744, 0.0744, 0.0766, 0.0788, 0.0810, 0.0810,\n",
       "             0.0810, 0.0832, 0.0853, 0.0875, 0.0897, 0.0919, 0.0941, 0.0941, 0.0963,\n",
       "             0.0985, 0.1007, 0.1028, 0.1028, 0.1050, 0.1072, 0.1094, 0.1116, 0.1138,\n",
       "             0.1160, 0.1182, 0.1204, 0.1225, 0.1247, 0.1269, 0.1291, 0.1313, 0.1335,\n",
       "             0.1335, 0.1357, 0.1357, 0.1379, 0.1400, 0.1422, 0.1444, 0.1466, 0.1488,\n",
       "             0.1510, 0.1532, 0.1554, 0.1575, 0.1597, 0.1619, 0.1641, 0.1663, 0.1685,\n",
       "             0.1707, 0.1729, 0.1751, 0.1772, 0.1794, 0.1816, 0.1838, 0.1860, 0.1882,\n",
       "             0.1904, 0.1926, 0.1926, 0.1947, 0.1947, 0.1969, 0.1991, 0.2013, 0.2035,\n",
       "             0.2035, 0.2057, 0.2079, 0.2101, 0.2123, 0.2144, 0.2166, 0.2188, 0.2210,\n",
       "             0.2232, 0.2254, 0.2254, 0.2276, 0.2298, 0.2319, 0.2341, 0.2363, 0.2385,\n",
       "             0.2407, 0.2429, 0.2451, 0.2473, 0.2495, 0.2516, 0.2538, 0.2560, 0.2582,\n",
       "             0.2604, 0.2626, 0.2648, 0.2670, 0.2691, 0.2713, 0.2735, 0.2757, 0.2779,\n",
       "             0.2801, 0.2823, 0.2845, 0.2867, 0.2888, 0.2910, 0.2932, 0.2954, 0.2976,\n",
       "             0.2998, 0.3020, 0.3042, 0.3063, 0.3085, 0.3107, 0.3129, 0.3151, 0.3173,\n",
       "             0.3195, 0.3217, 0.3239, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370,\n",
       "             0.3392, 0.3414, 0.3435, 0.3457, 0.3479, 0.3501, 0.3523, 0.3545, 0.3567,\n",
       "             0.3589, 0.3611, 0.3632, 0.3632, 0.3654, 0.3676, 0.3698, 0.3720, 0.3742,\n",
       "             0.3764, 0.3786, 0.3807, 0.3829, 0.3851, 0.3873, 0.3895, 0.3917, 0.3939,\n",
       "             0.3961, 0.3982, 0.4004, 0.4026, 0.4048, 0.4070, 0.4092, 0.4114, 0.4136,\n",
       "             0.4158, 0.4179, 0.4201, 0.4223, 0.4245, 0.4267, 0.4289, 0.4311, 0.4333,\n",
       "             0.4354, 0.4376, 0.4398, 0.4420, 0.4442, 0.4464, 0.4486, 0.4508, 0.4530,\n",
       "             0.4551, 0.4573, 0.4595, 0.4617, 0.4639, 0.4661, 0.4683, 0.4705, 0.4726,\n",
       "             0.4748, 0.4770, 0.4792, 0.4814, 0.4836, 0.4858, 0.4880, 0.4902, 0.4923,\n",
       "             0.4945, 0.4967, 0.4989, 0.5011, 0.5033, 0.5055, 0.5077, 0.5098, 0.5120,\n",
       "             0.5142, 0.5164, 0.5186, 0.5208, 0.5230, 0.5252, 0.5274, 0.5295, 0.5317,\n",
       "             0.5339, 0.5361, 0.5383, 0.5405, 0.5427, 0.5449, 0.5470, 0.5492, 0.5514,\n",
       "             0.5536, 0.5558, 0.5580, 0.5602, 0.5624, 0.5646, 0.5646, 0.5667, 0.5689,\n",
       "             0.5711, 0.5733, 0.5755, 0.5777, 0.5799, 0.5821, 0.5842, 0.5864, 0.5886,\n",
       "             0.5908, 0.5930, 0.5952, 0.5974, 0.5996, 0.6018, 0.6039, 0.6061, 0.6083,\n",
       "             0.6105, 0.6127, 0.6149, 0.6171, 0.6193, 0.6214, 0.6236, 0.6258, 0.6280,\n",
       "             0.6302, 0.6324, 0.6346, 0.6368, 0.6389, 0.6411, 0.6433, 0.6455, 0.6477,\n",
       "             0.6499, 0.6521, 0.6543, 0.6565, 0.6586, 0.6608, 0.6630, 0.6652, 0.6674,\n",
       "             0.6696, 0.6718, 0.6740, 0.6761, 0.6783, 0.6805, 0.6827, 0.6849, 0.6871,\n",
       "             0.6893, 0.6915, 0.6937, 0.6958, 0.6980, 0.7002, 0.7024, 0.7046, 0.7068,\n",
       "             0.7090, 0.7112, 0.7133, 0.7155, 0.7177, 0.7199, 0.7221, 0.7243, 0.7265,\n",
       "             0.7287, 0.7309, 0.7330, 0.7352, 0.7374, 0.7396, 0.7418, 0.7440, 0.7462,\n",
       "             0.7484, 0.7505, 0.7505, 0.7527, 0.7549, 0.7571, 0.7593, 0.7615, 0.7637,\n",
       "             0.7659, 0.7681, 0.7702, 0.7724, 0.7746, 0.7768, 0.7790, 0.7812, 0.7834,\n",
       "             0.7856, 0.7877, 0.7899, 0.7921, 0.7943, 0.7965, 0.7987, 0.8009, 0.8031,\n",
       "             0.8053, 0.8074, 0.8096, 0.8118, 0.8140, 0.8162, 0.8184, 0.8206, 0.8228,\n",
       "             0.8249, 0.8271, 0.8293, 0.8315, 0.8337, 0.8359, 0.8381, 0.8403, 0.8425,\n",
       "             0.8446, 0.8468, 0.8490, 0.8512, 0.8534, 0.8556, 0.8578, 0.8600, 0.8621,\n",
       "             0.8643, 0.8665, 0.8687, 0.8709, 0.8731, 0.8753, 0.8775, 0.8796, 0.8818,\n",
       "             0.8840, 0.8862, 0.8884, 0.8906, 0.8928, 0.8950, 0.8972, 0.8993, 0.9015,\n",
       "             0.9037, 0.9059, 0.9081, 0.9103, 0.9125, 0.9147, 0.9168, 0.9190, 0.9212,\n",
       "             0.9234, 0.9256, 0.9278, 0.9300, 0.9322, 0.9344, 0.9365, 0.9387, 0.9409,\n",
       "             0.9431, 0.9453, 0.9475, 0.9497, 0.9519, 0.9540, 0.9562, 0.9584, 0.9606,\n",
       "             0.9628, 0.9650, 0.9672, 0.9694, 0.9716, 0.9737, 0.9759, 0.9781, 0.9803,\n",
       "             0.9825, 0.9847, 0.9869, 0.9891, 0.9912, 0.9934, 0.9956, 0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8612, 0.8797, 0.8874, 0.8913, 0.8944, 0.8990, 0.9005, 0.9029,\n",
       "             0.9044, 0.9052, 0.9075, 0.9082, 0.9106, 0.9121, 0.9129, 0.9136, 0.9144,\n",
       "             0.9152, 0.9183, 0.9198, 0.9206, 0.9214, 0.9221, 0.9237, 0.9244, 0.9252,\n",
       "             0.9275, 0.9283, 0.9298, 0.9306, 0.9314, 0.9322, 0.9329, 0.9345, 0.9352,\n",
       "             0.9360, 0.9368, 0.9375, 0.9383, 0.9391, 0.9406, 0.9414, 0.9422, 0.9429,\n",
       "             0.9437, 0.9445, 0.9453, 0.9460, 0.9468, 0.9476, 0.9476, 0.9483, 0.9491,\n",
       "             0.9499, 0.9507, 0.9514, 0.9514, 0.9522, 0.9530, 0.9537, 0.9537, 0.9545,\n",
       "             0.9553, 0.9561, 0.9568, 0.9576, 0.9584, 0.9584, 0.9591, 0.9599, 0.9607,\n",
       "             0.9614, 0.9614, 0.9614, 0.9614, 0.9614, 0.9622, 0.9630, 0.9638, 0.9645,\n",
       "             0.9653, 0.9661, 0.9668, 0.9676, 0.9676, 0.9676, 0.9684, 0.9684, 0.9692,\n",
       "             0.9699, 0.9707, 0.9715, 0.9715, 0.9715, 0.9722, 0.9722, 0.9730, 0.9738,\n",
       "             0.9746, 0.9753, 0.9761, 0.9769, 0.9769, 0.9776, 0.9784, 0.9792, 0.9800,\n",
       "             0.9807, 0.9815, 0.9823, 0.9823, 0.9830, 0.9838, 0.9838, 0.9846, 0.9854,\n",
       "             0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9869, 0.9877, 0.9884,\n",
       "             0.9884, 0.9892, 0.9892, 0.9892, 0.9900, 0.9900, 0.9900, 0.9900, 0.9907,\n",
       "             0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9923, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9938, 0.9938, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9954, 0.9954, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9989e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9973e-01, 9.9970e-01, 9.9967e-01, 9.9963e-01, 9.9962e-01,\n",
       "             9.9960e-01, 9.9954e-01, 9.9952e-01, 9.9950e-01, 9.9946e-01, 9.9941e-01,\n",
       "             9.9940e-01, 9.9937e-01, 9.9925e-01, 9.9924e-01, 9.9919e-01, 9.9915e-01,\n",
       "             9.9910e-01, 9.9898e-01, 9.9889e-01, 9.9879e-01, 9.9853e-01, 9.9844e-01,\n",
       "             9.9842e-01, 9.9804e-01, 9.9776e-01, 9.9748e-01, 9.9727e-01, 9.9725e-01,\n",
       "             9.9700e-01, 9.9693e-01, 9.9627e-01, 9.9604e-01, 9.9576e-01, 9.9348e-01,\n",
       "             9.9288e-01, 9.9284e-01, 9.9176e-01, 9.9138e-01, 9.8968e-01, 9.8941e-01,\n",
       "             9.8808e-01, 9.8756e-01, 9.8673e-01, 9.8255e-01, 9.8105e-01, 9.8049e-01,\n",
       "             9.7908e-01, 9.7389e-01, 9.7133e-01, 9.7076e-01, 9.6489e-01, 9.6016e-01,\n",
       "             9.5748e-01, 9.4303e-01, 9.4140e-01, 9.3923e-01, 9.3547e-01, 9.3051e-01,\n",
       "             9.1939e-01, 9.1800e-01, 9.0882e-01, 8.8914e-01, 8.7361e-01, 8.4330e-01,\n",
       "             8.4302e-01, 8.1405e-01, 7.9321e-01, 6.9254e-01, 6.4680e-01, 5.6016e-01,\n",
       "             4.9429e-01, 4.6914e-01, 4.0065e-01, 3.2429e-01, 2.6105e-01, 1.9246e-01,\n",
       "             1.7287e-01, 1.2621e-01, 1.1859e-01, 9.0834e-02, 9.0490e-02, 8.5229e-02,\n",
       "             8.0666e-02, 8.0460e-02, 7.3405e-02, 6.7867e-02, 6.2976e-02, 5.1290e-02,\n",
       "             4.9805e-02, 4.7216e-02, 3.9957e-02, 3.7977e-02, 3.2399e-02, 3.1307e-02,\n",
       "             2.5890e-02, 2.3006e-02, 2.1108e-02, 1.9585e-02, 1.9095e-02, 1.9051e-02,\n",
       "             1.8518e-02, 1.5942e-02, 1.4798e-02, 1.4463e-02, 1.2637e-02, 1.2476e-02,\n",
       "             1.2122e-02, 8.1942e-03, 4.9587e-03, 3.9870e-03, 3.3604e-03, 3.2637e-03,\n",
       "             2.7590e-03, 2.7247e-03, 1.7660e-03, 1.3996e-03, 1.1209e-03, 1.0175e-03,\n",
       "             9.6615e-04, 8.7412e-04, 6.9914e-04, 5.3935e-04, 4.2014e-04, 3.6950e-04,\n",
       "             3.1751e-04, 2.9822e-04, 2.5887e-04, 2.4163e-04, 1.5639e-04, 1.4998e-04,\n",
       "             8.2712e-05, 5.2839e-05, 5.1503e-05, 4.9648e-05, 3.6174e-05, 3.5072e-05,\n",
       "             3.2282e-05, 2.9599e-05, 2.7624e-05, 2.5904e-05, 2.4456e-05, 2.4402e-05,\n",
       "             2.2335e-05, 2.1781e-05, 2.0442e-05, 2.0268e-05, 1.9972e-05, 1.9781e-05,\n",
       "             1.6943e-05, 1.5554e-05, 8.7083e-06, 6.4489e-06, 5.3789e-06, 4.1992e-06,\n",
       "             3.3791e-06, 3.3127e-06, 3.2363e-06, 3.1328e-06, 3.0461e-06, 2.8825e-06,\n",
       "             2.4452e-06, 2.4403e-06, 2.3429e-06, 2.2081e-06, 2.1281e-06, 2.0395e-06,\n",
       "             1.9451e-06, 1.7905e-06, 1.7841e-06, 1.7073e-06, 1.6384e-06, 1.5655e-06,\n",
       "             1.4151e-06, 1.0783e-06, 9.3222e-07, 7.5735e-07, 6.4190e-07, 5.4137e-07,\n",
       "             4.4314e-07, 4.2679e-07, 3.2619e-07, 3.2231e-07, 2.9474e-07, 2.5097e-07,\n",
       "             2.2882e-07, 2.1799e-07, 2.0041e-07, 1.8999e-07, 1.7613e-07, 1.6985e-07,\n",
       "             1.5717e-07, 1.5045e-07, 1.3537e-07, 1.1861e-07, 1.1290e-07, 8.9516e-08,\n",
       "             8.3275e-08, 7.1487e-08, 7.0458e-08, 6.8844e-08, 6.7830e-08, 5.3654e-08,\n",
       "             4.8727e-08, 4.7569e-08, 4.0275e-08, 3.6144e-08, 2.9299e-08, 2.6043e-08,\n",
       "             2.3134e-08, 1.8486e-08, 1.7400e-08, 1.6669e-08, 1.4982e-08, 1.1886e-08,\n",
       "             9.3732e-09, 8.5753e-09, 7.0610e-09, 6.2829e-09, 4.3129e-09, 4.2995e-09,\n",
       "             3.5683e-09, 3.1852e-09, 3.0217e-09, 2.8996e-09, 2.8223e-09, 2.7665e-09,\n",
       "             2.6980e-09, 2.3331e-09, 2.3115e-09, 2.2417e-09, 2.1432e-09, 2.0313e-09,\n",
       "             1.9975e-09, 1.9798e-09, 1.5981e-09, 1.4889e-09, 1.3266e-09, 1.2205e-09,\n",
       "             1.1048e-09, 9.4082e-10, 8.9883e-10, 8.3634e-10, 8.3121e-10, 7.2039e-10,\n",
       "             5.6852e-10, 5.4966e-10, 5.1356e-10, 3.9013e-10, 2.8748e-10, 2.7556e-10,\n",
       "             2.6535e-10, 2.5940e-10, 2.5773e-10, 2.5469e-10, 2.5037e-10, 2.3947e-10,\n",
       "             2.1778e-10, 1.8466e-10, 1.7843e-10, 1.7375e-10, 1.4761e-10, 1.1580e-10,\n",
       "             7.2451e-11, 7.2002e-11, 6.0529e-11, 5.4556e-11, 5.2244e-11, 5.1867e-11,\n",
       "             4.7524e-11, 4.1948e-11, 3.8015e-11, 3.6318e-11, 3.4748e-11, 3.4705e-11,\n",
       "             3.0618e-11, 2.6427e-11, 2.1211e-11, 2.0854e-11, 1.7198e-11, 1.6497e-11,\n",
       "             1.5434e-11, 1.5276e-11, 1.4218e-11, 1.4026e-11, 1.2383e-11, 1.1436e-11,\n",
       "             1.1409e-11, 1.0814e-11, 1.0485e-11, 5.5781e-12, 5.4761e-12, 5.0952e-12,\n",
       "             4.3532e-12, 3.9054e-12, 3.4321e-12, 3.4073e-12, 3.1228e-12, 3.0882e-12,\n",
       "             3.0382e-12, 2.3945e-12, 2.3772e-12, 1.9600e-12, 1.8485e-12, 1.3762e-12,\n",
       "             1.1008e-12, 1.0575e-12, 1.0542e-12, 8.6391e-13, 5.5636e-13, 5.2738e-13,\n",
       "             4.9326e-13, 4.8209e-13, 4.4188e-13, 3.6856e-13, 3.6209e-13, 3.4322e-13,\n",
       "             2.8833e-13, 2.5861e-13, 2.4781e-13, 2.3928e-13, 2.3488e-13, 2.2879e-13,\n",
       "             2.2427e-13, 2.1940e-13, 1.8424e-13, 1.7056e-13, 1.4525e-13, 1.2874e-13,\n",
       "             1.1858e-13, 1.1574e-13, 1.1330e-13, 1.1239e-13, 1.0723e-13, 8.2946e-14,\n",
       "             8.0912e-14, 7.9644e-14, 7.9498e-14, 7.8815e-14, 6.8428e-14, 4.7030e-14,\n",
       "             4.2887e-14, 4.2408e-14, 3.4878e-14, 3.3923e-14, 3.3566e-14, 3.3260e-14,\n",
       "             3.2268e-14, 2.8067e-14, 2.0751e-14, 2.0492e-14, 1.7632e-14, 1.5483e-14,\n",
       "             1.4196e-14, 1.2575e-14, 1.1654e-14, 1.0890e-14, 9.8754e-15, 8.4593e-15,\n",
       "             6.8597e-15, 6.7315e-15, 5.8479e-15, 4.8417e-15, 4.7060e-15, 3.9899e-15,\n",
       "             3.2056e-15, 3.0849e-15, 2.7876e-15, 2.5938e-15, 2.5712e-15, 1.9926e-15,\n",
       "             1.9576e-15, 1.7556e-15, 1.6712e-15, 1.5007e-15, 1.3955e-15, 1.1169e-15,\n",
       "             8.5329e-16, 8.3031e-16, 8.0820e-16, 6.8099e-16, 6.7782e-16, 6.6851e-16,\n",
       "             4.8068e-16, 3.7160e-16, 2.8544e-16, 2.8396e-16, 2.4914e-16, 2.1401e-16,\n",
       "             2.0745e-16, 1.2001e-16, 1.0332e-16, 5.1642e-17, 4.5650e-17, 4.5473e-17,\n",
       "             4.1552e-17, 2.9720e-17, 2.8006e-17, 2.3561e-17, 2.2147e-17, 1.9795e-17,\n",
       "             1.8080e-17, 1.6911e-17, 1.5028e-17, 1.3998e-17, 1.2997e-17, 1.1346e-17,\n",
       "             8.6088e-18, 7.8966e-18, 7.5184e-18, 6.8852e-18, 6.5716e-18, 5.6199e-18,\n",
       "             3.4737e-18, 3.3842e-18, 3.2611e-18, 2.9151e-18, 2.4474e-18, 2.2617e-18,\n",
       "             2.2072e-18, 1.7090e-18, 1.2654e-18, 1.2560e-18, 9.2743e-19, 4.6391e-19,\n",
       "             4.4084e-19, 3.3555e-19, 1.8081e-19, 1.6732e-19, 1.5638e-19, 1.3670e-19,\n",
       "             1.1096e-19, 8.5851e-20, 8.1767e-20, 6.7642e-20, 6.6162e-20, 5.9125e-20,\n",
       "             2.7388e-20, 2.5826e-20, 2.2872e-20, 2.2782e-20, 1.5390e-20, 1.4661e-20,\n",
       "             1.2474e-20, 8.9460e-21, 8.6605e-21, 6.4875e-21, 6.2470e-21, 4.7653e-21,\n",
       "             4.3157e-21, 4.0890e-21, 3.8508e-21, 3.5349e-21, 3.1075e-21, 2.3445e-21,\n",
       "             1.5330e-21, 1.3868e-21, 1.2418e-21, 1.1852e-21, 9.9868e-22, 9.4302e-22,\n",
       "             5.3198e-22, 4.4189e-22, 2.7340e-22, 2.4394e-22, 2.3863e-22, 1.7101e-22,\n",
       "             1.5685e-22, 1.3660e-22, 1.1627e-22, 1.0997e-22, 9.2217e-23, 6.1316e-23,\n",
       "             3.8986e-23, 2.9377e-23, 1.1047e-23, 1.0518e-23, 2.4482e-24, 1.5834e-24,\n",
       "             6.0070e-25, 5.3988e-25, 4.5190e-25, 4.3211e-25, 3.2498e-25, 2.6874e-25,\n",
       "             2.3192e-25, 2.0026e-25, 1.7080e-25, 1.0467e-25, 5.2602e-26, 4.5382e-26,\n",
       "             4.2922e-26, 1.7370e-26, 1.0046e-26, 3.0383e-27, 2.5722e-27, 1.4217e-27,\n",
       "             8.6320e-28, 1.2057e-28, 9.4894e-29, 5.4297e-29, 3.7964e-29, 2.3596e-32,\n",
       "             3.2989e-33, 4.8133e-38, 0.0000e+00])}},\n",
       "   {'fpr': np.float64(0.35667396061269147),\n",
       "    'tpr': np.float64(0.9953739398612182),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0569, 0.0635, 0.0678, 0.0700, 0.0810, 0.0810, 0.0832, 0.0875,\n",
       "             0.0897, 0.0897, 0.0897, 0.0919, 0.0919, 0.0919, 0.0919, 0.0919, 0.0919,\n",
       "             0.0919, 0.0941, 0.0963, 0.0985, 0.0985, 0.0985, 0.1007, 0.1028, 0.1028,\n",
       "             0.1050, 0.1050, 0.1072, 0.1072, 0.1094, 0.1094, 0.1094, 0.1116, 0.1138,\n",
       "             0.1138, 0.1160, 0.1160, 0.1182, 0.1204, 0.1204, 0.1225, 0.1247, 0.1247,\n",
       "             0.1269, 0.1291, 0.1313, 0.1335, 0.1357, 0.1379, 0.1400, 0.1422, 0.1444,\n",
       "             0.1444, 0.1466, 0.1488, 0.1510, 0.1510, 0.1532, 0.1554, 0.1554, 0.1575,\n",
       "             0.1597, 0.1597, 0.1619, 0.1641, 0.1663, 0.1685, 0.1707, 0.1729, 0.1751,\n",
       "             0.1772, 0.1794, 0.1816, 0.1838, 0.1838, 0.1860, 0.1882, 0.1904, 0.1904,\n",
       "             0.1926, 0.1947, 0.1969, 0.1991, 0.2013, 0.2035, 0.2035, 0.2057, 0.2079,\n",
       "             0.2101, 0.2123, 0.2144, 0.2166, 0.2188, 0.2210, 0.2232, 0.2254, 0.2276,\n",
       "             0.2298, 0.2319, 0.2319, 0.2341, 0.2363, 0.2385, 0.2407, 0.2429, 0.2451,\n",
       "             0.2473, 0.2473, 0.2495, 0.2516, 0.2538, 0.2560, 0.2582, 0.2604, 0.2626,\n",
       "             0.2648, 0.2670, 0.2691, 0.2713, 0.2735, 0.2757, 0.2779, 0.2801, 0.2823,\n",
       "             0.2845, 0.2867, 0.2888, 0.2910, 0.2910, 0.2932, 0.2954, 0.2976, 0.2998,\n",
       "             0.3020, 0.3042, 0.3063, 0.3085, 0.3107, 0.3129, 0.3151, 0.3173, 0.3195,\n",
       "             0.3217, 0.3239, 0.3260, 0.3282, 0.3304, 0.3326, 0.3348, 0.3370, 0.3392,\n",
       "             0.3414, 0.3435, 0.3457, 0.3479, 0.3501, 0.3523, 0.3545, 0.3567, 0.3589,\n",
       "             0.3611, 0.3632, 0.3654, 0.3676, 0.3698, 0.3720, 0.3742, 0.3764, 0.3786,\n",
       "             0.3807, 0.3829, 0.3851, 0.3873, 0.3895, 0.3917, 0.3939, 0.3961, 0.3961,\n",
       "             0.3982, 0.4004, 0.4026, 0.4048, 0.4070, 0.4092, 0.4114, 0.4136, 0.4158,\n",
       "             0.4179, 0.4201, 0.4223, 0.4245, 0.4267, 0.4289, 0.4311, 0.4333, 0.4354,\n",
       "             0.4376, 0.4398, 0.4420, 0.4442, 0.4464, 0.4486, 0.4508, 0.4530, 0.4551,\n",
       "             0.4573, 0.4595, 0.4595, 0.4617, 0.4639, 0.4661, 0.4683, 0.4705, 0.4726,\n",
       "             0.4748, 0.4770, 0.4792, 0.4814, 0.4836, 0.4858, 0.4880, 0.4880, 0.4902,\n",
       "             0.4923, 0.4945, 0.4967, 0.4989, 0.5011, 0.5033, 0.5033, 0.5055, 0.5077,\n",
       "             0.5098, 0.5120, 0.5142, 0.5164, 0.5186, 0.5208, 0.5230, 0.5252, 0.5274,\n",
       "             0.5295, 0.5317, 0.5339, 0.5361, 0.5383, 0.5405, 0.5427, 0.5449, 0.5470,\n",
       "             0.5470, 0.5492, 0.5514, 0.5536, 0.5558, 0.5580, 0.5602, 0.5624, 0.5646,\n",
       "             0.5667, 0.5689, 0.5711, 0.5733, 0.5755, 0.5777, 0.5799, 0.5821, 0.5842,\n",
       "             0.5864, 0.5886, 0.5908, 0.5930, 0.5952, 0.5974, 0.5996, 0.6018, 0.6039,\n",
       "             0.6061, 0.6083, 0.6105, 0.6127, 0.6149, 0.6171, 0.6193, 0.6214, 0.6236,\n",
       "             0.6258, 0.6280, 0.6302, 0.6324, 0.6346, 0.6368, 0.6389, 0.6411, 0.6433,\n",
       "             0.6455, 0.6477, 0.6499, 0.6521, 0.6543, 0.6565, 0.6586, 0.6608, 0.6630,\n",
       "             0.6652, 0.6674, 0.6696, 0.6718, 0.6740, 0.6761, 0.6783, 0.6805, 0.6827,\n",
       "             0.6849, 0.6871, 0.6893, 0.6915, 0.6937, 0.6958, 0.6980, 0.7002, 0.7024,\n",
       "             0.7046, 0.7068, 0.7090, 0.7112, 0.7133, 0.7155, 0.7177, 0.7199, 0.7221,\n",
       "             0.7243, 0.7265, 0.7287, 0.7309, 0.7330, 0.7352, 0.7374, 0.7396, 0.7418,\n",
       "             0.7440, 0.7462, 0.7484, 0.7505, 0.7527, 0.7549, 0.7571, 0.7593, 0.7615,\n",
       "             0.7637, 0.7659, 0.7681, 0.7702, 0.7724, 0.7746, 0.7768, 0.7790, 0.7812,\n",
       "             0.7834, 0.7856, 0.7877, 0.7899, 0.7921, 0.7943, 0.7965, 0.7987, 0.8009,\n",
       "             0.8031, 0.8053, 0.8074, 0.8096, 0.8118, 0.8140, 0.8162, 0.8184, 0.8184,\n",
       "             0.8206, 0.8228, 0.8249, 0.8271, 0.8293, 0.8315, 0.8337, 0.8359, 0.8381,\n",
       "             0.8403, 0.8425, 0.8446, 0.8468, 0.8490, 0.8512, 0.8534, 0.8556, 0.8578,\n",
       "             0.8600, 0.8621, 0.8643, 0.8665, 0.8687, 0.8709, 0.8731, 0.8753, 0.8775,\n",
       "             0.8796, 0.8818, 0.8840, 0.8862, 0.8884, 0.8906, 0.8928, 0.8950, 0.8972,\n",
       "             0.8993, 0.9015, 0.9037, 0.9059, 0.9081, 0.9103, 0.9125, 0.9147, 0.9168,\n",
       "             0.9190, 0.9212, 0.9234, 0.9256, 0.9278, 0.9300, 0.9322, 0.9344, 0.9365,\n",
       "             0.9387, 0.9409, 0.9431, 0.9453, 0.9475, 0.9497, 0.9519, 0.9540, 0.9562,\n",
       "             0.9584, 0.9606, 0.9628, 0.9650, 0.9672, 0.9694, 0.9716, 0.9737, 0.9759,\n",
       "             0.9781, 0.9803, 0.9825, 0.9847, 0.9869, 0.9891, 0.9912, 0.9934, 0.9956,\n",
       "             0.9978, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9483, 0.9561, 0.9622, 0.9645, 0.9676, 0.9684, 0.9692, 0.9692,\n",
       "             0.9707, 0.9715, 0.9730, 0.9738, 0.9753, 0.9761, 0.9769, 0.9776, 0.9784,\n",
       "             0.9792, 0.9792, 0.9792, 0.9792, 0.9800, 0.9807, 0.9807, 0.9807, 0.9815,\n",
       "             0.9815, 0.9823, 0.9823, 0.9830, 0.9830, 0.9838, 0.9846, 0.9846, 0.9846,\n",
       "             0.9854, 0.9854, 0.9861, 0.9861, 0.9861, 0.9869, 0.9869, 0.9869, 0.9877,\n",
       "             0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877, 0.9877,\n",
       "             0.9884, 0.9884, 0.9884, 0.9884, 0.9892, 0.9892, 0.9892, 0.9900, 0.9900,\n",
       "             0.9900, 0.9907, 0.9907, 0.9907, 0.9907, 0.9907, 0.9907, 0.9907, 0.9907,\n",
       "             0.9907, 0.9907, 0.9907, 0.9907, 0.9915, 0.9915, 0.9915, 0.9915, 0.9923,\n",
       "             0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938, 0.9938,\n",
       "             0.9938, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946, 0.9946,\n",
       "             0.9946, 0.9946, 0.9946, 0.9946, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "             0.9961, 0.9961, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969,\n",
       "             0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9969, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "             0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9992e-01, 9.9992e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9973e-01, 9.9970e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9962e-01, 9.9960e-01, 9.9952e-01, 9.9950e-01, 9.9948e-01, 9.9946e-01,\n",
       "             9.9946e-01, 9.9923e-01, 9.9911e-01, 9.9897e-01, 9.9897e-01, 9.9879e-01,\n",
       "             9.9879e-01, 9.9876e-01, 9.9875e-01, 9.9858e-01, 9.9837e-01, 9.9830e-01,\n",
       "             9.9805e-01, 9.9802e-01, 9.9786e-01, 9.9778e-01, 9.9711e-01, 9.9677e-01,\n",
       "             9.9671e-01, 9.9661e-01, 9.9623e-01, 9.9614e-01, 9.9610e-01, 9.9597e-01,\n",
       "             9.9567e-01, 9.9550e-01, 9.9514e-01, 9.9505e-01, 9.9462e-01, 9.9258e-01,\n",
       "             9.9196e-01, 9.9121e-01, 9.9087e-01, 9.8994e-01, 9.8934e-01, 9.8933e-01,\n",
       "             9.8928e-01, 9.8901e-01, 9.8896e-01, 9.8879e-01, 9.8815e-01, 9.8803e-01,\n",
       "             9.8481e-01, 9.8413e-01, 9.8279e-01, 9.8215e-01, 9.8201e-01, 9.8152e-01,\n",
       "             9.8107e-01, 9.7976e-01, 9.7831e-01, 9.7624e-01, 9.7060e-01, 9.6829e-01,\n",
       "             9.6582e-01, 9.6402e-01, 9.6234e-01, 9.5917e-01, 9.5814e-01, 9.5316e-01,\n",
       "             9.4538e-01, 9.4053e-01, 9.3882e-01, 9.3568e-01, 9.3174e-01, 9.2960e-01,\n",
       "             9.2419e-01, 9.1645e-01, 9.1568e-01, 9.0784e-01, 9.0720e-01, 9.0290e-01,\n",
       "             8.9641e-01, 8.8728e-01, 8.8616e-01, 8.7105e-01, 8.5844e-01, 8.4336e-01,\n",
       "             8.1653e-01, 8.0857e-01, 7.9602e-01, 7.9048e-01, 7.7801e-01, 7.3429e-01,\n",
       "             7.3083e-01, 7.2219e-01, 6.9537e-01, 6.7778e-01, 6.7520e-01, 6.4883e-01,\n",
       "             6.3751e-01, 6.3442e-01, 6.2483e-01, 5.8405e-01, 5.6498e-01, 5.5290e-01,\n",
       "             5.4700e-01, 5.4474e-01, 5.4038e-01, 5.2012e-01, 5.0540e-01, 4.8656e-01,\n",
       "             4.8598e-01, 4.5851e-01, 4.5247e-01, 4.3859e-01, 4.3512e-01, 4.2592e-01,\n",
       "             4.2307e-01, 4.1265e-01, 4.0043e-01, 3.8347e-01, 3.6757e-01, 3.5863e-01,\n",
       "             2.7891e-01, 2.7305e-01, 2.5816e-01, 2.5133e-01, 2.5012e-01, 2.3934e-01,\n",
       "             2.3750e-01, 2.1967e-01, 1.9182e-01, 1.9077e-01, 1.8676e-01, 1.7807e-01,\n",
       "             1.6950e-01, 1.6521e-01, 1.4239e-01, 1.3961e-01, 1.2843e-01, 1.2296e-01,\n",
       "             1.0701e-01, 9.4454e-02, 9.1386e-02, 8.3104e-02, 8.1315e-02, 7.7014e-02,\n",
       "             7.3812e-02, 6.4829e-02, 5.6731e-02, 5.1681e-02, 5.0987e-02, 4.6101e-02,\n",
       "             4.3777e-02, 4.1594e-02, 3.7772e-02, 3.7495e-02, 3.3825e-02, 3.2492e-02,\n",
       "             2.6281e-02, 2.6221e-02, 2.4834e-02, 2.2016e-02, 2.0270e-02, 1.9752e-02,\n",
       "             1.7167e-02, 1.6456e-02, 1.4263e-02, 1.3857e-02, 1.2806e-02, 1.1319e-02,\n",
       "             1.1201e-02, 1.0785e-02, 9.5936e-03, 9.3055e-03, 9.2317e-03, 9.1025e-03,\n",
       "             8.9481e-03, 8.8003e-03, 8.5741e-03, 8.4012e-03, 8.1486e-03, 7.4244e-03,\n",
       "             6.7688e-03, 5.9350e-03, 4.5613e-03, 3.8036e-03, 3.5067e-03, 3.0759e-03,\n",
       "             2.9994e-03, 2.9929e-03, 2.7751e-03, 2.7066e-03, 2.6489e-03, 2.6062e-03,\n",
       "             2.5307e-03, 2.3686e-03, 1.7232e-03, 1.6981e-03, 1.6233e-03, 1.5112e-03,\n",
       "             1.4875e-03, 1.3973e-03, 1.2396e-03, 1.2193e-03, 1.1043e-03, 1.0480e-03,\n",
       "             1.0271e-03, 9.7512e-04, 9.6173e-04, 9.5608e-04, 9.2902e-04, 8.9118e-04,\n",
       "             7.1972e-04, 6.9295e-04, 6.8792e-04, 6.0212e-04, 5.2652e-04, 5.0728e-04,\n",
       "             4.8466e-04, 4.3840e-04, 3.7539e-04, 3.4741e-04, 3.2790e-04, 3.1622e-04,\n",
       "             2.9742e-04, 2.9182e-04, 2.4194e-04, 2.2380e-04, 1.9873e-04, 1.9157e-04,\n",
       "             1.6675e-04, 1.3233e-04, 1.3159e-04, 1.2147e-04, 9.6558e-05, 9.4422e-05,\n",
       "             8.7190e-05, 8.1391e-05, 7.9201e-05, 7.4050e-05, 6.4454e-05, 6.3232e-05,\n",
       "             6.2474e-05, 5.6419e-05, 4.9489e-05, 4.6154e-05, 4.5868e-05, 3.2060e-05,\n",
       "             3.1814e-05, 2.3893e-05, 2.2919e-05, 2.1844e-05, 2.0055e-05, 1.9744e-05,\n",
       "             1.8658e-05, 1.5808e-05, 1.4242e-05, 1.3482e-05, 1.3166e-05, 1.2761e-05,\n",
       "             1.1908e-05, 1.1215e-05, 1.1192e-05, 1.1004e-05, 1.0548e-05, 1.0531e-05,\n",
       "             1.0484e-05, 9.9216e-06, 9.3845e-06, 8.8496e-06, 8.6414e-06, 8.2231e-06,\n",
       "             7.8258e-06, 7.6392e-06, 7.4473e-06, 6.7902e-06, 5.6466e-06, 5.4365e-06,\n",
       "             5.3659e-06, 4.7628e-06, 4.5215e-06, 4.2501e-06, 3.9975e-06, 3.5886e-06,\n",
       "             3.5501e-06, 3.2215e-06, 3.1198e-06, 2.6543e-06, 2.6154e-06, 2.3756e-06,\n",
       "             2.0948e-06, 1.9446e-06, 1.6292e-06, 1.6127e-06, 1.3409e-06, 1.2178e-06,\n",
       "             1.0304e-06, 9.2365e-07, 8.5911e-07, 8.2995e-07, 7.3000e-07, 6.8259e-07,\n",
       "             6.6265e-07, 6.3715e-07, 5.7880e-07, 5.7737e-07, 5.3296e-07, 5.2957e-07,\n",
       "             4.1760e-07, 3.6953e-07, 3.6056e-07, 2.9409e-07, 2.8501e-07, 1.9837e-07,\n",
       "             1.8718e-07, 1.5647e-07, 1.4319e-07, 1.3703e-07, 1.3391e-07, 1.1647e-07,\n",
       "             1.1323e-07, 8.8858e-08, 8.7004e-08, 7.5787e-08, 7.4233e-08, 6.1545e-08,\n",
       "             5.4018e-08, 4.0309e-08, 3.3415e-08, 3.1191e-08, 2.8518e-08, 2.5050e-08,\n",
       "             2.2916e-08, 1.3271e-08, 1.0728e-08, 1.0721e-08, 1.0116e-08, 8.4383e-09,\n",
       "             8.2161e-09, 7.9022e-09, 7.7041e-09, 7.1767e-09, 6.4010e-09, 5.1481e-09,\n",
       "             4.9323e-09, 3.5523e-09, 3.3910e-09, 2.7429e-09, 2.0171e-09, 1.4854e-09,\n",
       "             1.4427e-09, 1.4212e-09, 1.3433e-09, 1.2575e-09, 1.2118e-09, 1.1271e-09,\n",
       "             9.0450e-10, 7.3459e-10, 7.1656e-10, 7.1379e-10, 6.8931e-10, 5.9648e-10,\n",
       "             5.3883e-10, 5.3793e-10, 4.7557e-10, 4.6644e-10, 4.3060e-10, 3.6967e-10,\n",
       "             2.5513e-10, 2.2664e-10, 2.1787e-10, 1.8785e-10, 1.2437e-10, 1.1025e-10,\n",
       "             1.0224e-10, 7.7625e-11, 5.2311e-11, 5.2084e-11, 4.6312e-11, 4.0030e-11,\n",
       "             2.8984e-11, 2.6478e-11, 1.4448e-11, 7.1552e-12, 5.3668e-12, 4.2680e-12,\n",
       "             2.3544e-12, 1.8199e-12, 1.6684e-12, 6.5759e-13, 6.2091e-13, 5.9939e-13,\n",
       "             4.2378e-13, 2.6258e-13, 2.2124e-13, 1.9527e-13, 1.0564e-13, 1.0558e-13,\n",
       "             2.6211e-14, 1.9458e-14, 1.7162e-14, 1.1731e-14, 9.1303e-16, 1.0265e-16,\n",
       "             1.9905e-17, 1.2226e-18, 7.8505e-19, 2.2692e-20, 9.9993e-27])}}]],\n",
       " 'roc_results': {'fpr': tensor([0.0000, 0.0043, 0.0043, 0.0043, 0.0043, 0.0085, 0.0085, 0.0085, 0.0085,\n",
       "          0.0085, 0.0085, 0.0085, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
       "          0.0128, 0.0128, 0.0128, 0.0171, 0.0171, 0.0214, 0.0256, 0.0256, 0.0256,\n",
       "          0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.0299, 0.0299, 0.0299, 0.0299,\n",
       "          0.0299, 0.0299, 0.0299, 0.0299, 0.0299, 0.0299, 0.0342, 0.0385, 0.0385,\n",
       "          0.0385, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427,\n",
       "          0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0470, 0.0470, 0.0470,\n",
       "          0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470,\n",
       "          0.0470, 0.0513, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556, 0.0556,\n",
       "          0.0598, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641,\n",
       "          0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0684,\n",
       "          0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684,\n",
       "          0.0684, 0.0726, 0.0726, 0.0726, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
       "          0.0769, 0.0769, 0.0769, 0.0769, 0.0855, 0.0855, 0.0855, 0.0855, 0.0855,\n",
       "          0.0855, 0.0897, 0.0897, 0.0897, 0.0940, 0.0940, 0.0983, 0.1026, 0.1026,\n",
       "          0.1026, 0.1026, 0.1026, 0.1026, 0.1026, 0.1026, 0.1026, 0.1068, 0.1068,\n",
       "          0.1111, 0.1111, 0.1111, 0.1154, 0.1154, 0.1154, 0.1154, 0.1154, 0.1154,\n",
       "          0.1154, 0.1154, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197,\n",
       "          0.1239, 0.1239, 0.1239, 0.1239, 0.1239, 0.1239, 0.1239, 0.1239, 0.1239,\n",
       "          0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1325, 0.1325, 0.1368,\n",
       "          0.1368, 0.1368, 0.1368, 0.1368, 0.1368, 0.1410, 0.1410, 0.1410, 0.1453,\n",
       "          0.1496, 0.1496, 0.1496, 0.1538, 0.1581, 0.1581, 0.1581, 0.1624, 0.1667,\n",
       "          0.1667, 0.1709, 0.1752, 0.1795, 0.1838, 0.1838, 0.1838, 0.1880, 0.1880,\n",
       "          0.1923, 0.1923, 0.1966, 0.2009, 0.2009, 0.2009, 0.2051, 0.2051, 0.2051,\n",
       "          0.2051, 0.2051, 0.2051, 0.2051, 0.2094, 0.2094, 0.2137, 0.2137, 0.2179,\n",
       "          0.2222, 0.2265, 0.2308, 0.2350, 0.2393, 0.2393, 0.2393, 0.2393, 0.2436,\n",
       "          0.2479, 0.2521, 0.2564, 0.2564, 0.2564, 0.2564, 0.2564, 0.2607, 0.2650,\n",
       "          0.2692, 0.2735, 0.2735, 0.2778, 0.2821, 0.2863, 0.2906, 0.2949, 0.2991,\n",
       "          0.3034, 0.3034, 0.3077, 0.3120, 0.3120, 0.3120, 0.3162, 0.3162, 0.3205,\n",
       "          0.3248, 0.3291, 0.3333, 0.3376, 0.3419, 0.3419, 0.3462, 0.3504, 0.3547,\n",
       "          0.3590, 0.3632, 0.3675, 0.3718, 0.3761, 0.3803, 0.3846, 0.3889, 0.3932,\n",
       "          0.3974, 0.3974, 0.3974, 0.4017, 0.4060, 0.4103, 0.4145, 0.4188, 0.4188,\n",
       "          0.4231, 0.4274, 0.4316, 0.4359, 0.4359, 0.4359, 0.4359, 0.4402, 0.4444,\n",
       "          0.4530, 0.4573, 0.4615, 0.4658, 0.4658, 0.4701, 0.4744, 0.4744, 0.4786,\n",
       "          0.4829, 0.4872, 0.4872, 0.4915, 0.4957, 0.4957, 0.4957, 0.5000, 0.5043,\n",
       "          0.5085, 0.5128, 0.5171, 0.5214, 0.5256, 0.5299, 0.5342, 0.5342, 0.5385,\n",
       "          0.5427, 0.5470, 0.5513, 0.5556, 0.5641, 0.5684, 0.5726, 0.5769, 0.5812,\n",
       "          0.5855, 0.5855, 0.5897, 0.5940, 0.5983, 0.6026, 0.6068, 0.6111, 0.6154,\n",
       "          0.6197, 0.6239, 0.6282, 0.6325, 0.6368, 0.6453, 0.6453, 0.6538, 0.6581,\n",
       "          0.6624, 0.6667, 0.6709, 0.6752, 0.6795, 0.6838, 0.6880, 0.6923, 0.6966,\n",
       "          0.7009, 0.7051, 0.7094, 0.7137, 0.7222, 0.7265, 0.7308, 0.7350, 0.7393,\n",
       "          0.7436, 0.7479, 0.7521, 0.7564, 0.7607, 0.7650, 0.7692, 0.7735, 0.7778,\n",
       "          0.7821, 0.7863, 0.7906, 0.7949, 0.7991, 0.8034, 0.8077, 0.8120, 0.8162,\n",
       "          0.8205, 0.8248, 0.8291, 0.8333, 0.8376, 0.8419, 0.8462, 0.8504, 0.8547,\n",
       "          0.8590, 0.8632, 0.8675, 0.8718, 0.8761, 0.8803, 0.8846, 0.8889, 0.8932,\n",
       "          0.8974, 0.9017, 0.9060, 0.9103, 0.9145, 0.9188, 0.9231, 0.9274, 0.9316,\n",
       "          0.9359, 0.9402, 0.9444, 0.9487, 0.9530, 0.9573, 0.9615, 0.9658, 0.9701,\n",
       "          0.9744, 0.9786, 0.9829, 0.9872, 0.9915, 0.9957, 1.0000]),\n",
       "  'tpr': tensor([0.0000, 0.0000, 0.0051, 0.0077, 0.0179, 0.0410, 0.0564, 0.0718, 0.1000,\n",
       "          0.1256, 0.1385, 0.1692, 0.1897, 0.2026, 0.2103, 0.2256, 0.2359, 0.2487,\n",
       "          0.2615, 0.2718, 0.2744, 0.2872, 0.3000, 0.3077, 0.3179, 0.3256, 0.3308,\n",
       "          0.3359, 0.3410, 0.3513, 0.3564, 0.3615, 0.3667, 0.3692, 0.3744, 0.3821,\n",
       "          0.3872, 0.3897, 0.4051, 0.4154, 0.4231, 0.4282, 0.4385, 0.4385, 0.4410,\n",
       "          0.4436, 0.4487, 0.4538, 0.4641, 0.4744, 0.4872, 0.4897, 0.4949, 0.4974,\n",
       "          0.5000, 0.5026, 0.5051, 0.5154, 0.5205, 0.5282, 0.5308, 0.5333, 0.5385,\n",
       "          0.5410, 0.5436, 0.5462, 0.5487, 0.5513, 0.5538, 0.5564, 0.5615, 0.5641,\n",
       "          0.5667, 0.5692, 0.5692, 0.5718, 0.5795, 0.5846, 0.5872, 0.5897, 0.5923,\n",
       "          0.5949, 0.6000, 0.6051, 0.6077, 0.6128, 0.6154, 0.6179, 0.6205, 0.6231,\n",
       "          0.6256, 0.6282, 0.6359, 0.6385, 0.6410, 0.6436, 0.6462, 0.6513, 0.6538,\n",
       "          0.6564, 0.6590, 0.6615, 0.6641, 0.6667, 0.6692, 0.6718, 0.6744, 0.6769,\n",
       "          0.6795, 0.6795, 0.6846, 0.6872, 0.6897, 0.6923, 0.6974, 0.7000, 0.7026,\n",
       "          0.7051, 0.7077, 0.7103, 0.7128, 0.7128, 0.7154, 0.7205, 0.7231, 0.7256,\n",
       "          0.7282, 0.7308, 0.7333, 0.7385, 0.7385, 0.7410, 0.7436, 0.7436, 0.7462,\n",
       "          0.7487, 0.7513, 0.7538, 0.7564, 0.7590, 0.7615, 0.7641, 0.7667, 0.7692,\n",
       "          0.7692, 0.7718, 0.7769, 0.7769, 0.7795, 0.7821, 0.7846, 0.7872, 0.7897,\n",
       "          0.7923, 0.7949, 0.7949, 0.8000, 0.8026, 0.8051, 0.8077, 0.8103, 0.8128,\n",
       "          0.8128, 0.8154, 0.8179, 0.8205, 0.8231, 0.8256, 0.8282, 0.8308, 0.8333,\n",
       "          0.8333, 0.8359, 0.8385, 0.8436, 0.8462, 0.8513, 0.8513, 0.8538, 0.8564,\n",
       "          0.8590, 0.8615, 0.8641, 0.8667, 0.8692, 0.8692, 0.8718, 0.8744, 0.8744,\n",
       "          0.8744, 0.8769, 0.8795, 0.8795, 0.8795, 0.8821, 0.8846, 0.8846, 0.8846,\n",
       "          0.8872, 0.8872, 0.8872, 0.8872, 0.8872, 0.8897, 0.8923, 0.8923, 0.8949,\n",
       "          0.8974, 0.9000, 0.9000, 0.9000, 0.9026, 0.9051, 0.9051, 0.9077, 0.9103,\n",
       "          0.9128, 0.9154, 0.9179, 0.9205, 0.9205, 0.9231, 0.9256, 0.9282, 0.9282,\n",
       "          0.9282, 0.9282, 0.9282, 0.9282, 0.9282, 0.9308, 0.9333, 0.9359, 0.9359,\n",
       "          0.9359, 0.9359, 0.9359, 0.9385, 0.9410, 0.9436, 0.9462, 0.9462, 0.9462,\n",
       "          0.9462, 0.9462, 0.9487, 0.9487, 0.9487, 0.9487, 0.9487, 0.9487, 0.9487,\n",
       "          0.9487, 0.9513, 0.9513, 0.9513, 0.9538, 0.9564, 0.9564, 0.9590, 0.9590,\n",
       "          0.9590, 0.9590, 0.9590, 0.9590, 0.9590, 0.9615, 0.9615, 0.9615, 0.9615,\n",
       "          0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615,\n",
       "          0.9641, 0.9667, 0.9692, 0.9692, 0.9692, 0.9692, 0.9692, 0.9692, 0.9718,\n",
       "          0.9718, 0.9718, 0.9718, 0.9718, 0.9744, 0.9769, 0.9795, 0.9795, 0.9795,\n",
       "          0.9795, 0.9795, 0.9795, 0.9795, 0.9821, 0.9821, 0.9821, 0.9846, 0.9846,\n",
       "          0.9846, 0.9846, 0.9872, 0.9872, 0.9872, 0.9897, 0.9923, 0.9923, 0.9923,\n",
       "          0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9923, 0.9949, 0.9949,\n",
       "          0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949,\n",
       "          0.9949, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974,\n",
       "          0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "  'thresholds': tensor([1.0000, 1.0000, 0.9995, 0.9990, 0.9985, 0.9980, 0.9976, 0.9971, 0.9966,\n",
       "          0.9961, 0.9956, 0.9951, 0.9946, 0.9941, 0.9937, 0.9932, 0.9927, 0.9922,\n",
       "          0.9917, 0.9912, 0.9907, 0.9902, 0.9897, 0.9893, 0.9888, 0.9883, 0.9878,\n",
       "          0.9873, 0.9868, 0.9863, 0.9858, 0.9854, 0.9849, 0.9844, 0.9839, 0.9834,\n",
       "          0.9829, 0.9824, 0.9819, 0.9814, 0.9810, 0.9805, 0.9800, 0.9790, 0.9785,\n",
       "          0.9780, 0.9775, 0.9771, 0.9766, 0.9761, 0.9756, 0.9751, 0.9746, 0.9741,\n",
       "          0.9736, 0.9731, 0.9727, 0.9717, 0.9712, 0.9707, 0.9697, 0.9683, 0.9673,\n",
       "          0.9658, 0.9648, 0.9639, 0.9634, 0.9629, 0.9624, 0.9619, 0.9614, 0.9609,\n",
       "          0.9604, 0.9600, 0.9590, 0.9585, 0.9580, 0.9565, 0.9551, 0.9541, 0.9521,\n",
       "          0.9517, 0.9512, 0.9478, 0.9473, 0.9448, 0.9438, 0.9424, 0.9409, 0.9390,\n",
       "          0.9385, 0.9380, 0.9351, 0.9312, 0.9307, 0.9297, 0.9292, 0.9272, 0.9268,\n",
       "          0.9263, 0.9238, 0.9224, 0.9219, 0.9189, 0.9165, 0.9160, 0.9155, 0.9092,\n",
       "          0.9082, 0.9067, 0.9043, 0.9014, 0.8975, 0.8970, 0.8960, 0.8940, 0.8926,\n",
       "          0.8921, 0.8887, 0.8882, 0.8872, 0.8857, 0.8853, 0.8843, 0.8823, 0.8813,\n",
       "          0.8809, 0.8799, 0.8784, 0.8779, 0.8755, 0.8750, 0.8716, 0.8696, 0.8682,\n",
       "          0.8633, 0.8594, 0.8589, 0.8584, 0.8579, 0.8560, 0.8506, 0.8491, 0.8477,\n",
       "          0.8467, 0.8452, 0.8433, 0.8408, 0.8379, 0.8369, 0.8359, 0.8354, 0.8335,\n",
       "          0.8315, 0.8296, 0.8267, 0.8247, 0.8237, 0.8198, 0.8188, 0.8159, 0.8149,\n",
       "          0.8145, 0.8101, 0.8071, 0.8052, 0.8027, 0.7988, 0.7974, 0.7969, 0.7964,\n",
       "          0.7930, 0.7861, 0.7852, 0.7812, 0.7803, 0.7798, 0.7739, 0.7715, 0.7656,\n",
       "          0.7622, 0.7539, 0.7534, 0.7515, 0.7505, 0.7476, 0.7432, 0.7422, 0.7417,\n",
       "          0.7393, 0.7388, 0.7334, 0.7271, 0.7266, 0.7261, 0.7251, 0.7217, 0.7202,\n",
       "          0.7192, 0.7188, 0.7144, 0.7139, 0.7104, 0.6963, 0.6875, 0.6855, 0.6831,\n",
       "          0.6782, 0.6777, 0.6753, 0.6724, 0.6709, 0.6704, 0.6699, 0.6660, 0.6646,\n",
       "          0.6558, 0.6543, 0.6494, 0.6421, 0.6411, 0.6401, 0.6362, 0.6348, 0.6343,\n",
       "          0.6313, 0.6284, 0.6206, 0.6147, 0.6128, 0.5952, 0.5942, 0.5913, 0.5908,\n",
       "          0.5815, 0.5786, 0.5737, 0.5732, 0.5684, 0.5586, 0.5566, 0.5552, 0.5430,\n",
       "          0.5420, 0.5405, 0.5386, 0.5376, 0.5361, 0.5122, 0.5112, 0.5049, 0.5020,\n",
       "          0.4980, 0.4946, 0.4822, 0.4797, 0.4761, 0.4631, 0.4541, 0.4456, 0.4404,\n",
       "          0.4399, 0.4255, 0.4199, 0.4146, 0.4036, 0.4021, 0.4014, 0.3997, 0.3992,\n",
       "          0.3962, 0.3875, 0.3848, 0.3811, 0.3809, 0.3779, 0.3752, 0.3718, 0.3652,\n",
       "          0.3621, 0.3542, 0.3535, 0.3438, 0.3376, 0.3347, 0.3328, 0.3320, 0.3311,\n",
       "          0.3262, 0.3259, 0.3193, 0.3159, 0.3071, 0.2976, 0.2954, 0.2915, 0.2900,\n",
       "          0.2869, 0.2825, 0.2803, 0.2781, 0.2729, 0.2487, 0.2478, 0.2444, 0.2438,\n",
       "          0.2422, 0.2421, 0.2410, 0.2360, 0.2338, 0.2311, 0.2306, 0.2294, 0.2283,\n",
       "          0.2094, 0.2009, 0.1996, 0.1989, 0.1959, 0.1958, 0.1843, 0.1831, 0.1638,\n",
       "          0.1569, 0.1514, 0.1475, 0.1466, 0.1409, 0.1364, 0.1348, 0.1331, 0.1320,\n",
       "          0.1313, 0.1311, 0.1310, 0.1300, 0.1287, 0.1248, 0.1236, 0.1212, 0.1188,\n",
       "          0.1155, 0.1136, 0.1126, 0.1120, 0.1107, 0.1097, 0.1074, 0.1052, 0.1038,\n",
       "          0.1021, 0.1019, 0.1016, 0.1014, 0.0995, 0.0955, 0.0944, 0.0919, 0.0895,\n",
       "          0.0887, 0.0882, 0.0876, 0.0859, 0.0840, 0.0817, 0.0814, 0.0808, 0.0801,\n",
       "          0.0788, 0.0786, 0.0784, 0.0767, 0.0760, 0.0757, 0.0718, 0.0707, 0.0690,\n",
       "          0.0688, 0.0684, 0.0657, 0.0655, 0.0642, 0.0641, 0.0634, 0.0624, 0.0613,\n",
       "          0.0609, 0.0577, 0.0568, 0.0565, 0.0558, 0.0557, 0.0523, 0.0482, 0.0480,\n",
       "          0.0478, 0.0465, 0.0456, 0.0435, 0.0434, 0.0430, 0.0419, 0.0418, 0.0399,\n",
       "          0.0399, 0.0395, 0.0385, 0.0366, 0.0355, 0.0352, 0.0342, 0.0323, 0.0302,\n",
       "          0.0300, 0.0276, 0.0262, 0.0262, 0.0254, 0.0247, 0.0244, 0.0239, 0.0229,\n",
       "          0.0217, 0.0215, 0.0209, 0.0198, 0.0191, 0.0171, 0.0153],\n",
       "         dtype=torch.float16),\n",
       "  'name': 'Original NN PneumoniaMNIST',\n",
       "  'auc': tensor(0.9218, device='cuda:0'),\n",
       "  'model': LitSimpleCNN(\n",
       "    (model): SimpleCNN(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "        (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): ReLU()\n",
       "        (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "      (relu_fc): ReLU()\n",
       "      (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (criterion): BCEWithLogitsLoss()\n",
       "    (train_accuracy): BinaryAccuracy()\n",
       "    (val_accuracy): BinaryAccuracy()\n",
       "    (val_auc): BinaryAUROC()\n",
       "    (test_accuracy): BinaryAccuracy()\n",
       "    (test_auc): BinaryAUROC()\n",
       "    (test_precision): BinaryPrecision()\n",
       "    (test_recall): BinaryRecall()\n",
       "    (test_f1): BinaryF1Score()\n",
       "  )},\n",
       " 'test_loader': <torch.utils.data.dataloader.DataLoader at 0x76caf36a3c50>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Save the data\n",
    "save_to_pickle(list_folds_best_models, list_folds_weighted_clfs, results_original_roc, test_loader, filename='pickle/medMNIST_weighted_bootstrap.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d29cb",
   "metadata": {},
   "source": [
    "# Load pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bbf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_ensemble_and_evaluate_paper import save_to_pickle, load_from_pickle\n",
    "from predict_ensemble_and_evaluate_paper import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate_paper import plot_roc_comparison\n",
    "from predict_ensemble_and_evaluate_paper import make_curve_monotonic\n",
    "\n",
    "# Load the data\n",
    "list_folds_best_models, list_folds_weighted_clfs, results_original_roc, test_loader = load_from_pickle(filename='pickle/medMNIST_weighted_bootstrap.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a49e4",
   "metadata": {},
   "source": [
    "### Ensemble prediction using voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d77ee047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a single ensemble from 140 models across all folds.\n",
      "Extracting full dataset...\n",
      "Getting predictions from all models...\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Loop through all the FPR values to generate the ensemble ROC curve ---\n",
    "\n",
    "\n",
    "\n",
    "results_tuple, prior_proba = predict_ensemble_and_evaluate(list_folds_best_models=list_folds_best_models,\n",
    "    test_loader=test_loader)\n",
    "\n",
    "ensemble_results_soft = results_tuple['soft_voting']\n",
    "ensemble_results_hard = results_tuple['hard_voting']\n",
    "misclassification_risk = results_tuple['misclassification_risk']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f000d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "misclassification_risk_half_bootstrap = results_tuple['misclassification_risk_half']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a29a7a",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate Neyman Pearson ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca388dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class SklearnPyTorchCNNWrapper:\n",
    "    def __init__(self, model, epochs=5, lr=0.001, image_dims=(1, 224, 224)):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.image_dims = image_dims\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.train()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).view(-1, 1))\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        print(\"Starting PyTorch model training inside wrapper...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, labels in loader:\n",
    "                inputs = inputs.view(-1, *self.image_dims).to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            all_outputs = []\n",
    "            for i in range(0, len(X_tensor), BATCH_SIZE):\n",
    "                batch = X_tensor[i:i+BATCH_SIZE]\n",
    "                batch = batch.view(-1, *self.image_dims).to(self.device)\n",
    "                output_batch = self.model(batch).cpu().numpy()\n",
    "                all_outputs.append(output_batch)\n",
    "            return np.vstack(all_outputs).flatten()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            all_probs = []\n",
    "            for i in range(0, len(X_tensor), BATCH_SIZE):\n",
    "                batch = X_tensor[i:i+BATCH_SIZE]\n",
    "                batch = batch.view(-1, *self.image_dims).to(self.device)\n",
    "                prob_class_1_batch = self.model(batch).cpu().numpy()\n",
    "                all_probs.append(prob_class_1_batch)\n",
    "            \n",
    "            prob_class_1 = np.vstack(all_probs)\n",
    "            prob_class_0 = 1 - prob_class_1\n",
    "            \n",
    "            return np.hstack((prob_class_0, prob_class_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import scipy\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nproc import npc\n",
    "\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 1\n",
    "NUM_CHANNELS = 1\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = generate_data(\"pneumoniaMNIST\")\n",
    "\n",
    "\n",
    "# Ensure you have your train_loader and val_loader defined here\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset.transform = data_transforms\n",
    "test_dataset.transform = data_transforms\n",
    "val_dataset.transform = data_transforms\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#new_train_loader = data.DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) to be done inside the sample_ratio loop\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=int(BATCH_SIZE/4), shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "def flatten_dataset(dataset):\n",
    "    X_list, y_list = [], []\n",
    "    for img, label in dataset:\n",
    "        X_list.append(np.array(img).flatten())\n",
    "        y_list.append(label[0])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "X_train, y_train = flatten_dataset(train_dataset)\n",
    "X_test, y_test = flatten_dataset(test_dataset)\n",
    "\n",
    "print(\"Generating constrained ROC curve (Neyman-Pearson Simulation)...\")\n",
    "\n",
    "npc_instance = npc()\n",
    "\n",
    "# 1. Instantiate your PyTorch model\n",
    "pytorch_cnn = SimpleCNN(\n",
    "    in_channels=NUM_CHANNELS,\n",
    "    num_classes=1,\n",
    "    image_height=IMAGE_SIZE,\n",
    "    image_width=IMAGE_SIZE\n",
    ")\n",
    "wrapped_model = SklearnPyTorchCNNWrapper(\n",
    "    model=pytorch_cnn, \n",
    "    epochs=NUM_EPOCHS, \n",
    "    lr=LEARNING_RATE, \n",
    "    image_dims=(NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    ")\n",
    "\n",
    "print(\"Generating constrained ROC curve (Neyman-Pearson Simulation)...\")\n",
    "\n",
    "result = npc_instance.npc(\n",
    "    x=X_train, \n",
    "    y=y_train, \n",
    "    method=\"\", # Leave blank to use the provided model\n",
    "    model=wrapped_model\n",
    ")\n",
    "\n",
    "# Extract the results from the single run\n",
    "fit_results = result[0][0]\n",
    "final_model = fit_results[0]             # The single trained model object\n",
    "y_calib_labels = fit_results[1]          # The labels from the internal calibration set\n",
    "y_calib_scores = fit_results[2]          # The model's scores on the calibration set\n",
    "initial_sign = fit_results[4]\n",
    "print(\"Phase 1 Complete.\")\n",
    "\n",
    "# --- 5. Phase 2: Evaluate on the HELD-OUT Test Set ---\n",
    "print(\"\\nPhase 2: Evaluating on the unseen test set...\")\n",
    "alphas = np.linspace(0, 1, 51)\n",
    "roc_points = []\n",
    "\n",
    "# Get the model's scores on the completely separate test set\n",
    "#y_test_scores = final_model.predict_proba(X_test)[:, 1]\n",
    "y_test_scores = final_model.decision_function(X_test)\n",
    "\n",
    "# SAFETY CHECK: Align Units (Logits vs Probabilities)\n",
    "# If Calibration scores are Probs [0,1] but Test scores are Logits [-inf, inf], \n",
    "# we must convert Calibration scores to Logits.\n",
    "if y_calib_scores.min() >= 0 and y_calib_scores.max() <= 1.0:\n",
    "    print(\"⚠️ Mismatch Detected: Converting calibration probabilities to logits...\")\n",
    "    eps = 1e-15\n",
    "    y_calib_scores = np.clip(y_calib_scores, eps, 1 - eps)\n",
    "    y_calib_scores = scipy.special.logit(y_calib_scores)\n",
    "\n",
    "# The sign must be consistent between calibration and testing\n",
    "if initial_sign:\n",
    "    y_calib_scores = -y_calib_scores\n",
    "    y_test_scores = -y_test_scores\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    # Use the CALIBRATION data to find the optimal cutoff for this alpha\n",
    "    core_result = npc_instance.npc_core(y_calib_labels, y_calib_scores, alpha, 0.05, 1)\n",
    "    if not core_result or core_result[6]: continue\n",
    "    \n",
    "    cutoff = core_result[0]\n",
    "    \n",
    "    # Apply that cutoff to the TEST data scores to get final predictions\n",
    "    y_pred_test = (y_test_scores >= cutoff).astype(int)\n",
    "    \n",
    "    # Calculate performance on the TEST data\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "    current_fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    current_tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    roc_points.append({'fpr': current_fpr, 'tpr': current_tpr})\n",
    "    print(f\"Alpha {alpha:.2f} ({i+1}/{len(alphas)}): FPR={current_fpr:.3f}, TPR={current_tpr:.3f}\")\n",
    "\n",
    "# --- 4. Process and Plot the Results ---\n",
    "# Remove duplicate points\n",
    "unique_points_dict = {(p['fpr'], p['tpr']): p for p in roc_points}\n",
    "constrained_points = list(unique_points_dict.values())\n",
    "constrained_points = sorted(constrained_points, key=lambda x: x['fpr'])\n",
    "\n",
    "# ensure 0,0 and 1,1 are included\n",
    "if constrained_points[0]['fpr'] > 0:\n",
    "    constrained_points.insert(0, {'fpr': 0.0, 'tpr': 0.0})\n",
    "if constrained_points[-1]['fpr'] < 1:\n",
    "    constrained_points.append({'fpr': 1.0, 'tpr': 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccedd7b",
   "metadata": {},
   "source": [
    "\n",
    "## Store NP curve pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from predict_ensemble_and_evaluate_paper import save_to_pickle_constrained_roc\n",
    "\n",
    "# Save the constrained ROC curve results\n",
    "save_to_pickle_constrained_roc(constrained_points, filename='pickle/NN_pneumoniaMNIST_NP_roc_curve.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4a755",
   "metadata": {},
   "source": [
    "## Load NP curve pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "794916d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained ROC curve points loaded from pickle/NN_pneumoniaMNIST_NP_roc_curve.pkl\n"
     ]
    }
   ],
   "source": [
    "from predict_ensemble_and_evaluate_paper import load_from_pickle_constrained_roc\n",
    "\n",
    "# Load the constrained ROC curve results\n",
    "constrained_points = load_from_pickle_constrained_roc(filename='pickle/NN_pneumoniaMNIST_NP_roc_curve.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3173afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plotting all ROC curves for comparison ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "#results_lists.extend(list_folds_best_models)\n",
    "#results_lists.extend(list_folds_weighted_clfs)\n",
    "results_lists.append(constrained_points)\n",
    "results_lists.append(ensemble_results_hard)\n",
    "results_lists.append(list_full_weighted_clfs)\n",
    "#results_lists.append(misclassification_risk)\n",
    "\n",
    "\n",
    "# --- Plot the final comparison ---\n",
    "print(\"\\n--- Plotting all ROC curves for comparison ---\")\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    #names=[\"MaxROCFold 1\", \"MaxROCFold 2\", \"MaxROCFold 3\", \"MaxROCFold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\", \"Neyman_Pearson\", \"Ensemble_voting_hard\", \"Misclassification_Risk\"],\n",
    "    names=[\"Neyman_Pearson\", \"Ensemble_voting_hard\", \"Weighted Classifiers\"],\n",
    "    results_original_roc=results_original_roc, plot_name=\"NN_weighted_PneumoniaMNIST_bootstrap\", prior_prob=prior_proba, misclassification_risk=[misclassification_risk_orig, misclassification_risk_half_bootstrap]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3712cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plotting all ROC curves for comparison ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "results_lists.extend(list_folds_best_models)\n",
    "results_lists.extend(list_folds_weighted_clfs)\n",
    "#results_lists.append(constrained_points)\n",
    "#results_lists.append(ensemble_results_hard)\n",
    "#results_lists.append(misclassification_risk)\n",
    "\n",
    "\n",
    "# --- Plot the final comparison ---\n",
    "print(\"\\n--- Plotting all ROC curves for comparison ---\")\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    #names=[\"MaxROCFold 1\", \"MaxROCFold 2\", \"MaxROCFold 3\", \"MaxROCFold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\", \"Neyman_Pearson\", \"Ensemble_voting_hard\", \"Misclassification_Risk\"],\n",
    "    names=[\"MaxROCFold 1\", \"MaxROCFold 2\", \"MaxROCFold 3\", \"MaxROCFold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\"],\n",
    "    results_original_roc=results_original_roc, plot_name=\"NN_weighted_intermediate_pneumoniaMNIST_bootstrap\", prior_prob=prior_proba, misclassification_risk=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caef9ac",
   "metadata": {},
   "source": [
    "\n",
    "### Tabulate the misclassification risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e78d0ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassification Risk Comparison NN pneumoniaMNIST bootstrap:\n",
      "+-----------------------------+------------------------+--------+--------+--------+\n",
      "| Method                      | Misclassification Risk | F1     | FPR    | TPR    |\n",
      "+-----------------------------+------------------------+--------+--------+--------+\n",
      "| Original Model              | 0.1442                 | 0.8916 | 0.2991 | 0.9487 |\n",
      "+-----------------------------+------------------------+--------+--------+--------+\n",
      "| Expert Ensemble (Bootstrap) | 0.1522                 | 0.8907 | 0.3932 | 0.9923 |\n",
      "+-----------------------------+------------------------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import termtables as tt\n",
    "\n",
    "table_data = [\n",
    "    [\"Method\", \"Misclassification Risk\", \"F1\", \"FPR\", \"TPR\"],\n",
    "    [\"Original Model\", f\"{misclassification_risk_orig['risk']:.4f}\", f\"{misclassification_risk_orig['f1']:.4f}\", f\"{misclassification_risk_orig['fpr']:.4f}\", f\"{misclassification_risk_orig['tpr']:.4f}\"],\n",
    "    #[\"Expert Ensemble (CV)\", f\"{misclassification_risk_half_CV['risk']:.4f}\", f\"{misclassification_risk_half_CV['f1']:.4f}\", f\"{misclassification_risk_half_CV['fpr']:.4f}\", f\"{misclassification_risk_half_CV['tpr']:.4f}\"],\n",
    "    [\"Expert Ensemble (Bootstrap)\", f\"{misclassification_risk_half_bootstrap['risk']:.4f}\", f\"{misclassification_risk_half_bootstrap['f1']:.4f}\", f\"{misclassification_risk_half_bootstrap['fpr']:.4f}\", f\"{misclassification_risk_half_bootstrap['tpr']:.4f}\"],\n",
    "]\n",
    "\n",
    "headers = table_data[0]\n",
    "rows = table_data[1:]\n",
    "\n",
    "print(\"\\nMisclassification Risk Comparison NN pneumoniaMNIST bootstrap:\")\n",
    "print(tt.to_string(rows, header=headers, style=tt.styles.ascii_thin))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLlabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
