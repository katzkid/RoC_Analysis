{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4da4f6",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0556d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "from predict_ensemble_and_evaluate_paper import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate_paper import plot_roc_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d840881",
   "metadata": {},
   "source": [
    "## Display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c24af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for displaying ROC curves of multiple classifiers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def plot_roc_connected_points(results_list, results_original_roc, name=\"All Classifiers\"):\n",
    "    \"\"\"\n",
    "    Creates a plot showing the performance of all classifiers as points,\n",
    "    and connects them with a line, sorted by FPR.\n",
    "\n",
    "    Args:\n",
    "        results_list: A list of dictionaries, where each dict contains 'fpr', 'tpr'.\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # Convert results to a pandas DataFrame for easy sorting\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Sort the models by their False Positive Rate to create a left-to-right line\n",
    "    df_sorted = df.sort_values(by='fpr').reset_index(drop=True)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Plot all the individual model points as a scatter plot\n",
    "    # This helps see the actual performance of each model\n",
    "    plt.scatter(df_sorted['fpr'], df_sorted['tpr'], c='black', marker='o', \n",
    "                alpha=0.6, s=100, zorder=3) # zorder=3 puts dots on top\n",
    "\n",
    "    # Plot the line connecting the points\n",
    "    plt.plot(df_sorted['fpr'], df_sorted['tpr'], color='red', lw=1.5,\n",
    "             alpha=0.8, label=f'{name} (AUC = {auc(df_sorted[\"fpr\"], df_sorted[\"tpr\"]):.2f})')\n",
    "    \n",
    "    #plot the original ROC curve\n",
    "    plt.plot(results_original_roc[\"fpr\"], results_original_roc[\"tpr\"], color='blue', \n",
    "             label=f'{results_original_roc[\"name\"]} (AUC = {results_original_roc[\"auc\"]:.2f})')\n",
    "\n",
    "    # Plot the \"no-skill\" line for reference\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Performance Path of All Trained Classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23b1ea",
   "metadata": {},
   "source": [
    "## Best ROC envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e350c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def get_best_roc_envelope(results_list, results_original_roc=None):\n",
    "    \"\"\"\n",
    "    Constructs the upper envelope of multiple ROC curves to find the best\n",
    "    possible operating points from a collection of classifiers.\n",
    "\n",
    "    This function is guaranteed to produce a list of points that starts at\n",
    "    (FPR=0, TPR=0) and ends at (FPR=1, TPR=1), making it suitable for direct\n",
    "    plotting and AUC calculation.\n",
    "\n",
    "    Args:\n",
    "        results_list (list): A list of dictionaries. Each dictionary must\n",
    "                             contain a 'model' name and the full ROC results\n",
    "                             under the 'full_roc' key.\n",
    "        results_original_roc (dict, optional): A dictionary containing the\n",
    "                                               ROC results of a baseline model,\n",
    "                                               which will also be included in\n",
    "                                               the comparison. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of dictionaries representing the optimal ROC\n",
    "              envelope. Each dictionary includes 'model', 'fpr', 'tpr',\n",
    "              and 'threshold'.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "\n",
    "    # Step 1: Gather all ROC points from all provided models\n",
    "    all_results = results_list[:]\n",
    "    if results_original_roc:\n",
    "        all_results.append({\n",
    "            'model': 'original_baseline',\n",
    "            'full_roc': results_original_roc\n",
    "        })\n",
    "\n",
    "    for result in all_results:\n",
    "        model_name = result.get('model', 'Unknown Model')\n",
    "        roc = result.get('full_roc')\n",
    "        if not roc: continue\n",
    "\n",
    "        fprs = roc.get('fpr', [])\n",
    "        tprs = roc.get('tpr', [])\n",
    "        thresholds = roc.get('thresholds', [])\n",
    "\n",
    "        for i in range(len(fprs)):\n",
    "            threshold = thresholds[i] if i < len(thresholds) else thresholds[-1]\n",
    "            all_points.append({\n",
    "                'model': model_name, 'fpr': fprs[i],\n",
    "                'tpr': tprs[i], 'threshold': threshold\n",
    "            })\n",
    "\n",
    "    # Handle case with no data by returning a default diagonal line\n",
    "    if not all_points:\n",
    "        return [\n",
    "            {'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf},\n",
    "            {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}\n",
    "        ]\n",
    "\n",
    "    # Step 2: Sort all points by FPR (asc), then by TPR (desc)\n",
    "    all_points.sort(key=lambda p: (p['fpr'], -p['tpr']))\n",
    "\n",
    "    # Step 3: Build the core ROC envelope\n",
    "    best_roc_envelope = []\n",
    "    max_tpr_so_far = -1.0\n",
    "    for point in all_points:\n",
    "        # Add a point only if it has a strictly higher TPR\n",
    "        if point['tpr'] > max_tpr_so_far:\n",
    "            # To prevent adding multiple points at the same FPR, check if the last point\n",
    "            # has the same FPR and replace it if so (since this one has a higher TPR)\n",
    "            if best_roc_envelope and best_roc_envelope[-1]['fpr'] == point['fpr']:\n",
    "                best_roc_envelope[-1] = point\n",
    "            else:\n",
    "                best_roc_envelope.append(point)\n",
    "            max_tpr_so_far = point['tpr']\n",
    "    \n",
    "    # Step 4: Manually add extremities if they are missing\n",
    "    \n",
    "    # Ensure the curve starts at (0, 0)\n",
    "    if not best_roc_envelope or best_roc_envelope[0]['fpr'] > 0.0:\n",
    "        best_roc_envelope.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if best_roc_envelope[-1]['fpr'] < 1.0 or best_roc_envelope[-1]['tpr'] < 1.0:\n",
    "        best_roc_envelope.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "    return best_roc_envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b781e9",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c3deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(dataset):\n",
    "    if dataset == \"data1\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [0, 2], [2, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "    elif dataset == \"data2\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [1, 2], [1, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    elif dataset == \"pneumoniaMNIST\":\n",
    "        import medmnist\n",
    "        print(medmnist.__version__)\n",
    "        # read in the PneumoniaMNIST dataset\n",
    "        from medmnist import PneumoniaMNIST\n",
    "        train_data = PneumoniaMNIST(split='train', download=True, size=224)\n",
    "        test_data = PneumoniaMNIST(split='test', download=True, size=224)\n",
    "        val_data = PneumoniaMNIST(split='val', download=True, size=224)\n",
    "        # print the dataset info\n",
    "        print(f\"Number of training samples: {len(train_data)}\")\n",
    "        print(f\"Number of test samples: {len(test_data)}\")\n",
    "        print(f\"Number of validation samples: {len(val_data)}\")\n",
    "\n",
    "    return train_data, test_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfe355",
   "metadata": {},
   "source": [
    "## Calculate Statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af73be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_fpr_tpr(clf_model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the False Positive Rate (FPR) and True Positive Rate (TPR) at a given threshold.\n",
    "\n",
    "    Args:\n",
    "        X_test: The test features.\n",
    "        y_test: The true test labels (0 or 1).\n",
    "        threshold: The probability threshold.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the FPR and TPR. Returns None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_prob = clf_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  #Avoid division by zero\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0 #Avoid division by zero\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        misclassification_rate = 1 - accuracy\n",
    "\n",
    "        return {\"fpr\": fpr, \"tpr\": tpr, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy, \"misclassification_rate\": misclassification_rate}\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error calculating FPR and TPR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59034e",
   "metadata": {},
   "source": [
    "## PneumoniaMNIST: NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74133e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a060ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 1\n",
    "NUM_CHANNELS = 1\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 4\n",
    "K_FOLDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d23d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, image_height, image_width):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        #Convoluional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 224x224 -> 112x112\n",
    "        \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 112x112 -> 56x56\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 56x56 -> 28x28\n",
    "        \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 28x28 -> 14x14\n",
    "        )\n",
    "\n",
    "        # --- DYNAMIC FLATTENED SIZE CALCULATION ---\n",
    "        # Create a dummy tensor with the specified input dimensions\n",
    "        dummy_input = torch.randn(1, in_channels, image_height, image_width)\n",
    "        # Pass it through the feature extractor to see the output shape\n",
    "        dummy_output = self.features(dummy_input)\n",
    "        # The number of elements in the output tensor is our flattened size\n",
    "        self.flattened_size = dummy_output.numel()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        # Output layer: num_classes=1 for binary classification (outputting logits)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a702675",
   "metadata": {},
   "source": [
    "### Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac92eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitSimpleCNN(pl.LightningModule):\n",
    "    def __init__(self, in_channels, num_classes, learning_rate, image_height, image_width, training_mode='full_network'):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define the model\n",
    "        self.model = SimpleCNN(\n",
    "            in_channels=self.hparams.in_channels, \n",
    "            num_classes=self.hparams.num_classes,\n",
    "            image_height=self.hparams.image_height,\n",
    "            image_width=self.hparams.image_width\n",
    "        )\n",
    "        \n",
    "        # Define loss function\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Define metrics\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.val_auc = torchmetrics.AUROC(task=\"binary\")\n",
    "\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.test_auc = torchmetrics.AUROC(task=\"binary\")\n",
    "        self.test_precision = torchmetrics.Precision(task=\"binary\")\n",
    "        self.test_recall = torchmetrics.Recall(task=\"binary\")\n",
    "        self.test_f1 = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "        # This list will store outputs from each test step\n",
    "        self.last_test_results = {}\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def freeze_feature_extractor(self):\n",
    "        print(\"Freezing feature extractor layers...\")\n",
    "        for param in self.model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.fc1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        # For BCEWithLogitsLoss, labels must be float\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = self(inputs) # Forward pass\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # We need to handle which part of the network we are training\n",
    "        if self.hparams.training_mode == 'full_network':\n",
    "            self.log('train_loss_full', loss)\n",
    "        elif self.hparams.training_mode == 'classifier_only':\n",
    "            self.log('train_loss_classifier', loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.train_accuracy(outputs, labels.int())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.val_accuracy(outputs, labels.int())\n",
    "        self.val_auc(outputs, labels.int())\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('val_acc', self.val_accuracy, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('val_auc', self.val_auc, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels.float())\n",
    "        \n",
    "        # Append predictions and labels to our list\n",
    "        self.test_step_outputs.append({\"preds\": outputs.detach(), \"labels\": labels.detach()})\n",
    "        \n",
    "        # Log the loss for this batch\n",
    "        self.log('test_loss', loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Concatenate all predictions and labels from the list we built\n",
    "        all_preds = torch.cat([x['preds'] for x in self.test_step_outputs])\n",
    "        all_labels = torch.cat([x['labels'] for x in self.test_step_outputs])\n",
    "\n",
    "        # Calculate final metrics over the entire test set\n",
    "        test_acc = self.test_accuracy(all_preds, all_labels.int())\n",
    "        test_auc_val = self.test_auc(all_preds, all_labels.int())\n",
    "        test_prec = self.test_precision(all_preds, all_labels.int())\n",
    "        test_rec = self.test_recall(all_preds, all_labels.int())\n",
    "        test_f1_val = self.test_f1(all_preds, all_labels.int())\n",
    "        test_cm_val = torchmetrics.functional.confusion_matrix(all_preds, all_labels.int(), task=\"binary\")\n",
    "\n",
    "        # Log the final metrics\n",
    "        self.log(\"test_acc_epoch\", test_acc)\n",
    "        self.log(\"test_auc_epoch\", test_auc_val)\n",
    "\n",
    "        print(f\"\\n--- Final Test Metrics ---\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"AUC: {test_auc_val:.4f}\")\n",
    "        print(f\"Precision: {test_prec:.4f}\")\n",
    "        print(f\"Recall: {test_rec:.4f}\")\n",
    "        print(f\"F1-Score: {test_f1_val:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{test_cm_val}\")\n",
    "        print(\"--------------------------\\n\")\n",
    "\n",
    "        # Calculate data for the ROC Curve\n",
    "        fpr, tpr, thresholds = torchmetrics.functional.roc(\n",
    "            torch.sigmoid(all_preds),\n",
    "            all_labels.int(),\n",
    "            task=\"binary\"\n",
    "        )\n",
    "\n",
    "        # Store the results to be retrieved later in the main script\n",
    "        self.last_test_results = {\n",
    "            \"fpr\": fpr.cpu(),\n",
    "            \"tpr\": tpr.cpu(),\n",
    "            \"auc\": test_auc_val,\n",
    "            \"f1\": test_f1_val,\n",
    "            \"precision\": test_prec,\n",
    "            \"recall\": test_rec,\n",
    "            \"cm\": test_cm_val,\n",
    "            \"thresholds\": thresholds.cpu(),\n",
    "        }\n",
    "        # Free up memory\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ea652",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d785e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n",
      "Number of training samples: 4708\n",
      "Number of test samples: 624\n",
      "Number of validation samples: 524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "train_dataset, test_dataset, val_dataset = generate_data(\"pneumoniaMNIST\")\n",
    "\n",
    "# Ensure you have your train_loader and val_loader defined here\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset.transform = data_transforms\n",
    "test_dataset.transform = data_transforms\n",
    "val_dataset.transform = data_transforms\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#new_train_loader = data.DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) to be done inside the sample_ratio loop\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# initialize fold\n",
    "kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "971945fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/katzkid/miniforge3/envs/MLlabs/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/katzkid/Documents/RoC_Analysis/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training for 1 epochs on train_loader ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SimpleCNN         | 26.1 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | val_auc        | BinaryAUROC       | 0      | train\n",
      "5 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "6 | test_auc       | BinaryAUROC       | 0      | train\n",
      "7 | test_precision | BinaryPrecision   | 0      | train\n",
      "8 | test_recall    | BinaryRecall      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score     | 0      | train\n",
      "-------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.316   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 74/74 [00:05<00:00, 12.88it/s, v_num=6, train_loss_step=0.0846, train_acc_step=1.000, val_loss=0.195, val_acc=0.933, val_auc=0.977, train_loss_epoch=0.277, train_acc_epoch=0.878]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 74/74 [00:06<00:00, 10.66it/s, v_num=6, train_loss_step=0.0846, train_acc_step=1.000, val_loss=0.195, val_acc=0.933, val_auc=0.977, train_loss_epoch=0.277, train_acc_epoch=0.878]\n",
      "Best model from Phase 1 saved to: /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93-v4.ckpt\n",
      "\n",
      "--- Testing model from Phase 1 checkpoint: /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93-v4.ckpt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93-v4.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at /home/katzkid/Documents/RoC_Analysis/checkpoints/simple-cnn-full-epoch=00-val_acc=0.93-v4.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 42.15it/s]\n",
      "--- Final Test Metrics ---\n",
      "Accuracy: 0.8574\n",
      "AUC: 0.9214\n",
      "Precision: 0.8397\n",
      "Recall: 0.9538\n",
      "F1-Score: 0.8932\n",
      "Confusion Matrix:\n",
      "tensor([[163,  71],\n",
      "        [ 18, 372]], device='cuda:0')\n",
      "--------------------------\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 36.82it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.8573718070983887\n",
      "     test_auc_epoch         0.9213510751724243\n",
      "        test_loss           0.3700377643108368\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "--- Manually Calculating Metrics on Test Set ---\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import numpy as np\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "# 1. Instantiate the LightningModule\n",
    "model = LitSimpleCNN(\n",
    "    in_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    training_mode='full_network',  # Change to 'classifier_only' if you want to train only the classifier\n",
    "    image_height=IMAGE_SIZE,\n",
    "    image_width=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# Optional: Print model summary\n",
    "# You need to move the model to a device first for torchsummary to work\n",
    "# summary(model.to('cuda'), (NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE))\n",
    "# model.to('cpu') # Move it back if needed\n",
    "\n",
    "checkpoint_callback_full_model = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='simple-cnn-full-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "train_trainer_1 = pl.Trainer(\n",
    "    accelerator='gpu',                 # Use GPUs\n",
    "    devices=1,                         # Number of GPUs to use\n",
    "    #strategy='ddp_notebook',                    # DistributedDataParallel strategy (best for multi-GPU)\n",
    "    max_epochs=NUM_EPOCHS,              # Total number of epochs\n",
    "    callbacks=[checkpoint_callback_full_model, progress_bar],\n",
    "    logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"simple_cnn-full\")\n",
    ")\n",
    "\n",
    "test_trainer_1 = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,  # Use only one GPU\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"simple_cnn_test\"),\n",
    "    # Add the callback here so the Trainer can find the 'best' path\n",
    "    callbacks=[checkpoint_callback_full_model],\n",
    "    precision='16-mixed'  # Use mixed precision for testing\n",
    ")\n",
    "\n",
    "\n",
    "# --- Phase 1 Training ---\n",
    "print(f\"--- Starting Training for {NUM_EPOCHS} epochs on train_loader ---\")\n",
    "train_trainer_1.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# CRITICAL: Save the path to the best model from this phase\n",
    "path_after_phase1 = checkpoint_callback_full_model.best_model_path\n",
    "print(f\"Best model from Phase 1 saved to: {path_after_phase1}\")\n",
    "\n",
    "# --- Test 1: Evaluate the model from after Phase 1 ---\n",
    "print(f\"\\n--- Testing model from Phase 1 checkpoint: {path_after_phase1} ---\")\n",
    "model.current_test_title = \"ROC Curve after training on DataLoader 1\"\n",
    "model = LitSimpleCNN.load_from_checkpoint(path_after_phase1)  # Load the model from Phase 1\n",
    "model.eval().to('cuda:1')  # Ensure the model is in evaluation mode and on GPU\n",
    "test_trainer_1.test(model, dataloaders=test_loader, ckpt_path=path_after_phase1)\n",
    "results_phase1 = model.last_test_results.copy()\n",
    "results_original_roc = {\"fpr\": results_phase1[\"fpr\"], \"tpr\": results_phase1[\"tpr\"], \"thresholds\": results_phase1[\"thresholds\"], \"name\": \"Original NN PneumoniaMNIST\", \"auc\": results_phase1[\"auc\"], \"model\": model}\n",
    "\n",
    "# Metrics\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "final_predictions = [] # This will store binary predictions (0s or 1s)\n",
    "true_labels = []\n",
    "\n",
    "print(\"\\n--- Manually Calculating Metrics on Test Set ---\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move input data to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # 1. Get the raw model output (logits) and convert to probabilities\n",
    "        outputs = torch.sigmoid(model(inputs))\n",
    "\n",
    "        # 2. Convert probabilities to binary class predictions (0 or 1) using a 0.5 threshold\n",
    "        preds = (outputs > 0.5).int()\n",
    "\n",
    "        final_predictions.extend(preds.cpu().numpy().flatten())\n",
    "        true_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "# Ensure they are numpy arrays for sklearn\n",
    "true_labels = np.array(true_labels)\n",
    "final_predictions = np.array(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b9bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Confusion Matrix ---\n",
      "Misclassification Risk original model: {'risk': np.float64(0.14262820512820507), 'tpr': np.float64(0.9538461538461539), 'fpr': np.float64(0.3034188034188034), 'threshold': 0.625, 'f1': np.float64(0.8931572629051621)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, calculate metrics using the correct binary predictions\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "tn, fp, fn, tp = confusion_matrix(true_labels, final_predictions).ravel()\n",
    "\n",
    "#print the misclassification risk of the original model\n",
    "total_positive = 0\n",
    "total_samples = 0\n",
    "for _, labels in test_loader:\n",
    "    total_positive += labels.sum().item()\n",
    "    total_samples += labels.size(0)\n",
    "prior_proba = total_positive / total_samples\n",
    "tpr_orig = tp/(tp + fn) if(tp + fn) > 0 else 0.0\n",
    "fpr_orig = fp/(fp + tn) if(fp + tn) > 0 else 0.0\n",
    "risk = (prior_proba * (1 - tpr_orig)) + ((1 - prior_proba) * fpr_orig)\n",
    "f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n",
    "misclassification_risk_orig = {\n",
    "    \"risk\": risk,\n",
    "    \"tpr\": tpr_orig,\n",
    "    \"fpr\": fpr_orig,\n",
    "    \"threshold\": prior_proba,\n",
    "    \"f1\": f1\n",
    "}\n",
    "print(f\"Misclassification Risk original model: {misclassification_risk_orig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849b63a",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate Weighted ROC curve (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "226e70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSIFIERS = 20\n",
    "w = np.linspace(0.001, 0.999, NUM_CLASSIFIERS, endpoint=True)\n",
    "pos_weights = w/(1-w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515305ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import autocast\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#new_train_loader = data.DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) to be done inside the sample_ratio loop\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "list_weighted_clfs = []\n",
    "\n",
    "for i, pos_weight in enumerate(pos_weights):\n",
    "    model.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_acc',\n",
    "        dirpath=f'checkpoints/stage_{i+1}/',\n",
    "        filename=f'best-model-{{epoch:02d}}-{{val_acc:.2f}}',\n",
    "        save_top_k=1,\n",
    "        mode='max',\n",
    "    )\n",
    "    \n",
    "    # 3. Instantiate a NEW Trainer for this specific stage\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        #strategy='ddp_notebook',\n",
    "        max_epochs=NUM_EPOCHS, # Number of epochs for THIS stage\n",
    "        callbacks=[checkpoint_callback, progress_bar],\n",
    "        logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=f\"stage_{i+1}_ratio_{pos_weight}\"),\n",
    "        precision='16-mixed'\n",
    "        )\n",
    "    \n",
    "    # 4. Train the model. It will start with weights from the previous stage.\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    \n",
    "    # 5. Get the path to the best model from THIS stage and store it\n",
    "    if checkpoint_callback.best_model_path:\n",
    "        best_path_this_stage = checkpoint_callback.best_model_path\n",
    "        print(f\"--- Stage {i+1} complete. Best model path: {best_path_this_stage} ---\")\n",
    "        best_model_paths.append(best_path_this_stage)\n",
    "        \n",
    "        # 6. CRITICAL: Load the best weights back into the model object\n",
    "        # This ensures we carry forward the BEST model to the next stage, not the last.\n",
    "        print(f\"Loading best weights from {best_path_this_stage} to continue...\")\n",
    "        model = LitSimpleCNN.load_from_checkpoint(best_path_this_stage, strict=False)  # Use strict=False to ignore missing keys if any\n",
    "    else:\n",
    "        print(f\"--- Stage {i+1} complete. No checkpoint was saved. ---\")\n",
    "        # If no checkpoint was saved, the model object will just have the weights from the last epoch.\n",
    "\n",
    "# 7. Test the model after each stage\n",
    "# Loop through each saved model checkpoint\n",
    "for i, checkpoint_path in enumerate(best_model_paths):\n",
    "    print(f\"\\n--- Testing model from checkpoint: {checkpoint_path} ---\")\n",
    "\n",
    "    # 1. Load the PyTorch model from the checkpoint\n",
    "    pytorch_model = LitSimpleCNN.load_from_checkpoint(checkpoint_path, strict=False)\n",
    "    pytorch_model.eval()  # Set model to evaluation mode\n",
    "    pytorch_model.to('cuda:1') # Move model to GPU\n",
    "\n",
    "    # --- Generate Predictions for the ENTIRE test set ---\n",
    "    # We will collect the raw model outputs (logits) and true labels\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Wrap the loop in torch.no_grad() for efficiency\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Predicting with model {i+1}\"):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Move inputs to GPU\n",
    "            inputs = inputs.to(device, non_blocking=True) # non_blocking=True helps speed up transfer\n",
    "            \n",
    "            # Use Mixed Precision to save memory\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                logits = pytorch_model(inputs)\n",
    "                \n",
    "            # Move back to CPU immediately to keep GPU memory free\n",
    "            # .detach() ensures no hidden gradients are tracked\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_labels.append(labels.detach().cpu())\n",
    "\n",
    "    # Concatenate all batch results into single tensors\n",
    "    # These now contain the predictions and labels for the full test set\n",
    "    full_dataset_logits = torch.cat(all_logits)\n",
    "    full_dataset_labels = torch.cat(all_labels).int() # Ensure labels are integers\n",
    "\n",
    "    # --- Now, Calculate ALL Metrics using the generated predictions ---\n",
    "\n",
    "    # 2. Calculate the full ROC curve data\n",
    "    # Use the raw logits, torchmetrics will handle applying all thresholds\n",
    "    array_of_all_fprs, array_of_all_tprs, threshold_vals = torchmetrics.functional.roc(\n",
    "        preds=full_dataset_logits,\n",
    "        target=full_dataset_labels,\n",
    "        task=\"binary\"\n",
    "    )\n",
    "\n",
    "    # 3. Calculate confusion matrix based on a hard threshold (e.g., 0.5)\n",
    "    # Note: The model outputs logits, so the threshold 0.5 for probabilities corresponds to 0.0 for logits.\n",
    "    hard_preds = (full_dataset_logits > 0.0).int().numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(full_dataset_labels.numpy(), hard_preds).ravel()\n",
    "\n",
    "    # 4. Calculate metrics from the confusion matrix\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # This is also Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "    \n",
    "    print(f\"Results at threshold 0.5: TPR={tpr:.4f}, FPR={fpr:.4f}, F1-Score={f1:.4f}\")\n",
    "    \n",
    "    # 5. Store the comprehensive results for this model\n",
    "    list_weighted_clfs.append({\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"threshold\": 0.5,\n",
    "        \"full_roc\": {\n",
    "            \"fpr\": array_of_all_fprs,\n",
    "            \"tpr\": array_of_all_tprs,\n",
    "            \"thresholds\": threshold_vals\n",
    "        }\n",
    "    })\n",
    "list_full_weighted_clfs = list_weighted_clfs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d529b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained ROC curve points saved to pickle/medMNIST_weighted_full_weighted_roc.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save to pickle\n",
    "from predict_ensemble_and_evaluate_paper import save_to_pickle_full_weighted_roc\n",
    "save_to_pickle_full_weighted_roc(list_full_weighted_clfs, filename='pickle/medMNIST_weighted_full_weighted_roc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7976e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load from pickle\n",
    "from predict_ensemble_and_evaluate_paper import load_from_pickle_full_weighted_roc\n",
    "list_full_weighted_clfs = load_from_pickle_full_weighted_roc(filename='pickle/medMNIST_weighted_full_weighted_roc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c2d0a",
   "metadata": {},
   "source": [
    "### Weighted ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import numpy as np\n",
    "\n",
    "# --- Place any constants or unchanging objects outside the loop ---\n",
    "pl.seed_everything(42, workers=True)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "\n",
    "#store the weighted classifiers of all the folds\n",
    "list_folds_weighted_clfs = []  # List to store classifiers from all folds\n",
    "#store the best classfiers of all the folds\n",
    "list_folds_best_models = []  # List to store best models from all folds\n",
    "\n",
    "# Dictionary to store the history of each fold\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# 3. K-fold cross-validation loop\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "    list_weighted_clfs = [] # List to store the models trained at each stage\n",
    "    best_clfs = [] # List to store the best models from each stage\n",
    "    best_model_paths = [] # List to store the path of the best model from each stage\n",
    "\n",
    "    # 1. Instantiate the LightningModule\n",
    "    model = LitSimpleCNN(\n",
    "        in_channels=NUM_CHANNELS,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        training_mode='full_network',  # Change to 'classifier_only' if you want to train only the classifier\n",
    "        image_height=IMAGE_SIZE,\n",
    "        image_width=IMAGE_SIZE\n",
    "    )\n",
    "    print(f\"--- Starting Fold {fold + 1}/{K_FOLDS} ---\")\n",
    "\n",
    "    # 4. Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "                      train_dataset,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      sampler=train_subsampler,\n",
    "                      num_workers=NUM_WORKERS)\n",
    "    fold_loader = data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sampler=val_subsampler,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "\n",
    "    for i, pos_weight in enumerate(pos_weights):\n",
    "        model.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_acc',\n",
    "            dirpath=f'checkpoints/stage_{i+1}/',\n",
    "            filename=f'best-model-{{epoch:02d}}-{{val_acc:.2f}}',\n",
    "            save_top_k=1,\n",
    "            mode='max',\n",
    "        )\n",
    "        \n",
    "        # 3. Instantiate a NEW Trainer for this specific stage\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator='gpu',\n",
    "            devices=1,\n",
    "            #strategy='ddp_notebook',\n",
    "            max_epochs=NUM_EPOCHS, # Number of epochs for THIS stage\n",
    "            callbacks=[checkpoint_callback, progress_bar],\n",
    "            logger=pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=f\"stage_{i+1}_ratio_{pos_weight}\"),\n",
    "            precision='16-mixed'\n",
    "            )\n",
    "        \n",
    "        # 4. Train the model. It will start with weights from the previous stage.\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "        \n",
    "        # 5. Get the path to the best model from THIS stage and store it\n",
    "        if checkpoint_callback.best_model_path:\n",
    "            best_path_this_stage = checkpoint_callback.best_model_path\n",
    "            print(f\"--- Stage {i+1} complete. Best model path: {best_path_this_stage} ---\")\n",
    "            best_model_paths.append(best_path_this_stage)\n",
    "            \n",
    "            # 6. CRITICAL: Load the best weights back into the model object\n",
    "            # This ensures we carry forward the BEST model to the next stage, not the last.\n",
    "            print(f\"Loading best weights from {best_path_this_stage} to continue...\")\n",
    "            model = LitSimpleCNN.load_from_checkpoint(best_path_this_stage, strict=False)  # Use strict=False to ignore missing keys if any\n",
    "        else:\n",
    "            print(f\"--- Stage {i+1} complete. No checkpoint was saved. ---\")\n",
    "            # If no checkpoint was saved, the model object will just have the weights from the last epoch.\n",
    "\n",
    "    # 7. Test the model after each stage\n",
    "    # Loop through each saved model checkpoint\n",
    "    for i, checkpoint_path in enumerate(best_model_paths):\n",
    "        print(f\"\\n--- Testing model from checkpoint: {checkpoint_path} ---\")\n",
    "\n",
    "        # 1. Load the PyTorch model from the checkpoint\n",
    "        pytorch_model = LitSimpleCNN.load_from_checkpoint(checkpoint_path, strict=False)\n",
    "        pytorch_model.eval()  # Set model to evaluation mode\n",
    "        pytorch_model.to('cuda:1') # Move model to GPU\n",
    "\n",
    "        # --- Generate Predictions for the ENTIRE test set ---\n",
    "        # We will collect the raw model outputs (logits) and true labels\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Wrap the loop in torch.no_grad() for efficiency\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(fold_loader, desc=f\"Predicting with model {i+1}\"):\n",
    "                inputs, labels = batch\n",
    "                \n",
    "                # Move data to the GPU\n",
    "                inputs = inputs.to('cuda:1')\n",
    "                \n",
    "                # Get model output (raw logits) for the batch\n",
    "                logits = pytorch_model(inputs)\n",
    "                \n",
    "                # Append batch results to lists (move back to CPU to prevent GPU memory buildup)\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        # Concatenate all batch results into single tensors\n",
    "        # These now contain the predictions and labels for the full test set\n",
    "        full_dataset_logits = torch.cat(all_logits)\n",
    "        full_dataset_labels = torch.cat(all_labels).int() # Ensure labels are integers\n",
    "\n",
    "        # --- Now, Calculate ALL Metrics using the generated predictions ---\n",
    "\n",
    "        # 2. Calculate the full ROC curve data\n",
    "        # Use the raw logits, torchmetrics will handle applying all thresholds\n",
    "        array_of_all_fprs, array_of_all_tprs, threshold_vals = torchmetrics.functional.roc(\n",
    "            preds=full_dataset_logits,\n",
    "            target=full_dataset_labels,\n",
    "            task=\"binary\"\n",
    "        )\n",
    "\n",
    "        # 3. Calculate confusion matrix based on a hard threshold (e.g., 0.5)\n",
    "        # Note: The model outputs logits, so the threshold 0.5 for probabilities corresponds to 0.0 for logits.\n",
    "        hard_preds = (full_dataset_logits > 0.0).int().numpy()\n",
    "        tn, fp, fn, tp = confusion_matrix(full_dataset_labels.numpy(), hard_preds).ravel()\n",
    "\n",
    "        # 4. Calculate metrics from the confusion matrix\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # This is also Recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "        \n",
    "        print(f\"Results at threshold 0.5: TPR={tpr:.4f}, FPR={fpr:.4f}, F1-Score={f1:.4f}\")\n",
    "        \n",
    "        # 5. Store the comprehensive results for this model\n",
    "        list_weighted_clfs.append({\n",
    "            \"fpr\": fpr,\n",
    "            \"tpr\": tpr,\n",
    "            \"model\": pytorch_model, # Optional: store the model object itself\n",
    "            \"threshold\": 0.5,\n",
    "            \"full_roc\": {\n",
    "                \"fpr\": array_of_all_fprs,\n",
    "                \"tpr\": array_of_all_tprs,\n",
    "                \"thresholds\": threshold_vals\n",
    "            }\n",
    "        })\n",
    "    best_clfs = get_best_roc_envelope(list_weighted_clfs, results_original_roc)\n",
    "    list_folds_weighted_clfs.append(list_weighted_clfs)\n",
    "    list_folds_best_models.append(best_clfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f49a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_ensemble_and_evaluate_paper import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate_paper import plot_roc_comparison\n",
    "from predict_ensemble_and_evaluate_paper import make_curve_monotonic\n",
    "from predict_ensemble_and_evaluate_paper import save_to_pickle, load_from_pickle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "168ae7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to pickle/medMNIST_weighted.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_models': [[{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.8959),\n",
       "    'threshold': tensor(0.9962)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0031),\n",
       "    'tpr': tensor(0.9181),\n",
       "    'threshold': tensor(0.9893)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0062),\n",
       "    'tpr': tensor(0.9263),\n",
       "    'threshold': tensor(0.9968)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0093),\n",
       "    'tpr': tensor(0.9474),\n",
       "    'threshold': tensor(0.9653)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0124),\n",
       "    'tpr': tensor(0.9579),\n",
       "    'threshold': tensor(0.9351)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0186),\n",
       "    'tpr': tensor(0.9626),\n",
       "    'threshold': tensor(0.9893)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0248),\n",
       "    'tpr': tensor(0.9684),\n",
       "    'threshold': tensor(0.8447)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0280),\n",
       "    'tpr': tensor(0.9731),\n",
       "    'threshold': tensor(0.8055)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0311),\n",
       "    'tpr': tensor(0.9743),\n",
       "    'threshold': tensor(0.7291)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0373),\n",
       "    'tpr': tensor(0.9766),\n",
       "    'threshold': tensor(0.6680)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0435),\n",
       "    'tpr': tensor(0.9801),\n",
       "    'threshold': tensor(0.6223)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0466),\n",
       "    'tpr': tensor(0.9825),\n",
       "    'threshold': tensor(0.5767)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0497),\n",
       "    'tpr': tensor(0.9836),\n",
       "    'threshold': tensor(0.5509)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0528),\n",
       "    'tpr': tensor(0.9848),\n",
       "    'threshold': tensor(0.5242)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0590),\n",
       "    'tpr': tensor(0.9860),\n",
       "    'threshold': tensor(0.4446)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0621),\n",
       "    'tpr': tensor(0.9871),\n",
       "    'threshold': tensor(0.8282)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0652),\n",
       "    'tpr': tensor(0.9883),\n",
       "    'threshold': tensor(0.7987)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0714),\n",
       "    'tpr': tensor(0.9895),\n",
       "    'threshold': tensor(0.7156)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0776),\n",
       "    'tpr': tensor(0.9906),\n",
       "    'threshold': tensor(0.5714)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0807),\n",
       "    'tpr': tensor(0.9918),\n",
       "    'threshold': tensor(0.5252)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0839),\n",
       "    'tpr': tensor(0.9930),\n",
       "    'threshold': tensor(0.4872)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0932),\n",
       "    'tpr': tensor(0.9942),\n",
       "    'threshold': tensor(0.3953)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1025),\n",
       "    'tpr': tensor(0.9953),\n",
       "    'threshold': tensor(0.3216)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1118),\n",
       "    'tpr': tensor(0.9965),\n",
       "    'threshold': tensor(0.7186)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1522),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(3.4976e-05)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1739),\n",
       "    'tpr': tensor(0.9988),\n",
       "    'threshold': tensor(0.0308)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.3168),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0008)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.8954),\n",
       "    'threshold': tensor(0.7445)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0033),\n",
       "    'tpr': tensor(0.9264),\n",
       "    'threshold': tensor(0.9999)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0065),\n",
       "    'tpr': tensor(0.9322),\n",
       "    'threshold': tensor(0.7920)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0098),\n",
       "    'tpr': tensor(0.9540),\n",
       "    'threshold': tensor(0.9999)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0163),\n",
       "    'tpr': tensor(0.9632),\n",
       "    'threshold': tensor(0.9713)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0195),\n",
       "    'tpr': tensor(0.9667),\n",
       "    'threshold': tensor(0.6785)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0228),\n",
       "    'tpr': tensor(0.9713),\n",
       "    'threshold': tensor(0.1952)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0293),\n",
       "    'tpr': tensor(0.9759),\n",
       "    'threshold': tensor(0.9817)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0326),\n",
       "    'tpr': tensor(0.9816),\n",
       "    'threshold': tensor(0.5871)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0358),\n",
       "    'tpr': tensor(0.9839),\n",
       "    'threshold': tensor(0.5320)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0456),\n",
       "    'tpr': tensor(0.9851),\n",
       "    'threshold': tensor(0.9519)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0489),\n",
       "    'tpr': tensor(0.9874),\n",
       "    'threshold': tensor(0.9445)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0619),\n",
       "    'tpr': tensor(0.9885),\n",
       "    'threshold': tensor(0.9117)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0684),\n",
       "    'tpr': tensor(0.9908),\n",
       "    'threshold': tensor(0.2237)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0717),\n",
       "    'tpr': tensor(0.9920),\n",
       "    'threshold': tensor(0.1898)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0879),\n",
       "    'tpr': tensor(0.9931),\n",
       "    'threshold': tensor(0.1112)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1368),\n",
       "    'tpr': tensor(0.9943),\n",
       "    'threshold': tensor(0.0233)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1564),\n",
       "    'tpr': tensor(0.9954),\n",
       "    'threshold': tensor(0.0002)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1629),\n",
       "    'tpr': tensor(0.9966),\n",
       "    'threshold': tensor(0.0435)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1792),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(0.0211)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1922),\n",
       "    'tpr': tensor(0.9989),\n",
       "    'threshold': tensor(9.3917e-06)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1954),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0104)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.7109),\n",
       "    'threshold': tensor(5.0512e-05)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0036),\n",
       "    'tpr': tensor(0.8973),\n",
       "    'threshold': tensor(0.9677)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0071),\n",
       "    'tpr': tensor(0.9453),\n",
       "    'threshold': tensor(0.8899)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0107),\n",
       "    'tpr': tensor(0.9464),\n",
       "    'threshold': tensor(0.8624)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0142),\n",
       "    'tpr': tensor(0.9509),\n",
       "    'threshold': tensor(0.8457)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0178),\n",
       "    'tpr': tensor(0.9520),\n",
       "    'threshold': tensor(0.8377)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0214),\n",
       "    'tpr': tensor(0.9676),\n",
       "    'threshold': tensor(0.4480)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0249),\n",
       "    'tpr': tensor(0.9721),\n",
       "    'threshold': tensor(0.2419)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0285),\n",
       "    'tpr': tensor(0.9866),\n",
       "    'threshold': tensor(0.1562)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0391),\n",
       "    'tpr': tensor(0.9888),\n",
       "    'threshold': tensor(0.0956)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0463),\n",
       "    'tpr': tensor(0.9900),\n",
       "    'threshold': tensor(0.0772)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0534),\n",
       "    'tpr': tensor(0.9911),\n",
       "    'threshold': tensor(0.7811)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0569),\n",
       "    'tpr': tensor(0.9922),\n",
       "    'threshold': tensor(0.3512)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0676),\n",
       "    'tpr': tensor(0.9933),\n",
       "    'threshold': tensor(0.4385)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0712),\n",
       "    'tpr': tensor(0.9944),\n",
       "    'threshold': tensor(0.4095)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0783),\n",
       "    'tpr': tensor(0.9955),\n",
       "    'threshold': tensor(0.2223)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0890),\n",
       "    'tpr': tensor(0.9967),\n",
       "    'threshold': tensor(0.1113)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1530),\n",
       "    'tpr': tensor(0.9978),\n",
       "    'threshold': tensor(0.0054)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1957),\n",
       "    'tpr': tensor(0.9989),\n",
       "    'threshold': tensor(0.0017)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2420),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0079)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.),\n",
       "    'tpr': tensor(0.9084),\n",
       "    'threshold': tensor(0.8700)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0033),\n",
       "    'tpr': tensor(0.9473),\n",
       "    'threshold': tensor(0.4369)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0066),\n",
       "    'tpr': tensor(0.9679),\n",
       "    'threshold': tensor(0.4760)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0099),\n",
       "    'tpr': tensor(0.9702),\n",
       "    'threshold': tensor(0.4565)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0132),\n",
       "    'tpr': tensor(0.9771),\n",
       "    'threshold': tensor(0.3745)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0164),\n",
       "    'tpr': tensor(0.9794),\n",
       "    'threshold': tensor(0.3416)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0230),\n",
       "    'tpr': tensor(0.9817),\n",
       "    'threshold': tensor(0.2385)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0329),\n",
       "    'tpr': tensor(0.9828),\n",
       "    'threshold': tensor(0.2042)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0428),\n",
       "    'tpr': tensor(0.9851),\n",
       "    'threshold': tensor(0.1586)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0493),\n",
       "    'tpr': tensor(0.9885),\n",
       "    'threshold': tensor(0.1194)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0526),\n",
       "    'tpr': tensor(0.9897),\n",
       "    'threshold': tensor(0.1101)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0625),\n",
       "    'tpr': tensor(0.9908),\n",
       "    'threshold': tensor(0.0726)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0658),\n",
       "    'tpr': tensor(0.9920),\n",
       "    'threshold': tensor(0.0719)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0724),\n",
       "    'tpr': tensor(0.9931),\n",
       "    'threshold': tensor(0.0536)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0789),\n",
       "    'tpr': tensor(0.9943),\n",
       "    'threshold': tensor(0.0426)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.0855),\n",
       "    'tpr': tensor(0.9954),\n",
       "    'threshold': tensor(0.0332)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1086),\n",
       "    'tpr': tensor(0.9966),\n",
       "    'threshold': tensor(0.5559)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1184),\n",
       "    'tpr': tensor(0.9977),\n",
       "    'threshold': tensor(0.1695)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.1447),\n",
       "    'tpr': tensor(0.9989),\n",
       "    'threshold': tensor(0.1260)},\n",
       "   {'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'fpr': tensor(0.2336),\n",
       "    'tpr': tensor(1.),\n",
       "    'threshold': tensor(0.0423)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}]],\n",
       " 'weighted_clfs': [[{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.10292397660818714),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.6811e-01, 9.5809e-01,  ..., 1.6914e-08, 2.4539e-09,\n",
       "             2.1526e-09])}},\n",
       "   {'fpr': np.float64(0.015527950310559006),\n",
       "    'tpr': np.float64(0.8105263157894737),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9755e-01, 9.9675e-01,  ..., 2.8364e-06, 2.0510e-06,\n",
       "             8.6191e-07])}},\n",
       "   {'fpr': np.float64(0.040372670807453416),\n",
       "    'tpr': np.float64(0.9555555555555556),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9968e-01, 9.9966e-01,  ..., 6.0520e-06, 3.9669e-06,\n",
       "             7.6337e-07])}},\n",
       "   {'fpr': np.float64(0.06832298136645963),\n",
       "    'tpr': np.float64(0.9730994152046784),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0047,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.7860e-07, 1.0749e-07,\n",
       "             2.1376e-08])}},\n",
       "   {'fpr': np.float64(0.015527950310559006),\n",
       "    'tpr': np.float64(0.9064327485380117),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9998e-01,  ..., 5.8526e-09, 5.7954e-10,\n",
       "             3.9462e-10])}},\n",
       "   {'fpr': np.float64(0.09627329192546584),\n",
       "    'tpr': np.float64(0.9859649122807017),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9998e-01,  ..., 8.5523e-06, 6.4789e-06,\n",
       "             5.1900e-06])}},\n",
       "   {'fpr': np.float64(0.09937888198757763),\n",
       "    'tpr': np.float64(0.9894736842105263),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 9.9999e-01,  ..., 6.7978e-06, 4.9337e-06,\n",
       "             4.0759e-06])}},\n",
       "   {'fpr': np.float64(0.055900621118012424),\n",
       "    'tpr': np.float64(0.9812865497076023),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9999e-01,  ..., 1.0555e-08, 4.6581e-09,\n",
       "             1.0192e-09])}},\n",
       "   {'fpr': np.float64(0.08385093167701864),\n",
       "    'tpr': np.float64(0.991812865497076),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0124, 0.0124, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0217, 0.0248, 0.0280, 0.0280, 0.0280, 0.0280,\n",
       "             0.0311, 0.0311, 0.0311, 0.0311, 0.0342, 0.0342, 0.0373, 0.0373, 0.0373,\n",
       "             0.0404, 0.0435, 0.0466, 0.0466, 0.0497, 0.0528, 0.0528, 0.0528, 0.0528,\n",
       "             0.0528, 0.0559, 0.0559, 0.0559, 0.0559, 0.0590, 0.0590, 0.0590, 0.0621,\n",
       "             0.0621, 0.0621, 0.0652, 0.0652, 0.0683, 0.0714, 0.0714, 0.0745, 0.0776,\n",
       "             0.0776, 0.0807, 0.0807, 0.0839, 0.0839, 0.0870, 0.0932, 0.0932, 0.0963,\n",
       "             0.0994, 0.1025, 0.1025, 0.1056, 0.1087, 0.1118, 0.1149, 0.1180, 0.1211,\n",
       "             0.1242, 0.1242, 0.1273, 0.1304, 0.1335, 0.1366, 0.1398, 0.1429, 0.1460,\n",
       "             0.1491, 0.1522, 0.1553, 0.1584, 0.1584, 0.1615, 0.1646, 0.1677, 0.1708,\n",
       "             0.1739, 0.1739, 0.1770, 0.1801, 0.1832, 0.1863, 0.1894, 0.1925, 0.1957,\n",
       "             0.1988, 0.2019, 0.2050, 0.2081, 0.2112, 0.2143, 0.2174, 0.2205, 0.2236,\n",
       "             0.2267, 0.2298, 0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516,\n",
       "             0.2547, 0.2578, 0.2609, 0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795,\n",
       "             0.2826, 0.2857, 0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075,\n",
       "             0.3106, 0.3137, 0.3168, 0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354,\n",
       "             0.3385, 0.3416, 0.3447, 0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634,\n",
       "             0.3665, 0.3696, 0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913,\n",
       "             0.3944, 0.3975, 0.4006, 0.4037, 0.4068, 0.4099, 0.4130, 0.4161, 0.4193,\n",
       "             0.4224, 0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4410, 0.4441, 0.4472,\n",
       "             0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752,\n",
       "             0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031,\n",
       "             0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311,\n",
       "             0.5342, 0.5373, 0.5404, 0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590,\n",
       "             0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870,\n",
       "             0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6118,\n",
       "             0.6149, 0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398,\n",
       "             0.6429, 0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677,\n",
       "             0.6708, 0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957,\n",
       "             0.6988, 0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236,\n",
       "             0.7267, 0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453, 0.7484, 0.7516,\n",
       "             0.7547, 0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795,\n",
       "             0.7826, 0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075,\n",
       "             0.8106, 0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354,\n",
       "             0.8385, 0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634,\n",
       "             0.8665, 0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913,\n",
       "             0.8944, 0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193,\n",
       "             0.9224, 0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472,\n",
       "             0.9503, 0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752,\n",
       "             0.9783, 0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0058, 0.0094, 0.0129, 0.0164, 0.0211, 0.0234, 0.0316,\n",
       "             0.0398, 0.0421, 0.0480, 0.0573, 0.0608, 0.0702, 0.0760, 0.0784, 0.0807,\n",
       "             0.0830, 0.0865, 0.0889, 0.0971, 0.0994, 0.1018, 0.1029, 0.1053, 0.1076,\n",
       "             0.1123, 0.1205, 0.1287, 0.1333, 0.1357, 0.1380, 0.1415, 0.1497, 0.1556,\n",
       "             0.1602, 0.1614, 0.1673, 0.1719, 0.1754, 0.1778, 0.1789, 0.1848, 0.1883,\n",
       "             0.1895, 0.1918, 0.1930, 0.1953, 0.2035, 0.2094, 0.2117, 0.2140, 0.2187,\n",
       "             0.2269, 0.2304, 0.2327, 0.2363, 0.2386, 0.2398, 0.2433, 0.2444, 0.2456,\n",
       "             0.2480, 0.2503, 0.2538, 0.2573, 0.2608, 0.2620, 0.2655, 0.2667, 0.2690,\n",
       "             0.2725, 0.2737, 0.2749, 0.2772, 0.2807, 0.2830, 0.2854, 0.2889, 0.2912,\n",
       "             0.2947, 0.2959, 0.2982, 0.3006, 0.3053, 0.3064, 0.3111, 0.3146, 0.3181,\n",
       "             0.3205, 0.3240, 0.3275, 0.3310, 0.3333, 0.3404, 0.3415, 0.3427, 0.3439,\n",
       "             0.3462, 0.3474, 0.3497, 0.3520, 0.3532, 0.3544, 0.3567, 0.3626, 0.3637,\n",
       "             0.3661, 0.3673, 0.3684, 0.3708, 0.3719, 0.3731, 0.3743, 0.3766, 0.3789,\n",
       "             0.3825, 0.3836, 0.3848, 0.3860, 0.3871, 0.3883, 0.3895, 0.3918, 0.3930,\n",
       "             0.3953, 0.3965, 0.3988, 0.4000, 0.4047, 0.4058, 0.4082, 0.4094, 0.4105,\n",
       "             0.4117, 0.4129, 0.4140, 0.4152, 0.4175, 0.4211, 0.4222, 0.4234, 0.4246,\n",
       "             0.4257, 0.4281, 0.4292, 0.4304, 0.4327, 0.4351, 0.4363, 0.4374, 0.4398,\n",
       "             0.4409, 0.4421, 0.4456, 0.4468, 0.4480, 0.4491, 0.4503, 0.4526, 0.4550,\n",
       "             0.4561, 0.4573, 0.4585, 0.4608, 0.4620, 0.4632, 0.4667, 0.4678, 0.4702,\n",
       "             0.4713, 0.4737, 0.4749, 0.4760, 0.4784, 0.4795, 0.4807, 0.4819, 0.4830,\n",
       "             0.4854, 0.4877, 0.4889, 0.4912, 0.4924, 0.4936, 0.4947, 0.4959, 0.4982,\n",
       "             0.5006, 0.5018, 0.5029, 0.5053, 0.5064, 0.5076, 0.5099, 0.5111, 0.5135,\n",
       "             0.5146, 0.5170, 0.5193, 0.5205, 0.5216, 0.5240, 0.5251, 0.5263, 0.5275,\n",
       "             0.5287, 0.5298, 0.5310, 0.5322, 0.5333, 0.5345, 0.5357, 0.5380, 0.5392,\n",
       "             0.5404, 0.5415, 0.5427, 0.5439, 0.5450, 0.5474, 0.5485, 0.5497, 0.5509,\n",
       "             0.5520, 0.5532, 0.5556, 0.5567, 0.5579, 0.5591, 0.5602, 0.5614, 0.5649,\n",
       "             0.5661, 0.5684, 0.5696, 0.5708, 0.5731, 0.5754, 0.5766, 0.5778, 0.5789,\n",
       "             0.5801, 0.5813, 0.5825, 0.5836, 0.5848, 0.5871, 0.5883, 0.5895, 0.5906,\n",
       "             0.5918, 0.5930, 0.5942, 0.5965, 0.5977, 0.5988, 0.6023, 0.6035, 0.6047,\n",
       "             0.6058, 0.6070, 0.6094, 0.6105, 0.6117, 0.6129, 0.6152, 0.6164, 0.6175,\n",
       "             0.6187, 0.6199, 0.6211, 0.6222, 0.6234, 0.6246, 0.6257, 0.6269, 0.6281,\n",
       "             0.6292, 0.6304, 0.6316, 0.6327, 0.6339, 0.6351, 0.6363, 0.6374, 0.6386,\n",
       "             0.6398, 0.6409, 0.6421, 0.6433, 0.6444, 0.6456, 0.6468, 0.6491, 0.6503,\n",
       "             0.6515, 0.6526, 0.6538, 0.6550, 0.6561, 0.6573, 0.6585, 0.6596, 0.6608,\n",
       "             0.6620, 0.6632, 0.6643, 0.6655, 0.6667, 0.6678, 0.6690, 0.6702, 0.6713,\n",
       "             0.6725, 0.6737, 0.6772, 0.6784, 0.6795, 0.6807, 0.6819, 0.6830, 0.6842,\n",
       "             0.6854, 0.6889, 0.6901, 0.6912, 0.6924, 0.6936, 0.6947, 0.6971, 0.6982,\n",
       "             0.6994, 0.7006, 0.7018, 0.7029, 0.7041, 0.7053, 0.7064, 0.7076, 0.7088,\n",
       "             0.7088, 0.7099, 0.7111, 0.7123, 0.7135, 0.7146, 0.7158, 0.7170, 0.7181,\n",
       "             0.7193, 0.7205, 0.7216, 0.7228, 0.7240, 0.7251, 0.7263, 0.7275, 0.7287,\n",
       "             0.7298, 0.7310, 0.7333, 0.7345, 0.7357, 0.7368, 0.7380, 0.7392, 0.7404,\n",
       "             0.7415, 0.7427, 0.7439, 0.7450, 0.7474, 0.7485, 0.7497, 0.7509, 0.7520,\n",
       "             0.7532, 0.7544, 0.7556, 0.7567, 0.7579, 0.7591, 0.7602, 0.7614, 0.7626,\n",
       "             0.7637, 0.7649, 0.7661, 0.7673, 0.7684, 0.7696, 0.7708, 0.7719, 0.7731,\n",
       "             0.7754, 0.7766, 0.7778, 0.7789, 0.7801, 0.7813, 0.7825, 0.7836, 0.7848,\n",
       "             0.7860, 0.7871, 0.7883, 0.7895, 0.7906, 0.7918, 0.7930, 0.7942, 0.7953,\n",
       "             0.7965, 0.7977, 0.7988, 0.8000, 0.8012, 0.8023, 0.8035, 0.8047, 0.8058,\n",
       "             0.8070, 0.8082, 0.8094, 0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164,\n",
       "             0.8175, 0.8187, 0.8199, 0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269,\n",
       "             0.8281, 0.8292, 0.8304, 0.8316, 0.8327, 0.8339, 0.8351, 0.8363, 0.8374,\n",
       "             0.8386, 0.8398, 0.8409, 0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480,\n",
       "             0.8480, 0.8491, 0.8503, 0.8515, 0.8526, 0.8538, 0.8550, 0.8561, 0.8573,\n",
       "             0.8585, 0.8596, 0.8608, 0.8620, 0.8632, 0.8643, 0.8655, 0.8667, 0.8678,\n",
       "             0.8690, 0.8702, 0.8713, 0.8725, 0.8737, 0.8749, 0.8760, 0.8772, 0.8784,\n",
       "             0.8795, 0.8807, 0.8819, 0.8830, 0.8842, 0.8854, 0.8865, 0.8877, 0.8889,\n",
       "             0.8901, 0.8912, 0.8924, 0.8936, 0.8947, 0.8959, 0.8971, 0.8982, 0.8994,\n",
       "             0.9006, 0.9018, 0.9029, 0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099,\n",
       "             0.9111, 0.9123, 0.9135, 0.9146, 0.9158, 0.9170, 0.9181, 0.9193, 0.9205,\n",
       "             0.9216, 0.9228, 0.9240, 0.9251, 0.9263, 0.9275, 0.9287, 0.9298, 0.9310,\n",
       "             0.9322, 0.9333, 0.9333, 0.9345, 0.9345, 0.9357, 0.9368, 0.9380, 0.9392,\n",
       "             0.9404, 0.9415, 0.9427, 0.9439, 0.9450, 0.9462, 0.9474, 0.9485, 0.9497,\n",
       "             0.9509, 0.9520, 0.9520, 0.9532, 0.9544, 0.9556, 0.9567, 0.9579, 0.9591,\n",
       "             0.9602, 0.9614, 0.9626, 0.9626, 0.9626, 0.9626, 0.9637, 0.9649, 0.9661,\n",
       "             0.9661, 0.9673, 0.9684, 0.9696, 0.9696, 0.9708, 0.9708, 0.9719, 0.9731,\n",
       "             0.9731, 0.9731, 0.9731, 0.9743, 0.9743, 0.9743, 0.9754, 0.9766, 0.9778,\n",
       "             0.9789, 0.9789, 0.9801, 0.9813, 0.9825, 0.9825, 0.9836, 0.9848, 0.9848,\n",
       "             0.9860, 0.9871, 0.9871, 0.9883, 0.9883, 0.9883, 0.9895, 0.9895, 0.9895,\n",
       "             0.9906, 0.9906, 0.9918, 0.9918, 0.9930, 0.9930, 0.9930, 0.9942, 0.9942,\n",
       "             0.9942, 0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953,\n",
       "             0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01, 9.9967e-01,\n",
       "             9.9966e-01, 9.9965e-01, 9.9963e-01, 9.9961e-01, 9.9961e-01, 9.9961e-01,\n",
       "             9.9960e-01, 9.9960e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9957e-01, 9.9956e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9954e-01,\n",
       "             9.9954e-01, 9.9953e-01, 9.9952e-01, 9.9952e-01, 9.9952e-01, 9.9951e-01,\n",
       "             9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01, 9.9948e-01,\n",
       "             9.9948e-01, 9.9946e-01, 9.9946e-01, 9.9945e-01, 9.9943e-01, 9.9943e-01,\n",
       "             9.9943e-01, 9.9943e-01, 9.9942e-01, 9.9939e-01, 9.9938e-01, 9.9937e-01,\n",
       "             9.9936e-01, 9.9936e-01, 9.9934e-01, 9.9932e-01, 9.9932e-01, 9.9931e-01,\n",
       "             9.9931e-01, 9.9929e-01, 9.9928e-01, 9.9928e-01, 9.9925e-01, 9.9917e-01,\n",
       "             9.9914e-01, 9.9914e-01, 9.9913e-01, 9.9911e-01, 9.9906e-01, 9.9901e-01,\n",
       "             9.9900e-01, 9.9897e-01, 9.9894e-01, 9.9893e-01, 9.9891e-01, 9.9889e-01,\n",
       "             9.9888e-01, 9.9883e-01, 9.9882e-01, 9.9872e-01, 9.9872e-01, 9.9870e-01,\n",
       "             9.9860e-01, 9.9824e-01, 9.9822e-01, 9.9820e-01, 9.9805e-01, 9.9802e-01,\n",
       "             9.9792e-01, 9.9790e-01, 9.9780e-01, 9.9766e-01, 9.9758e-01, 9.9746e-01,\n",
       "             9.9729e-01, 9.9701e-01, 9.9700e-01, 9.9672e-01, 9.9670e-01, 9.9613e-01,\n",
       "             9.9603e-01, 9.9540e-01, 9.9492e-01, 9.9467e-01, 9.9439e-01, 9.9397e-01,\n",
       "             9.9350e-01, 9.9248e-01, 9.9241e-01, 9.9206e-01, 9.9174e-01, 9.9108e-01,\n",
       "             9.9106e-01, 9.9051e-01, 9.8933e-01, 9.8887e-01, 9.8814e-01, 9.8645e-01,\n",
       "             9.8637e-01, 9.8569e-01, 9.8377e-01, 9.8373e-01, 9.8175e-01, 9.8056e-01,\n",
       "             9.7799e-01, 9.7518e-01, 9.7357e-01, 9.7264e-01, 9.7199e-01, 9.7151e-01,\n",
       "             9.7097e-01, 9.7040e-01, 9.5185e-01, 9.5025e-01, 9.4346e-01, 9.3734e-01,\n",
       "             9.2660e-01, 9.1663e-01, 9.1640e-01, 9.1344e-01, 9.1174e-01, 9.0249e-01,\n",
       "             8.8831e-01, 8.8627e-01, 8.6988e-01, 8.4859e-01, 8.3528e-01, 8.3296e-01,\n",
       "             8.2852e-01, 8.2818e-01, 8.0287e-01, 7.9875e-01, 7.5960e-01, 7.3094e-01,\n",
       "             7.1562e-01, 6.3391e-01, 5.8839e-01, 5.7138e-01, 5.3788e-01, 5.2519e-01,\n",
       "             5.2425e-01, 4.8719e-01, 4.3461e-01, 4.0020e-01, 3.9526e-01, 3.9109e-01,\n",
       "             3.8767e-01, 3.2592e-01, 3.2159e-01, 3.2154e-01, 2.6625e-01, 2.5528e-01,\n",
       "             2.2931e-01, 2.0071e-01, 1.9903e-01, 1.7496e-01, 1.5961e-01, 1.5591e-01,\n",
       "             1.4948e-01, 1.3845e-01, 1.3595e-01, 1.2526e-01, 1.0610e-01, 9.8423e-02,\n",
       "             8.1208e-02, 8.1030e-02, 6.5904e-02, 6.0172e-02, 5.8927e-02, 5.5145e-02,\n",
       "             4.5247e-02, 3.7269e-02, 3.5846e-02, 3.1247e-02, 3.0751e-02, 2.7358e-02,\n",
       "             2.5782e-02, 2.4694e-02, 2.4614e-02, 2.3028e-02, 2.2704e-02, 2.1418e-02,\n",
       "             2.0628e-02, 1.8869e-02, 1.8263e-02, 1.8223e-02, 1.6607e-02, 1.6053e-02,\n",
       "             1.5109e-02, 1.4262e-02, 1.3804e-02, 1.2374e-02, 1.1946e-02, 1.1693e-02,\n",
       "             1.0677e-02, 9.5601e-03, 9.2858e-03, 8.7968e-03, 8.4620e-03, 8.1275e-03,\n",
       "             7.7085e-03, 7.3499e-03, 6.9200e-03, 6.5637e-03, 6.3062e-03, 6.2846e-03,\n",
       "             6.2455e-03, 6.0762e-03, 5.6932e-03, 5.4689e-03, 5.2184e-03, 5.2165e-03,\n",
       "             5.1562e-03, 4.9957e-03, 4.9788e-03, 4.9192e-03, 4.6402e-03, 4.5143e-03,\n",
       "             4.4777e-03, 4.3175e-03, 4.1082e-03, 3.6142e-03, 3.1572e-03, 2.9143e-03,\n",
       "             2.7688e-03, 2.7195e-03, 2.6845e-03, 2.4225e-03, 2.3721e-03, 2.3678e-03,\n",
       "             2.3524e-03, 2.3011e-03, 2.2247e-03, 2.1328e-03, 2.0225e-03, 2.0197e-03,\n",
       "             1.9784e-03, 1.9359e-03, 1.4415e-03, 1.4337e-03, 1.4089e-03, 1.3272e-03,\n",
       "             1.3084e-03, 1.2895e-03, 1.2043e-03, 1.1921e-03, 1.1802e-03, 1.1441e-03,\n",
       "             1.0800e-03, 1.0533e-03, 9.8002e-04, 9.6132e-04, 9.5280e-04, 9.3237e-04,\n",
       "             8.3664e-04, 8.1528e-04, 7.9224e-04, 7.8799e-04, 7.5532e-04, 7.2397e-04,\n",
       "             7.1543e-04, 6.1752e-04, 6.0764e-04, 6.0502e-04, 5.8073e-04, 5.3572e-04,\n",
       "             5.3510e-04, 4.7376e-04, 4.6297e-04, 4.2371e-04, 4.2185e-04, 4.1816e-04,\n",
       "             4.0948e-04, 4.0509e-04, 3.8681e-04, 3.4242e-04, 3.3970e-04, 3.3284e-04,\n",
       "             3.1564e-04, 3.0515e-04, 3.0351e-04, 2.6740e-04, 2.6377e-04, 2.5922e-04,\n",
       "             1.9763e-04, 1.8684e-04, 1.8198e-04, 1.5828e-04, 1.5731e-04, 1.5682e-04,\n",
       "             1.5596e-04, 1.5421e-04, 1.5165e-04, 1.4247e-04, 1.3757e-04, 1.3208e-04,\n",
       "             1.2993e-04, 1.2945e-04, 1.2617e-04, 1.2533e-04, 1.1646e-04, 1.1359e-04,\n",
       "             1.1098e-04, 9.9745e-05, 9.3604e-05, 8.8096e-05, 8.4372e-05, 8.3564e-05,\n",
       "             8.1304e-05, 7.8911e-05, 6.9437e-05, 6.9167e-05, 6.5499e-05, 6.2762e-05,\n",
       "             6.1573e-05, 5.8674e-05, 5.5373e-05, 5.2780e-05, 5.2306e-05, 5.2086e-05,\n",
       "             5.1457e-05, 5.1276e-05, 4.8034e-05, 4.7527e-05, 4.4964e-05, 4.4163e-05,\n",
       "             4.2980e-05, 4.0618e-05, 4.0235e-05, 4.0136e-05, 3.7933e-05, 3.6996e-05,\n",
       "             3.6444e-05, 3.5089e-05, 3.5037e-05, 3.3193e-05, 3.2788e-05, 3.2289e-05,\n",
       "             2.9950e-05, 2.8664e-05, 2.7299e-05, 2.7256e-05, 2.4558e-05, 2.4210e-05,\n",
       "             2.0512e-05, 1.9745e-05, 1.9074e-05, 1.6846e-05, 1.6716e-05, 1.5659e-05,\n",
       "             1.5062e-05, 1.4790e-05, 1.4210e-05, 1.3411e-05, 1.3198e-05, 1.3136e-05,\n",
       "             1.1997e-05, 1.1664e-05, 1.1516e-05, 1.1515e-05, 1.0395e-05, 9.9239e-06,\n",
       "             9.5760e-06, 9.0351e-06, 8.9948e-06, 8.8463e-06, 8.7636e-06, 8.6112e-06,\n",
       "             8.5290e-06, 8.3978e-06, 7.9923e-06, 7.4216e-06, 7.4080e-06, 6.0595e-06,\n",
       "             5.6752e-06, 4.9410e-06, 4.7968e-06, 4.6467e-06, 4.6433e-06, 4.5249e-06,\n",
       "             4.4731e-06, 4.4555e-06, 4.4032e-06, 3.3367e-06, 3.3261e-06, 2.9426e-06,\n",
       "             2.8519e-06, 2.7308e-06, 2.6432e-06, 2.1274e-06, 2.0084e-06, 1.6573e-06,\n",
       "             1.6287e-06, 1.4047e-06, 1.3194e-06, 1.3105e-06, 1.2928e-06, 1.2821e-06,\n",
       "             1.2154e-06, 1.1541e-06, 1.1438e-06, 9.7147e-07, 9.5821e-07, 9.3931e-07,\n",
       "             8.8988e-07, 8.3880e-07, 7.9923e-07, 7.8753e-07, 7.3331e-07, 7.1067e-07,\n",
       "             7.0272e-07, 6.7246e-07, 5.0719e-07, 4.5623e-07, 4.3629e-07, 3.4729e-07,\n",
       "             1.9427e-07, 1.7614e-07, 1.7591e-07, 1.6876e-07, 1.6370e-07, 1.3587e-07,\n",
       "             1.1187e-07, 1.0920e-07, 9.9363e-08, 9.8519e-08, 9.5520e-08, 8.7987e-08,\n",
       "             6.5062e-08, 5.7512e-08, 4.0855e-08, 3.9901e-08, 3.8460e-08, 3.1499e-08,\n",
       "             2.1621e-08, 9.5133e-09, 6.1396e-09, 5.1411e-09, 1.6146e-09, 1.4212e-09,\n",
       "             1.3560e-09, 6.4432e-10])}},\n",
       "   {'fpr': np.float64(0.09006211180124224),\n",
       "    'tpr': np.float64(0.9836257309941521),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.2759e-07, 1.0988e-07,\n",
       "             8.1581e-08])}},\n",
       "   {'fpr': np.float64(0.052795031055900624),\n",
       "    'tpr': np.float64(0.9847953216374269),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9999e-01,  ..., 5.0046e-08, 4.9964e-08,\n",
       "             4.0131e-08])}},\n",
       "   {'fpr': np.float64(0.13043478260869565),\n",
       "    'tpr': np.float64(0.9964912280701754),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0217,\n",
       "             0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0280,\n",
       "             0.0280, 0.0280, 0.0280, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342,\n",
       "             0.0373, 0.0404, 0.0435, 0.0466, 0.0497, 0.0497, 0.0528, 0.0559, 0.0590,\n",
       "             0.0590, 0.0621, 0.0621, 0.0621, 0.0621, 0.0652, 0.0652, 0.0652, 0.0683,\n",
       "             0.0683, 0.0714, 0.0745, 0.0745, 0.0745, 0.0776, 0.0776, 0.0776, 0.0807,\n",
       "             0.0839, 0.0839, 0.0839, 0.0870, 0.0901, 0.0932, 0.0963, 0.0963, 0.0994,\n",
       "             0.0994, 0.1025, 0.1025, 0.1025, 0.1025, 0.1056, 0.1087, 0.1118, 0.1118,\n",
       "             0.1149, 0.1180, 0.1211, 0.1242, 0.1273, 0.1304, 0.1335, 0.1366, 0.1398,\n",
       "             0.1429, 0.1460, 0.1491, 0.1522, 0.1553, 0.1584, 0.1615, 0.1646, 0.1677,\n",
       "             0.1708, 0.1708, 0.1739, 0.1770, 0.1801, 0.1832, 0.1863, 0.1894, 0.1925,\n",
       "             0.1957, 0.1988, 0.2019, 0.2050, 0.2081, 0.2112, 0.2143, 0.2174, 0.2205,\n",
       "             0.2236, 0.2267, 0.2298, 0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484,\n",
       "             0.2516, 0.2547, 0.2578, 0.2609, 0.2640, 0.2671, 0.2671, 0.2702, 0.2733,\n",
       "             0.2764, 0.2795, 0.2826, 0.2857, 0.2888, 0.2919, 0.2950, 0.2981, 0.3012,\n",
       "             0.3043, 0.3075, 0.3106, 0.3137, 0.3168, 0.3199, 0.3230, 0.3261, 0.3292,\n",
       "             0.3323, 0.3354, 0.3385, 0.3416, 0.3447, 0.3478, 0.3509, 0.3540, 0.3571,\n",
       "             0.3602, 0.3634, 0.3665, 0.3696, 0.3727, 0.3758, 0.3789, 0.3820, 0.3851,\n",
       "             0.3882, 0.3913, 0.3944, 0.3975, 0.4006, 0.4037, 0.4068, 0.4099, 0.4130,\n",
       "             0.4161, 0.4193, 0.4224, 0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4410,\n",
       "             0.4441, 0.4472, 0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689,\n",
       "             0.4720, 0.4720, 0.4752, 0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938,\n",
       "             0.4969, 0.5000, 0.5031, 0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217,\n",
       "             0.5248, 0.5280, 0.5311, 0.5342, 0.5373, 0.5404, 0.5435, 0.5466, 0.5497,\n",
       "             0.5528, 0.5559, 0.5590, 0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776,\n",
       "             0.5807, 0.5839, 0.5870, 0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056,\n",
       "             0.6087, 0.6118, 0.6149, 0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335,\n",
       "             0.6366, 0.6398, 0.6429, 0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615,\n",
       "             0.6646, 0.6677, 0.6708, 0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894,\n",
       "             0.6925, 0.6957, 0.6988, 0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174,\n",
       "             0.7205, 0.7236, 0.7267, 0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453,\n",
       "             0.7484, 0.7516, 0.7547, 0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733,\n",
       "             0.7764, 0.7795, 0.7826, 0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012,\n",
       "             0.8043, 0.8075, 0.8106, 0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292,\n",
       "             0.8323, 0.8354, 0.8385, 0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571,\n",
       "             0.8602, 0.8634, 0.8665, 0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851,\n",
       "             0.8882, 0.8913, 0.8944, 0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130,\n",
       "             0.9161, 0.9193, 0.9224, 0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410,\n",
       "             0.9441, 0.9472, 0.9503, 0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689,\n",
       "             0.9720, 0.9752, 0.9783, 0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0702, 0.1146, 0.1520, 0.1813, 0.2035, 0.2281, 0.2456, 0.2620,\n",
       "             0.2784, 0.2865, 0.2936, 0.3053, 0.3170, 0.3275, 0.3357, 0.3439, 0.3497,\n",
       "             0.3556, 0.3602, 0.3649, 0.3684, 0.3754, 0.3848, 0.3871, 0.3953, 0.4012,\n",
       "             0.4070, 0.4117, 0.4140, 0.4164, 0.4187, 0.4222, 0.4246, 0.4292, 0.4316,\n",
       "             0.4351, 0.4398, 0.4433, 0.4491, 0.4526, 0.4561, 0.4596, 0.4632, 0.4655,\n",
       "             0.4667, 0.4713, 0.4737, 0.4772, 0.4807, 0.4854, 0.4901, 0.4936, 0.4947,\n",
       "             0.4971, 0.4982, 0.5006, 0.5018, 0.5053, 0.5076, 0.5088, 0.5111, 0.5146,\n",
       "             0.5158, 0.5181, 0.5216, 0.5228, 0.5240, 0.5251, 0.5275, 0.5287, 0.5322,\n",
       "             0.5333, 0.5357, 0.5368, 0.5380, 0.5404, 0.5415, 0.5439, 0.5450, 0.5462,\n",
       "             0.5485, 0.5544, 0.5567, 0.5591, 0.5614, 0.5637, 0.5673, 0.5696, 0.5719,\n",
       "             0.5731, 0.5754, 0.5766, 0.5778, 0.5801, 0.5813, 0.5848, 0.5871, 0.5883,\n",
       "             0.5906, 0.5930, 0.5942, 0.5965, 0.5977, 0.5988, 0.6000, 0.6035, 0.6058,\n",
       "             0.6070, 0.6094, 0.6105, 0.6117, 0.6140, 0.6164, 0.6175, 0.6187, 0.6211,\n",
       "             0.6234, 0.6246, 0.6257, 0.6281, 0.6292, 0.6316, 0.6327, 0.6351, 0.6363,\n",
       "             0.6374, 0.6409, 0.6421, 0.6444, 0.6480, 0.6491, 0.6503, 0.6515, 0.6538,\n",
       "             0.6550, 0.6561, 0.6573, 0.6596, 0.6608, 0.6620, 0.6632, 0.6643, 0.6655,\n",
       "             0.6667, 0.6690, 0.6702, 0.6713, 0.6725, 0.6737, 0.6749, 0.6772, 0.6784,\n",
       "             0.6795, 0.6807, 0.6819, 0.6842, 0.6854, 0.6865, 0.6877, 0.6901, 0.6924,\n",
       "             0.6936, 0.6947, 0.6971, 0.6982, 0.7006, 0.7018, 0.7029, 0.7041, 0.7076,\n",
       "             0.7088, 0.7111, 0.7123, 0.7135, 0.7146, 0.7158, 0.7170, 0.7181, 0.7193,\n",
       "             0.7205, 0.7216, 0.7228, 0.7251, 0.7263, 0.7287, 0.7298, 0.7310, 0.7322,\n",
       "             0.7333, 0.7345, 0.7357, 0.7368, 0.7380, 0.7392, 0.7415, 0.7427, 0.7439,\n",
       "             0.7450, 0.7462, 0.7474, 0.7485, 0.7497, 0.7509, 0.7520, 0.7532, 0.7544,\n",
       "             0.7556, 0.7567, 0.7579, 0.7591, 0.7602, 0.7614, 0.7637, 0.7637, 0.7649,\n",
       "             0.7661, 0.7673, 0.7673, 0.7684, 0.7696, 0.7708, 0.7719, 0.7731, 0.7743,\n",
       "             0.7754, 0.7766, 0.7778, 0.7789, 0.7801, 0.7813, 0.7825, 0.7836, 0.7860,\n",
       "             0.7871, 0.7883, 0.7895, 0.7906, 0.7918, 0.7930, 0.7942, 0.7953, 0.7965,\n",
       "             0.7977, 0.7988, 0.8000, 0.8000, 0.8012, 0.8023, 0.8035, 0.8047, 0.8058,\n",
       "             0.8082, 0.8094, 0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164, 0.8175,\n",
       "             0.8187, 0.8199, 0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269, 0.8281,\n",
       "             0.8292, 0.8304, 0.8316, 0.8327, 0.8339, 0.8363, 0.8374, 0.8386, 0.8409,\n",
       "             0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480, 0.8491, 0.8515, 0.8538,\n",
       "             0.8550, 0.8561, 0.8573, 0.8585, 0.8596, 0.8608, 0.8620, 0.8620, 0.8632,\n",
       "             0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702, 0.8713, 0.8725, 0.8737,\n",
       "             0.8749, 0.8760, 0.8772, 0.8772, 0.8784, 0.8795, 0.8807, 0.8819, 0.8830,\n",
       "             0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912, 0.8924, 0.8936,\n",
       "             0.8947, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029, 0.9041,\n",
       "             0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135, 0.9146,\n",
       "             0.9158, 0.9170, 0.9181, 0.9193, 0.9205, 0.9216, 0.9228, 0.9240, 0.9251,\n",
       "             0.9263, 0.9275, 0.9287, 0.9298, 0.9310, 0.9322, 0.9333, 0.9345, 0.9357,\n",
       "             0.9368, 0.9380, 0.9392, 0.9404, 0.9415, 0.9427, 0.9439, 0.9450, 0.9462,\n",
       "             0.9474, 0.9485, 0.9497, 0.9509, 0.9520, 0.9532, 0.9544, 0.9556, 0.9556,\n",
       "             0.9556, 0.9567, 0.9579, 0.9591, 0.9602, 0.9614, 0.9626, 0.9637, 0.9637,\n",
       "             0.9649, 0.9661, 0.9673, 0.9673, 0.9684, 0.9696, 0.9708, 0.9719, 0.9731,\n",
       "             0.9731, 0.9731, 0.9731, 0.9731, 0.9731, 0.9743, 0.9743, 0.9743, 0.9743,\n",
       "             0.9754, 0.9754, 0.9766, 0.9778, 0.9789, 0.9789, 0.9801, 0.9813, 0.9813,\n",
       "             0.9825, 0.9825, 0.9825, 0.9836, 0.9848, 0.9848, 0.9860, 0.9871, 0.9871,\n",
       "             0.9871, 0.9883, 0.9895, 0.9895, 0.9895, 0.9895, 0.9895, 0.9906, 0.9906,\n",
       "             0.9918, 0.9918, 0.9930, 0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9974e-01,\n",
       "             9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9967e-01, 9.9967e-01, 9.9966e-01,\n",
       "             9.9966e-01, 9.9964e-01, 9.9962e-01, 9.9962e-01, 9.9961e-01, 9.9960e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9956e-01, 9.9953e-01, 9.9953e-01, 9.9953e-01,\n",
       "             9.9953e-01, 9.9952e-01, 9.9950e-01, 9.9948e-01, 9.9948e-01, 9.9946e-01,\n",
       "             9.9944e-01, 9.9941e-01, 9.9940e-01, 9.9939e-01, 9.9937e-01, 9.9933e-01,\n",
       "             9.9933e-01, 9.9931e-01, 9.9930e-01, 9.9928e-01, 9.9925e-01, 9.9912e-01,\n",
       "             9.9911e-01, 9.9909e-01, 9.9905e-01, 9.9902e-01, 9.9901e-01, 9.9898e-01,\n",
       "             9.9897e-01, 9.9894e-01, 9.9889e-01, 9.9863e-01, 9.9851e-01, 9.9840e-01,\n",
       "             9.9808e-01, 9.9792e-01, 9.9771e-01, 9.9742e-01, 9.9734e-01, 9.9733e-01,\n",
       "             9.9722e-01, 9.9675e-01, 9.9667e-01, 9.9648e-01, 9.9648e-01, 9.9619e-01,\n",
       "             9.9521e-01, 9.9485e-01, 9.9398e-01, 9.9348e-01, 9.9331e-01, 9.9176e-01,\n",
       "             9.9127e-01, 9.9086e-01, 9.9064e-01, 9.9055e-01, 9.9045e-01, 9.9033e-01,\n",
       "             9.8991e-01, 9.8990e-01, 9.8970e-01, 9.8969e-01, 9.8872e-01, 9.8745e-01,\n",
       "             9.8418e-01, 9.8400e-01, 9.8076e-01, 9.7691e-01, 9.7610e-01, 9.7309e-01,\n",
       "             9.7008e-01, 9.6915e-01, 9.6643e-01, 9.6264e-01, 9.6111e-01, 9.6104e-01,\n",
       "             9.6030e-01, 9.5655e-01, 9.4843e-01, 9.4010e-01, 9.3323e-01, 9.2804e-01,\n",
       "             9.2103e-01, 9.1534e-01, 9.0595e-01, 9.0586e-01, 8.9822e-01, 8.2039e-01,\n",
       "             8.0339e-01, 7.9290e-01, 7.7936e-01, 7.7585e-01, 7.4083e-01, 7.2917e-01,\n",
       "             7.2477e-01, 7.1995e-01, 7.1863e-01, 7.0122e-01, 6.7885e-01, 6.4721e-01,\n",
       "             6.4587e-01, 6.0774e-01, 5.2762e-01, 4.9415e-01, 4.7062e-01, 4.6448e-01,\n",
       "             3.7526e-01, 3.7366e-01, 3.2093e-01, 2.4752e-01, 2.3327e-01, 1.9566e-01,\n",
       "             1.9548e-01, 1.9357e-01, 1.8723e-01, 1.8512e-01, 1.6990e-01, 1.6064e-01,\n",
       "             1.5320e-01, 1.4431e-01, 1.3224e-01, 1.2423e-01, 1.1806e-01, 1.0216e-01,\n",
       "             9.9304e-02, 9.4088e-02, 9.2573e-02, 9.0637e-02, 8.7438e-02, 8.0440e-02,\n",
       "             7.3338e-02, 7.1258e-02, 6.9927e-02, 6.8470e-02, 6.4102e-02, 6.3445e-02,\n",
       "             6.2413e-02, 6.1455e-02, 6.1008e-02, 5.2117e-02, 4.8495e-02, 4.8455e-02,\n",
       "             4.6697e-02, 4.0245e-02, 3.9506e-02, 3.7011e-02, 3.5734e-02, 3.2748e-02,\n",
       "             3.2424e-02, 3.2129e-02, 3.1507e-02, 3.0637e-02, 2.8133e-02, 2.5197e-02,\n",
       "             2.4834e-02, 2.3590e-02, 2.3214e-02, 2.1111e-02, 2.0012e-02, 1.8084e-02,\n",
       "             1.7801e-02, 1.5759e-02, 1.5528e-02, 1.5513e-02, 1.3313e-02, 1.3307e-02,\n",
       "             1.2056e-02, 1.1787e-02, 1.1679e-02, 1.1628e-02, 1.1269e-02, 1.1041e-02,\n",
       "             1.0838e-02, 1.0516e-02, 9.2187e-03, 8.3959e-03, 8.0289e-03, 7.9882e-03,\n",
       "             7.8948e-03, 7.4960e-03, 6.9939e-03, 6.7101e-03, 6.2653e-03, 5.9041e-03,\n",
       "             5.5621e-03, 5.2729e-03, 5.1650e-03, 4.6244e-03, 4.5796e-03, 4.4222e-03,\n",
       "             4.2776e-03, 4.2535e-03, 4.2098e-03, 4.1089e-03, 3.8131e-03, 3.4957e-03,\n",
       "             3.3570e-03, 3.0788e-03, 3.0523e-03, 2.7258e-03, 2.5992e-03, 2.5346e-03,\n",
       "             2.5240e-03, 2.4655e-03, 2.3805e-03, 2.3296e-03, 2.1767e-03, 2.0197e-03,\n",
       "             1.9827e-03, 1.8749e-03, 1.8494e-03, 1.5683e-03, 1.5622e-03, 1.4809e-03,\n",
       "             1.4247e-03, 1.2551e-03, 1.2005e-03, 1.1621e-03, 1.0572e-03, 1.0502e-03,\n",
       "             1.0436e-03, 9.7898e-04, 9.6862e-04, 9.4402e-04, 9.2912e-04, 9.0861e-04,\n",
       "             7.8332e-04, 7.5445e-04, 7.3559e-04, 6.9711e-04, 6.7643e-04, 6.6384e-04,\n",
       "             6.5225e-04, 6.3411e-04, 6.2184e-04, 5.8800e-04, 5.6021e-04, 5.0001e-04,\n",
       "             4.8860e-04, 4.5942e-04, 4.5760e-04, 4.1313e-04, 4.0806e-04, 3.9620e-04,\n",
       "             3.9514e-04, 3.9281e-04, 3.6979e-04, 3.4874e-04, 3.2872e-04, 3.2726e-04,\n",
       "             3.1227e-04, 3.0896e-04, 3.0404e-04, 2.8220e-04, 2.7405e-04, 2.4840e-04,\n",
       "             2.4734e-04, 2.3485e-04, 2.3255e-04, 2.1873e-04, 2.1552e-04, 2.0717e-04,\n",
       "             1.9784e-04, 1.8448e-04, 1.7236e-04, 1.6037e-04, 1.5910e-04, 1.5679e-04,\n",
       "             1.5334e-04, 1.4623e-04, 1.4359e-04, 1.4306e-04, 1.3501e-04, 1.2408e-04,\n",
       "             1.2212e-04, 1.2060e-04, 1.1393e-04, 1.1258e-04, 8.8704e-05, 8.5756e-05,\n",
       "             8.3212e-05, 7.6273e-05, 7.6158e-05, 7.6158e-05, 7.2116e-05, 6.9937e-05,\n",
       "             6.8471e-05, 6.8124e-05, 6.7853e-05, 6.3841e-05, 6.1188e-05, 5.0184e-05,\n",
       "             4.9545e-05, 4.8682e-05, 4.4902e-05, 4.3442e-05, 4.1302e-05, 4.0886e-05,\n",
       "             4.0363e-05, 4.0359e-05, 3.9248e-05, 3.9228e-05, 3.8600e-05, 3.6987e-05,\n",
       "             3.6651e-05, 3.1628e-05, 3.1582e-05, 2.9110e-05, 2.6033e-05, 2.5320e-05,\n",
       "             2.5135e-05, 2.4627e-05, 2.2220e-05, 2.1501e-05, 1.8835e-05, 1.8551e-05,\n",
       "             1.8473e-05, 1.8434e-05, 1.6160e-05, 1.2801e-05, 1.2449e-05, 1.2412e-05,\n",
       "             1.2310e-05, 1.2226e-05, 1.1033e-05, 1.0836e-05, 1.0087e-05, 9.7774e-06,\n",
       "             8.9031e-06, 8.8440e-06, 8.6992e-06, 8.6008e-06, 8.0304e-06, 6.8588e-06,\n",
       "             6.8440e-06, 6.2252e-06, 5.3562e-06, 4.7625e-06, 4.4356e-06, 3.9464e-06,\n",
       "             3.9299e-06, 3.8445e-06, 3.7558e-06, 3.4928e-06, 3.2070e-06, 3.1365e-06,\n",
       "             3.1219e-06, 3.0256e-06, 2.7174e-06, 2.7046e-06, 2.6634e-06, 2.5008e-06,\n",
       "             2.4971e-06, 2.3053e-06, 2.2337e-06, 2.1830e-06, 2.1561e-06, 1.5681e-06,\n",
       "             9.9449e-07, 9.6866e-07, 9.0567e-07, 8.7263e-07, 7.0394e-07, 6.2281e-07,\n",
       "             4.7540e-07, 4.5872e-07, 4.3515e-07, 3.7700e-07, 3.7243e-07, 3.0021e-07,\n",
       "             2.7268e-07, 2.0363e-07, 1.5338e-07, 1.5234e-07, 1.0633e-07, 1.0049e-07,\n",
       "             6.7709e-08, 5.6624e-08, 5.5146e-08, 4.1702e-08, 3.3875e-08, 2.9401e-08,\n",
       "             1.2102e-08, 1.1202e-08, 5.4592e-09, 4.0980e-09])}},\n",
       "   {'fpr': np.float64(0.049689440993788817),\n",
       "    'tpr': np.float64(0.9812865497076023),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0012, 0.0047,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.6908e-10, 1.1577e-10,\n",
       "             1.1370e-10])}},\n",
       "   {'fpr': np.float64(0.024844720496894408),\n",
       "    'tpr': np.float64(0.9625730994152046),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0186, 0.0186, 0.0217, 0.0217,\n",
       "             0.0217, 0.0248, 0.0248, 0.0248, 0.0248, 0.0280, 0.0280, 0.0311, 0.0311,\n",
       "             0.0342, 0.0373, 0.0404, 0.0404, 0.0404, 0.0404, 0.0404, 0.0435, 0.0435,\n",
       "             0.0435, 0.0435, 0.0435, 0.0435, 0.0466, 0.0466, 0.0466, 0.0497, 0.0497,\n",
       "             0.0528, 0.0559, 0.0559, 0.0559, 0.0621, 0.0652, 0.0652, 0.0683, 0.0714,\n",
       "             0.0745, 0.0776, 0.0807, 0.0839, 0.0870, 0.0901, 0.0901, 0.0901, 0.0932,\n",
       "             0.0932, 0.0963, 0.0994, 0.1025, 0.1025, 0.1056, 0.1056, 0.1087, 0.1118,\n",
       "             0.1118, 0.1149, 0.1180, 0.1211, 0.1242, 0.1242, 0.1242, 0.1273, 0.1304,\n",
       "             0.1335, 0.1335, 0.1366, 0.1398, 0.1429, 0.1460, 0.1491, 0.1522, 0.1553,\n",
       "             0.1584, 0.1615, 0.1646, 0.1677, 0.1708, 0.1739, 0.1739, 0.1739, 0.1770,\n",
       "             0.1801, 0.1832, 0.1863, 0.1894, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050,\n",
       "             0.2081, 0.2112, 0.2143, 0.2174, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329,\n",
       "             0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578,\n",
       "             0.2609, 0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857,\n",
       "             0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137,\n",
       "             0.3168, 0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416,\n",
       "             0.3447, 0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696,\n",
       "             0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975,\n",
       "             0.4006, 0.4037, 0.4068, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224, 0.4255,\n",
       "             0.4286, 0.4317, 0.4348, 0.4379, 0.4410, 0.4441, 0.4472, 0.4503, 0.4534,\n",
       "             0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752, 0.4783, 0.4814,\n",
       "             0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031, 0.5062, 0.5093,\n",
       "             0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311, 0.5342, 0.5373,\n",
       "             0.5404, 0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590, 0.5621, 0.5652,\n",
       "             0.5683, 0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870, 0.5901, 0.5932,\n",
       "             0.5963, 0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149, 0.6180, 0.6211,\n",
       "             0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429, 0.6460, 0.6491,\n",
       "             0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708, 0.6739, 0.6770,\n",
       "             0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988, 0.7019, 0.7050,\n",
       "             0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267, 0.7298, 0.7329,\n",
       "             0.7360, 0.7391, 0.7422, 0.7453, 0.7484, 0.7484, 0.7516, 0.7547, 0.7578,\n",
       "             0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826, 0.7857,\n",
       "             0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106, 0.8137,\n",
       "             0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385, 0.8416,\n",
       "             0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665, 0.8696,\n",
       "             0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944, 0.8975,\n",
       "             0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224, 0.9255,\n",
       "             0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503, 0.9534,\n",
       "             0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783, 0.9814,\n",
       "             0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.2199, 0.2947, 0.3322, 0.3673, 0.3813, 0.3953, 0.4070, 0.4152,\n",
       "             0.4246, 0.4327, 0.4398, 0.4515, 0.4573, 0.4632, 0.4702, 0.4795, 0.4854,\n",
       "             0.4901, 0.4959, 0.5029, 0.5123, 0.5170, 0.5193, 0.5251, 0.5287, 0.5310,\n",
       "             0.5345, 0.5392, 0.5415, 0.5427, 0.5439, 0.5474, 0.5485, 0.5497, 0.5520,\n",
       "             0.5567, 0.5579, 0.5591, 0.5614, 0.5626, 0.5684, 0.5719, 0.5731, 0.5743,\n",
       "             0.5766, 0.5801, 0.5813, 0.5825, 0.5836, 0.5871, 0.5883, 0.5895, 0.5906,\n",
       "             0.5942, 0.5953, 0.5965, 0.5977, 0.5988, 0.6000, 0.6012, 0.6023, 0.6047,\n",
       "             0.6070, 0.6082, 0.6105, 0.6117, 0.6129, 0.6140, 0.6175, 0.6187, 0.6199,\n",
       "             0.6234, 0.6246, 0.6257, 0.6281, 0.6292, 0.6304, 0.6327, 0.6339, 0.6351,\n",
       "             0.6363, 0.6386, 0.6398, 0.6409, 0.6433, 0.6444, 0.6468, 0.6480, 0.6491,\n",
       "             0.6503, 0.6515, 0.6538, 0.6550, 0.6561, 0.6585, 0.6608, 0.6632, 0.6643,\n",
       "             0.6655, 0.6678, 0.6690, 0.6702, 0.6713, 0.6737, 0.6749, 0.6760, 0.6772,\n",
       "             0.6795, 0.6807, 0.6819, 0.6830, 0.6842, 0.6854, 0.6865, 0.6889, 0.6901,\n",
       "             0.6912, 0.6924, 0.6936, 0.6947, 0.6959, 0.6971, 0.6982, 0.6994, 0.7006,\n",
       "             0.7018, 0.7041, 0.7053, 0.7064, 0.7088, 0.7099, 0.7111, 0.7123, 0.7135,\n",
       "             0.7146, 0.7158, 0.7170, 0.7181, 0.7193, 0.7216, 0.7228, 0.7240, 0.7251,\n",
       "             0.7263, 0.7275, 0.7287, 0.7298, 0.7310, 0.7322, 0.7333, 0.7345, 0.7357,\n",
       "             0.7368, 0.7380, 0.7392, 0.7404, 0.7415, 0.7427, 0.7439, 0.7450, 0.7462,\n",
       "             0.7474, 0.7485, 0.7497, 0.7509, 0.7520, 0.7532, 0.7544, 0.7556, 0.7567,\n",
       "             0.7579, 0.7591, 0.7602, 0.7614, 0.7626, 0.7637, 0.7649, 0.7661, 0.7673,\n",
       "             0.7684, 0.7696, 0.7708, 0.7719, 0.7731, 0.7743, 0.7754, 0.7766, 0.7778,\n",
       "             0.7801, 0.7813, 0.7825, 0.7836, 0.7848, 0.7860, 0.7871, 0.7883, 0.7895,\n",
       "             0.7906, 0.7918, 0.7930, 0.7942, 0.7953, 0.7965, 0.7977, 0.7988, 0.8000,\n",
       "             0.8012, 0.8023, 0.8035, 0.8047, 0.8058, 0.8070, 0.8082, 0.8082, 0.8094,\n",
       "             0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164, 0.8175, 0.8187, 0.8199,\n",
       "             0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269, 0.8281, 0.8292, 0.8304,\n",
       "             0.8316, 0.8327, 0.8339, 0.8351, 0.8363, 0.8374, 0.8386, 0.8398, 0.8409,\n",
       "             0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480, 0.8491, 0.8503, 0.8515,\n",
       "             0.8526, 0.8538, 0.8550, 0.8561, 0.8573, 0.8585, 0.8596, 0.8608, 0.8620,\n",
       "             0.8632, 0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702, 0.8713, 0.8725,\n",
       "             0.8737, 0.8749, 0.8760, 0.8772, 0.8784, 0.8795, 0.8807, 0.8819, 0.8830,\n",
       "             0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912, 0.8924, 0.8936,\n",
       "             0.8947, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029, 0.9041,\n",
       "             0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135, 0.9146,\n",
       "             0.9158, 0.9170, 0.9170, 0.9181, 0.9193, 0.9205, 0.9216, 0.9216, 0.9228,\n",
       "             0.9240, 0.9251, 0.9263, 0.9275, 0.9287, 0.9298, 0.9310, 0.9322, 0.9333,\n",
       "             0.9345, 0.9357, 0.9368, 0.9380, 0.9392, 0.9404, 0.9415, 0.9415, 0.9427,\n",
       "             0.9439, 0.9450, 0.9462, 0.9474, 0.9485, 0.9497, 0.9509, 0.9520, 0.9532,\n",
       "             0.9532, 0.9544, 0.9556, 0.9567, 0.9579, 0.9579, 0.9591, 0.9591, 0.9602,\n",
       "             0.9614, 0.9614, 0.9626, 0.9637, 0.9649, 0.9649, 0.9661, 0.9661, 0.9673,\n",
       "             0.9673, 0.9673, 0.9673, 0.9684, 0.9696, 0.9708, 0.9719, 0.9719, 0.9731,\n",
       "             0.9743, 0.9754, 0.9766, 0.9778, 0.9778, 0.9789, 0.9801, 0.9801, 0.9813,\n",
       "             0.9813, 0.9813, 0.9825, 0.9836, 0.9836, 0.9836, 0.9848, 0.9848, 0.9848,\n",
       "             0.9848, 0.9848, 0.9848, 0.9848, 0.9848, 0.9848, 0.9860, 0.9871, 0.9871,\n",
       "             0.9883, 0.9883, 0.9883, 0.9883, 0.9895, 0.9895, 0.9906, 0.9906, 0.9906,\n",
       "             0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9930, 0.9942, 0.9942, 0.9942,\n",
       "             0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953,\n",
       "             0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9965, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01, 9.9967e-01,\n",
       "             9.9966e-01, 9.9966e-01, 9.9964e-01, 9.9964e-01, 9.9964e-01, 9.9964e-01,\n",
       "             9.9963e-01, 9.9960e-01, 9.9960e-01, 9.9960e-01, 9.9958e-01, 9.9958e-01,\n",
       "             9.9956e-01, 9.9955e-01, 9.9952e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01,\n",
       "             9.9949e-01, 9.9947e-01, 9.9946e-01, 9.9944e-01, 9.9944e-01, 9.9943e-01,\n",
       "             9.9943e-01, 9.9939e-01, 9.9939e-01, 9.9938e-01, 9.9937e-01, 9.9937e-01,\n",
       "             9.9937e-01, 9.9936e-01, 9.9935e-01, 9.9932e-01, 9.9928e-01, 9.9924e-01,\n",
       "             9.9924e-01, 9.9922e-01, 9.9920e-01, 9.9917e-01, 9.9917e-01, 9.9916e-01,\n",
       "             9.9912e-01, 9.9901e-01, 9.9899e-01, 9.9899e-01, 9.9898e-01, 9.9891e-01,\n",
       "             9.9880e-01, 9.9879e-01, 9.9878e-01, 9.9874e-01, 9.9871e-01, 9.9859e-01,\n",
       "             9.9858e-01, 9.9857e-01, 9.9856e-01, 9.9855e-01, 9.9850e-01, 9.9850e-01,\n",
       "             9.9846e-01, 9.9824e-01, 9.9813e-01, 9.9804e-01, 9.9804e-01, 9.9800e-01,\n",
       "             9.9769e-01, 9.9763e-01, 9.9749e-01, 9.9695e-01, 9.9679e-01, 9.9663e-01,\n",
       "             9.9656e-01, 9.9636e-01, 9.9622e-01, 9.9602e-01, 9.9588e-01, 9.9581e-01,\n",
       "             9.9561e-01, 9.9506e-01, 9.9489e-01, 9.9485e-01, 9.9438e-01, 9.9390e-01,\n",
       "             9.9309e-01, 9.9115e-01, 9.8982e-01, 9.8945e-01, 9.8927e-01, 9.8886e-01,\n",
       "             9.8807e-01, 9.8765e-01, 9.8619e-01, 9.8519e-01, 9.8488e-01, 9.8459e-01,\n",
       "             9.8125e-01, 9.7978e-01, 9.7854e-01, 9.7459e-01, 9.7214e-01, 9.7100e-01,\n",
       "             9.5879e-01, 9.5249e-01, 9.4223e-01, 9.3883e-01, 9.2238e-01, 8.9377e-01,\n",
       "             8.8712e-01, 8.7001e-01, 8.6169e-01, 8.5482e-01, 8.5128e-01, 8.4625e-01,\n",
       "             8.3321e-01, 7.4329e-01, 7.1342e-01, 6.9087e-01, 6.8968e-01, 6.8869e-01,\n",
       "             5.9620e-01, 5.9043e-01, 5.8882e-01, 5.6096e-01, 5.4645e-01, 5.2808e-01,\n",
       "             4.9399e-01, 4.8397e-01, 4.4516e-01, 4.3210e-01, 3.8514e-01, 3.7327e-01,\n",
       "             3.6647e-01, 3.1820e-01, 2.8244e-01, 2.7387e-01, 2.5822e-01, 2.3351e-01,\n",
       "             2.1121e-01, 1.8512e-01, 1.7332e-01, 1.4859e-01, 1.4741e-01, 1.3658e-01,\n",
       "             1.3160e-01, 8.7399e-02, 8.5643e-02, 8.4207e-02, 8.3852e-02, 7.3874e-02,\n",
       "             7.1828e-02, 7.1108e-02, 6.4114e-02, 2.0555e-02, 1.7006e-02, 1.1338e-02,\n",
       "             1.0861e-02, 1.0218e-02, 1.0093e-02, 9.4810e-03, 9.2038e-03, 6.9609e-03,\n",
       "             6.8632e-03, 5.4629e-03, 4.0514e-03, 3.6236e-03, 2.7599e-03, 2.7198e-03,\n",
       "             2.4963e-03, 2.3744e-03, 2.3660e-03, 1.8366e-03, 1.5576e-03, 1.5351e-03,\n",
       "             1.0059e-03, 8.4018e-04, 5.4894e-04, 5.3065e-04, 3.4762e-04, 2.8891e-04,\n",
       "             2.0172e-04, 1.9667e-04, 1.9541e-04, 1.9154e-04, 1.4127e-04, 1.1453e-04,\n",
       "             1.1125e-04, 8.7960e-05, 6.6651e-05, 6.5621e-05, 6.3718e-05, 6.2754e-05,\n",
       "             5.6882e-05, 4.5279e-05, 4.3804e-05, 3.9136e-05, 3.8264e-05, 3.6171e-05,\n",
       "             2.9058e-05, 2.8253e-05, 2.3588e-05, 2.3295e-05, 1.7672e-05, 1.6979e-05,\n",
       "             1.6827e-05, 1.1412e-05, 1.0411e-05, 8.3529e-06, 7.9781e-06, 6.8888e-06,\n",
       "             6.8548e-06, 6.2552e-06, 5.9188e-06, 5.7607e-06, 4.5956e-06, 3.9626e-06,\n",
       "             3.9451e-06, 3.7434e-06, 3.5788e-06, 2.9749e-06, 2.6925e-06, 2.5224e-06,\n",
       "             2.5069e-06, 2.1091e-06, 1.9132e-06, 1.8314e-06, 1.6307e-06, 1.4544e-06,\n",
       "             1.3544e-06, 1.1646e-06, 1.1163e-06, 1.0402e-06, 1.0321e-06, 9.2471e-07,\n",
       "             9.0520e-07, 8.5011e-07, 7.0312e-07, 6.9550e-07, 6.8458e-07, 6.6047e-07,\n",
       "             6.5476e-07, 6.4768e-07, 5.4162e-07, 5.3646e-07, 5.0525e-07, 4.6344e-07,\n",
       "             4.5294e-07, 4.1586e-07, 4.0503e-07, 4.0189e-07, 3.7007e-07, 3.3643e-07,\n",
       "             3.0727e-07, 2.6859e-07, 2.4986e-07, 2.1514e-07, 2.1232e-07, 1.8010e-07,\n",
       "             1.7582e-07, 1.7115e-07, 1.5130e-07, 1.2453e-07, 1.2379e-07, 1.0541e-07,\n",
       "             1.0494e-07, 8.0588e-08, 8.0543e-08, 7.5449e-08, 7.1513e-08, 6.7391e-08,\n",
       "             5.9522e-08, 5.7161e-08, 4.9180e-08, 4.5189e-08, 3.8383e-08, 3.4527e-08,\n",
       "             3.1759e-08, 3.0051e-08, 2.7791e-08, 2.3861e-08, 2.2406e-08, 1.9448e-08,\n",
       "             1.8990e-08, 1.7096e-08, 1.6984e-08, 1.5201e-08, 1.5105e-08, 1.4879e-08,\n",
       "             1.3586e-08, 1.3506e-08, 1.2699e-08, 1.2144e-08, 1.1245e-08, 1.1128e-08,\n",
       "             1.1048e-08, 7.7469e-09, 7.5692e-09, 7.2357e-09, 6.8173e-09, 6.6823e-09,\n",
       "             6.2322e-09, 6.2306e-09, 6.0154e-09, 5.4115e-09, 5.3944e-09, 5.0181e-09,\n",
       "             4.5178e-09, 4.2152e-09, 4.0449e-09, 3.7003e-09, 3.4506e-09, 3.0531e-09,\n",
       "             3.0296e-09, 2.9738e-09, 2.8191e-09, 2.6937e-09, 2.6585e-09, 2.4881e-09,\n",
       "             2.2591e-09, 2.2123e-09, 2.1504e-09, 2.0526e-09, 1.9688e-09, 1.9463e-09,\n",
       "             1.9208e-09, 1.5275e-09, 1.4027e-09, 1.1968e-09, 1.1857e-09, 1.1686e-09,\n",
       "             1.0184e-09, 9.3239e-10, 9.2102e-10, 8.8411e-10, 8.3967e-10, 7.2878e-10,\n",
       "             6.8458e-10, 6.3693e-10, 5.9220e-10, 5.8782e-10, 5.8725e-10, 5.7672e-10,\n",
       "             5.3212e-10, 4.7535e-10, 4.6048e-10, 4.4851e-10, 4.4605e-10, 4.3769e-10,\n",
       "             3.9835e-10, 3.9275e-10, 3.6789e-10, 3.3907e-10, 3.1478e-10, 3.1226e-10,\n",
       "             3.0663e-10, 2.9064e-10, 2.7265e-10, 2.7194e-10, 2.6177e-10, 2.4158e-10,\n",
       "             2.3722e-10, 2.2857e-10, 2.2607e-10, 2.2434e-10, 1.9313e-10, 1.8799e-10,\n",
       "             1.8795e-10, 1.6272e-10, 1.4340e-10, 1.3400e-10, 1.3046e-10, 1.1769e-10,\n",
       "             1.1226e-10, 1.0291e-10, 9.7018e-11, 9.5419e-11, 7.4549e-11, 7.1441e-11,\n",
       "             6.2949e-11, 5.6079e-11, 5.3517e-11, 5.1983e-11, 5.1222e-11, 4.8505e-11,\n",
       "             4.6338e-11, 4.2978e-11, 4.1657e-11, 3.9208e-11, 3.7615e-11, 3.7573e-11,\n",
       "             3.6683e-11, 3.4821e-11, 3.4361e-11, 3.2329e-11, 3.1474e-11, 3.0893e-11,\n",
       "             2.9129e-11, 2.8532e-11, 2.7166e-11, 2.5653e-11, 2.1989e-11, 1.9322e-11,\n",
       "             1.8670e-11, 1.8464e-11, 1.7880e-11, 1.7356e-11, 1.7063e-11, 1.7011e-11,\n",
       "             1.4461e-11, 1.4104e-11, 1.4041e-11, 1.2905e-11, 1.2447e-11, 1.2200e-11,\n",
       "             1.0124e-11, 9.8870e-12, 9.4026e-12, 7.8529e-12, 7.0735e-12, 7.0571e-12,\n",
       "             6.5709e-12, 5.9698e-12, 5.8775e-12, 5.4859e-12, 4.7793e-12, 4.7061e-12,\n",
       "             3.9108e-12, 3.1147e-12, 2.7307e-12, 2.6464e-12, 2.5216e-12, 2.2176e-12,\n",
       "             2.1963e-12, 1.8792e-12, 1.7441e-12, 1.7247e-12, 1.4578e-12, 1.4025e-12,\n",
       "             1.0591e-12, 9.7671e-13, 8.4407e-13, 5.1422e-13, 4.5371e-13, 3.8772e-13,\n",
       "             3.8517e-13, 3.4303e-13, 3.3784e-13, 3.2623e-13, 2.4517e-13, 2.0895e-13,\n",
       "             1.9274e-13, 1.0803e-13, 7.8687e-14, 6.1254e-14, 5.7251e-14, 5.3346e-14,\n",
       "             3.7853e-14, 2.8975e-14, 1.9319e-14, 1.9047e-14, 1.3243e-14, 8.9379e-15,\n",
       "             6.0585e-15, 2.9088e-15, 1.3383e-15, 1.3226e-15, 1.3158e-15, 1.0706e-15,\n",
       "             9.1079e-16, 6.4738e-16, 2.7909e-16])}},\n",
       "   {'fpr': np.float64(0.037267080745341616),\n",
       "    'tpr': np.float64(0.9695906432748538),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0186, 0.0186, 0.0217, 0.0217, 0.0217, 0.0217,\n",
       "             0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0280,\n",
       "             0.0280, 0.0280, 0.0280, 0.0280, 0.0311, 0.0342, 0.0342, 0.0342, 0.0342,\n",
       "             0.0342, 0.0373, 0.0373, 0.0373, 0.0373, 0.0404, 0.0435, 0.0435, 0.0466,\n",
       "             0.0466, 0.0497, 0.0528, 0.0528, 0.0528, 0.0528, 0.0528, 0.0528, 0.0528,\n",
       "             0.0559, 0.0559, 0.0559, 0.0590, 0.0621, 0.0652, 0.0652, 0.0652, 0.0683,\n",
       "             0.0745, 0.0776, 0.0776, 0.0776, 0.0776, 0.0807, 0.0839, 0.0839, 0.0870,\n",
       "             0.0870, 0.0870, 0.0901, 0.0932, 0.0932, 0.0932, 0.0932, 0.0963, 0.0994,\n",
       "             0.1025, 0.1056, 0.1087, 0.1087, 0.1118, 0.1149, 0.1180, 0.1211, 0.1242,\n",
       "             0.1242, 0.1273, 0.1304, 0.1335, 0.1366, 0.1398, 0.1429, 0.1460, 0.1491,\n",
       "             0.1522, 0.1522, 0.1553, 0.1584, 0.1615, 0.1646, 0.1677, 0.1708, 0.1739,\n",
       "             0.1770, 0.1801, 0.1832, 0.1863, 0.1894, 0.1925, 0.1957, 0.1988, 0.2019,\n",
       "             0.2050, 0.2081, 0.2112, 0.2143, 0.2174, 0.2205, 0.2236, 0.2267, 0.2298,\n",
       "             0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578,\n",
       "             0.2609, 0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857,\n",
       "             0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137,\n",
       "             0.3168, 0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416,\n",
       "             0.3447, 0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696,\n",
       "             0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975,\n",
       "             0.4006, 0.4037, 0.4068, 0.4099, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224,\n",
       "             0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4379, 0.4410, 0.4441, 0.4472,\n",
       "             0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752,\n",
       "             0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031,\n",
       "             0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311,\n",
       "             0.5342, 0.5373, 0.5404, 0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590,\n",
       "             0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870,\n",
       "             0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149,\n",
       "             0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429,\n",
       "             0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708,\n",
       "             0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988,\n",
       "             0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267,\n",
       "             0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453, 0.7484, 0.7516, 0.7547,\n",
       "             0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826,\n",
       "             0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106,\n",
       "             0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385,\n",
       "             0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665,\n",
       "             0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944,\n",
       "             0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224,\n",
       "             0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503,\n",
       "             0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783,\n",
       "             0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.1287, 0.1731, 0.2058, 0.2327, 0.2491, 0.2538, 0.2678, 0.2795,\n",
       "             0.2924, 0.3029, 0.3111, 0.3146, 0.3193, 0.3275, 0.3322, 0.3415, 0.3497,\n",
       "             0.3567, 0.3591, 0.3637, 0.3661, 0.3719, 0.3766, 0.3801, 0.3813, 0.3860,\n",
       "             0.3918, 0.3930, 0.3942, 0.3977, 0.4012, 0.4023, 0.4047, 0.4094, 0.4140,\n",
       "             0.4164, 0.4175, 0.4187, 0.4211, 0.4234, 0.4257, 0.4269, 0.4316, 0.4327,\n",
       "             0.4339, 0.4374, 0.4386, 0.4409, 0.4444, 0.4468, 0.4491, 0.4538, 0.4550,\n",
       "             0.4585, 0.4596, 0.4667, 0.4702, 0.4713, 0.4737, 0.4749, 0.4760, 0.4784,\n",
       "             0.4795, 0.4807, 0.4819, 0.4842, 0.4854, 0.4865, 0.4877, 0.4889, 0.4912,\n",
       "             0.4924, 0.4982, 0.4994, 0.5006, 0.5018, 0.5041, 0.5053, 0.5064, 0.5076,\n",
       "             0.5088, 0.5099, 0.5111, 0.5135, 0.5158, 0.5170, 0.5181, 0.5193, 0.5205,\n",
       "             0.5216, 0.5228, 0.5240, 0.5251, 0.5287, 0.5298, 0.5310, 0.5322, 0.5333,\n",
       "             0.5380, 0.5404, 0.5415, 0.5450, 0.5462, 0.5474, 0.5485, 0.5497, 0.5509,\n",
       "             0.5520, 0.5532, 0.5544, 0.5567, 0.5579, 0.5591, 0.5602, 0.5614, 0.5626,\n",
       "             0.5637, 0.5649, 0.5661, 0.5673, 0.5684, 0.5696, 0.5708, 0.5731, 0.5754,\n",
       "             0.5766, 0.5789, 0.5801, 0.5813, 0.5825, 0.5836, 0.5848, 0.5871, 0.5883,\n",
       "             0.5895, 0.5906, 0.5918, 0.5930, 0.5942, 0.5953, 0.5965, 0.5988, 0.6000,\n",
       "             0.6012, 0.6023, 0.6035, 0.6058, 0.6070, 0.6082, 0.6094, 0.6105, 0.6117,\n",
       "             0.6129, 0.6140, 0.6152, 0.6164, 0.6175, 0.6187, 0.6199, 0.6211, 0.6222,\n",
       "             0.6234, 0.6246, 0.6269, 0.6281, 0.6292, 0.6304, 0.6316, 0.6327, 0.6339,\n",
       "             0.6351, 0.6363, 0.6374, 0.6386, 0.6398, 0.6409, 0.6421, 0.6444, 0.6456,\n",
       "             0.6468, 0.6480, 0.6503, 0.6515, 0.6526, 0.6538, 0.6561, 0.6573, 0.6585,\n",
       "             0.6608, 0.6620, 0.6632, 0.6643, 0.6655, 0.6667, 0.6678, 0.6690, 0.6702,\n",
       "             0.6713, 0.6725, 0.6737, 0.6749, 0.6760, 0.6772, 0.6784, 0.6795, 0.6807,\n",
       "             0.6819, 0.6830, 0.6842, 0.6854, 0.6865, 0.6877, 0.6901, 0.6912, 0.6924,\n",
       "             0.6936, 0.6947, 0.6959, 0.6971, 0.6982, 0.6994, 0.7018, 0.7029, 0.7041,\n",
       "             0.7053, 0.7064, 0.7076, 0.7088, 0.7099, 0.7111, 0.7123, 0.7135, 0.7146,\n",
       "             0.7158, 0.7170, 0.7181, 0.7193, 0.7205, 0.7216, 0.7228, 0.7240, 0.7251,\n",
       "             0.7263, 0.7275, 0.7287, 0.7298, 0.7310, 0.7322, 0.7333, 0.7345, 0.7357,\n",
       "             0.7368, 0.7380, 0.7392, 0.7404, 0.7415, 0.7427, 0.7439, 0.7450, 0.7462,\n",
       "             0.7474, 0.7485, 0.7497, 0.7509, 0.7520, 0.7532, 0.7544, 0.7556, 0.7567,\n",
       "             0.7579, 0.7591, 0.7602, 0.7614, 0.7626, 0.7637, 0.7649, 0.7661, 0.7673,\n",
       "             0.7684, 0.7696, 0.7708, 0.7719, 0.7731, 0.7743, 0.7754, 0.7766, 0.7778,\n",
       "             0.7789, 0.7801, 0.7813, 0.7825, 0.7836, 0.7848, 0.7860, 0.7871, 0.7883,\n",
       "             0.7895, 0.7906, 0.7918, 0.7930, 0.7942, 0.7953, 0.7965, 0.7977, 0.7988,\n",
       "             0.8000, 0.8012, 0.8023, 0.8035, 0.8047, 0.8058, 0.8070, 0.8082, 0.8094,\n",
       "             0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164, 0.8175, 0.8187, 0.8199,\n",
       "             0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269, 0.8281, 0.8292, 0.8304,\n",
       "             0.8316, 0.8327, 0.8339, 0.8351, 0.8363, 0.8374, 0.8386, 0.8398, 0.8409,\n",
       "             0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480, 0.8491, 0.8503, 0.8515,\n",
       "             0.8526, 0.8538, 0.8550, 0.8561, 0.8573, 0.8585, 0.8596, 0.8608, 0.8620,\n",
       "             0.8632, 0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702, 0.8713, 0.8725,\n",
       "             0.8737, 0.8749, 0.8760, 0.8772, 0.8784, 0.8795, 0.8807, 0.8819, 0.8830,\n",
       "             0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912, 0.8924, 0.8936,\n",
       "             0.8947, 0.8959, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029,\n",
       "             0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135,\n",
       "             0.9146, 0.9158, 0.9170, 0.9181, 0.9181, 0.9193, 0.9205, 0.9216, 0.9228,\n",
       "             0.9240, 0.9251, 0.9251, 0.9263, 0.9275, 0.9287, 0.9298, 0.9298, 0.9310,\n",
       "             0.9322, 0.9333, 0.9345, 0.9357, 0.9368, 0.9380, 0.9392, 0.9404, 0.9404,\n",
       "             0.9415, 0.9427, 0.9439, 0.9439, 0.9450, 0.9450, 0.9462, 0.9474, 0.9485,\n",
       "             0.9485, 0.9497, 0.9509, 0.9520, 0.9532, 0.9544, 0.9556, 0.9567, 0.9567,\n",
       "             0.9579, 0.9591, 0.9602, 0.9614, 0.9614, 0.9614, 0.9626, 0.9637, 0.9649,\n",
       "             0.9661, 0.9661, 0.9673, 0.9684, 0.9696, 0.9696, 0.9696, 0.9708, 0.9708,\n",
       "             0.9719, 0.9719, 0.9719, 0.9731, 0.9743, 0.9754, 0.9766, 0.9778, 0.9789,\n",
       "             0.9789, 0.9801, 0.9813, 0.9813, 0.9813, 0.9813, 0.9825, 0.9836, 0.9836,\n",
       "             0.9836, 0.9836, 0.9848, 0.9860, 0.9871, 0.9871, 0.9871, 0.9883, 0.9883,\n",
       "             0.9895, 0.9906, 0.9906, 0.9906, 0.9918, 0.9930, 0.9942, 0.9942, 0.9942,\n",
       "             0.9942, 0.9942, 0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01,\n",
       "             9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9967e-01, 9.9967e-01, 9.9967e-01, 9.9967e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9964e-01, 9.9964e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9952e-01,\n",
       "             9.9952e-01, 9.9949e-01, 9.9946e-01, 9.9946e-01, 9.9943e-01, 9.9937e-01,\n",
       "             9.9937e-01, 9.9937e-01, 9.9936e-01, 9.9935e-01, 9.9934e-01, 9.9933e-01,\n",
       "             9.9933e-01, 9.9933e-01, 9.9932e-01, 9.9931e-01, 9.9929e-01, 9.9929e-01,\n",
       "             9.9929e-01, 9.9928e-01, 9.9925e-01, 9.9925e-01, 9.9924e-01, 9.9924e-01,\n",
       "             9.9924e-01, 9.9914e-01, 9.9912e-01, 9.9911e-01, 9.9911e-01, 9.9911e-01,\n",
       "             9.9910e-01, 9.9909e-01, 9.9907e-01, 9.9904e-01, 9.9904e-01, 9.9903e-01,\n",
       "             9.9901e-01, 9.9901e-01, 9.9900e-01, 9.9899e-01, 9.9899e-01, 9.9894e-01,\n",
       "             9.9892e-01, 9.9891e-01, 9.9890e-01, 9.9881e-01, 9.9880e-01, 9.9876e-01,\n",
       "             9.9871e-01, 9.9869e-01, 9.9868e-01, 9.9862e-01, 9.9859e-01, 9.9856e-01,\n",
       "             9.9851e-01, 9.9836e-01, 9.9834e-01, 9.9833e-01, 9.9813e-01, 9.9812e-01,\n",
       "             9.9780e-01, 9.9767e-01, 9.9749e-01, 9.9739e-01, 9.9736e-01, 9.9730e-01,\n",
       "             9.9703e-01, 9.9684e-01, 9.9644e-01, 9.9637e-01, 9.9621e-01, 9.9565e-01,\n",
       "             9.9563e-01, 9.9562e-01, 9.9523e-01, 9.9495e-01, 9.9488e-01, 9.9476e-01,\n",
       "             9.9455e-01, 9.9445e-01, 9.9358e-01, 9.9334e-01, 9.9305e-01, 9.9272e-01,\n",
       "             9.9205e-01, 9.9194e-01, 9.9193e-01, 9.9129e-01, 9.9055e-01, 9.9043e-01,\n",
       "             9.8934e-01, 9.8932e-01, 9.8752e-01, 9.8735e-01, 9.8663e-01, 9.8521e-01,\n",
       "             9.8365e-01, 9.8323e-01, 9.8297e-01, 9.8224e-01, 9.8165e-01, 9.7888e-01,\n",
       "             9.7733e-01, 9.7472e-01, 9.7057e-01, 9.6692e-01, 9.6571e-01, 9.6566e-01,\n",
       "             9.5949e-01, 9.5941e-01, 9.5627e-01, 9.5432e-01, 9.5408e-01, 9.5180e-01,\n",
       "             9.5133e-01, 9.5075e-01, 9.5029e-01, 9.4750e-01, 9.4481e-01, 9.4332e-01,\n",
       "             9.3577e-01, 9.0903e-01, 8.7078e-01, 8.6963e-01, 8.6572e-01, 8.5339e-01,\n",
       "             8.4431e-01, 8.2653e-01, 8.1046e-01, 8.0885e-01, 7.9484e-01, 7.3398e-01,\n",
       "             7.3351e-01, 7.1080e-01, 6.9687e-01, 6.9647e-01, 6.7831e-01, 6.6799e-01,\n",
       "             6.6536e-01, 6.6258e-01, 6.4771e-01, 6.2360e-01, 5.7386e-01, 5.7329e-01,\n",
       "             5.4786e-01, 5.1237e-01, 3.8948e-01, 3.2945e-01, 2.8302e-01, 2.6444e-01,\n",
       "             2.5163e-01, 1.9449e-01, 1.6960e-01, 1.2610e-01, 1.2307e-01, 1.2240e-01,\n",
       "             8.2377e-02, 7.9617e-02, 6.7875e-02, 5.8376e-02, 5.1055e-02, 4.5639e-02,\n",
       "             4.1239e-02, 3.9828e-02, 1.9723e-02, 1.3784e-02, 1.0338e-02, 9.9810e-03,\n",
       "             9.2076e-03, 6.5902e-03, 6.1816e-03, 4.7033e-03, 4.6474e-03, 3.2582e-03,\n",
       "             2.4727e-03, 2.0522e-03, 2.0069e-03, 1.8530e-03, 1.1253e-03, 1.0133e-03,\n",
       "             9.5740e-04, 6.5166e-04, 5.1912e-04, 4.8577e-04, 4.0967e-04, 3.5222e-04,\n",
       "             3.1056e-04, 2.8880e-04, 2.6389e-04, 2.3162e-04, 2.1592e-04, 2.0790e-04,\n",
       "             2.0458e-04, 1.5475e-04, 1.4757e-04, 1.3237e-04, 1.2220e-04, 9.3050e-05,\n",
       "             9.1224e-05, 7.7155e-05, 7.0172e-05, 4.5904e-05, 4.3770e-05, 3.8270e-05,\n",
       "             3.6115e-05, 3.4976e-05, 3.3970e-05, 2.7419e-05, 2.2394e-05, 1.6358e-05,\n",
       "             1.4966e-05, 1.4054e-05, 1.3395e-05, 1.3171e-05, 1.0469e-05, 9.5442e-06,\n",
       "             9.4850e-06, 9.0082e-06, 8.3090e-06, 8.0027e-06, 7.4962e-06, 5.6397e-06,\n",
       "             5.5363e-06, 4.3459e-06, 3.7830e-06, 2.3001e-06, 1.5313e-06, 1.4935e-06,\n",
       "             1.4604e-06, 1.4375e-06, 1.2601e-06, 8.5394e-07, 7.3993e-07, 6.8337e-07,\n",
       "             6.6283e-07, 6.0589e-07, 5.8810e-07, 4.9761e-07, 4.9267e-07, 4.8517e-07,\n",
       "             3.9040e-07, 3.7533e-07, 3.5296e-07, 3.4059e-07, 3.2411e-07, 3.2369e-07,\n",
       "             3.0366e-07, 2.9381e-07, 2.6069e-07, 2.4354e-07, 2.3653e-07, 2.2307e-07,\n",
       "             2.1999e-07, 1.6717e-07, 1.5718e-07, 1.4810e-07, 1.3560e-07, 1.3421e-07,\n",
       "             1.1395e-07, 1.1381e-07, 1.0138e-07, 9.2308e-08, 8.8958e-08, 8.0152e-08,\n",
       "             7.3872e-08, 6.8589e-08, 6.0627e-08, 5.7493e-08, 5.1736e-08, 5.1356e-08,\n",
       "             5.0864e-08, 4.9848e-08, 4.1111e-08, 3.2431e-08, 2.2115e-08, 1.9664e-08,\n",
       "             1.6653e-08, 1.3618e-08, 1.1900e-08, 1.0596e-08, 1.0197e-08, 9.8132e-09,\n",
       "             9.6105e-09, 9.2149e-09, 8.0859e-09, 7.1598e-09, 6.8433e-09, 5.9391e-09,\n",
       "             5.3274e-09, 5.2962e-09, 5.1820e-09, 5.1539e-09, 4.6638e-09, 4.5897e-09,\n",
       "             4.5209e-09, 4.4792e-09, 4.2905e-09, 3.5812e-09, 3.2694e-09, 2.5325e-09,\n",
       "             2.3983e-09, 2.3811e-09, 2.1005e-09, 1.9757e-09, 1.8304e-09, 1.7900e-09,\n",
       "             1.7558e-09, 1.7115e-09, 1.2674e-09, 1.1803e-09, 1.1766e-09, 1.0806e-09,\n",
       "             1.0448e-09, 1.0038e-09, 9.8270e-10, 9.6163e-10, 9.2558e-10, 8.6843e-10,\n",
       "             8.4920e-10, 8.2498e-10, 7.7608e-10, 7.6404e-10, 7.5202e-10, 7.3177e-10,\n",
       "             6.6063e-10, 6.2674e-10, 4.8089e-10, 4.8007e-10, 3.8527e-10, 3.2701e-10,\n",
       "             3.1737e-10, 2.9158e-10, 2.7978e-10, 2.6864e-10, 2.4453e-10, 2.4122e-10,\n",
       "             2.3882e-10, 2.1868e-10, 2.0962e-10, 2.0672e-10, 2.0015e-10, 1.9351e-10,\n",
       "             1.9244e-10, 1.6379e-10, 1.6071e-10, 1.4852e-10, 1.3720e-10, 1.3323e-10,\n",
       "             1.1854e-10, 1.0142e-10, 9.5962e-11, 9.3767e-11, 9.2219e-11, 9.1830e-11,\n",
       "             8.2490e-11, 7.2973e-11, 6.4405e-11, 5.9965e-11, 5.8740e-11, 5.7661e-11,\n",
       "             5.4622e-11, 5.1939e-11, 4.7126e-11, 4.6983e-11, 4.6002e-11, 4.5326e-11,\n",
       "             4.1469e-11, 3.2582e-11, 2.7774e-11, 2.5735e-11, 2.1363e-11, 2.0753e-11,\n",
       "             2.0711e-11, 1.9563e-11, 1.8510e-11, 1.7377e-11, 1.7109e-11, 1.5576e-11,\n",
       "             1.4144e-11, 1.2709e-11, 1.1771e-11, 9.9888e-12, 9.8758e-12, 9.6564e-12,\n",
       "             9.3433e-12, 8.7429e-12, 8.5015e-12, 8.2870e-12, 7.7341e-12, 7.6414e-12,\n",
       "             7.5857e-12, 7.0144e-12, 6.8053e-12, 6.2786e-12, 6.2763e-12, 5.9503e-12,\n",
       "             5.5646e-12, 4.5211e-12, 4.5112e-12, 3.9688e-12, 3.6616e-12, 3.3388e-12,\n",
       "             3.2903e-12, 2.7866e-12, 2.5032e-12, 2.4417e-12, 2.4114e-12, 2.1940e-12,\n",
       "             1.8920e-12, 1.7893e-12, 1.7364e-12, 1.5391e-12, 1.4701e-12, 1.2509e-12,\n",
       "             1.2185e-12, 1.1767e-12, 1.1299e-12, 1.1120e-12, 9.9484e-13, 9.3755e-13,\n",
       "             8.7498e-13, 8.4463e-13, 7.0548e-13, 6.9472e-13, 6.5608e-13, 4.0157e-13,\n",
       "             3.3892e-13, 3.2158e-13, 2.3159e-13, 2.0639e-13, 1.6023e-13, 9.8482e-14,\n",
       "             9.4109e-14, 8.6920e-14, 7.7514e-14, 5.7854e-14, 5.6908e-14, 5.1140e-14,\n",
       "             4.3229e-14, 3.9362e-14, 3.7480e-14, 3.6012e-14, 3.5393e-14, 3.4375e-14,\n",
       "             3.1893e-14, 3.1764e-14, 3.0686e-14, 2.8250e-14, 2.4819e-14, 2.2841e-14,\n",
       "             2.1064e-14, 1.5246e-14, 1.1748e-14, 1.1396e-14, 1.0010e-14, 6.5199e-15,\n",
       "             6.4978e-15, 6.0702e-15, 6.0684e-15, 4.4946e-15, 4.3267e-15, 3.5316e-15,\n",
       "             2.8273e-15, 2.1040e-15, 1.9740e-15, 1.6929e-15, 1.4184e-15, 1.1182e-15,\n",
       "             8.3385e-16, 6.4369e-16, 4.6935e-16, 3.5618e-16, 1.9582e-16, 1.9325e-16,\n",
       "             1.3851e-16, 8.2571e-17, 6.9200e-17, 2.7894e-17, 1.3862e-17, 1.1048e-17,\n",
       "             2.5355e-18])}},\n",
       "   {'fpr': np.float64(0.09006211180124224),\n",
       "    'tpr': np.float64(0.9906432748538012),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155, 0.0155, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0217, 0.0248, 0.0248, 0.0248,\n",
       "             0.0248, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280,\n",
       "             0.0280, 0.0280, 0.0280, 0.0311, 0.0311, 0.0311, 0.0311, 0.0342, 0.0342,\n",
       "             0.0342, 0.0342, 0.0373, 0.0373, 0.0404, 0.0435, 0.0435, 0.0466, 0.0466,\n",
       "             0.0466, 0.0497, 0.0528, 0.0528, 0.0559, 0.0559, 0.0559, 0.0559, 0.0559,\n",
       "             0.0590, 0.0590, 0.0590, 0.0621, 0.0621, 0.0652, 0.0683, 0.0683, 0.0714,\n",
       "             0.0714, 0.0714, 0.0745, 0.0745, 0.0776, 0.0776, 0.0807, 0.0839, 0.0870,\n",
       "             0.0901, 0.0932, 0.0963, 0.0963, 0.0994, 0.1025, 0.1056, 0.1087, 0.1118,\n",
       "             0.1149, 0.1180, 0.1180, 0.1211, 0.1242, 0.1273, 0.1273, 0.1304, 0.1335,\n",
       "             0.1398, 0.1429, 0.1460, 0.1491, 0.1491, 0.1522, 0.1553, 0.1584, 0.1615,\n",
       "             0.1646, 0.1646, 0.1677, 0.1708, 0.1739, 0.1770, 0.1801, 0.1832, 0.1863,\n",
       "             0.1894, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050, 0.2081, 0.2112, 0.2143,\n",
       "             0.2174, 0.2174, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329, 0.2360, 0.2391,\n",
       "             0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578, 0.2609, 0.2640, 0.2671,\n",
       "             0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857, 0.2888, 0.2919, 0.2919,\n",
       "             0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137, 0.3168, 0.3199,\n",
       "             0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416, 0.3447, 0.3478,\n",
       "             0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696, 0.3727, 0.3758,\n",
       "             0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975, 0.4006, 0.4037,\n",
       "             0.4068, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224, 0.4255, 0.4286, 0.4317,\n",
       "             0.4348, 0.4379, 0.4410, 0.4441, 0.4472, 0.4503, 0.4534, 0.4565, 0.4596,\n",
       "             0.4627, 0.4658, 0.4689, 0.4720, 0.4752, 0.4783, 0.4814, 0.4845, 0.4876,\n",
       "             0.4907, 0.4938, 0.4969, 0.5000, 0.5031, 0.5062, 0.5093, 0.5124, 0.5155,\n",
       "             0.5186, 0.5217, 0.5248, 0.5280, 0.5311, 0.5342, 0.5373, 0.5404, 0.5435,\n",
       "             0.5466, 0.5497, 0.5528, 0.5559, 0.5590, 0.5621, 0.5652, 0.5683, 0.5714,\n",
       "             0.5745, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870, 0.5901, 0.5932, 0.5963,\n",
       "             0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149, 0.6180, 0.6211, 0.6242,\n",
       "             0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429, 0.6460, 0.6491, 0.6522,\n",
       "             0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708, 0.6739, 0.6770, 0.6801,\n",
       "             0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988, 0.7019, 0.7050, 0.7081,\n",
       "             0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267, 0.7298, 0.7329, 0.7360,\n",
       "             0.7391, 0.7422, 0.7453, 0.7484, 0.7516, 0.7547, 0.7578, 0.7609, 0.7640,\n",
       "             0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826, 0.7857, 0.7888, 0.7919,\n",
       "             0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106, 0.8137, 0.8168, 0.8199,\n",
       "             0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385, 0.8416, 0.8447, 0.8478,\n",
       "             0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665, 0.8696, 0.8727, 0.8758,\n",
       "             0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944, 0.8975, 0.9006, 0.9037,\n",
       "             0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224, 0.9255, 0.9286, 0.9317,\n",
       "             0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503, 0.9534, 0.9565, 0.9596,\n",
       "             0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783, 0.9814, 0.9845, 0.9876,\n",
       "             0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.5532, 0.6257, 0.6596, 0.6901, 0.7064, 0.7181, 0.7275, 0.7380,\n",
       "             0.7404, 0.7450, 0.7497, 0.7556, 0.7567, 0.7626, 0.7661, 0.7743, 0.7789,\n",
       "             0.7836, 0.7895, 0.7965, 0.7988, 0.8035, 0.8047, 0.8047, 0.8082, 0.8094,\n",
       "             0.8105, 0.8140, 0.8152, 0.8175, 0.8187, 0.8222, 0.8234, 0.8246, 0.8269,\n",
       "             0.8292, 0.8304, 0.8327, 0.8351, 0.8363, 0.8374, 0.8386, 0.8409, 0.8421,\n",
       "             0.8433, 0.8444, 0.8468, 0.8503, 0.8515, 0.8526, 0.8550, 0.8561, 0.8573,\n",
       "             0.8596, 0.8608, 0.8620, 0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702,\n",
       "             0.8713, 0.8725, 0.8737, 0.8749, 0.8760, 0.8772, 0.8784, 0.8795, 0.8807,\n",
       "             0.8819, 0.8830, 0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912,\n",
       "             0.8924, 0.8936, 0.8947, 0.8959, 0.8971, 0.8994, 0.9006, 0.9018, 0.9029,\n",
       "             0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135,\n",
       "             0.9146, 0.9158, 0.9170, 0.9181, 0.9193, 0.9205, 0.9216, 0.9228, 0.9240,\n",
       "             0.9251, 0.9263, 0.9275, 0.9298, 0.9310, 0.9322, 0.9333, 0.9345, 0.9357,\n",
       "             0.9368, 0.9368, 0.9380, 0.9392, 0.9404, 0.9415, 0.9415, 0.9427, 0.9427,\n",
       "             0.9439, 0.9450, 0.9462, 0.9474, 0.9485, 0.9485, 0.9485, 0.9497, 0.9509,\n",
       "             0.9520, 0.9520, 0.9532, 0.9544, 0.9556, 0.9567, 0.9579, 0.9591, 0.9602,\n",
       "             0.9614, 0.9626, 0.9637, 0.9637, 0.9649, 0.9661, 0.9673, 0.9673, 0.9684,\n",
       "             0.9696, 0.9708, 0.9708, 0.9719, 0.9719, 0.9719, 0.9731, 0.9731, 0.9743,\n",
       "             0.9754, 0.9754, 0.9754, 0.9766, 0.9766, 0.9778, 0.9789, 0.9801, 0.9813,\n",
       "             0.9813, 0.9825, 0.9836, 0.9836, 0.9848, 0.9848, 0.9848, 0.9860, 0.9860,\n",
       "             0.9871, 0.9883, 0.9883, 0.9895, 0.9895, 0.9906, 0.9906, 0.9906, 0.9906,\n",
       "             0.9906, 0.9906, 0.9906, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9930, 0.9930, 0.9930, 0.9930, 0.9942, 0.9942, 0.9942,\n",
       "             0.9942, 0.9942, 0.9942, 0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9953,\n",
       "             0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9981e-01, 9.9979e-01,\n",
       "             9.9976e-01, 9.9974e-01, 9.9973e-01, 9.9970e-01, 9.9969e-01, 9.9929e-01,\n",
       "             9.9927e-01, 9.9925e-01, 9.9921e-01, 9.9920e-01, 9.9908e-01, 9.9896e-01,\n",
       "             9.9893e-01, 9.9886e-01, 9.9875e-01, 9.9851e-01, 9.9813e-01, 9.9808e-01,\n",
       "             9.9789e-01, 9.9657e-01, 9.9629e-01, 9.9380e-01, 9.9316e-01, 9.9167e-01,\n",
       "             9.9163e-01, 9.9136e-01, 9.9049e-01, 9.9004e-01, 9.8975e-01, 9.8927e-01,\n",
       "             9.8879e-01, 9.8851e-01, 9.8815e-01, 9.8040e-01, 9.8014e-01, 9.7928e-01,\n",
       "             9.7909e-01, 9.7524e-01, 9.7475e-01, 9.7357e-01, 9.6962e-01, 9.6770e-01,\n",
       "             9.6570e-01, 9.6373e-01, 9.5682e-01, 9.3728e-01, 9.2733e-01, 9.2274e-01,\n",
       "             9.2192e-01, 9.2058e-01, 8.9217e-01, 8.8001e-01, 8.2719e-01, 8.2641e-01,\n",
       "             8.2349e-01, 8.0750e-01, 8.0006e-01, 7.9848e-01, 7.6287e-01, 7.2380e-01,\n",
       "             7.1645e-01, 7.0158e-01, 6.6471e-01, 5.7198e-01, 4.2366e-01, 3.8505e-01,\n",
       "             3.7568e-01, 3.1181e-01, 3.0851e-01, 2.7914e-01, 2.0760e-01, 1.8551e-01,\n",
       "             1.8378e-01, 1.0344e-01, 1.0122e-01, 9.6229e-02, 7.6813e-02, 7.5443e-02,\n",
       "             7.0978e-02, 6.6701e-02, 5.1244e-02, 5.0178e-02, 4.2760e-02, 3.8457e-02,\n",
       "             3.8068e-02, 3.3891e-02, 3.2148e-02, 3.1429e-02, 2.4254e-02, 1.9640e-02,\n",
       "             9.5321e-03, 8.6675e-03, 8.0861e-03, 7.0225e-03, 6.8615e-03, 6.2967e-03,\n",
       "             5.6492e-03, 4.8242e-03, 2.9979e-03, 2.7864e-03, 2.6160e-03, 2.3569e-03,\n",
       "             2.2324e-03, 2.0693e-03, 1.9756e-03, 1.4392e-03, 1.3612e-03, 1.1801e-03,\n",
       "             1.0323e-03, 9.8521e-04, 9.6125e-04, 7.6637e-04, 7.2549e-04, 7.1389e-04,\n",
       "             4.4926e-04, 3.6241e-04, 3.5738e-04, 3.2735e-04, 2.6902e-04, 2.3885e-04,\n",
       "             2.3332e-04, 2.3321e-04, 1.8972e-04, 1.6383e-04, 1.5052e-04, 1.4053e-04,\n",
       "             1.2720e-04, 1.2542e-04, 1.0886e-04, 1.0580e-04, 1.0293e-04, 1.0227e-04,\n",
       "             9.4871e-05, 8.8618e-05, 8.3294e-05, 7.5842e-05, 7.3299e-05, 6.9430e-05,\n",
       "             6.7963e-05, 6.5154e-05, 6.4734e-05, 6.1497e-05, 5.7116e-05, 5.6674e-05,\n",
       "             5.2548e-05, 4.0558e-05, 3.7893e-05, 3.7865e-05, 3.6541e-05, 3.5985e-05,\n",
       "             3.4754e-05, 3.1816e-05, 3.0476e-05, 2.8148e-05, 2.7137e-05, 2.4859e-05,\n",
       "             2.2549e-05, 2.1503e-05, 2.0350e-05, 1.8450e-05, 1.7532e-05, 1.5830e-05,\n",
       "             1.1816e-05, 1.1620e-05, 1.1327e-05, 1.0030e-05, 9.3735e-06, 8.4569e-06,\n",
       "             7.8722e-06, 6.7338e-06, 6.5977e-06, 6.3683e-06, 3.5486e-06, 3.4315e-06,\n",
       "             3.3007e-06, 3.2881e-06, 2.8759e-06, 2.8421e-06, 2.7807e-06, 2.7073e-06,\n",
       "             2.6965e-06, 2.5796e-06, 2.5151e-06, 2.2375e-06, 2.1484e-06, 2.1463e-06,\n",
       "             2.0599e-06, 1.9367e-06, 1.6327e-06, 1.6249e-06, 1.4644e-06, 1.4613e-06,\n",
       "             1.2550e-06, 1.2107e-06, 1.1353e-06, 1.1297e-06, 1.0889e-06, 1.0738e-06,\n",
       "             1.0098e-06, 1.0020e-06, 9.7890e-07, 9.3705e-07, 7.8094e-07, 7.6406e-07,\n",
       "             7.1105e-07, 6.6427e-07, 6.4926e-07, 6.2019e-07, 5.5546e-07, 5.5151e-07,\n",
       "             4.8874e-07, 4.7762e-07, 4.6936e-07, 4.4859e-07, 4.4743e-07, 4.3212e-07,\n",
       "             3.2674e-07, 3.1205e-07, 2.8726e-07, 2.5418e-07, 2.4886e-07, 2.1760e-07,\n",
       "             2.0127e-07, 1.9121e-07, 1.7606e-07, 1.6297e-07, 1.5925e-07, 1.4238e-07,\n",
       "             1.3712e-07, 1.3617e-07, 1.3110e-07, 1.2347e-07, 1.2123e-07, 1.1232e-07,\n",
       "             1.0851e-07, 1.0793e-07, 1.0227e-07, 9.7076e-08, 9.6169e-08, 9.3649e-08,\n",
       "             7.9102e-08, 7.4486e-08, 6.9820e-08, 6.7831e-08, 5.7823e-08, 5.3810e-08,\n",
       "             5.3772e-08, 5.1485e-08, 4.6102e-08, 4.5806e-08, 4.5220e-08, 4.2884e-08,\n",
       "             3.9634e-08, 3.9370e-08, 3.7421e-08, 3.6869e-08, 3.4242e-08, 3.3720e-08,\n",
       "             3.0766e-08, 2.8069e-08, 2.2664e-08, 2.1095e-08, 2.0524e-08, 1.9951e-08,\n",
       "             1.9509e-08, 1.8773e-08, 1.8481e-08, 1.8404e-08, 1.4890e-08, 1.2406e-08,\n",
       "             1.1866e-08, 1.1318e-08, 1.0400e-08, 9.6622e-09, 9.4428e-09, 8.8255e-09,\n",
       "             8.2776e-09, 7.8900e-09, 7.6243e-09, 7.3107e-09, 7.1522e-09, 6.6423e-09,\n",
       "             6.6157e-09, 6.0270e-09, 5.9445e-09, 5.8779e-09, 3.9993e-09, 3.8831e-09,\n",
       "             3.8403e-09, 3.8103e-09, 3.3099e-09, 2.9585e-09, 2.8175e-09, 2.7972e-09,\n",
       "             2.1258e-09, 2.0586e-09, 2.0108e-09, 1.6930e-09, 1.5586e-09, 1.4203e-09,\n",
       "             1.4077e-09, 1.3145e-09, 1.1066e-09, 1.0551e-09, 9.6316e-10, 9.2644e-10,\n",
       "             9.1525e-10, 9.1000e-10, 7.9007e-10, 6.4545e-10, 6.4460e-10, 3.7273e-10,\n",
       "             3.2577e-10, 2.7113e-10, 2.4478e-10, 2.1416e-10, 1.9619e-10, 1.8936e-10,\n",
       "             1.5442e-10, 1.5097e-10, 1.4089e-10, 1.3681e-10, 1.3454e-10, 1.1683e-10,\n",
       "             1.0565e-10, 7.6365e-11, 3.9805e-11, 1.9182e-11, 1.6170e-11, 1.4803e-11,\n",
       "             1.4470e-11, 1.0974e-11, 7.6006e-12, 7.5502e-12, 4.9702e-12, 4.1519e-12,\n",
       "             3.8748e-12, 3.8690e-12, 3.8316e-12, 3.7488e-12, 3.1854e-12, 2.5145e-12,\n",
       "             1.7968e-12, 1.2822e-12, 1.0936e-12, 9.0786e-13, 8.2926e-13, 6.0966e-13,\n",
       "             5.4975e-13, 5.1912e-13, 4.9868e-13, 2.5181e-13, 2.1941e-13, 8.2916e-14,\n",
       "             5.3200e-14, 5.0077e-14, 4.9232e-14, 4.8896e-14, 5.2021e-15, 4.9387e-15,\n",
       "             3.7060e-15, 1.4136e-15, 1.0967e-15, 7.6160e-17])}},\n",
       "   {'fpr': np.float64(0.062111801242236024),\n",
       "    'tpr': np.float64(0.9801169590643275),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0155, 0.0186, 0.0186, 0.0186, 0.0186, 0.0217, 0.0217,\n",
       "             0.0248, 0.0248, 0.0248, 0.0280, 0.0311, 0.0311, 0.0311, 0.0311, 0.0342,\n",
       "             0.0342, 0.0342, 0.0373, 0.0373, 0.0404, 0.0404, 0.0404, 0.0404, 0.0404,\n",
       "             0.0435, 0.0435, 0.0466, 0.0497, 0.0497, 0.0497, 0.0497, 0.0528, 0.0559,\n",
       "             0.0559, 0.0559, 0.0559, 0.0590, 0.0621, 0.0652, 0.0652, 0.0683, 0.0683,\n",
       "             0.0683, 0.0714, 0.0745, 0.0776, 0.0807, 0.0807, 0.0839, 0.0870, 0.0870,\n",
       "             0.0870, 0.0870, 0.0870, 0.0901, 0.0963, 0.0963, 0.0994, 0.1025, 0.1056,\n",
       "             0.1087, 0.1118, 0.1149, 0.1149, 0.1180, 0.1211, 0.1242, 0.1273, 0.1304,\n",
       "             0.1335, 0.1366, 0.1366, 0.1398, 0.1429, 0.1429, 0.1460, 0.1491, 0.1522,\n",
       "             0.1522, 0.1553, 0.1553, 0.1584, 0.1615, 0.1646, 0.1677, 0.1708, 0.1739,\n",
       "             0.1770, 0.1801, 0.1801, 0.1832, 0.1863, 0.1894, 0.1925, 0.1957, 0.1988,\n",
       "             0.1988, 0.2019, 0.2050, 0.2081, 0.2112, 0.2143, 0.2174, 0.2205, 0.2236,\n",
       "             0.2267, 0.2298, 0.2329, 0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516,\n",
       "             0.2547, 0.2578, 0.2609, 0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795,\n",
       "             0.2826, 0.2857, 0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075,\n",
       "             0.3106, 0.3137, 0.3168, 0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354,\n",
       "             0.3385, 0.3416, 0.3447, 0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634,\n",
       "             0.3665, 0.3696, 0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913,\n",
       "             0.3944, 0.3975, 0.4006, 0.4037, 0.4068, 0.4099, 0.4130, 0.4161, 0.4193,\n",
       "             0.4224, 0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4410, 0.4441, 0.4472,\n",
       "             0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752,\n",
       "             0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031,\n",
       "             0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311,\n",
       "             0.5342, 0.5373, 0.5404, 0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590,\n",
       "             0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870,\n",
       "             0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149,\n",
       "             0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429,\n",
       "             0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708,\n",
       "             0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988,\n",
       "             0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267,\n",
       "             0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453, 0.7453, 0.7484, 0.7516,\n",
       "             0.7547, 0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795,\n",
       "             0.7826, 0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075,\n",
       "             0.8106, 0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354,\n",
       "             0.8385, 0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634,\n",
       "             0.8665, 0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913,\n",
       "             0.8944, 0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193,\n",
       "             0.9224, 0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472,\n",
       "             0.9503, 0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752,\n",
       "             0.9783, 0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0912, 0.1368, 0.1719, 0.2023, 0.2152, 0.2339, 0.2503, 0.2596,\n",
       "             0.2713, 0.2819, 0.2901, 0.2971, 0.3064, 0.3123, 0.3216, 0.3263, 0.3322,\n",
       "             0.3333, 0.3392, 0.3427, 0.3474, 0.3567, 0.3579, 0.3626, 0.3637, 0.3661,\n",
       "             0.3684, 0.3731, 0.3789, 0.3836, 0.3883, 0.3965, 0.4000, 0.4035, 0.4082,\n",
       "             0.4117, 0.4140, 0.4164, 0.4187, 0.4269, 0.4304, 0.4327, 0.4339, 0.4363,\n",
       "             0.4386, 0.4421, 0.4433, 0.4444, 0.4491, 0.4526, 0.4561, 0.4573, 0.4585,\n",
       "             0.4608, 0.4632, 0.4678, 0.4713, 0.4749, 0.4760, 0.4772, 0.4807, 0.4819,\n",
       "             0.4854, 0.4877, 0.4924, 0.4936, 0.4971, 0.4982, 0.5018, 0.5053, 0.5064,\n",
       "             0.5076, 0.5123, 0.5135, 0.5158, 0.5170, 0.5205, 0.5228, 0.5251, 0.5263,\n",
       "             0.5287, 0.5298, 0.5310, 0.5322, 0.5345, 0.5357, 0.5368, 0.5380, 0.5415,\n",
       "             0.5427, 0.5439, 0.5450, 0.5462, 0.5474, 0.5485, 0.5532, 0.5544, 0.5556,\n",
       "             0.5567, 0.5591, 0.5602, 0.5637, 0.5649, 0.5673, 0.5684, 0.5719, 0.5731,\n",
       "             0.5743, 0.5766, 0.5778, 0.5789, 0.5825, 0.5848, 0.5860, 0.5871, 0.5883,\n",
       "             0.5906, 0.5918, 0.5930, 0.5942, 0.5953, 0.5977, 0.5988, 0.6000, 0.6023,\n",
       "             0.6035, 0.6047, 0.6070, 0.6082, 0.6094, 0.6105, 0.6117, 0.6129, 0.6152,\n",
       "             0.6175, 0.6187, 0.6199, 0.6222, 0.6234, 0.6246, 0.6257, 0.6269, 0.6281,\n",
       "             0.6304, 0.6351, 0.6363, 0.6374, 0.6398, 0.6421, 0.6433, 0.6456, 0.6468,\n",
       "             0.6480, 0.6491, 0.6503, 0.6526, 0.6538, 0.6550, 0.6573, 0.6585, 0.6596,\n",
       "             0.6608, 0.6632, 0.6643, 0.6655, 0.6667, 0.6678, 0.6690, 0.6702, 0.6713,\n",
       "             0.6725, 0.6737, 0.6749, 0.6760, 0.6772, 0.6784, 0.6795, 0.6807, 0.6819,\n",
       "             0.6830, 0.6842, 0.6854, 0.6865, 0.6877, 0.6889, 0.6912, 0.6924, 0.6936,\n",
       "             0.6947, 0.6959, 0.6971, 0.6982, 0.7006, 0.7018, 0.7029, 0.7041, 0.7053,\n",
       "             0.7064, 0.7076, 0.7088, 0.7099, 0.7111, 0.7123, 0.7146, 0.7170, 0.7181,\n",
       "             0.7193, 0.7205, 0.7216, 0.7228, 0.7240, 0.7251, 0.7263, 0.7287, 0.7298,\n",
       "             0.7310, 0.7322, 0.7333, 0.7345, 0.7357, 0.7368, 0.7380, 0.7392, 0.7404,\n",
       "             0.7427, 0.7439, 0.7450, 0.7462, 0.7474, 0.7485, 0.7497, 0.7509, 0.7520,\n",
       "             0.7532, 0.7544, 0.7556, 0.7567, 0.7579, 0.7591, 0.7602, 0.7614, 0.7626,\n",
       "             0.7637, 0.7649, 0.7673, 0.7684, 0.7696, 0.7708, 0.7719, 0.7743, 0.7754,\n",
       "             0.7766, 0.7778, 0.7789, 0.7801, 0.7813, 0.7825, 0.7836, 0.7848, 0.7860,\n",
       "             0.7871, 0.7883, 0.7895, 0.7906, 0.7918, 0.7930, 0.7942, 0.7953, 0.7965,\n",
       "             0.7977, 0.7988, 0.8000, 0.8012, 0.8023, 0.8035, 0.8047, 0.8058, 0.8082,\n",
       "             0.8094, 0.8105, 0.8117, 0.8129, 0.8140, 0.8152, 0.8164, 0.8175, 0.8187,\n",
       "             0.8199, 0.8211, 0.8222, 0.8234, 0.8246, 0.8257, 0.8269, 0.8281, 0.8292,\n",
       "             0.8304, 0.8316, 0.8316, 0.8327, 0.8339, 0.8351, 0.8363, 0.8374, 0.8386,\n",
       "             0.8398, 0.8409, 0.8421, 0.8433, 0.8444, 0.8456, 0.8468, 0.8480, 0.8491,\n",
       "             0.8503, 0.8515, 0.8526, 0.8538, 0.8550, 0.8561, 0.8573, 0.8585, 0.8596,\n",
       "             0.8608, 0.8620, 0.8632, 0.8643, 0.8655, 0.8667, 0.8678, 0.8690, 0.8702,\n",
       "             0.8713, 0.8725, 0.8737, 0.8749, 0.8760, 0.8772, 0.8784, 0.8795, 0.8807,\n",
       "             0.8819, 0.8830, 0.8842, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912,\n",
       "             0.8924, 0.8936, 0.8947, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018,\n",
       "             0.9029, 0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123,\n",
       "             0.9135, 0.9146, 0.9158, 0.9170, 0.9170, 0.9181, 0.9193, 0.9205, 0.9216,\n",
       "             0.9228, 0.9240, 0.9251, 0.9263, 0.9263, 0.9275, 0.9287, 0.9298, 0.9298,\n",
       "             0.9310, 0.9322, 0.9333, 0.9345, 0.9357, 0.9368, 0.9380, 0.9392, 0.9392,\n",
       "             0.9404, 0.9415, 0.9427, 0.9439, 0.9450, 0.9462, 0.9474, 0.9485, 0.9497,\n",
       "             0.9509, 0.9520, 0.9532, 0.9532, 0.9544, 0.9556, 0.9567, 0.9567, 0.9579,\n",
       "             0.9579, 0.9591, 0.9602, 0.9602, 0.9602, 0.9614, 0.9626, 0.9637, 0.9637,\n",
       "             0.9649, 0.9661, 0.9661, 0.9673, 0.9673, 0.9684, 0.9696, 0.9708, 0.9719,\n",
       "             0.9719, 0.9731, 0.9731, 0.9731, 0.9743, 0.9754, 0.9766, 0.9766, 0.9766,\n",
       "             0.9778, 0.9789, 0.9801, 0.9801, 0.9801, 0.9801, 0.9813, 0.9813, 0.9825,\n",
       "             0.9836, 0.9836, 0.9836, 0.9836, 0.9836, 0.9848, 0.9848, 0.9848, 0.9860,\n",
       "             0.9871, 0.9883, 0.9895, 0.9895, 0.9895, 0.9906, 0.9906, 0.9906, 0.9906,\n",
       "             0.9906, 0.9906, 0.9906, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9930, 0.9930, 0.9930, 0.9942, 0.9942, 0.9942, 0.9942,\n",
       "             0.9953, 0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9974e-01,\n",
       "             9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9970e-01, 9.9968e-01, 9.9968e-01, 9.9967e-01, 9.9966e-01,\n",
       "             9.9965e-01, 9.9965e-01, 9.9963e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01,\n",
       "             9.9961e-01, 9.9960e-01, 9.9959e-01, 9.9958e-01, 9.9958e-01, 9.9958e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9957e-01, 9.9956e-01, 9.9956e-01, 9.9951e-01,\n",
       "             9.9951e-01, 9.9949e-01, 9.9947e-01, 9.9946e-01, 9.9946e-01, 9.9946e-01,\n",
       "             9.9945e-01, 9.9942e-01, 9.9938e-01, 9.9936e-01, 9.9935e-01, 9.9935e-01,\n",
       "             9.9933e-01, 9.9929e-01, 9.9927e-01, 9.9926e-01, 9.9926e-01, 9.9925e-01,\n",
       "             9.9922e-01, 9.9916e-01, 9.9913e-01, 9.9913e-01, 9.9912e-01, 9.9911e-01,\n",
       "             9.9904e-01, 9.9903e-01, 9.9896e-01, 9.9895e-01, 9.9892e-01, 9.9882e-01,\n",
       "             9.9870e-01, 9.9867e-01, 9.9865e-01, 9.9863e-01, 9.9854e-01, 9.9848e-01,\n",
       "             9.9827e-01, 9.9767e-01, 9.9765e-01, 9.9758e-01, 9.9755e-01, 9.9743e-01,\n",
       "             9.9715e-01, 9.9684e-01, 9.9682e-01, 9.9681e-01, 9.9657e-01, 9.9653e-01,\n",
       "             9.9629e-01, 9.9555e-01, 9.9515e-01, 9.9460e-01, 9.9404e-01, 9.9332e-01,\n",
       "             9.9319e-01, 9.9169e-01, 9.9053e-01, 9.9018e-01, 9.9011e-01, 9.8918e-01,\n",
       "             9.8915e-01, 9.8904e-01, 9.8828e-01, 9.8754e-01, 9.8562e-01, 9.8426e-01,\n",
       "             9.8323e-01, 9.8316e-01, 9.8268e-01, 9.8104e-01, 9.8061e-01, 9.7881e-01,\n",
       "             9.7871e-01, 9.7648e-01, 9.7478e-01, 9.7269e-01, 9.7117e-01, 9.6875e-01,\n",
       "             9.5974e-01, 9.5775e-01, 9.5650e-01, 9.5525e-01, 9.5046e-01, 9.4791e-01,\n",
       "             9.4681e-01, 9.4679e-01, 9.3961e-01, 9.3188e-01, 9.2754e-01, 9.2580e-01,\n",
       "             9.1374e-01, 8.9594e-01, 8.9390e-01, 8.7414e-01, 8.5576e-01, 8.4683e-01,\n",
       "             8.4038e-01, 7.6522e-01, 7.3357e-01, 6.8654e-01, 6.6460e-01, 6.5654e-01,\n",
       "             5.9787e-01, 5.9110e-01, 5.8694e-01, 5.7571e-01, 5.6230e-01, 5.6139e-01,\n",
       "             5.4059e-01, 5.3975e-01, 4.3494e-01, 4.2379e-01, 4.1642e-01, 3.9378e-01,\n",
       "             3.9240e-01, 3.8397e-01, 3.4263e-01, 3.1778e-01, 3.0925e-01, 2.9305e-01,\n",
       "             2.5360e-01, 2.3998e-01, 2.3511e-01, 2.3166e-01, 2.2873e-01, 2.1781e-01,\n",
       "             1.6513e-01, 1.5591e-01, 1.3420e-01, 1.1281e-01, 1.0749e-01, 1.0260e-01,\n",
       "             1.0089e-01, 8.9059e-02, 8.4140e-02, 7.2579e-02, 5.6859e-02, 5.5044e-02,\n",
       "             5.1218e-02, 5.0626e-02, 4.7996e-02, 3.9026e-02, 3.1123e-02, 2.6023e-02,\n",
       "             2.5023e-02, 2.3283e-02, 2.2218e-02, 1.8872e-02, 1.6317e-02, 1.5813e-02,\n",
       "             1.5764e-02, 1.5745e-02, 1.4086e-02, 1.2520e-02, 1.1574e-02, 9.7933e-03,\n",
       "             9.4929e-03, 9.0557e-03, 6.6811e-03, 4.8944e-03, 4.8650e-03, 4.4537e-03,\n",
       "             3.8652e-03, 3.2489e-03, 2.8807e-03, 2.7353e-03, 2.5237e-03, 1.9850e-03,\n",
       "             1.9719e-03, 1.7987e-03, 1.6047e-03, 1.5936e-03, 1.5782e-03, 1.5606e-03,\n",
       "             1.3601e-03, 1.1404e-03, 9.9522e-04, 9.7936e-04, 9.3635e-04, 8.9686e-04,\n",
       "             6.9698e-04, 6.1655e-04, 4.4839e-04, 3.6302e-04, 3.0874e-04, 2.6727e-04,\n",
       "             2.0457e-04, 1.7484e-04, 1.5338e-04, 1.5258e-04, 1.4446e-04, 1.2564e-04,\n",
       "             9.0419e-05, 9.0044e-05, 8.8842e-05, 7.2270e-05, 6.9659e-05, 6.9315e-05,\n",
       "             6.7682e-05, 6.3464e-05, 6.3336e-05, 6.1306e-05, 5.9376e-05, 5.4764e-05,\n",
       "             4.9775e-05, 4.6604e-05, 3.9276e-05, 3.5562e-05, 3.4214e-05, 2.6691e-05,\n",
       "             2.4375e-05, 2.3943e-05, 2.3221e-05, 2.1677e-05, 2.1603e-05, 1.9819e-05,\n",
       "             1.8631e-05, 1.8408e-05, 1.5440e-05, 1.5098e-05, 1.4534e-05, 1.3794e-05,\n",
       "             1.2688e-05, 1.1600e-05, 1.0440e-05, 1.0011e-05, 9.8639e-06, 9.7714e-06,\n",
       "             8.1048e-06, 8.0513e-06, 7.5136e-06, 6.2176e-06, 6.0435e-06, 5.7502e-06,\n",
       "             5.6422e-06, 5.1519e-06, 5.0908e-06, 4.8295e-06, 4.2379e-06, 4.1290e-06,\n",
       "             3.5986e-06, 3.3542e-06, 3.3532e-06, 3.3526e-06, 3.2850e-06, 3.2155e-06,\n",
       "             2.9997e-06, 2.8038e-06, 2.7274e-06, 2.7265e-06, 2.6802e-06, 1.9901e-06,\n",
       "             1.8825e-06, 1.8282e-06, 1.8281e-06, 1.7869e-06, 1.7860e-06, 1.7742e-06,\n",
       "             1.7047e-06, 1.4645e-06, 1.3873e-06, 1.3810e-06, 1.3792e-06, 1.2444e-06,\n",
       "             1.2128e-06, 1.1780e-06, 1.1508e-06, 1.0353e-06, 7.7131e-07, 7.0021e-07,\n",
       "             6.3695e-07, 6.3510e-07, 6.0253e-07, 5.4878e-07, 5.4510e-07, 5.3300e-07,\n",
       "             5.3037e-07, 5.1898e-07, 4.8088e-07, 4.7701e-07, 4.6926e-07, 4.6654e-07,\n",
       "             4.5392e-07, 4.0313e-07, 3.8961e-07, 3.6986e-07, 3.6085e-07, 3.4583e-07,\n",
       "             3.3207e-07, 3.2348e-07, 2.7127e-07, 2.5781e-07, 2.4541e-07, 2.1174e-07,\n",
       "             1.9530e-07, 1.9090e-07, 1.8872e-07, 1.8820e-07, 1.5225e-07, 1.5052e-07,\n",
       "             1.4198e-07, 1.3928e-07, 1.3119e-07, 1.2141e-07, 1.1703e-07, 1.0713e-07,\n",
       "             1.0282e-07, 8.1098e-08, 7.8796e-08, 7.8205e-08, 7.5841e-08, 7.2074e-08,\n",
       "             7.0426e-08, 6.0368e-08, 5.8911e-08, 5.7925e-08, 5.6503e-08, 4.4927e-08,\n",
       "             4.1097e-08, 4.1084e-08, 4.0797e-08, 4.0070e-08, 3.8378e-08, 3.7102e-08,\n",
       "             3.1372e-08, 2.9787e-08, 2.9735e-08, 2.7959e-08, 2.7734e-08, 2.7386e-08,\n",
       "             2.7059e-08, 2.3895e-08, 2.2708e-08, 2.2144e-08, 1.9909e-08, 1.9069e-08,\n",
       "             1.8852e-08, 1.7549e-08, 1.7304e-08, 1.7018e-08, 1.6068e-08, 1.6063e-08,\n",
       "             1.5723e-08, 1.3419e-08, 1.1642e-08, 1.1133e-08, 1.0979e-08, 1.0454e-08,\n",
       "             1.0355e-08, 1.0176e-08, 1.0088e-08, 9.9727e-09, 9.9063e-09, 9.7593e-09,\n",
       "             9.4924e-09, 8.9815e-09, 8.9112e-09, 8.3503e-09, 7.8922e-09, 6.6900e-09,\n",
       "             6.3569e-09, 5.2363e-09, 4.3507e-09, 4.3313e-09, 4.1666e-09, 3.8315e-09,\n",
       "             3.2605e-09, 3.0608e-09, 2.6899e-09, 2.2469e-09, 2.0303e-09, 1.8684e-09,\n",
       "             1.8158e-09, 1.7971e-09, 1.6817e-09, 1.6068e-09, 1.3495e-09, 1.3391e-09,\n",
       "             1.3177e-09, 1.2932e-09, 1.2738e-09, 1.2274e-09, 6.0086e-10, 4.8605e-10,\n",
       "             4.2667e-10, 4.0971e-10, 3.9959e-10, 3.7145e-10, 3.5057e-10, 3.1851e-10,\n",
       "             3.1669e-10, 3.1209e-10, 2.9082e-10, 2.8049e-10, 1.7508e-10, 1.5819e-10,\n",
       "             1.5520e-10, 1.4708e-10, 1.3032e-10, 1.0714e-10, 9.1004e-11, 8.4280e-11,\n",
       "             6.5978e-11, 5.9665e-11, 5.4206e-11, 5.2972e-11, 5.2329e-11, 4.7457e-11,\n",
       "             4.5788e-11, 4.5277e-11, 3.9103e-11, 3.5957e-11, 2.9274e-11, 2.5356e-11,\n",
       "             1.9775e-11, 1.7755e-11, 1.3895e-11, 1.1464e-11, 1.1277e-11, 1.0338e-11,\n",
       "             4.4409e-12, 3.5993e-12, 1.6673e-12, 1.1832e-12, 2.8369e-13, 2.4564e-13,\n",
       "             5.5330e-14, 3.9045e-15])}},\n",
       "   {'fpr': np.float64(0.09316770186335403),\n",
       "    'tpr': np.float64(0.9847953216374269),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0031, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "             0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0155, 0.0155, 0.0155, 0.0186,\n",
       "             0.0186, 0.0186, 0.0186, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,\n",
       "             0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0248, 0.0280, 0.0280,\n",
       "             0.0280, 0.0311, 0.0311, 0.0311, 0.0311, 0.0342, 0.0342, 0.0342, 0.0342,\n",
       "             0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342,\n",
       "             0.0342, 0.0342, 0.0342, 0.0342, 0.0373, 0.0373, 0.0373, 0.0373, 0.0373,\n",
       "             0.0435, 0.0435, 0.0435, 0.0435, 0.0466, 0.0497, 0.0528, 0.0559, 0.0590,\n",
       "             0.0621, 0.0621, 0.0621, 0.0621, 0.0652, 0.0683, 0.0714, 0.0714, 0.0745,\n",
       "             0.0745, 0.0745, 0.0745, 0.0776, 0.0776, 0.0776, 0.0776, 0.0807, 0.0807,\n",
       "             0.0839, 0.0870, 0.0870, 0.0901, 0.0932, 0.0963, 0.0963, 0.0994, 0.1025,\n",
       "             0.1056, 0.1087, 0.1118, 0.1149, 0.1180, 0.1211, 0.1211, 0.1242, 0.1273,\n",
       "             0.1273, 0.1273, 0.1273, 0.1304, 0.1304, 0.1335, 0.1366, 0.1366, 0.1398,\n",
       "             0.1398, 0.1398, 0.1429, 0.1460, 0.1491, 0.1491, 0.1522, 0.1553, 0.1584,\n",
       "             0.1615, 0.1646, 0.1677, 0.1708, 0.1739, 0.1770, 0.1801, 0.1801, 0.1832,\n",
       "             0.1863, 0.1894, 0.1925, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050, 0.2081,\n",
       "             0.2112, 0.2143, 0.2174, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329, 0.2360,\n",
       "             0.2391, 0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578, 0.2609, 0.2640,\n",
       "             0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857, 0.2888, 0.2919,\n",
       "             0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137, 0.3168, 0.3199,\n",
       "             0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416, 0.3447, 0.3478,\n",
       "             0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696, 0.3727, 0.3758,\n",
       "             0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975, 0.4006, 0.4037,\n",
       "             0.4068, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224, 0.4255, 0.4286, 0.4317,\n",
       "             0.4348, 0.4379, 0.4410, 0.4441, 0.4472, 0.4503, 0.4534, 0.4565, 0.4596,\n",
       "             0.4627, 0.4658, 0.4689, 0.4720, 0.4752, 0.4783, 0.4814, 0.4845, 0.4876,\n",
       "             0.4907, 0.4938, 0.4969, 0.5000, 0.5031, 0.5062, 0.5093, 0.5124, 0.5155,\n",
       "             0.5186, 0.5217, 0.5248, 0.5280, 0.5311, 0.5342, 0.5373, 0.5404, 0.5435,\n",
       "             0.5466, 0.5497, 0.5528, 0.5559, 0.5590, 0.5621, 0.5652, 0.5683, 0.5714,\n",
       "             0.5745, 0.5776, 0.5807, 0.5839, 0.5870, 0.5901, 0.5932, 0.5963, 0.5994,\n",
       "             0.6025, 0.6056, 0.6087, 0.6118, 0.6149, 0.6180, 0.6211, 0.6242, 0.6273,\n",
       "             0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429, 0.6460, 0.6491, 0.6522,\n",
       "             0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708, 0.6739, 0.6770, 0.6801,\n",
       "             0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988, 0.7019, 0.7050, 0.7081,\n",
       "             0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267, 0.7298, 0.7329, 0.7360,\n",
       "             0.7391, 0.7422, 0.7453, 0.7484, 0.7516, 0.7547, 0.7578, 0.7609, 0.7640,\n",
       "             0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826, 0.7857, 0.7888, 0.7919,\n",
       "             0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106, 0.8137, 0.8168, 0.8199,\n",
       "             0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385, 0.8416, 0.8447, 0.8478,\n",
       "             0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665, 0.8696, 0.8727, 0.8758,\n",
       "             0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944, 0.8975, 0.9006, 0.9037,\n",
       "             0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224, 0.9255, 0.9286, 0.9317,\n",
       "             0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503, 0.9534, 0.9565, 0.9596,\n",
       "             0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783, 0.9814, 0.9845, 0.9876,\n",
       "             0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7801, 0.8164, 0.8234, 0.8327, 0.8386, 0.8421, 0.8444, 0.8480,\n",
       "             0.8503, 0.8515, 0.8538, 0.8561, 0.8596, 0.8608, 0.8620, 0.8632, 0.8655,\n",
       "             0.8667, 0.8678, 0.8690, 0.8702, 0.8713, 0.8749, 0.8760, 0.8772, 0.8784,\n",
       "             0.8819, 0.8830, 0.8854, 0.8865, 0.8877, 0.8889, 0.8901, 0.8912, 0.8912,\n",
       "             0.8936, 0.8947, 0.8959, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029,\n",
       "             0.9041, 0.9053, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9123, 0.9135,\n",
       "             0.9146, 0.9158, 0.9170, 0.9181, 0.9193, 0.9193, 0.9205, 0.9216, 0.9216,\n",
       "             0.9228, 0.9240, 0.9251, 0.9251, 0.9263, 0.9275, 0.9287, 0.9298, 0.9310,\n",
       "             0.9322, 0.9333, 0.9345, 0.9357, 0.9368, 0.9380, 0.9380, 0.9380, 0.9392,\n",
       "             0.9404, 0.9404, 0.9415, 0.9427, 0.9439, 0.9439, 0.9450, 0.9462, 0.9474,\n",
       "             0.9485, 0.9497, 0.9509, 0.9520, 0.9532, 0.9544, 0.9556, 0.9567, 0.9579,\n",
       "             0.9591, 0.9602, 0.9614, 0.9626, 0.9626, 0.9637, 0.9649, 0.9661, 0.9673,\n",
       "             0.9673, 0.9684, 0.9696, 0.9708, 0.9708, 0.9708, 0.9708, 0.9708, 0.9708,\n",
       "             0.9708, 0.9719, 0.9731, 0.9743, 0.9743, 0.9743, 0.9743, 0.9754, 0.9754,\n",
       "             0.9766, 0.9778, 0.9789, 0.9789, 0.9801, 0.9813, 0.9825, 0.9825, 0.9836,\n",
       "             0.9836, 0.9836, 0.9848, 0.9848, 0.9848, 0.9848, 0.9860, 0.9860, 0.9860,\n",
       "             0.9860, 0.9860, 0.9860, 0.9860, 0.9860, 0.9860, 0.9871, 0.9871, 0.9871,\n",
       "             0.9883, 0.9895, 0.9906, 0.9906, 0.9918, 0.9918, 0.9918, 0.9930, 0.9930,\n",
       "             0.9942, 0.9953, 0.9953, 0.9953, 0.9953, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9986e-01, 9.9986e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9974e-01, 9.9970e-01, 9.9964e-01, 9.9963e-01, 9.9963e-01, 9.9961e-01,\n",
       "             9.9960e-01, 9.9947e-01, 9.9928e-01, 9.9920e-01, 9.9904e-01, 9.9895e-01,\n",
       "             9.9870e-01, 9.9848e-01, 9.9847e-01, 9.9842e-01, 9.9822e-01, 9.9792e-01,\n",
       "             9.9759e-01, 9.9756e-01, 9.9603e-01, 9.9532e-01, 9.9514e-01, 9.9508e-01,\n",
       "             9.9412e-01, 9.9362e-01, 9.9247e-01, 9.9172e-01, 9.9144e-01, 9.8978e-01,\n",
       "             9.8945e-01, 9.8631e-01, 9.8621e-01, 9.8562e-01, 9.8456e-01, 9.8126e-01,\n",
       "             9.7415e-01, 9.6739e-01, 9.6465e-01, 9.5946e-01, 9.5643e-01, 9.2045e-01,\n",
       "             9.1133e-01, 9.0409e-01, 8.9416e-01, 8.8065e-01, 8.7301e-01, 8.6537e-01,\n",
       "             8.6297e-01, 8.2097e-01, 7.6777e-01, 7.2511e-01, 6.4897e-01, 6.3145e-01,\n",
       "             6.2145e-01, 5.4173e-01, 4.9489e-01, 4.3679e-01, 4.0620e-01, 4.0258e-01,\n",
       "             3.1320e-01, 2.8717e-01, 2.2740e-01, 2.0463e-01, 1.7436e-01, 1.3094e-01,\n",
       "             1.2889e-01, 1.0356e-01, 1.0049e-01, 9.9163e-02, 7.8544e-02, 6.9117e-02,\n",
       "             6.6926e-02, 5.4271e-02, 5.0900e-02, 3.6344e-02, 3.4948e-02, 3.4099e-02,\n",
       "             3.1693e-02, 2.8171e-02, 2.6546e-02, 1.8426e-02, 1.7046e-02, 1.4114e-02,\n",
       "             1.3685e-02, 1.3678e-02, 1.2382e-02, 1.0706e-02, 9.4864e-03, 6.8747e-03,\n",
       "             3.0937e-03, 3.0170e-03, 2.5345e-03, 2.1557e-03, 2.1343e-03, 1.7282e-03,\n",
       "             1.5548e-03, 1.1460e-03, 1.0073e-03, 7.3169e-04, 6.4867e-04, 6.4480e-04,\n",
       "             6.0775e-04, 5.6530e-04, 5.2462e-04, 4.7387e-04, 4.3459e-04, 3.8681e-04,\n",
       "             2.8552e-04, 2.6634e-04, 2.5764e-04, 2.4083e-04, 1.9493e-04, 1.8878e-04,\n",
       "             1.8563e-04, 1.7560e-04, 1.7157e-04, 1.7053e-04, 1.3929e-04, 1.2784e-04,\n",
       "             1.1387e-04, 1.0162e-04, 9.4559e-05, 8.8382e-05, 8.7043e-05, 8.2056e-05,\n",
       "             7.4692e-05, 6.8827e-05, 6.1004e-05, 5.5532e-05, 5.5213e-05, 5.1485e-05,\n",
       "             4.8254e-05, 4.5171e-05, 3.9352e-05, 3.1170e-05, 3.1105e-05, 3.1024e-05,\n",
       "             2.6681e-05, 2.6371e-05, 2.3571e-05, 2.2055e-05, 1.4407e-05, 1.2640e-05,\n",
       "             1.1394e-05, 1.0412e-05, 9.9274e-06, 9.7592e-06, 8.9308e-06, 8.3266e-06,\n",
       "             7.9939e-06, 4.8042e-06, 4.0091e-06, 3.9986e-06, 3.9647e-06, 3.7029e-06,\n",
       "             3.2056e-06, 3.1047e-06, 2.8351e-06, 2.6484e-06, 2.1960e-06, 1.7630e-06,\n",
       "             1.5739e-06, 1.4356e-06, 1.4337e-06, 1.2508e-06, 1.2148e-06, 1.1706e-06,\n",
       "             1.1010e-06, 1.0543e-06, 9.5953e-07, 8.6402e-07, 7.3858e-07, 6.7851e-07,\n",
       "             6.1787e-07, 5.8617e-07, 5.6447e-07, 5.5444e-07, 5.5306e-07, 5.2112e-07,\n",
       "             4.9334e-07, 4.9070e-07, 4.8979e-07, 4.7245e-07, 4.4378e-07, 4.0346e-07,\n",
       "             3.9111e-07, 3.7299e-07, 3.4088e-07, 2.8016e-07, 2.7871e-07, 2.7194e-07,\n",
       "             2.6028e-07, 2.3129e-07, 2.2342e-07, 2.1575e-07, 2.0713e-07, 2.0095e-07,\n",
       "             1.7458e-07, 1.7245e-07, 1.7206e-07, 1.7072e-07, 1.7048e-07, 1.7035e-07,\n",
       "             1.5501e-07, 1.5388e-07, 1.4742e-07, 1.2580e-07, 1.2079e-07, 1.1474e-07,\n",
       "             1.1345e-07, 1.0921e-07, 9.7691e-08, 7.5275e-08, 7.5198e-08, 7.4485e-08,\n",
       "             7.0056e-08, 5.7016e-08, 5.5413e-08, 5.5353e-08, 5.1624e-08, 4.7474e-08,\n",
       "             4.7070e-08, 4.5684e-08, 4.5095e-08, 3.5425e-08, 3.5254e-08, 3.1353e-08,\n",
       "             3.1216e-08, 2.9876e-08, 2.8170e-08, 2.7054e-08, 2.6495e-08, 2.3217e-08,\n",
       "             2.2681e-08, 2.2675e-08, 1.9891e-08, 1.9801e-08, 1.5587e-08, 1.5536e-08,\n",
       "             1.2731e-08, 1.2553e-08, 1.0306e-08, 9.2562e-09, 8.6314e-09, 8.1738e-09,\n",
       "             7.4849e-09, 5.5705e-09, 4.6980e-09, 4.5901e-09, 4.5527e-09, 4.3414e-09,\n",
       "             3.6377e-09, 3.2895e-09, 3.2663e-09, 3.1705e-09, 3.1370e-09, 2.9094e-09,\n",
       "             2.6626e-09, 2.2804e-09, 2.1271e-09, 1.9659e-09, 1.9643e-09, 1.8952e-09,\n",
       "             1.7425e-09, 1.4847e-09, 1.4128e-09, 1.2881e-09, 1.2320e-09, 1.2119e-09,\n",
       "             1.1740e-09, 1.1303e-09, 7.8291e-10, 7.0888e-10, 6.7403e-10, 6.3173e-10,\n",
       "             4.6952e-10, 4.1640e-10, 3.4420e-10, 3.4085e-10, 3.3725e-10, 3.2089e-10,\n",
       "             3.1329e-10, 2.7871e-10, 2.7663e-10, 2.5127e-10, 2.5075e-10, 2.1168e-10,\n",
       "             1.4683e-10, 1.1747e-10, 1.1572e-10, 1.1099e-10, 1.0788e-10, 1.0187e-10,\n",
       "             7.3614e-11, 7.3367e-11, 6.7211e-11, 6.5142e-11, 5.7478e-11, 5.4840e-11,\n",
       "             3.9901e-11, 3.9378e-11, 3.5684e-11, 2.9241e-11, 2.7604e-11, 2.3350e-11,\n",
       "             2.0187e-11, 1.6790e-11, 1.5379e-11, 1.5348e-11, 1.3175e-11, 1.2195e-11,\n",
       "             1.2022e-11, 1.1100e-11, 1.0794e-11, 8.0653e-12, 7.7166e-12, 7.7034e-12,\n",
       "             7.4244e-12, 6.2023e-12, 4.7455e-12, 4.5878e-12, 3.8299e-12, 3.5114e-12,\n",
       "             3.4860e-12, 3.0375e-12, 2.8006e-12, 2.6465e-12, 2.1459e-12, 8.3182e-13,\n",
       "             7.3573e-13, 6.0612e-13, 5.7404e-13, 4.7201e-13, 2.5967e-13, 2.2770e-13,\n",
       "             2.2404e-13, 1.9268e-13, 1.8713e-13, 1.8669e-13, 1.5467e-13, 1.1924e-13,\n",
       "             6.6008e-14, 4.2140e-14, 4.1196e-14, 3.6550e-14, 3.2684e-14, 1.9403e-14,\n",
       "             1.0320e-14, 7.6686e-15, 7.5353e-15, 6.8435e-15, 6.1599e-15, 5.3941e-15,\n",
       "             5.2841e-15, 3.9905e-15, 1.1319e-15, 1.3452e-16, 3.5270e-18, 2.8903e-18,\n",
       "             6.4887e-20])}},\n",
       "   {'fpr': np.float64(0.08385093167701864),\n",
       "    'tpr': np.float64(0.9812865497076023),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
       "             0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0062,\n",
       "             0.0062, 0.0062, 0.0062, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
       "             0.0093, 0.0093, 0.0093, 0.0093, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
       "             0.0124, 0.0124, 0.0124, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,\n",
       "             0.0155, 0.0155, 0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0217, 0.0248,\n",
       "             0.0248, 0.0248, 0.0280, 0.0280, 0.0280, 0.0311, 0.0311, 0.0311, 0.0311,\n",
       "             0.0311, 0.0311, 0.0311, 0.0311, 0.0342, 0.0342, 0.0342, 0.0342, 0.0373,\n",
       "             0.0404, 0.0435, 0.0435, 0.0466, 0.0466, 0.0466, 0.0466, 0.0466, 0.0466,\n",
       "             0.0466, 0.0466, 0.0466, 0.0466, 0.0497, 0.0528, 0.0559, 0.0590, 0.0590,\n",
       "             0.0621, 0.0621, 0.0621, 0.0621, 0.0621, 0.0621, 0.0652, 0.0683, 0.0714,\n",
       "             0.0745, 0.0745, 0.0745, 0.0745, 0.0745, 0.0776, 0.0776, 0.0776, 0.0807,\n",
       "             0.0839, 0.0839, 0.0870, 0.0901, 0.0901, 0.0901, 0.0932, 0.0932, 0.0994,\n",
       "             0.0994, 0.1025, 0.1025, 0.1056, 0.1087, 0.1118, 0.1149, 0.1180, 0.1180,\n",
       "             0.1211, 0.1242, 0.1273, 0.1304, 0.1335, 0.1366, 0.1366, 0.1398, 0.1429,\n",
       "             0.1460, 0.1491, 0.1522, 0.1553, 0.1584, 0.1615, 0.1615, 0.1646, 0.1677,\n",
       "             0.1708, 0.1708, 0.1739, 0.1739, 0.1770, 0.1801, 0.1832, 0.1832, 0.1863,\n",
       "             0.1894, 0.1925, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050, 0.2081, 0.2112,\n",
       "             0.2143, 0.2143, 0.2174, 0.2205, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329,\n",
       "             0.2360, 0.2391, 0.2422, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578, 0.2609,\n",
       "             0.2640, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826, 0.2857, 0.2888,\n",
       "             0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106, 0.3137, 0.3168,\n",
       "             0.3199, 0.3230, 0.3261, 0.3292, 0.3323, 0.3354, 0.3385, 0.3416, 0.3447,\n",
       "             0.3478, 0.3509, 0.3540, 0.3571, 0.3602, 0.3634, 0.3665, 0.3696, 0.3727,\n",
       "             0.3758, 0.3789, 0.3820, 0.3851, 0.3882, 0.3913, 0.3944, 0.3975, 0.4006,\n",
       "             0.4037, 0.4068, 0.4099, 0.4130, 0.4161, 0.4193, 0.4224, 0.4255, 0.4286,\n",
       "             0.4317, 0.4348, 0.4379, 0.4410, 0.4441, 0.4472, 0.4503, 0.4534, 0.4565,\n",
       "             0.4596, 0.4627, 0.4658, 0.4689, 0.4720, 0.4752, 0.4783, 0.4814, 0.4845,\n",
       "             0.4876, 0.4907, 0.4938, 0.4969, 0.5000, 0.5031, 0.5062, 0.5093, 0.5124,\n",
       "             0.5155, 0.5186, 0.5217, 0.5248, 0.5280, 0.5311, 0.5342, 0.5373, 0.5404,\n",
       "             0.5435, 0.5466, 0.5497, 0.5528, 0.5559, 0.5590, 0.5621, 0.5652, 0.5683,\n",
       "             0.5714, 0.5745, 0.5776, 0.5807, 0.5839, 0.5870, 0.5901, 0.5932, 0.5963,\n",
       "             0.5994, 0.6025, 0.6056, 0.6087, 0.6118, 0.6149, 0.6180, 0.6180, 0.6211,\n",
       "             0.6242, 0.6273, 0.6304, 0.6335, 0.6366, 0.6398, 0.6429, 0.6460, 0.6491,\n",
       "             0.6522, 0.6553, 0.6584, 0.6615, 0.6646, 0.6677, 0.6708, 0.6739, 0.6770,\n",
       "             0.6801, 0.6832, 0.6863, 0.6894, 0.6925, 0.6957, 0.6988, 0.7019, 0.7050,\n",
       "             0.7081, 0.7112, 0.7143, 0.7174, 0.7205, 0.7236, 0.7267, 0.7298, 0.7329,\n",
       "             0.7360, 0.7391, 0.7422, 0.7453, 0.7484, 0.7516, 0.7547, 0.7578, 0.7609,\n",
       "             0.7640, 0.7671, 0.7702, 0.7733, 0.7764, 0.7795, 0.7826, 0.7857, 0.7888,\n",
       "             0.7919, 0.7950, 0.7981, 0.8012, 0.8043, 0.8075, 0.8106, 0.8137, 0.8168,\n",
       "             0.8199, 0.8230, 0.8261, 0.8292, 0.8323, 0.8354, 0.8385, 0.8416, 0.8447,\n",
       "             0.8478, 0.8509, 0.8540, 0.8571, 0.8602, 0.8634, 0.8665, 0.8696, 0.8727,\n",
       "             0.8758, 0.8789, 0.8820, 0.8851, 0.8882, 0.8913, 0.8944, 0.8975, 0.9006,\n",
       "             0.9037, 0.9068, 0.9099, 0.9130, 0.9161, 0.9193, 0.9224, 0.9255, 0.9286,\n",
       "             0.9317, 0.9348, 0.9379, 0.9410, 0.9441, 0.9472, 0.9503, 0.9534, 0.9565,\n",
       "             0.9596, 0.9627, 0.9658, 0.9689, 0.9720, 0.9752, 0.9783, 0.9814, 0.9845,\n",
       "             0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8058, 0.8468, 0.8550, 0.8620, 0.8667, 0.8713, 0.8737, 0.8749,\n",
       "             0.8760, 0.8784, 0.8807, 0.8830, 0.8842, 0.8865, 0.8877, 0.8889, 0.8912,\n",
       "             0.8924, 0.8936, 0.8971, 0.8982, 0.8994, 0.9006, 0.9018, 0.9029, 0.9029,\n",
       "             0.9041, 0.9053, 0.9064, 0.9064, 0.9076, 0.9088, 0.9099, 0.9111, 0.9135,\n",
       "             0.9146, 0.9158, 0.9170, 0.9181, 0.9181, 0.9193, 0.9205, 0.9216, 0.9228,\n",
       "             0.9240, 0.9251, 0.9263, 0.9263, 0.9275, 0.9287, 0.9298, 0.9310, 0.9322,\n",
       "             0.9333, 0.9345, 0.9345, 0.9357, 0.9368, 0.9380, 0.9392, 0.9392, 0.9392,\n",
       "             0.9404, 0.9415, 0.9415, 0.9427, 0.9439, 0.9439, 0.9450, 0.9462, 0.9474,\n",
       "             0.9485, 0.9497, 0.9509, 0.9520, 0.9520, 0.9532, 0.9544, 0.9556, 0.9556,\n",
       "             0.9556, 0.9556, 0.9567, 0.9567, 0.9579, 0.9591, 0.9602, 0.9614, 0.9626,\n",
       "             0.9637, 0.9649, 0.9661, 0.9673, 0.9673, 0.9673, 0.9673, 0.9673, 0.9684,\n",
       "             0.9684, 0.9696, 0.9708, 0.9719, 0.9731, 0.9743, 0.9743, 0.9743, 0.9743,\n",
       "             0.9743, 0.9754, 0.9766, 0.9778, 0.9789, 0.9789, 0.9801, 0.9813, 0.9813,\n",
       "             0.9813, 0.9825, 0.9825, 0.9825, 0.9836, 0.9848, 0.9848, 0.9860, 0.9860,\n",
       "             0.9871, 0.9871, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9895,\n",
       "             0.9895, 0.9895, 0.9895, 0.9895, 0.9895, 0.9895, 0.9906, 0.9906, 0.9906,\n",
       "             0.9906, 0.9906, 0.9906, 0.9906, 0.9906, 0.9906, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9930, 0.9930, 0.9942, 0.9942, 0.9942, 0.9942, 0.9953, 0.9953,\n",
       "             0.9953, 0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9992e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9983e-01,\n",
       "             9.9978e-01, 9.9976e-01, 9.9972e-01, 9.9967e-01, 9.9965e-01, 9.9965e-01,\n",
       "             9.9932e-01, 9.9932e-01, 9.9919e-01, 9.9886e-01, 9.9878e-01, 9.9843e-01,\n",
       "             9.9762e-01, 9.9755e-01, 9.9736e-01, 9.9673e-01, 9.9514e-01, 9.9331e-01,\n",
       "             9.9168e-01, 9.9105e-01, 9.9095e-01, 9.9086e-01, 9.8620e-01, 9.8572e-01,\n",
       "             9.8391e-01, 9.8165e-01, 9.7696e-01, 9.7015e-01, 9.6908e-01, 9.6093e-01,\n",
       "             9.5334e-01, 9.5210e-01, 9.4990e-01, 9.4944e-01, 9.4743e-01, 9.3309e-01,\n",
       "             9.2961e-01, 9.2706e-01, 8.9564e-01, 8.8655e-01, 8.6148e-01, 8.5254e-01,\n",
       "             8.3796e-01, 8.3783e-01, 8.3530e-01, 8.0618e-01, 8.0123e-01, 7.8192e-01,\n",
       "             7.5934e-01, 7.0955e-01, 6.6221e-01, 6.1397e-01, 4.8420e-01, 4.2637e-01,\n",
       "             4.0318e-01, 3.1889e-01, 3.1660e-01, 2.8038e-01, 2.3820e-01, 1.9981e-01,\n",
       "             1.0221e-01, 8.1979e-02, 7.3678e-02, 6.4238e-02, 6.1261e-02, 5.8058e-02,\n",
       "             5.1044e-02, 4.7807e-02, 4.7450e-02, 4.1562e-02, 3.4676e-02, 3.0256e-02,\n",
       "             2.7546e-02, 2.6934e-02, 1.9995e-02, 1.9316e-02, 1.8481e-02, 1.3998e-02,\n",
       "             1.2985e-02, 1.2250e-02, 8.7663e-03, 7.7890e-03, 6.7868e-03, 5.3589e-03,\n",
       "             5.2696e-03, 4.4807e-03, 3.6576e-03, 3.5111e-03, 3.2479e-03, 2.9070e-03,\n",
       "             2.8364e-03, 1.4679e-03, 1.3025e-03, 1.2024e-03, 1.1658e-03, 8.3814e-04,\n",
       "             6.4906e-04, 5.3067e-04, 5.1563e-04, 4.2084e-04, 3.6098e-04, 3.5401e-04,\n",
       "             2.8493e-04, 2.7606e-04, 2.7205e-04, 2.5706e-04, 2.3720e-04, 1.7091e-04,\n",
       "             1.4135e-04, 1.3478e-04, 1.1851e-04, 1.1528e-04, 9.7758e-05, 9.4147e-05,\n",
       "             6.4507e-05, 5.3369e-05, 4.9678e-05, 4.4655e-05, 2.3512e-05, 1.8150e-05,\n",
       "             1.7660e-05, 1.6659e-05, 1.5505e-05, 1.2201e-05, 7.2255e-06, 6.9747e-06,\n",
       "             5.6848e-06, 5.6587e-06, 5.5841e-06, 3.6261e-06, 3.2292e-06, 2.4548e-06,\n",
       "             2.4416e-06, 2.1593e-06, 1.9839e-06, 1.8672e-06, 1.6805e-06, 1.3490e-06,\n",
       "             1.3197e-06, 1.2487e-06, 1.1499e-06, 1.1499e-06, 1.0262e-06, 1.0174e-06,\n",
       "             6.4327e-07, 6.1269e-07, 5.9379e-07, 5.5342e-07, 5.3835e-07, 5.2348e-07,\n",
       "             5.0125e-07, 3.8802e-07, 3.8780e-07, 3.1393e-07, 2.2876e-07, 2.0718e-07,\n",
       "             1.8050e-07, 1.5143e-07, 1.4242e-07, 1.4029e-07, 1.2957e-07, 9.7678e-08,\n",
       "             9.6642e-08, 9.2883e-08, 6.7276e-08, 5.7022e-08, 5.5618e-08, 5.0851e-08,\n",
       "             4.7183e-08, 4.5253e-08, 4.1970e-08, 4.0848e-08, 3.9362e-08, 3.8345e-08,\n",
       "             3.0957e-08, 2.7627e-08, 2.7496e-08, 2.6497e-08, 1.9845e-08, 1.7688e-08,\n",
       "             1.7344e-08, 1.6981e-08, 1.6278e-08, 1.6264e-08, 1.2383e-08, 1.2164e-08,\n",
       "             1.2005e-08, 1.1884e-08, 1.1129e-08, 9.5948e-09, 7.9753e-09, 6.3150e-09,\n",
       "             6.2451e-09, 5.3078e-09, 5.0197e-09, 3.3229e-09, 3.0949e-09, 2.8245e-09,\n",
       "             2.5422e-09, 2.1303e-09, 1.6813e-09, 1.6131e-09, 1.5603e-09, 1.3062e-09,\n",
       "             1.2539e-09, 1.1925e-09, 1.1503e-09, 9.7336e-10, 9.3987e-10, 8.3410e-10,\n",
       "             7.4847e-10, 7.0472e-10, 6.8478e-10, 6.6546e-10, 6.4704e-10, 5.9293e-10,\n",
       "             5.1954e-10, 5.1604e-10, 4.9321e-10, 4.7593e-10, 3.8548e-10, 3.4066e-10,\n",
       "             3.2813e-10, 3.0462e-10, 2.8342e-10, 2.6921e-10, 2.4916e-10, 2.4147e-10,\n",
       "             2.2007e-10, 1.7540e-10, 1.5564e-10, 1.1980e-10, 1.1883e-10, 6.2806e-11,\n",
       "             5.4755e-11, 5.2877e-11, 5.1174e-11, 4.7711e-11, 4.1144e-11, 4.1014e-11,\n",
       "             4.0614e-11, 3.1172e-11, 2.9304e-11, 2.8300e-11, 2.4753e-11, 1.9723e-11,\n",
       "             1.9681e-11, 1.9143e-11, 1.6715e-11, 1.5116e-11, 1.2795e-11, 1.0370e-11,\n",
       "             1.0113e-11, 9.8687e-12, 8.6615e-12, 7.6264e-12, 5.4488e-12, 4.6091e-12,\n",
       "             4.4513e-12, 4.0769e-12, 3.5615e-12, 2.9118e-12, 2.8379e-12, 2.6984e-12,\n",
       "             2.6681e-12, 2.5949e-12, 2.4565e-12, 2.4249e-12, 1.9007e-12, 1.6961e-12,\n",
       "             1.6767e-12, 1.5598e-12, 1.0883e-12, 9.7640e-13, 8.3941e-13, 8.0427e-13,\n",
       "             6.8360e-13, 6.3895e-13, 5.8586e-13, 2.8836e-13, 2.8520e-13, 1.5032e-13,\n",
       "             9.7554e-14, 9.2196e-14, 8.7452e-14, 8.4850e-14, 8.4839e-14, 6.8710e-14,\n",
       "             6.5212e-14, 6.2010e-14, 6.1670e-14, 5.1365e-14, 4.9966e-14, 4.8317e-14,\n",
       "             3.8208e-14, 3.6646e-14, 2.6163e-14, 2.5219e-14, 1.8087e-14, 1.7539e-14,\n",
       "             1.2126e-14, 1.2115e-14, 1.1689e-14, 1.0398e-14, 8.6621e-15, 7.6090e-15,\n",
       "             6.0257e-15, 5.5555e-15, 5.2393e-15, 4.9608e-15, 4.4371e-15, 3.9620e-15,\n",
       "             3.2477e-15, 2.1309e-15, 2.0552e-15, 1.6385e-15, 1.5309e-15, 1.4616e-15,\n",
       "             1.2701e-15, 7.6677e-16, 5.8988e-16, 4.9271e-16, 4.8618e-16, 4.6411e-16,\n",
       "             4.4265e-16, 3.1547e-16, 2.7249e-16, 2.1928e-16, 1.9539e-16, 1.7345e-16,\n",
       "             1.7270e-16, 1.7009e-16, 1.6924e-16, 1.2466e-16, 1.2366e-16, 8.8483e-17,\n",
       "             8.3273e-17, 5.8747e-17, 4.6490e-17, 4.1308e-17, 3.3506e-17, 2.7496e-17,\n",
       "             2.3214e-17, 1.7775e-17, 4.2948e-18, 3.6238e-18, 2.7307e-18, 2.1757e-18,\n",
       "             2.0221e-18, 1.3412e-18, 1.7796e-19, 1.5693e-19, 5.3715e-20, 4.1828e-20,\n",
       "             8.5080e-21, 3.3408e-21, 1.8983e-21, 7.2684e-22, 6.8294e-22, 7.5465e-23,\n",
       "             8.7096e-24, 1.8537e-25])}},\n",
       "   {'fpr': np.float64(0.32608695652173914),\n",
       "    'tpr': np.float64(0.9976608187134502),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.1056, 0.1087, 0.1149, 0.1211, 0.1242, 0.1304, 0.1304, 0.1335,\n",
       "             0.1366, 0.1366, 0.1366, 0.1398, 0.1429, 0.1460, 0.1491, 0.1522, 0.1553,\n",
       "             0.1584, 0.1615, 0.1646, 0.1677, 0.1708, 0.1739, 0.1770, 0.1801, 0.1832,\n",
       "             0.1863, 0.1894, 0.1925, 0.1957, 0.1988, 0.2019, 0.2050, 0.2081, 0.2112,\n",
       "             0.2143, 0.2174, 0.2205, 0.2205, 0.2236, 0.2267, 0.2298, 0.2329, 0.2360,\n",
       "             0.2391, 0.2422, 0.2453, 0.2453, 0.2484, 0.2516, 0.2547, 0.2578, 0.2609,\n",
       "             0.2640, 0.2640, 0.2671, 0.2671, 0.2702, 0.2733, 0.2764, 0.2795, 0.2826,\n",
       "             0.2857, 0.2888, 0.2919, 0.2950, 0.2981, 0.3012, 0.3043, 0.3075, 0.3106,\n",
       "             0.3137, 0.3168, 0.3199, 0.3230, 0.3261, 0.3261, 0.3292, 0.3323, 0.3354,\n",
       "             0.3385, 0.3416, 0.3447, 0.3478, 0.3509, 0.3509, 0.3540, 0.3571, 0.3602,\n",
       "             0.3634, 0.3665, 0.3696, 0.3727, 0.3758, 0.3789, 0.3820, 0.3851, 0.3882,\n",
       "             0.3913, 0.3944, 0.3975, 0.4006, 0.4037, 0.4068, 0.4099, 0.4130, 0.4161,\n",
       "             0.4193, 0.4224, 0.4255, 0.4286, 0.4317, 0.4348, 0.4379, 0.4410, 0.4441,\n",
       "             0.4472, 0.4503, 0.4534, 0.4565, 0.4596, 0.4627, 0.4658, 0.4689, 0.4720,\n",
       "             0.4752, 0.4783, 0.4814, 0.4845, 0.4876, 0.4907, 0.4938, 0.4969, 0.5000,\n",
       "             0.5031, 0.5062, 0.5093, 0.5124, 0.5155, 0.5186, 0.5217, 0.5248, 0.5280,\n",
       "             0.5311, 0.5342, 0.5373, 0.5404, 0.5404, 0.5435, 0.5466, 0.5497, 0.5528,\n",
       "             0.5559, 0.5590, 0.5621, 0.5652, 0.5683, 0.5714, 0.5745, 0.5776, 0.5807,\n",
       "             0.5839, 0.5870, 0.5901, 0.5932, 0.5963, 0.5994, 0.6025, 0.6056, 0.6087,\n",
       "             0.6118, 0.6149, 0.6180, 0.6211, 0.6242, 0.6273, 0.6304, 0.6335, 0.6366,\n",
       "             0.6398, 0.6429, 0.6460, 0.6491, 0.6522, 0.6553, 0.6584, 0.6615, 0.6646,\n",
       "             0.6677, 0.6708, 0.6739, 0.6770, 0.6801, 0.6832, 0.6863, 0.6894, 0.6925,\n",
       "             0.6957, 0.6988, 0.7019, 0.7050, 0.7081, 0.7112, 0.7143, 0.7174, 0.7205,\n",
       "             0.7236, 0.7267, 0.7298, 0.7329, 0.7360, 0.7391, 0.7422, 0.7453, 0.7484,\n",
       "             0.7516, 0.7547, 0.7578, 0.7609, 0.7640, 0.7671, 0.7702, 0.7733, 0.7764,\n",
       "             0.7795, 0.7826, 0.7857, 0.7888, 0.7919, 0.7950, 0.7981, 0.8012, 0.8043,\n",
       "             0.8075, 0.8106, 0.8137, 0.8168, 0.8199, 0.8230, 0.8261, 0.8292, 0.8323,\n",
       "             0.8354, 0.8385, 0.8416, 0.8447, 0.8478, 0.8509, 0.8540, 0.8571, 0.8602,\n",
       "             0.8634, 0.8665, 0.8696, 0.8727, 0.8758, 0.8789, 0.8820, 0.8851, 0.8882,\n",
       "             0.8913, 0.8944, 0.8975, 0.9006, 0.9037, 0.9068, 0.9099, 0.9130, 0.9161,\n",
       "             0.9193, 0.9224, 0.9255, 0.9286, 0.9317, 0.9348, 0.9379, 0.9410, 0.9441,\n",
       "             0.9472, 0.9503, 0.9534, 0.9565, 0.9596, 0.9627, 0.9658, 0.9689, 0.9720,\n",
       "             0.9752, 0.9783, 0.9814, 0.9845, 0.9876, 0.9907, 0.9938, 0.9969, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9848, 0.9871, 0.9883, 0.9883, 0.9883, 0.9883, 0.9895, 0.9895,\n",
       "             0.9895, 0.9906, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918,\n",
       "             0.9918, 0.9918, 0.9918, 0.9930, 0.9930, 0.9930, 0.9930, 0.9930, 0.9930,\n",
       "             0.9930, 0.9930, 0.9930, 0.9942, 0.9942, 0.9942, 0.9942, 0.9942, 0.9942,\n",
       "             0.9942, 0.9953, 0.9953, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965,\n",
       "             0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988, 0.9988,\n",
       "             0.9988, 0.9988, 0.9988, 0.9988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9988e-01, 9.9987e-01, 9.9976e-01, 9.9973e-01, 9.9971e-01,\n",
       "             9.9961e-01, 9.9959e-01, 9.9948e-01, 9.9941e-01, 9.9935e-01, 9.9935e-01,\n",
       "             9.9922e-01, 9.9874e-01, 9.9873e-01, 9.9851e-01, 9.9800e-01, 9.9770e-01,\n",
       "             9.9765e-01, 9.9651e-01, 9.9341e-01, 9.9041e-01, 9.9003e-01, 9.8966e-01,\n",
       "             9.8871e-01, 9.8835e-01, 9.8764e-01, 9.8506e-01, 9.8431e-01, 9.7369e-01,\n",
       "             9.7185e-01, 9.7052e-01, 9.6633e-01, 9.6224e-01, 9.3711e-01, 9.1429e-01,\n",
       "             9.1396e-01, 9.0290e-01, 8.9741e-01, 8.7448e-01, 8.4321e-01, 8.1718e-01,\n",
       "             7.1689e-01, 6.9231e-01, 6.8093e-01, 6.5880e-01, 6.4598e-01, 6.3359e-01,\n",
       "             6.3015e-01, 6.1000e-01, 5.8041e-01, 5.7097e-01, 5.5384e-01, 5.1107e-01,\n",
       "             4.7310e-01, 4.6307e-01, 3.9370e-01, 3.4249e-01, 2.4926e-01, 1.2969e-01,\n",
       "             1.1653e-01, 1.1503e-01, 1.0350e-01, 8.8735e-02, 8.2611e-02, 5.8413e-02,\n",
       "             5.6214e-02, 5.5117e-02, 3.7596e-02, 3.1057e-02, 3.0085e-02, 2.1304e-02,\n",
       "             1.8983e-02, 1.4684e-02, 1.3920e-02, 1.3721e-02, 1.1626e-02, 9.5057e-03,\n",
       "             8.8799e-03, 7.9458e-03, 6.0053e-03, 5.2737e-03, 5.2701e-03, 4.5554e-03,\n",
       "             2.9964e-03, 2.6346e-03, 2.3433e-03, 1.7318e-03, 1.5242e-03, 1.5004e-03,\n",
       "             1.4885e-03, 8.6824e-04, 7.8189e-04, 5.9473e-04, 4.6227e-04, 4.3505e-04,\n",
       "             3.9865e-04, 3.1952e-04, 2.7376e-04, 2.1427e-04, 1.9301e-04, 1.5010e-04,\n",
       "             1.2737e-04, 9.9608e-05, 7.1173e-05, 6.5225e-05, 6.4852e-05, 3.7387e-05,\n",
       "             3.3503e-05, 2.4758e-05, 2.3590e-05, 2.2455e-05, 2.1017e-05, 1.9796e-05,\n",
       "             1.5583e-05, 1.2649e-05, 1.2154e-05, 1.0929e-05, 1.0700e-05, 9.4701e-06,\n",
       "             9.3337e-06, 8.1069e-06, 7.7053e-06, 6.9737e-06, 5.9994e-06, 5.7345e-06,\n",
       "             5.6452e-06, 5.4064e-06, 5.1242e-06, 4.9790e-06, 4.0469e-06, 3.7948e-06,\n",
       "             3.4882e-06, 2.6549e-06, 2.5634e-06, 2.4011e-06, 2.0602e-06, 1.5935e-06,\n",
       "             1.4896e-06, 1.4785e-06, 9.7481e-07, 8.4032e-07, 8.2330e-07, 6.1266e-07,\n",
       "             3.2305e-07, 2.7316e-07, 2.7240e-07, 2.0246e-07, 1.7810e-07, 1.5911e-07,\n",
       "             1.5465e-07, 1.3565e-07, 1.1472e-07, 9.8920e-08, 8.1445e-08, 6.4686e-08,\n",
       "             5.6957e-08, 5.1807e-08, 4.2411e-08, 2.9935e-08, 2.9387e-08, 2.6815e-08,\n",
       "             2.1540e-08, 1.7808e-08, 1.7793e-08, 1.6605e-08, 1.5588e-08, 1.4327e-08,\n",
       "             1.4228e-08, 1.4140e-08, 1.3829e-08, 1.3133e-08, 9.2442e-09, 8.2227e-09,\n",
       "             8.0394e-09, 5.7966e-09, 5.7457e-09, 5.5544e-09, 4.7366e-09, 4.7359e-09,\n",
       "             4.6315e-09, 4.4006e-09, 4.2763e-09, 4.0770e-09, 3.4945e-09, 2.5141e-09,\n",
       "             2.4433e-09, 2.2627e-09, 2.0880e-09, 1.7019e-09, 1.0217e-09, 9.3294e-10,\n",
       "             8.8528e-10, 8.3056e-10, 5.9552e-10, 5.3968e-10, 5.2002e-10, 5.0427e-10,\n",
       "             4.2780e-10, 3.6892e-10, 3.5474e-10, 3.0384e-10, 3.0348e-10, 2.2387e-10,\n",
       "             1.7521e-10, 1.4499e-10, 1.4389e-10, 1.3990e-10, 1.2617e-10, 1.0779e-10,\n",
       "             9.6119e-11, 9.3841e-11, 9.3238e-11, 5.9814e-11, 3.7590e-11, 1.9130e-11,\n",
       "             1.2878e-11, 1.1631e-11, 1.0759e-11, 1.0743e-11, 9.1412e-12, 7.5731e-12,\n",
       "             7.2839e-12, 2.9458e-12, 2.8052e-12, 2.2655e-12, 1.3693e-12, 1.1810e-12,\n",
       "             9.0490e-13, 8.8672e-13, 7.0597e-13, 4.9461e-13, 4.6003e-13, 4.1339e-13,\n",
       "             3.6748e-13, 3.3477e-13, 2.7632e-13, 2.7431e-13, 2.4528e-13, 1.5832e-13,\n",
       "             1.4975e-13, 1.3941e-13, 7.4367e-14, 7.2630e-14, 6.9017e-14, 6.5100e-14,\n",
       "             5.5042e-14, 4.1141e-14, 3.3974e-14, 3.1612e-14, 1.8090e-14, 9.3151e-15,\n",
       "             7.8049e-15, 2.1157e-15, 1.9628e-15, 1.2713e-15, 1.0910e-15, 3.3358e-16,\n",
       "             2.4268e-16, 2.2475e-16, 1.4497e-16, 1.3840e-16, 1.0285e-16, 4.3161e-17,\n",
       "             3.7137e-17, 2.1802e-17, 1.4141e-17, 4.7474e-18, 2.2913e-18, 2.3045e-19,\n",
       "             6.4051e-20, 4.2582e-20, 2.7734e-23])}}],\n",
       "  [{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.005747126436781609),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 6.6341e-01, 6.4758e-01,  ..., 1.0260e-07, 1.8032e-08,\n",
       "             8.5904e-09])}},\n",
       "   {'fpr': np.float64(0.006514657980456026),\n",
       "    'tpr': np.float64(0.8528735632183908),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9993e-01, 9.9969e-01,  ..., 1.8457e-06, 1.7331e-06,\n",
       "             1.4368e-06])}},\n",
       "   {'fpr': np.float64(0.02280130293159609),\n",
       "    'tpr': np.float64(0.9091954022988505),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 9.9998e-01,  ..., 1.2266e-07, 9.7331e-08,\n",
       "             4.0143e-08])}},\n",
       "   {'fpr': np.float64(0.04234527687296417),\n",
       "    'tpr': np.float64(0.9517241379310345),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9989e-01,  ..., 2.3299e-07, 2.0987e-07,\n",
       "             7.9773e-08])}},\n",
       "   {'fpr': np.float64(0.03257328990228013),\n",
       "    'tpr': np.float64(0.9620689655172414),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9998e-01, 9.9991e-01,  ..., 3.3779e-07, 1.3030e-07,\n",
       "             1.9506e-08])}},\n",
       "   {'fpr': np.float64(0.03908794788273615),\n",
       "    'tpr': np.float64(0.9712643678160919),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.8149e-09, 7.8341e-10,\n",
       "             6.2051e-11])}},\n",
       "   {'fpr': np.float64(0.016286644951140065),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9959e-01, 9.9895e-01,  ..., 1.5764e-09, 1.3717e-09,\n",
       "             8.6240e-12])}},\n",
       "   {'fpr': np.float64(0.06188925081433225),\n",
       "    'tpr': np.float64(0.9770114942528736),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9996e-01, 9.9995e-01,  ..., 3.2982e-08, 2.1384e-08,\n",
       "             4.5861e-10])}},\n",
       "   {'fpr': np.float64(0.07166123778501629),\n",
       "    'tpr': np.float64(0.9862068965517241),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9998e-01, 9.9997e-01,  ..., 1.5995e-09, 5.1251e-10,\n",
       "             1.5140e-12])}},\n",
       "   {'fpr': np.float64(0.05863192182410423),\n",
       "    'tpr': np.float64(0.9816091954022989),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9999e-01, 9.9999e-01,  ..., 1.0564e-11, 9.0585e-14,\n",
       "             6.5259e-16])}},\n",
       "   {'fpr': np.float64(0.08794788273615635),\n",
       "    'tpr': np.float64(0.9862068965517241),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0130, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
       "             0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
       "             0.0163, 0.0163, 0.0195, 0.0195, 0.0228, 0.0228, 0.0228, 0.0228, 0.0261,\n",
       "             0.0293, 0.0293, 0.0326, 0.0326, 0.0358, 0.0358, 0.0391, 0.0423, 0.0456,\n",
       "             0.0489, 0.0489, 0.0489, 0.0521, 0.0554, 0.0586, 0.0586, 0.0586, 0.0586,\n",
       "             0.0619, 0.0619, 0.0619, 0.0651, 0.0651, 0.0684, 0.0684, 0.0684, 0.0717,\n",
       "             0.0717, 0.0717, 0.0749, 0.0749, 0.0782, 0.0814, 0.0847, 0.0879, 0.0879,\n",
       "             0.0912, 0.0912, 0.0945, 0.0945, 0.0977, 0.1010, 0.1042, 0.1075, 0.1107,\n",
       "             0.1140, 0.1173, 0.1173, 0.1205, 0.1238, 0.1238, 0.1270, 0.1303, 0.1336,\n",
       "             0.1368, 0.1401, 0.1401, 0.1433, 0.1466, 0.1498, 0.1531, 0.1564, 0.1596,\n",
       "             0.1629, 0.1629, 0.1661, 0.1694, 0.1726, 0.1759, 0.1792, 0.1824, 0.1857,\n",
       "             0.1889, 0.1922, 0.1954, 0.1954, 0.1987, 0.2020, 0.2052, 0.2085, 0.2117,\n",
       "             0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345, 0.2378,\n",
       "             0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2606, 0.2638, 0.2671,\n",
       "             0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899, 0.2932,\n",
       "             0.2964, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160, 0.3192, 0.3225,\n",
       "             0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453, 0.3485, 0.3518,\n",
       "             0.3550, 0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779, 0.3811,\n",
       "             0.3844, 0.3876, 0.3909, 0.3941, 0.3974, 0.4007, 0.4039, 0.4072, 0.4072,\n",
       "             0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267, 0.4300, 0.4332, 0.4365,\n",
       "             0.4397, 0.4430, 0.4463, 0.4495, 0.4528, 0.4560, 0.4593, 0.4625, 0.4658,\n",
       "             0.4691, 0.4723, 0.4756, 0.4788, 0.4821, 0.4853, 0.4853, 0.4886, 0.4919,\n",
       "             0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114, 0.5147, 0.5179, 0.5212,\n",
       "             0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407, 0.5440, 0.5472, 0.5505,\n",
       "             0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700, 0.5733, 0.5765, 0.5798,\n",
       "             0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5993, 0.6026, 0.6059, 0.6091,\n",
       "             0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319, 0.6352, 0.6384,\n",
       "             0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612, 0.6645, 0.6678,\n",
       "             0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906, 0.6938, 0.6971,\n",
       "             0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199, 0.7231, 0.7264,\n",
       "             0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492, 0.7524, 0.7557,\n",
       "             0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785, 0.7818, 0.7850,\n",
       "             0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078, 0.8111, 0.8143,\n",
       "             0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371, 0.8404, 0.8436,\n",
       "             0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664, 0.8697, 0.8730,\n",
       "             0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958, 0.8990, 0.9023,\n",
       "             0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251, 0.9283, 0.9316,\n",
       "             0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544, 0.9577, 0.9609,\n",
       "             0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837, 0.9870, 0.9902,\n",
       "             0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0080, 0.0299, 0.0483, 0.0598, 0.0747, 0.0862, 0.0989, 0.1046,\n",
       "             0.1115, 0.1184, 0.1241, 0.1356, 0.1391, 0.1506, 0.1586, 0.1621, 0.1667,\n",
       "             0.1759, 0.1805, 0.1874, 0.1943, 0.1989, 0.2011, 0.2057, 0.2080, 0.2103,\n",
       "             0.2149, 0.2218, 0.2310, 0.2333, 0.2379, 0.2425, 0.2448, 0.2483, 0.2529,\n",
       "             0.2575, 0.2609, 0.2632, 0.2644, 0.2678, 0.2713, 0.2747, 0.2770, 0.2816,\n",
       "             0.2874, 0.2908, 0.2943, 0.3023, 0.3057, 0.3080, 0.3138, 0.3172, 0.3184,\n",
       "             0.3195, 0.3241, 0.3322, 0.3345, 0.3356, 0.3414, 0.3448, 0.3460, 0.3483,\n",
       "             0.3494, 0.3506, 0.3540, 0.3575, 0.3598, 0.3621, 0.3678, 0.3690, 0.3713,\n",
       "             0.3736, 0.3747, 0.3770, 0.3793, 0.3816, 0.3828, 0.3839, 0.3897, 0.3931,\n",
       "             0.3943, 0.3966, 0.3989, 0.4023, 0.4057, 0.4092, 0.4126, 0.4138, 0.4161,\n",
       "             0.4184, 0.4195, 0.4230, 0.4264, 0.4299, 0.4322, 0.4333, 0.4345, 0.4379,\n",
       "             0.4402, 0.4414, 0.4425, 0.4448, 0.4460, 0.4494, 0.4506, 0.4517, 0.4540,\n",
       "             0.4575, 0.4598, 0.4621, 0.4655, 0.4701, 0.4724, 0.4736, 0.4747, 0.4759,\n",
       "             0.4793, 0.4805, 0.4828, 0.4839, 0.4851, 0.4874, 0.4885, 0.4897, 0.4908,\n",
       "             0.4920, 0.4966, 0.5000, 0.5011, 0.5023, 0.5034, 0.5069, 0.5080, 0.5103,\n",
       "             0.5149, 0.5172, 0.5195, 0.5207, 0.5218, 0.5276, 0.5287, 0.5299, 0.5310,\n",
       "             0.5322, 0.5356, 0.5368, 0.5379, 0.5414, 0.5437, 0.5448, 0.5471, 0.5506,\n",
       "             0.5517, 0.5529, 0.5540, 0.5563, 0.5575, 0.5598, 0.5609, 0.5621, 0.5632,\n",
       "             0.5644, 0.5655, 0.5667, 0.5690, 0.5713, 0.5724, 0.5747, 0.5759, 0.5770,\n",
       "             0.5793, 0.5805, 0.5816, 0.5828, 0.5839, 0.5851, 0.5862, 0.5874, 0.5885,\n",
       "             0.5897, 0.5908, 0.5920, 0.5943, 0.5954, 0.5966, 0.5977, 0.6000, 0.6011,\n",
       "             0.6023, 0.6034, 0.6046, 0.6057, 0.6092, 0.6103, 0.6115, 0.6126, 0.6138,\n",
       "             0.6149, 0.6161, 0.6172, 0.6184, 0.6195, 0.6207, 0.6218, 0.6241, 0.6253,\n",
       "             0.6276, 0.6287, 0.6299, 0.6310, 0.6322, 0.6333, 0.6345, 0.6356, 0.6368,\n",
       "             0.6379, 0.6391, 0.6402, 0.6414, 0.6437, 0.6448, 0.6460, 0.6471, 0.6483,\n",
       "             0.6494, 0.6506, 0.6517, 0.6529, 0.6540, 0.6552, 0.6563, 0.6575, 0.6598,\n",
       "             0.6609, 0.6621, 0.6632, 0.6644, 0.6655, 0.6667, 0.6678, 0.6690, 0.6701,\n",
       "             0.6713, 0.6724, 0.6736, 0.6759, 0.6770, 0.6782, 0.6793, 0.6805, 0.6828,\n",
       "             0.6839, 0.6851, 0.6862, 0.6874, 0.6885, 0.6897, 0.6908, 0.6931, 0.6943,\n",
       "             0.6954, 0.6966, 0.6977, 0.6989, 0.7000, 0.7011, 0.7023, 0.7034, 0.7046,\n",
       "             0.7057, 0.7069, 0.7092, 0.7103, 0.7115, 0.7149, 0.7161, 0.7172, 0.7184,\n",
       "             0.7195, 0.7207, 0.7218, 0.7230, 0.7241, 0.7253, 0.7264, 0.7276, 0.7287,\n",
       "             0.7299, 0.7310, 0.7333, 0.7345, 0.7356, 0.7368, 0.7379, 0.7391, 0.7402,\n",
       "             0.7414, 0.7425, 0.7437, 0.7448, 0.7471, 0.7483, 0.7506, 0.7517, 0.7529,\n",
       "             0.7540, 0.7552, 0.7563, 0.7575, 0.7586, 0.7598, 0.7609, 0.7621, 0.7632,\n",
       "             0.7644, 0.7667, 0.7678, 0.7690, 0.7701, 0.7713, 0.7724, 0.7736, 0.7747,\n",
       "             0.7770, 0.7782, 0.7793, 0.7816, 0.7828, 0.7839, 0.7851, 0.7862, 0.7874,\n",
       "             0.7885, 0.7897, 0.7908, 0.7920, 0.7931, 0.7943, 0.7954, 0.7966, 0.7977,\n",
       "             0.7989, 0.8011, 0.8023, 0.8034, 0.8046, 0.8057, 0.8069, 0.8080, 0.8092,\n",
       "             0.8103, 0.8115, 0.8126, 0.8126, 0.8138, 0.8149, 0.8161, 0.8172, 0.8184,\n",
       "             0.8195, 0.8207, 0.8218, 0.8230, 0.8241, 0.8253, 0.8264, 0.8276, 0.8287,\n",
       "             0.8299, 0.8310, 0.8322, 0.8333, 0.8345, 0.8356, 0.8368, 0.8379, 0.8391,\n",
       "             0.8402, 0.8414, 0.8425, 0.8437, 0.8437, 0.8448, 0.8460, 0.8471, 0.8483,\n",
       "             0.8494, 0.8506, 0.8517, 0.8529, 0.8540, 0.8552, 0.8563, 0.8575, 0.8586,\n",
       "             0.8598, 0.8609, 0.8632, 0.8644, 0.8655, 0.8667, 0.8678, 0.8690, 0.8701,\n",
       "             0.8713, 0.8724, 0.8736, 0.8747, 0.8759, 0.8770, 0.8782, 0.8793, 0.8805,\n",
       "             0.8816, 0.8828, 0.8839, 0.8851, 0.8862, 0.8874, 0.8885, 0.8897, 0.8908,\n",
       "             0.8920, 0.8931, 0.8943, 0.8954, 0.8966, 0.8977, 0.8989, 0.9000, 0.9011,\n",
       "             0.9023, 0.9034, 0.9046, 0.9057, 0.9069, 0.9080, 0.9092, 0.9103, 0.9115,\n",
       "             0.9126, 0.9138, 0.9149, 0.9161, 0.9172, 0.9184, 0.9195, 0.9207, 0.9218,\n",
       "             0.9230, 0.9241, 0.9253, 0.9264, 0.9276, 0.9287, 0.9299, 0.9310, 0.9322,\n",
       "             0.9333, 0.9345, 0.9356, 0.9368, 0.9379, 0.9391, 0.9402, 0.9414, 0.9425,\n",
       "             0.9425, 0.9425, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483, 0.9494, 0.9506,\n",
       "             0.9517, 0.9529, 0.9540, 0.9552, 0.9563, 0.9575, 0.9586, 0.9598, 0.9609,\n",
       "             0.9621, 0.9632, 0.9632, 0.9644, 0.9644, 0.9655, 0.9667, 0.9678, 0.9678,\n",
       "             0.9678, 0.9690, 0.9690, 0.9701, 0.9701, 0.9713, 0.9713, 0.9713, 0.9713,\n",
       "             0.9713, 0.9724, 0.9736, 0.9736, 0.9736, 0.9736, 0.9747, 0.9759, 0.9770,\n",
       "             0.9770, 0.9782, 0.9793, 0.9793, 0.9805, 0.9805, 0.9816, 0.9828, 0.9828,\n",
       "             0.9839, 0.9851, 0.9851, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9874,\n",
       "             0.9874, 0.9885, 0.9885, 0.9897, 0.9897, 0.9897, 0.9897, 0.9897, 0.9897,\n",
       "             0.9897, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9968e-01, 9.9965e-01,\n",
       "             9.9965e-01, 9.9965e-01, 9.9965e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01,\n",
       "             9.9962e-01, 9.9960e-01, 9.9957e-01, 9.9955e-01, 9.9952e-01, 9.9950e-01,\n",
       "             9.9950e-01, 9.9949e-01, 9.9944e-01, 9.9943e-01, 9.9943e-01, 9.9941e-01,\n",
       "             9.9935e-01, 9.9935e-01, 9.9933e-01, 9.9933e-01, 9.9932e-01, 9.9929e-01,\n",
       "             9.9923e-01, 9.9920e-01, 9.9919e-01, 9.9917e-01, 9.9915e-01, 9.9904e-01,\n",
       "             9.9903e-01, 9.9902e-01, 9.9898e-01, 9.9898e-01, 9.9892e-01, 9.9891e-01,\n",
       "             9.9884e-01, 9.9883e-01, 9.9880e-01, 9.9870e-01, 9.9857e-01, 9.9843e-01,\n",
       "             9.9843e-01, 9.9819e-01, 9.9818e-01, 9.9816e-01, 9.9812e-01, 9.9811e-01,\n",
       "             9.9773e-01, 9.9770e-01, 9.9765e-01, 9.9763e-01, 9.9750e-01, 9.9730e-01,\n",
       "             9.9665e-01, 9.9621e-01, 9.9583e-01, 9.9568e-01, 9.9556e-01, 9.9553e-01,\n",
       "             9.9539e-01, 9.9498e-01, 9.9419e-01, 9.9394e-01, 9.9387e-01, 9.9363e-01,\n",
       "             9.9358e-01, 9.9282e-01, 9.9269e-01, 9.8922e-01, 9.8901e-01, 9.8709e-01,\n",
       "             9.8570e-01, 9.8372e-01, 9.8227e-01, 9.7905e-01, 9.7399e-01, 9.7332e-01,\n",
       "             9.7199e-01, 9.7134e-01, 9.6883e-01, 9.5658e-01, 9.5282e-01, 9.5203e-01,\n",
       "             9.5033e-01, 9.4726e-01, 9.4609e-01, 9.3831e-01, 9.3820e-01, 9.0950e-01,\n",
       "             9.0740e-01, 9.0683e-01, 9.0468e-01, 9.0239e-01, 9.0064e-01, 8.9594e-01,\n",
       "             8.8067e-01, 8.6048e-01, 8.4932e-01, 8.4895e-01, 8.4470e-01, 8.4183e-01,\n",
       "             8.4090e-01, 8.2322e-01, 8.2298e-01, 8.1869e-01, 7.9267e-01, 7.7822e-01,\n",
       "             7.5022e-01, 7.0970e-01, 6.8094e-01, 6.5522e-01, 6.3982e-01, 6.3887e-01,\n",
       "             5.9598e-01, 5.8128e-01, 5.7190e-01, 5.6596e-01, 5.5240e-01, 5.2526e-01,\n",
       "             5.2326e-01, 5.0350e-01, 4.8105e-01, 4.6043e-01, 4.4914e-01, 4.2718e-01,\n",
       "             4.2353e-01, 3.3186e-01, 3.2890e-01, 2.9456e-01, 2.9293e-01, 2.6535e-01,\n",
       "             2.6519e-01, 2.4431e-01, 2.3305e-01, 2.1277e-01, 2.0407e-01, 1.7358e-01,\n",
       "             1.6478e-01, 1.3434e-01, 1.0434e-01, 9.3784e-02, 9.3211e-02, 6.9953e-02,\n",
       "             6.4321e-02, 5.9418e-02, 5.0855e-02, 3.9550e-02, 3.9306e-02, 3.8529e-02,\n",
       "             2.8798e-02, 2.0844e-02, 2.0039e-02, 1.8982e-02, 1.6152e-02, 1.4864e-02,\n",
       "             1.4815e-02, 1.4468e-02, 1.4203e-02, 1.3499e-02, 1.2925e-02, 1.1250e-02,\n",
       "             1.1212e-02, 8.2794e-03, 7.8332e-03, 6.4686e-03, 5.8510e-03, 5.4187e-03,\n",
       "             5.1389e-03, 4.9220e-03, 4.8021e-03, 4.7165e-03, 4.3060e-03, 3.9611e-03,\n",
       "             3.7490e-03, 3.6051e-03, 3.3285e-03, 3.0394e-03, 2.8644e-03, 2.5307e-03,\n",
       "             2.2977e-03, 1.9712e-03, 1.8854e-03, 1.8735e-03, 1.8061e-03, 1.6939e-03,\n",
       "             1.5841e-03, 1.5164e-03, 1.4572e-03, 1.2735e-03, 1.1581e-03, 1.0796e-03,\n",
       "             9.0568e-04, 8.9472e-04, 8.4538e-04, 8.0034e-04, 7.3117e-04, 7.0359e-04,\n",
       "             6.2964e-04, 5.3917e-04, 4.9778e-04, 4.9354e-04, 4.2015e-04, 4.0565e-04,\n",
       "             3.9730e-04, 3.7460e-04, 3.6964e-04, 3.0380e-04, 2.6928e-04, 2.6096e-04,\n",
       "             2.4296e-04, 2.3559e-04, 2.3396e-04, 2.0434e-04, 2.0253e-04, 1.9602e-04,\n",
       "             1.8266e-04, 1.5106e-04, 1.5068e-04, 1.4721e-04, 1.4487e-04, 1.3582e-04,\n",
       "             1.3010e-04, 1.0812e-04, 1.0210e-04, 9.9881e-05, 9.2780e-05, 9.0828e-05,\n",
       "             5.5168e-05, 5.4297e-05, 5.1277e-05, 4.8369e-05, 3.8687e-05, 3.6103e-05,\n",
       "             3.4989e-05, 3.2002e-05, 2.9512e-05, 2.9228e-05, 2.9197e-05, 2.4989e-05,\n",
       "             2.2114e-05, 2.1645e-05, 2.1273e-05, 1.9667e-05, 1.9601e-05, 1.7037e-05,\n",
       "             1.6835e-05, 1.6195e-05, 1.5827e-05, 1.4139e-05, 1.3979e-05, 1.2451e-05,\n",
       "             1.2068e-05, 1.1532e-05, 1.0920e-05, 1.0163e-05, 9.5923e-06, 8.4884e-06,\n",
       "             8.2577e-06, 8.1191e-06, 7.5312e-06, 6.8969e-06, 5.4802e-06, 5.3313e-06,\n",
       "             4.6125e-06, 4.4836e-06, 4.3343e-06, 4.0860e-06, 3.9145e-06, 3.8564e-06,\n",
       "             3.2821e-06, 3.0162e-06, 2.6952e-06, 2.5683e-06, 2.4746e-06, 2.4252e-06,\n",
       "             2.3089e-06, 2.2232e-06, 2.1977e-06, 2.1027e-06, 2.0070e-06, 1.7779e-06,\n",
       "             1.5348e-06, 1.5162e-06, 1.2301e-06, 1.1961e-06, 1.1494e-06, 1.0960e-06,\n",
       "             9.9898e-07, 7.8528e-07, 7.8287e-07, 7.6818e-07, 7.6231e-07, 7.3440e-07,\n",
       "             7.2557e-07, 6.5241e-07, 5.3437e-07, 5.1865e-07, 5.1723e-07, 4.9314e-07,\n",
       "             4.8839e-07, 4.1197e-07, 3.8418e-07, 3.8016e-07, 3.6283e-07, 2.9303e-07,\n",
       "             2.4306e-07, 1.9523e-07, 1.7260e-07, 1.6365e-07, 1.6365e-07, 1.5656e-07,\n",
       "             1.5430e-07, 1.0911e-07, 1.0609e-07, 1.0337e-07, 7.7484e-08, 6.3685e-08,\n",
       "             6.1099e-08, 5.8518e-08, 5.7172e-08, 5.5387e-08, 5.5380e-08, 5.4188e-08,\n",
       "             5.1931e-08, 2.2741e-08, 2.2692e-08, 2.2675e-08, 2.1699e-08, 1.5399e-08,\n",
       "             1.4894e-08, 1.4719e-08, 1.3455e-08, 1.3261e-08, 1.1600e-08, 1.1580e-08,\n",
       "             9.2946e-09, 7.6039e-09, 7.5820e-09, 6.9398e-09, 6.2841e-09, 6.2553e-09,\n",
       "             6.1933e-09, 5.7231e-09, 5.5901e-09, 5.5242e-09, 5.5213e-09, 4.6268e-09,\n",
       "             4.1640e-09, 3.9599e-09, 3.1197e-09, 3.0882e-09, 2.7754e-09, 2.6787e-09,\n",
       "             2.5860e-09, 2.3616e-09, 1.8200e-09, 1.7108e-09, 1.5087e-09, 1.3598e-09,\n",
       "             1.1359e-09, 9.9370e-10, 3.2699e-10, 3.2549e-10, 2.8960e-10, 2.4566e-10,\n",
       "             2.3952e-10, 2.3266e-10, 2.2448e-10, 2.2055e-10, 2.1543e-10, 2.0714e-10,\n",
       "             1.9374e-10, 1.5197e-10, 1.3791e-10, 9.0903e-11, 8.1619e-11, 7.9870e-11,\n",
       "             6.8205e-11, 4.1550e-11, 3.9774e-11, 3.6577e-11, 2.9590e-11, 2.5180e-11,\n",
       "             1.8480e-11, 1.0701e-11, 6.2669e-12, 6.1410e-12, 5.9147e-12, 5.2881e-12,\n",
       "             4.6165e-12, 4.3445e-12, 2.4902e-12, 2.3470e-12, 1.7215e-12, 1.5485e-12,\n",
       "             8.8624e-13, 5.8804e-13, 4.8832e-13, 4.2099e-13, 3.2063e-13, 2.7400e-13,\n",
       "             2.4604e-13, 1.6772e-13, 1.1842e-13, 9.8197e-14, 7.7500e-14, 2.7754e-14,\n",
       "             1.9097e-14, 1.1242e-14, 4.2806e-15, 9.3274e-17, 1.2610e-17, 9.0356e-23])}},\n",
       "   {'fpr': np.float64(0.11400651465798045),\n",
       "    'tpr': np.float64(0.993103448275862),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0163, 0.0163, 0.0163, 0.0195, 0.0195, 0.0195, 0.0195, 0.0228, 0.0228,\n",
       "             0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228,\n",
       "             0.0228, 0.0228, 0.0228, 0.0261, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293,\n",
       "             0.0293, 0.0293, 0.0293, 0.0293, 0.0326, 0.0358, 0.0358, 0.0358, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0423, 0.0456, 0.0456, 0.0456, 0.0456, 0.0489,\n",
       "             0.0489, 0.0489, 0.0521, 0.0554, 0.0586, 0.0619, 0.0619, 0.0651, 0.0684,\n",
       "             0.0717, 0.0749, 0.0782, 0.0814, 0.0847, 0.0879, 0.0912, 0.0945, 0.0977,\n",
       "             0.0977, 0.1010, 0.1010, 0.1010, 0.1042, 0.1075, 0.1107, 0.1140, 0.1140,\n",
       "             0.1173, 0.1205, 0.1238, 0.1270, 0.1303, 0.1336, 0.1368, 0.1401, 0.1433,\n",
       "             0.1466, 0.1466, 0.1498, 0.1531, 0.1564, 0.1596, 0.1629, 0.1661, 0.1694,\n",
       "             0.1726, 0.1759, 0.1792, 0.1824, 0.1857, 0.1889, 0.1922, 0.1922, 0.1954,\n",
       "             0.1987, 0.2020, 0.2052, 0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2248,\n",
       "             0.2280, 0.2313, 0.2345, 0.2378, 0.2410, 0.2443, 0.2476, 0.2476, 0.2508,\n",
       "             0.2541, 0.2573, 0.2606, 0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801,\n",
       "             0.2834, 0.2866, 0.2899, 0.2932, 0.2964, 0.2964, 0.2997, 0.3029, 0.3062,\n",
       "             0.3094, 0.3127, 0.3127, 0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3322,\n",
       "             0.3355, 0.3388, 0.3420, 0.3453, 0.3453, 0.3485, 0.3518, 0.3550, 0.3583,\n",
       "             0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779, 0.3811, 0.3844, 0.3876,\n",
       "             0.3909, 0.3941, 0.3974, 0.4007, 0.4039, 0.4072, 0.4104, 0.4137, 0.4169,\n",
       "             0.4202, 0.4235, 0.4267, 0.4300, 0.4332, 0.4365, 0.4397, 0.4430, 0.4463,\n",
       "             0.4495, 0.4528, 0.4560, 0.4593, 0.4625, 0.4658, 0.4691, 0.4723, 0.4756,\n",
       "             0.4788, 0.4821, 0.4853, 0.4886, 0.4919, 0.4951, 0.4984, 0.5016, 0.5049,\n",
       "             0.5081, 0.5114, 0.5147, 0.5179, 0.5212, 0.5244, 0.5277, 0.5309, 0.5342,\n",
       "             0.5375, 0.5407, 0.5440, 0.5472, 0.5505, 0.5537, 0.5570, 0.5603, 0.5635,\n",
       "             0.5668, 0.5700, 0.5733, 0.5765, 0.5798, 0.5831, 0.5863, 0.5896, 0.5928,\n",
       "             0.5961, 0.5993, 0.6026, 0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221,\n",
       "             0.6254, 0.6287, 0.6319, 0.6352, 0.6384, 0.6417, 0.6450, 0.6482, 0.6515,\n",
       "             0.6547, 0.6580, 0.6612, 0.6645, 0.6678, 0.6710, 0.6743, 0.6775, 0.6808,\n",
       "             0.6840, 0.6873, 0.6906, 0.6938, 0.6971, 0.7003, 0.7036, 0.7068, 0.7101,\n",
       "             0.7134, 0.7166, 0.7199, 0.7231, 0.7264, 0.7296, 0.7329, 0.7362, 0.7394,\n",
       "             0.7427, 0.7459, 0.7492, 0.7524, 0.7557, 0.7590, 0.7622, 0.7655, 0.7687,\n",
       "             0.7720, 0.7752, 0.7785, 0.7818, 0.7850, 0.7883, 0.7915, 0.7948, 0.7980,\n",
       "             0.8013, 0.8046, 0.8078, 0.8111, 0.8143, 0.8176, 0.8208, 0.8241, 0.8274,\n",
       "             0.8306, 0.8339, 0.8371, 0.8404, 0.8436, 0.8469, 0.8502, 0.8534, 0.8567,\n",
       "             0.8599, 0.8632, 0.8664, 0.8697, 0.8730, 0.8762, 0.8795, 0.8827, 0.8860,\n",
       "             0.8893, 0.8925, 0.8958, 0.8990, 0.9023, 0.9055, 0.9088, 0.9121, 0.9153,\n",
       "             0.9186, 0.9218, 0.9251, 0.9283, 0.9316, 0.9349, 0.9381, 0.9414, 0.9446,\n",
       "             0.9479, 0.9511, 0.9544, 0.9577, 0.9609, 0.9642, 0.9674, 0.9707, 0.9739,\n",
       "             0.9772, 0.9805, 0.9837, 0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0414, 0.1034, 0.1391, 0.1678, 0.2034, 0.2276, 0.2506, 0.2759,\n",
       "             0.2966, 0.3149, 0.3310, 0.3437, 0.3540, 0.3678, 0.3759, 0.3839, 0.3920,\n",
       "             0.4023, 0.4080, 0.4161, 0.4241, 0.4299, 0.4356, 0.4494, 0.4563, 0.4621,\n",
       "             0.4667, 0.4678, 0.4724, 0.4770, 0.4874, 0.4954, 0.4977, 0.5023, 0.5103,\n",
       "             0.5149, 0.5218, 0.5241, 0.5299, 0.5322, 0.5379, 0.5471, 0.5506, 0.5517,\n",
       "             0.5563, 0.5609, 0.5644, 0.5678, 0.5690, 0.5724, 0.5759, 0.5805, 0.5839,\n",
       "             0.5851, 0.5874, 0.5897, 0.5920, 0.5943, 0.5989, 0.6000, 0.6034, 0.6057,\n",
       "             0.6080, 0.6103, 0.6126, 0.6149, 0.6161, 0.6172, 0.6207, 0.6253, 0.6264,\n",
       "             0.6287, 0.6322, 0.6333, 0.6368, 0.6379, 0.6402, 0.6414, 0.6448, 0.6460,\n",
       "             0.6471, 0.6483, 0.6506, 0.6529, 0.6552, 0.6575, 0.6586, 0.6598, 0.6632,\n",
       "             0.6655, 0.6678, 0.6701, 0.6713, 0.6736, 0.6759, 0.6770, 0.6782, 0.6793,\n",
       "             0.6816, 0.6874, 0.6885, 0.6897, 0.6908, 0.6931, 0.6943, 0.6954, 0.6977,\n",
       "             0.6989, 0.7000, 0.7011, 0.7023, 0.7034, 0.7057, 0.7069, 0.7080, 0.7092,\n",
       "             0.7115, 0.7115, 0.7138, 0.7149, 0.7161, 0.7172, 0.7184, 0.7195, 0.7207,\n",
       "             0.7218, 0.7230, 0.7241, 0.7264, 0.7287, 0.7310, 0.7333, 0.7345, 0.7379,\n",
       "             0.7391, 0.7402, 0.7414, 0.7437, 0.7448, 0.7460, 0.7471, 0.7483, 0.7506,\n",
       "             0.7540, 0.7552, 0.7563, 0.7575, 0.7598, 0.7609, 0.7621, 0.7632, 0.7644,\n",
       "             0.7655, 0.7667, 0.7678, 0.7690, 0.7713, 0.7724, 0.7736, 0.7747, 0.7759,\n",
       "             0.7770, 0.7782, 0.7793, 0.7805, 0.7828, 0.7839, 0.7851, 0.7862, 0.7885,\n",
       "             0.7897, 0.7908, 0.7920, 0.7931, 0.7943, 0.7954, 0.7977, 0.7989, 0.8000,\n",
       "             0.8011, 0.8023, 0.8034, 0.8046, 0.8057, 0.8069, 0.8080, 0.8092, 0.8103,\n",
       "             0.8115, 0.8126, 0.8138, 0.8149, 0.8161, 0.8172, 0.8184, 0.8207, 0.8218,\n",
       "             0.8230, 0.8241, 0.8253, 0.8264, 0.8276, 0.8287, 0.8299, 0.8310, 0.8333,\n",
       "             0.8345, 0.8356, 0.8368, 0.8379, 0.8391, 0.8402, 0.8414, 0.8425, 0.8437,\n",
       "             0.8448, 0.8460, 0.8471, 0.8483, 0.8494, 0.8506, 0.8517, 0.8529, 0.8540,\n",
       "             0.8563, 0.8575, 0.8586, 0.8598, 0.8609, 0.8621, 0.8632, 0.8644, 0.8655,\n",
       "             0.8667, 0.8678, 0.8690, 0.8701, 0.8713, 0.8724, 0.8736, 0.8747, 0.8759,\n",
       "             0.8770, 0.8782, 0.8793, 0.8805, 0.8816, 0.8828, 0.8839, 0.8851, 0.8862,\n",
       "             0.8874, 0.8885, 0.8897, 0.8908, 0.8920, 0.8943, 0.8954, 0.8966, 0.8977,\n",
       "             0.8989, 0.9000, 0.9011, 0.9023, 0.9034, 0.9046, 0.9057, 0.9069, 0.9080,\n",
       "             0.9092, 0.9103, 0.9115, 0.9126, 0.9138, 0.9149, 0.9161, 0.9172, 0.9184,\n",
       "             0.9195, 0.9207, 0.9218, 0.9230, 0.9241, 0.9241, 0.9253, 0.9264, 0.9276,\n",
       "             0.9276, 0.9287, 0.9299, 0.9310, 0.9322, 0.9333, 0.9345, 0.9356, 0.9368,\n",
       "             0.9379, 0.9391, 0.9402, 0.9402, 0.9414, 0.9425, 0.9437, 0.9448, 0.9460,\n",
       "             0.9460, 0.9471, 0.9483, 0.9483, 0.9494, 0.9506, 0.9517, 0.9517, 0.9529,\n",
       "             0.9540, 0.9552, 0.9563, 0.9575, 0.9586, 0.9598, 0.9609, 0.9621, 0.9632,\n",
       "             0.9644, 0.9655, 0.9667, 0.9667, 0.9667, 0.9678, 0.9690, 0.9701, 0.9713,\n",
       "             0.9724, 0.9736, 0.9747, 0.9759, 0.9759, 0.9759, 0.9770, 0.9782, 0.9782,\n",
       "             0.9793, 0.9805, 0.9816, 0.9816, 0.9816, 0.9828, 0.9839, 0.9851, 0.9851,\n",
       "             0.9862, 0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9897, 0.9897, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9978e-01, 9.9977e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9974e-01, 9.9971e-01, 9.9970e-01,\n",
       "             9.9967e-01, 9.9967e-01, 9.9963e-01, 9.9963e-01, 9.9961e-01, 9.9959e-01,\n",
       "             9.9955e-01, 9.9955e-01, 9.9949e-01, 9.9946e-01, 9.9945e-01, 9.9942e-01,\n",
       "             9.9942e-01, 9.9940e-01, 9.9938e-01, 9.9934e-01, 9.9931e-01, 9.9930e-01,\n",
       "             9.9925e-01, 9.9908e-01, 9.9908e-01, 9.9902e-01, 9.9884e-01, 9.9878e-01,\n",
       "             9.9870e-01, 9.9850e-01, 9.9834e-01, 9.9826e-01, 9.9819e-01, 9.9776e-01,\n",
       "             9.9765e-01, 9.9749e-01, 9.9743e-01, 9.9704e-01, 9.9679e-01, 9.9645e-01,\n",
       "             9.9576e-01, 9.9430e-01, 9.9178e-01, 9.9173e-01, 9.9135e-01, 9.9052e-01,\n",
       "             9.9003e-01, 9.8704e-01, 9.8402e-01, 9.8345e-01, 9.8303e-01, 9.8276e-01,\n",
       "             9.8165e-01, 9.7925e-01, 9.7797e-01, 9.7430e-01, 9.7104e-01, 9.7014e-01,\n",
       "             9.6956e-01, 9.6922e-01, 9.6888e-01, 9.6315e-01, 9.6270e-01, 9.5655e-01,\n",
       "             9.5448e-01, 9.5193e-01, 9.4720e-01, 9.4499e-01, 9.4447e-01, 9.4168e-01,\n",
       "             9.3243e-01, 9.2787e-01, 9.2650e-01, 9.1169e-01, 9.0856e-01, 9.0028e-01,\n",
       "             8.9174e-01, 8.8520e-01, 8.6301e-01, 8.3965e-01, 8.0003e-01, 7.8095e-01,\n",
       "             7.4096e-01, 7.2836e-01, 7.2497e-01, 6.8682e-01, 6.8677e-01, 6.4197e-01,\n",
       "             6.1810e-01, 6.0888e-01, 5.9492e-01, 5.5693e-01, 5.4905e-01, 5.1410e-01,\n",
       "             4.9463e-01, 4.1575e-01, 3.9382e-01, 3.8034e-01, 3.6208e-01, 3.0569e-01,\n",
       "             2.9485e-01, 2.6749e-01, 2.3734e-01, 1.7912e-01, 1.7223e-01, 1.4109e-01,\n",
       "             1.3426e-01, 1.2759e-01, 1.1836e-01, 1.1139e-01, 8.1937e-02, 6.5975e-02,\n",
       "             6.0604e-02, 5.5214e-02, 4.2179e-02, 4.2052e-02, 3.7957e-02, 3.6368e-02,\n",
       "             2.7580e-02, 2.0908e-02, 1.8605e-02, 1.8287e-02, 1.8064e-02, 1.7526e-02,\n",
       "             1.7271e-02, 1.6256e-02, 1.3934e-02, 1.1255e-02, 9.0980e-03, 7.9946e-03,\n",
       "             6.7570e-03, 6.5020e-03, 6.4250e-03, 5.9858e-03, 5.9789e-03, 5.7433e-03,\n",
       "             4.2883e-03, 3.9133e-03, 3.2235e-03, 3.0987e-03, 3.0286e-03, 2.9457e-03,\n",
       "             2.8193e-03, 2.5732e-03, 2.1708e-03, 1.9119e-03, 1.7915e-03, 1.7856e-03,\n",
       "             1.7459e-03, 1.6551e-03, 1.5382e-03, 1.3651e-03, 1.3052e-03, 1.1659e-03,\n",
       "             1.1624e-03, 1.1286e-03, 9.9338e-04, 9.0260e-04, 6.3523e-04, 5.4330e-04,\n",
       "             4.7457e-04, 4.6988e-04, 4.5831e-04, 4.3416e-04, 4.0147e-04, 3.6961e-04,\n",
       "             2.9542e-04, 2.8476e-04, 2.5666e-04, 1.9995e-04, 1.9075e-04, 1.8433e-04,\n",
       "             1.6947e-04, 1.2933e-04, 1.1749e-04, 1.1539e-04, 1.0836e-04, 1.0333e-04,\n",
       "             9.6401e-05, 9.3927e-05, 9.3026e-05, 8.9419e-05, 8.0269e-05, 7.2671e-05,\n",
       "             6.8377e-05, 6.8296e-05, 6.5055e-05, 6.4250e-05, 6.2272e-05, 6.1827e-05,\n",
       "             5.9885e-05, 5.7816e-05, 5.2432e-05, 5.1061e-05, 4.3870e-05, 4.2641e-05,\n",
       "             3.9425e-05, 3.4407e-05, 2.7487e-05, 1.9433e-05, 1.8544e-05, 1.8122e-05,\n",
       "             1.7546e-05, 1.6353e-05, 1.6235e-05, 1.6180e-05, 1.4853e-05, 1.4692e-05,\n",
       "             1.3828e-05, 1.3772e-05, 1.3731e-05, 1.2949e-05, 1.2538e-05, 1.1919e-05,\n",
       "             1.0748e-05, 1.0722e-05, 1.0620e-05, 9.6870e-06, 9.5094e-06, 9.4744e-06,\n",
       "             9.0815e-06, 8.7687e-06, 8.5640e-06, 6.3773e-06, 6.1972e-06, 5.6939e-06,\n",
       "             5.5007e-06, 5.0205e-06, 4.8978e-06, 4.3905e-06, 3.5950e-06, 3.5359e-06,\n",
       "             3.2909e-06, 2.5013e-06, 2.3130e-06, 2.3112e-06, 2.1007e-06, 2.0966e-06,\n",
       "             1.8978e-06, 1.5834e-06, 1.1126e-06, 1.0807e-06, 9.3839e-07, 8.7994e-07,\n",
       "             8.4179e-07, 7.7146e-07, 7.3514e-07, 7.2515e-07, 6.8576e-07, 6.5821e-07,\n",
       "             6.5802e-07, 5.8776e-07, 5.5537e-07, 5.4697e-07, 5.2649e-07, 4.8515e-07,\n",
       "             4.4073e-07, 4.3006e-07, 3.7501e-07, 3.5082e-07, 3.2897e-07, 3.1024e-07,\n",
       "             3.0352e-07, 2.8952e-07, 2.8777e-07, 2.6367e-07, 2.5735e-07, 2.4714e-07,\n",
       "             2.4015e-07, 2.1875e-07, 1.8564e-07, 1.7999e-07, 1.5567e-07, 1.5092e-07,\n",
       "             1.2433e-07, 1.1302e-07, 1.1046e-07, 1.0373e-07, 9.4705e-08, 9.3751e-08,\n",
       "             8.8966e-08, 8.5871e-08, 8.5581e-08, 6.3354e-08, 4.2257e-08, 4.0888e-08,\n",
       "             3.0968e-08, 2.5715e-08, 2.3036e-08, 2.2113e-08, 2.1378e-08, 1.9023e-08,\n",
       "             1.8280e-08, 1.6829e-08, 1.6494e-08, 1.5542e-08, 1.2835e-08, 1.2797e-08,\n",
       "             1.2017e-08, 1.1885e-08, 1.1376e-08, 1.1030e-08, 9.0075e-09, 8.8770e-09,\n",
       "             7.8891e-09, 7.2810e-09, 7.0868e-09, 6.6148e-09, 4.8167e-09, 4.2661e-09,\n",
       "             4.1877e-09, 3.7860e-09, 3.6308e-09, 3.4069e-09, 3.2547e-09, 2.5682e-09,\n",
       "             2.5567e-09, 1.9865e-09, 1.6306e-09, 1.4478e-09, 1.2483e-09, 1.1897e-09,\n",
       "             8.6711e-10, 5.9899e-10, 5.8071e-10, 4.9381e-10, 4.8197e-10, 4.5925e-10,\n",
       "             4.2629e-10, 3.8071e-10, 2.8899e-10, 2.6931e-10, 2.4642e-10, 2.1126e-10,\n",
       "             1.7416e-10, 1.6236e-10, 1.6168e-10, 1.0182e-10, 9.7824e-11, 7.1669e-11,\n",
       "             6.1804e-11, 5.7015e-11, 5.0758e-11, 3.1442e-11, 3.0097e-11, 2.0186e-11,\n",
       "             1.8574e-11, 1.4762e-11, 1.3685e-11, 4.3706e-12, 3.9533e-12, 2.9620e-12,\n",
       "             2.7596e-12, 2.6796e-12, 2.4872e-12, 2.2198e-12, 1.4835e-12, 6.0769e-13,\n",
       "             4.7270e-13, 4.2972e-13, 3.7506e-13, 1.2248e-13, 7.9074e-14, 5.0236e-14,\n",
       "             4.2574e-14, 1.6817e-14, 9.2434e-15, 7.8856e-15, 6.0922e-15, 7.7734e-16,\n",
       "             1.4407e-18, 7.6096e-21])}},\n",
       "   {'fpr': np.float64(0.04234527687296417),\n",
       "    'tpr': np.float64(0.9839080459770115),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0023, 0.0034,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.7643e-16, 2.7343e-16,\n",
       "             4.8041e-20])}},\n",
       "   {'fpr': np.float64(0.09771986970684039),\n",
       "    'tpr': np.float64(0.9873563218390805),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0163,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0228, 0.0228, 0.0261, 0.0293, 0.0293, 0.0326, 0.0326, 0.0358,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0423, 0.0423, 0.0423,\n",
       "             0.0423, 0.0423, 0.0423, 0.0423, 0.0456, 0.0456, 0.0489, 0.0489, 0.0489,\n",
       "             0.0521, 0.0521, 0.0554, 0.0554, 0.0586, 0.0586, 0.0619, 0.0651, 0.0651,\n",
       "             0.0651, 0.0651, 0.0684, 0.0717, 0.0749, 0.0782, 0.0814, 0.0814, 0.0814,\n",
       "             0.0847, 0.0879, 0.0912, 0.0945, 0.0977, 0.1010, 0.1042, 0.1042, 0.1075,\n",
       "             0.1107, 0.1107, 0.1140, 0.1140, 0.1173, 0.1205, 0.1205, 0.1238, 0.1270,\n",
       "             0.1303, 0.1336, 0.1368, 0.1401, 0.1433, 0.1466, 0.1466, 0.1498, 0.1531,\n",
       "             0.1564, 0.1596, 0.1629, 0.1661, 0.1694, 0.1726, 0.1726, 0.1759, 0.1759,\n",
       "             0.1792, 0.1824, 0.1857, 0.1889, 0.1922, 0.1954, 0.1987, 0.2020, 0.2052,\n",
       "             0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345,\n",
       "             0.2378, 0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2573, 0.2606,\n",
       "             0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899,\n",
       "             0.2932, 0.2964, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160, 0.3192,\n",
       "             0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453, 0.3485,\n",
       "             0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779,\n",
       "             0.3811, 0.3844, 0.3876, 0.3909, 0.3941, 0.3941, 0.3974, 0.4007, 0.4039,\n",
       "             0.4072, 0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267, 0.4300, 0.4332,\n",
       "             0.4365, 0.4397, 0.4430, 0.4463, 0.4495, 0.4528, 0.4528, 0.4560, 0.4593,\n",
       "             0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788, 0.4821, 0.4853, 0.4886,\n",
       "             0.4919, 0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114, 0.5147, 0.5179,\n",
       "             0.5212, 0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407, 0.5440, 0.5472,\n",
       "             0.5505, 0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700, 0.5733, 0.5765,\n",
       "             0.5798, 0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5961, 0.5993, 0.6026,\n",
       "             0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319,\n",
       "             0.6352, 0.6384, 0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612,\n",
       "             0.6645, 0.6678, 0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906,\n",
       "             0.6938, 0.6971, 0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199,\n",
       "             0.7231, 0.7264, 0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492,\n",
       "             0.7524, 0.7557, 0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785,\n",
       "             0.7818, 0.7850, 0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078,\n",
       "             0.8111, 0.8143, 0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371,\n",
       "             0.8404, 0.8436, 0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664,\n",
       "             0.8697, 0.8730, 0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958,\n",
       "             0.8990, 0.9023, 0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251,\n",
       "             0.9283, 0.9316, 0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544,\n",
       "             0.9577, 0.9609, 0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837,\n",
       "             0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.3782, 0.4690, 0.5080, 0.5368, 0.5586, 0.5736, 0.5874, 0.5977,\n",
       "             0.6046, 0.6126, 0.6253, 0.6345, 0.6414, 0.6471, 0.6517, 0.6575, 0.6621,\n",
       "             0.6678, 0.6701, 0.6747, 0.6759, 0.6805, 0.6851, 0.6897, 0.6943, 0.6977,\n",
       "             0.7000, 0.7057, 0.7069, 0.7103, 0.7126, 0.7138, 0.7172, 0.7195, 0.7207,\n",
       "             0.7218, 0.7230, 0.7241, 0.7276, 0.7310, 0.7345, 0.7356, 0.7379, 0.7414,\n",
       "             0.7425, 0.7448, 0.7460, 0.7494, 0.7517, 0.7540, 0.7552, 0.7563, 0.7575,\n",
       "             0.7586, 0.7598, 0.7632, 0.7644, 0.7655, 0.7678, 0.7690, 0.7713, 0.7736,\n",
       "             0.7747, 0.7770, 0.7793, 0.7805, 0.7816, 0.7839, 0.7851, 0.7862, 0.7874,\n",
       "             0.7885, 0.7897, 0.7908, 0.7920, 0.7931, 0.7966, 0.7977, 0.7989, 0.8000,\n",
       "             0.8011, 0.8011, 0.8023, 0.8057, 0.8069, 0.8080, 0.8092, 0.8103, 0.8115,\n",
       "             0.8126, 0.8138, 0.8149, 0.8161, 0.8172, 0.8184, 0.8195, 0.8207, 0.8218,\n",
       "             0.8230, 0.8241, 0.8253, 0.8264, 0.8276, 0.8287, 0.8299, 0.8310, 0.8322,\n",
       "             0.8333, 0.8345, 0.8356, 0.8379, 0.8391, 0.8402, 0.8414, 0.8437, 0.8448,\n",
       "             0.8460, 0.8460, 0.8471, 0.8483, 0.8494, 0.8506, 0.8517, 0.8529, 0.8540,\n",
       "             0.8552, 0.8563, 0.8575, 0.8598, 0.8609, 0.8621, 0.8632, 0.8644, 0.8655,\n",
       "             0.8667, 0.8678, 0.8690, 0.8701, 0.8713, 0.8724, 0.8736, 0.8747, 0.8759,\n",
       "             0.8770, 0.8782, 0.8793, 0.8805, 0.8816, 0.8828, 0.8839, 0.8862, 0.8874,\n",
       "             0.8885, 0.8897, 0.8908, 0.8920, 0.8931, 0.8943, 0.8954, 0.8966, 0.8977,\n",
       "             0.8989, 0.9000, 0.9011, 0.9023, 0.9034, 0.9046, 0.9057, 0.9069, 0.9080,\n",
       "             0.9092, 0.9103, 0.9115, 0.9126, 0.9138, 0.9149, 0.9161, 0.9172, 0.9184,\n",
       "             0.9195, 0.9207, 0.9218, 0.9230, 0.9241, 0.9253, 0.9264, 0.9276, 0.9287,\n",
       "             0.9299, 0.9310, 0.9322, 0.9333, 0.9345, 0.9356, 0.9368, 0.9379, 0.9391,\n",
       "             0.9402, 0.9414, 0.9425, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483, 0.9483,\n",
       "             0.9483, 0.9494, 0.9506, 0.9517, 0.9529, 0.9540, 0.9552, 0.9563, 0.9575,\n",
       "             0.9586, 0.9586, 0.9598, 0.9598, 0.9598, 0.9609, 0.9609, 0.9621, 0.9621,\n",
       "             0.9621, 0.9632, 0.9644, 0.9655, 0.9667, 0.9678, 0.9678, 0.9690, 0.9701,\n",
       "             0.9713, 0.9724, 0.9736, 0.9747, 0.9747, 0.9759, 0.9759, 0.9770, 0.9782,\n",
       "             0.9782, 0.9793, 0.9793, 0.9805, 0.9805, 0.9816, 0.9816, 0.9816, 0.9828,\n",
       "             0.9839, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9862, 0.9874,\n",
       "             0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9885, 0.9885,\n",
       "             0.9885, 0.9897, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9977e-01, 9.9976e-01, 9.9975e-01, 9.9974e-01,\n",
       "             9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9970e-01,\n",
       "             9.9970e-01, 9.9968e-01, 9.9967e-01, 9.9966e-01, 9.9965e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9961e-01, 9.9959e-01, 9.9958e-01, 9.9952e-01, 9.9944e-01,\n",
       "             9.9939e-01, 9.9939e-01, 9.9936e-01, 9.9936e-01, 9.9926e-01, 9.9922e-01,\n",
       "             9.9920e-01, 9.9899e-01, 9.9893e-01, 9.9884e-01, 9.9884e-01, 9.9881e-01,\n",
       "             9.9872e-01, 9.9847e-01, 9.9847e-01, 9.9844e-01, 9.9813e-01, 9.9813e-01,\n",
       "             9.9800e-01, 9.9790e-01, 9.9696e-01, 9.9625e-01, 9.9613e-01, 9.9600e-01,\n",
       "             9.9559e-01, 9.9346e-01, 9.9304e-01, 9.9199e-01, 9.9167e-01, 9.9084e-01,\n",
       "             9.9022e-01, 9.8951e-01, 9.8804e-01, 9.8243e-01, 9.8223e-01, 9.8222e-01,\n",
       "             9.8056e-01, 9.7723e-01, 9.7150e-01, 9.7058e-01, 9.6984e-01, 9.6625e-01,\n",
       "             9.6345e-01, 9.6291e-01, 9.5242e-01, 9.4761e-01, 9.4452e-01, 9.2851e-01,\n",
       "             9.1617e-01, 9.1141e-01, 9.0435e-01, 9.0412e-01, 9.0029e-01, 8.9675e-01,\n",
       "             8.9502e-01, 8.9188e-01, 8.8051e-01, 8.7633e-01, 8.6376e-01, 8.6185e-01,\n",
       "             8.0444e-01, 7.9866e-01, 7.8598e-01, 6.6012e-01, 5.8413e-01, 5.7297e-01,\n",
       "             5.4604e-01, 5.4162e-01, 4.5494e-01, 4.5164e-01, 3.8360e-01, 3.4572e-01,\n",
       "             3.0826e-01, 2.6988e-01, 2.6882e-01, 2.4493e-01, 2.2866e-01, 2.0006e-01,\n",
       "             1.2812e-01, 1.2120e-01, 1.0324e-01, 7.0203e-02, 6.7474e-02, 4.3949e-02,\n",
       "             4.3364e-02, 3.3114e-02, 2.4396e-02, 2.4393e-02, 1.5633e-02, 1.2673e-02,\n",
       "             1.1630e-02, 9.8748e-03, 9.0113e-03, 4.7637e-03, 4.1516e-03, 3.9449e-03,\n",
       "             3.6723e-03, 3.2913e-03, 2.5499e-03, 1.8347e-03, 1.7068e-03, 1.3341e-03,\n",
       "             1.2995e-03, 1.2421e-03, 1.1279e-03, 9.9742e-04, 8.7402e-04, 8.6474e-04,\n",
       "             8.2781e-04, 7.2728e-04, 6.5964e-04, 6.3613e-04, 6.1952e-04, 5.0152e-04,\n",
       "             5.0070e-04, 4.3525e-04, 3.9762e-04, 3.9752e-04, 3.7626e-04, 3.6085e-04,\n",
       "             2.1716e-04, 1.9346e-04, 1.6025e-04, 1.4757e-04, 1.3378e-04, 1.2942e-04,\n",
       "             1.2564e-04, 8.5708e-05, 6.0053e-05, 5.6658e-05, 5.0631e-05, 4.9361e-05,\n",
       "             4.5395e-05, 4.3919e-05, 4.1840e-05, 4.0268e-05, 3.4960e-05, 3.4187e-05,\n",
       "             2.7266e-05, 2.5679e-05, 2.0943e-05, 2.0387e-05, 2.0243e-05, 1.9505e-05,\n",
       "             1.8159e-05, 1.6932e-05, 1.3129e-05, 1.1180e-05, 1.0407e-05, 1.0010e-05,\n",
       "             9.5320e-06, 8.5665e-06, 6.6798e-06, 5.2199e-06, 3.8980e-06, 3.8857e-06,\n",
       "             2.9071e-06, 2.5078e-06, 2.4102e-06, 1.5709e-06, 1.4707e-06, 1.4239e-06,\n",
       "             1.2684e-06, 1.1389e-06, 1.0304e-06, 1.0012e-06, 8.4316e-07, 8.0709e-07,\n",
       "             7.3860e-07, 5.0078e-07, 4.6623e-07, 4.0567e-07, 4.0426e-07, 2.8575e-07,\n",
       "             2.7209e-07, 2.5882e-07, 2.5422e-07, 2.4575e-07, 2.2051e-07, 1.7803e-07,\n",
       "             1.7609e-07, 1.7167e-07, 1.6840e-07, 1.6544e-07, 1.6130e-07, 1.5380e-07,\n",
       "             1.4783e-07, 1.4187e-07, 1.3679e-07, 1.2008e-07, 9.0872e-08, 8.5406e-08,\n",
       "             6.7498e-08, 6.7251e-08, 6.6230e-08, 6.3096e-08, 5.7148e-08, 5.2042e-08,\n",
       "             4.3851e-08, 4.2255e-08, 3.3636e-08, 3.1706e-08, 2.9307e-08, 2.8543e-08,\n",
       "             2.6966e-08, 2.4296e-08, 1.8526e-08, 1.3419e-08, 1.2795e-08, 1.1372e-08,\n",
       "             1.0730e-08, 1.0549e-08, 1.0250e-08, 6.2524e-09, 5.9015e-09, 5.8225e-09,\n",
       "             5.5102e-09, 5.4801e-09, 5.2137e-09, 5.2023e-09, 4.6065e-09, 3.9554e-09,\n",
       "             3.5804e-09, 3.5621e-09, 3.5331e-09, 3.2667e-09, 3.0359e-09, 2.8941e-09,\n",
       "             2.7788e-09, 2.7685e-09, 2.3736e-09, 2.2246e-09, 2.0986e-09, 1.9920e-09,\n",
       "             1.7729e-09, 1.5649e-09, 1.5582e-09, 1.3348e-09, 1.2802e-09, 9.3261e-10,\n",
       "             8.4614e-10, 7.4867e-10, 7.4040e-10, 6.6483e-10, 6.3694e-10, 5.9691e-10,\n",
       "             4.3975e-10, 4.2840e-10, 3.7391e-10, 3.6270e-10, 2.4987e-10, 2.4387e-10,\n",
       "             2.3170e-10, 2.0532e-10, 2.0060e-10, 1.9692e-10, 1.6991e-10, 1.2221e-10,\n",
       "             1.2013e-10, 1.0222e-10, 1.0163e-10, 9.1884e-11, 8.8632e-11, 6.4194e-11,\n",
       "             5.8958e-11, 5.5513e-11, 5.4055e-11, 4.1409e-11, 3.9722e-11, 3.3512e-11,\n",
       "             2.9758e-11, 2.5905e-11, 2.4786e-11, 2.3457e-11, 2.0936e-11, 1.4142e-11,\n",
       "             9.2129e-12, 8.6717e-12, 8.5043e-12, 7.9353e-12, 7.0145e-12, 5.7313e-12,\n",
       "             5.6063e-12, 4.7422e-12, 3.7463e-12, 2.7882e-12, 2.5217e-12, 2.0905e-12,\n",
       "             2.0764e-12, 1.5886e-12, 1.5250e-12, 1.4539e-12, 9.0989e-13, 8.0342e-13,\n",
       "             7.7908e-13, 7.4713e-13, 7.0434e-13, 6.4507e-13, 6.4426e-13, 5.8715e-13,\n",
       "             5.2868e-13, 3.9695e-13, 3.7366e-13, 3.5262e-13, 3.5014e-13, 3.1289e-13,\n",
       "             2.7373e-13, 2.5930e-13, 1.9469e-13, 1.8037e-13, 1.7459e-13, 1.5266e-13,\n",
       "             1.0141e-13, 7.9443e-14, 6.7057e-14, 6.6785e-14, 5.5097e-14, 4.7392e-14,\n",
       "             4.3886e-14, 4.3481e-14, 4.3187e-14, 3.8722e-14, 3.6286e-14, 1.8950e-14,\n",
       "             1.8386e-14, 1.5228e-14, 1.1074e-14, 5.7466e-15, 4.4986e-15, 2.4932e-15,\n",
       "             1.7114e-15, 9.2564e-16, 8.7869e-16, 5.7563e-16, 4.1172e-16, 3.3619e-16,\n",
       "             3.0930e-16, 2.1111e-16, 1.9483e-16, 6.9953e-17, 2.9938e-17, 2.6411e-17,\n",
       "             1.5826e-17, 1.3799e-17, 8.6584e-18, 6.9443e-18, 3.8188e-18, 3.5587e-18,\n",
       "             1.3885e-18, 5.9354e-19, 1.1461e-19, 1.6392e-20, 4.7275e-21, 1.5034e-21,\n",
       "             1.9517e-23, 3.6825e-27])}},\n",
       "   {'fpr': np.float64(0.10097719869706841),\n",
       "    'tpr': np.float64(0.9919540229885058),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0130, 0.0130, 0.0130, 0.0130, 0.0163, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0261,\n",
       "             0.0261, 0.0293, 0.0293, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0358,\n",
       "             0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0391, 0.0391, 0.0391,\n",
       "             0.0423, 0.0423, 0.0423, 0.0423, 0.0423, 0.0456, 0.0489, 0.0489, 0.0489,\n",
       "             0.0521, 0.0521, 0.0554, 0.0554, 0.0586, 0.0619, 0.0651, 0.0684, 0.0684,\n",
       "             0.0717, 0.0717, 0.0717, 0.0749, 0.0782, 0.0782, 0.0814, 0.0847, 0.0847,\n",
       "             0.0879, 0.0912, 0.0945, 0.0977, 0.1010, 0.1042, 0.1075, 0.1107, 0.1107,\n",
       "             0.1140, 0.1173, 0.1205, 0.1238, 0.1270, 0.1303, 0.1336, 0.1368, 0.1401,\n",
       "             0.1433, 0.1466, 0.1498, 0.1498, 0.1531, 0.1564, 0.1596, 0.1629, 0.1629,\n",
       "             0.1629, 0.1661, 0.1694, 0.1726, 0.1759, 0.1792, 0.1792, 0.1824, 0.1857,\n",
       "             0.1889, 0.1922, 0.1954, 0.1954, 0.1954, 0.1987, 0.2020, 0.2052, 0.2085,\n",
       "             0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345, 0.2378,\n",
       "             0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2606, 0.2638, 0.2671,\n",
       "             0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899, 0.2932, 0.2964,\n",
       "             0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160, 0.3192, 0.3225, 0.3257,\n",
       "             0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453, 0.3485, 0.3518, 0.3550,\n",
       "             0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779, 0.3811, 0.3844,\n",
       "             0.3876, 0.3909, 0.3941, 0.3974, 0.4007, 0.4039, 0.4072, 0.4104, 0.4137,\n",
       "             0.4169, 0.4202, 0.4235, 0.4267, 0.4300, 0.4332, 0.4365, 0.4397, 0.4430,\n",
       "             0.4463, 0.4495, 0.4528, 0.4560, 0.4593, 0.4625, 0.4658, 0.4691, 0.4723,\n",
       "             0.4756, 0.4788, 0.4821, 0.4853, 0.4886, 0.4919, 0.4951, 0.4984, 0.5016,\n",
       "             0.5049, 0.5081, 0.5114, 0.5147, 0.5179, 0.5212, 0.5244, 0.5277, 0.5309,\n",
       "             0.5342, 0.5375, 0.5407, 0.5440, 0.5472, 0.5505, 0.5537, 0.5570, 0.5603,\n",
       "             0.5635, 0.5668, 0.5700, 0.5733, 0.5765, 0.5798, 0.5831, 0.5863, 0.5896,\n",
       "             0.5928, 0.5961, 0.5993, 0.6026, 0.6059, 0.6091, 0.6124, 0.6156, 0.6189,\n",
       "             0.6221, 0.6254, 0.6287, 0.6319, 0.6352, 0.6384, 0.6417, 0.6450, 0.6482,\n",
       "             0.6515, 0.6547, 0.6580, 0.6612, 0.6645, 0.6678, 0.6710, 0.6743, 0.6775,\n",
       "             0.6808, 0.6840, 0.6873, 0.6906, 0.6938, 0.6971, 0.7003, 0.7036, 0.7068,\n",
       "             0.7101, 0.7134, 0.7166, 0.7199, 0.7231, 0.7264, 0.7296, 0.7329, 0.7362,\n",
       "             0.7394, 0.7427, 0.7459, 0.7492, 0.7524, 0.7557, 0.7590, 0.7622, 0.7655,\n",
       "             0.7687, 0.7720, 0.7752, 0.7785, 0.7818, 0.7850, 0.7883, 0.7915, 0.7948,\n",
       "             0.7980, 0.8013, 0.8046, 0.8078, 0.8111, 0.8143, 0.8176, 0.8208, 0.8241,\n",
       "             0.8274, 0.8306, 0.8339, 0.8371, 0.8404, 0.8436, 0.8469, 0.8502, 0.8534,\n",
       "             0.8567, 0.8599, 0.8632, 0.8664, 0.8697, 0.8730, 0.8762, 0.8795, 0.8827,\n",
       "             0.8860, 0.8893, 0.8925, 0.8958, 0.8990, 0.9023, 0.9055, 0.9088, 0.9121,\n",
       "             0.9153, 0.9186, 0.9218, 0.9251, 0.9283, 0.9316, 0.9349, 0.9381, 0.9414,\n",
       "             0.9446, 0.9479, 0.9511, 0.9544, 0.9577, 0.9609, 0.9642, 0.9674, 0.9707,\n",
       "             0.9739, 0.9772, 0.9805, 0.9837, 0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0678, 0.1149, 0.1471, 0.1759, 0.1943, 0.2069, 0.2264, 0.2368,\n",
       "             0.2529, 0.2644, 0.2747, 0.2885, 0.3023, 0.3115, 0.3195, 0.3218, 0.3276,\n",
       "             0.3356, 0.3402, 0.3460, 0.3552, 0.3632, 0.3736, 0.3759, 0.3793, 0.3897,\n",
       "             0.3966, 0.4057, 0.4092, 0.4149, 0.4195, 0.4230, 0.4310, 0.4368, 0.4391,\n",
       "             0.4460, 0.4506, 0.4529, 0.4563, 0.4609, 0.4644, 0.4713, 0.4724, 0.4736,\n",
       "             0.4770, 0.4816, 0.4862, 0.4874, 0.4908, 0.4954, 0.4966, 0.5023, 0.5034,\n",
       "             0.5057, 0.5080, 0.5092, 0.5103, 0.5149, 0.5195, 0.5230, 0.5264, 0.5287,\n",
       "             0.5299, 0.5322, 0.5333, 0.5356, 0.5402, 0.5425, 0.5437, 0.5448, 0.5471,\n",
       "             0.5506, 0.5529, 0.5540, 0.5552, 0.5563, 0.5586, 0.5598, 0.5609, 0.5621,\n",
       "             0.5655, 0.5667, 0.5701, 0.5736, 0.5782, 0.5793, 0.5805, 0.5816, 0.5828,\n",
       "             0.5851, 0.5885, 0.5897, 0.5920, 0.5931, 0.5943, 0.5954, 0.5989, 0.6011,\n",
       "             0.6046, 0.6092, 0.6115, 0.6126, 0.6138, 0.6149, 0.6161, 0.6172, 0.6184,\n",
       "             0.6207, 0.6230, 0.6241, 0.6264, 0.6276, 0.6287, 0.6310, 0.6322, 0.6356,\n",
       "             0.6368, 0.6379, 0.6391, 0.6402, 0.6414, 0.6425, 0.6437, 0.6448, 0.6460,\n",
       "             0.6471, 0.6483, 0.6494, 0.6517, 0.6552, 0.6563, 0.6598, 0.6609, 0.6621,\n",
       "             0.6644, 0.6655, 0.6678, 0.6690, 0.6713, 0.6724, 0.6736, 0.6747, 0.6759,\n",
       "             0.6770, 0.6782, 0.6793, 0.6816, 0.6828, 0.6839, 0.6851, 0.6874, 0.6908,\n",
       "             0.6920, 0.6931, 0.6943, 0.6954, 0.6966, 0.6977, 0.7000, 0.7011, 0.7023,\n",
       "             0.7034, 0.7046, 0.7069, 0.7092, 0.7103, 0.7115, 0.7126, 0.7138, 0.7161,\n",
       "             0.7184, 0.7195, 0.7207, 0.7218, 0.7230, 0.7241, 0.7253, 0.7264, 0.7276,\n",
       "             0.7287, 0.7299, 0.7310, 0.7322, 0.7333, 0.7345, 0.7368, 0.7379, 0.7391,\n",
       "             0.7402, 0.7414, 0.7425, 0.7437, 0.7460, 0.7471, 0.7483, 0.7494, 0.7506,\n",
       "             0.7517, 0.7529, 0.7540, 0.7552, 0.7563, 0.7586, 0.7609, 0.7621, 0.7632,\n",
       "             0.7644, 0.7655, 0.7667, 0.7678, 0.7690, 0.7701, 0.7713, 0.7724, 0.7736,\n",
       "             0.7747, 0.7759, 0.7770, 0.7793, 0.7805, 0.7816, 0.7828, 0.7839, 0.7851,\n",
       "             0.7862, 0.7874, 0.7885, 0.7897, 0.7908, 0.7920, 0.7943, 0.7954, 0.7966,\n",
       "             0.7977, 0.7989, 0.8000, 0.8011, 0.8023, 0.8034, 0.8057, 0.8069, 0.8080,\n",
       "             0.8092, 0.8103, 0.8115, 0.8126, 0.8138, 0.8149, 0.8161, 0.8172, 0.8184,\n",
       "             0.8195, 0.8207, 0.8218, 0.8230, 0.8241, 0.8253, 0.8264, 0.8287, 0.8299,\n",
       "             0.8310, 0.8322, 0.8333, 0.8345, 0.8356, 0.8368, 0.8391, 0.8402, 0.8414,\n",
       "             0.8425, 0.8437, 0.8448, 0.8460, 0.8471, 0.8483, 0.8494, 0.8506, 0.8517,\n",
       "             0.8529, 0.8540, 0.8552, 0.8563, 0.8575, 0.8586, 0.8598, 0.8609, 0.8621,\n",
       "             0.8632, 0.8644, 0.8667, 0.8678, 0.8690, 0.8701, 0.8713, 0.8724, 0.8736,\n",
       "             0.8747, 0.8759, 0.8770, 0.8782, 0.8793, 0.8805, 0.8816, 0.8828, 0.8839,\n",
       "             0.8851, 0.8851, 0.8862, 0.8874, 0.8885, 0.8897, 0.8908, 0.8920, 0.8931,\n",
       "             0.8943, 0.8954, 0.8966, 0.8977, 0.8989, 0.9000, 0.9011, 0.9023, 0.9034,\n",
       "             0.9046, 0.9057, 0.9069, 0.9080, 0.9092, 0.9103, 0.9115, 0.9126, 0.9138,\n",
       "             0.9149, 0.9161, 0.9172, 0.9184, 0.9195, 0.9207, 0.9218, 0.9230, 0.9241,\n",
       "             0.9253, 0.9264, 0.9276, 0.9299, 0.9310, 0.9322, 0.9333, 0.9345, 0.9356,\n",
       "             0.9368, 0.9379, 0.9391, 0.9402, 0.9414, 0.9425, 0.9437, 0.9448, 0.9460,\n",
       "             0.9471, 0.9483, 0.9494, 0.9494, 0.9506, 0.9517, 0.9529, 0.9529, 0.9529,\n",
       "             0.9540, 0.9552, 0.9563, 0.9563, 0.9575, 0.9586, 0.9598, 0.9609, 0.9609,\n",
       "             0.9621, 0.9621, 0.9632, 0.9632, 0.9644, 0.9655, 0.9667, 0.9678, 0.9678,\n",
       "             0.9690, 0.9701, 0.9713, 0.9724, 0.9736, 0.9747, 0.9747, 0.9759, 0.9770,\n",
       "             0.9770, 0.9782, 0.9793, 0.9805, 0.9816, 0.9816, 0.9816, 0.9828, 0.9839,\n",
       "             0.9839, 0.9851, 0.9851, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9874,\n",
       "             0.9874, 0.9885, 0.9897, 0.9897, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9967e-01, 9.9967e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9964e-01, 9.9962e-01, 9.9961e-01, 9.9960e-01, 9.9959e-01,\n",
       "             9.9956e-01, 9.9950e-01, 9.9945e-01, 9.9945e-01, 9.9940e-01, 9.9940e-01,\n",
       "             9.9937e-01, 9.9937e-01, 9.9936e-01, 9.9935e-01, 9.9932e-01, 9.9932e-01,\n",
       "             9.9925e-01, 9.9918e-01, 9.9917e-01, 9.9915e-01, 9.9907e-01, 9.9904e-01,\n",
       "             9.9891e-01, 9.9889e-01, 9.9875e-01, 9.9871e-01, 9.9870e-01, 9.9856e-01,\n",
       "             9.9814e-01, 9.9805e-01, 9.9804e-01, 9.9789e-01, 9.9784e-01, 9.9758e-01,\n",
       "             9.9741e-01, 9.9732e-01, 9.9706e-01, 9.9696e-01, 9.9593e-01, 9.9590e-01,\n",
       "             9.9586e-01, 9.9547e-01, 9.9416e-01, 9.9297e-01, 9.9219e-01, 9.9105e-01,\n",
       "             9.9070e-01, 9.9054e-01, 9.9016e-01, 9.8956e-01, 9.8940e-01, 9.8777e-01,\n",
       "             9.8438e-01, 9.8241e-01, 9.8156e-01, 9.8029e-01, 9.7996e-01, 9.7422e-01,\n",
       "             9.7166e-01, 9.6590e-01, 9.6134e-01, 9.6075e-01, 9.5926e-01, 9.5035e-01,\n",
       "             9.4447e-01, 9.3930e-01, 9.2147e-01, 9.1811e-01, 8.9488e-01, 8.9041e-01,\n",
       "             8.8100e-01, 8.7280e-01, 8.6244e-01, 8.0915e-01, 8.0055e-01, 7.9973e-01,\n",
       "             6.5120e-01, 5.8015e-01, 4.8745e-01, 4.6553e-01, 4.5896e-01, 4.4619e-01,\n",
       "             4.3885e-01, 3.9628e-01, 3.6930e-01, 3.5925e-01, 3.5414e-01, 3.3104e-01,\n",
       "             2.1664e-01, 1.9269e-01, 1.9219e-01, 1.6442e-01, 1.5866e-01, 1.4438e-01,\n",
       "             1.3321e-01, 1.2527e-01, 1.1452e-01, 1.1264e-01, 8.5215e-02, 6.8434e-02,\n",
       "             4.3517e-02, 3.7875e-02, 3.5798e-02, 3.2913e-02, 2.6401e-02, 2.6258e-02,\n",
       "             2.1116e-02, 1.4696e-02, 1.4442e-02, 1.3120e-02, 1.2718e-02, 1.2386e-02,\n",
       "             1.2121e-02, 1.0369e-02, 8.4407e-03, 7.1779e-03, 6.3945e-03, 6.1934e-03,\n",
       "             6.0073e-03, 5.7138e-03, 5.2611e-03, 5.1392e-03, 2.8693e-03, 2.8394e-03,\n",
       "             2.2969e-03, 1.9845e-03, 1.9263e-03, 1.8695e-03, 1.8611e-03, 1.7550e-03,\n",
       "             1.7333e-03, 1.7121e-03, 1.3448e-03, 1.3214e-03, 1.1134e-03, 9.6958e-04,\n",
       "             5.7092e-04, 4.7745e-04, 4.7075e-04, 4.5386e-04, 4.3645e-04, 4.2557e-04,\n",
       "             4.2455e-04, 3.6097e-04, 3.5109e-04, 3.3412e-04, 3.3260e-04, 3.1376e-04,\n",
       "             3.0379e-04, 3.0129e-04, 2.8897e-04, 2.6410e-04, 1.5190e-04, 1.1697e-04,\n",
       "             1.1144e-04, 9.7797e-05, 9.0482e-05, 8.4457e-05, 8.3666e-05, 6.4592e-05,\n",
       "             6.4115e-05, 6.3919e-05, 6.3387e-05, 6.1487e-05, 5.4799e-05, 4.9111e-05,\n",
       "             4.6658e-05, 3.9171e-05, 3.1601e-05, 3.1303e-05, 3.0248e-05, 2.9544e-05,\n",
       "             2.7041e-05, 2.6639e-05, 2.5154e-05, 2.5073e-05, 2.2353e-05, 2.0154e-05,\n",
       "             1.7645e-05, 1.7559e-05, 1.7369e-05, 1.6191e-05, 1.5536e-05, 1.4340e-05,\n",
       "             1.3882e-05, 1.3344e-05, 1.1909e-05, 1.0818e-05, 9.5869e-06, 9.5774e-06,\n",
       "             9.0463e-06, 8.8431e-06, 6.6371e-06, 5.0889e-06, 4.6608e-06, 4.5413e-06,\n",
       "             4.3050e-06, 3.6752e-06, 3.6336e-06, 2.8554e-06, 2.6596e-06, 2.6384e-06,\n",
       "             2.5252e-06, 2.4992e-06, 2.4393e-06, 2.2802e-06, 2.2481e-06, 2.1881e-06,\n",
       "             1.8388e-06, 1.6673e-06, 1.4584e-06, 1.3513e-06, 1.1960e-06, 1.1882e-06,\n",
       "             9.7368e-07, 9.1552e-07, 8.6922e-07, 7.5976e-07, 6.6324e-07, 6.4426e-07,\n",
       "             5.7016e-07, 5.2572e-07, 4.8927e-07, 4.7348e-07, 4.6771e-07, 4.6328e-07,\n",
       "             4.5226e-07, 4.4374e-07, 4.4363e-07, 3.5775e-07, 3.3208e-07, 2.9304e-07,\n",
       "             2.8233e-07, 2.5940e-07, 2.2791e-07, 2.1451e-07, 1.9918e-07, 1.9752e-07,\n",
       "             1.7329e-07, 1.6964e-07, 1.4778e-07, 1.3140e-07, 9.8608e-08, 9.6355e-08,\n",
       "             9.3034e-08, 7.3228e-08, 6.6734e-08, 5.9012e-08, 5.6352e-08, 5.4704e-08,\n",
       "             5.0422e-08, 4.9995e-08, 4.6170e-08, 3.5381e-08, 3.3738e-08, 3.1314e-08,\n",
       "             3.0089e-08, 2.8853e-08, 2.7305e-08, 2.6898e-08, 2.4658e-08, 2.1200e-08,\n",
       "             1.9117e-08, 1.8527e-08, 1.7402e-08, 1.3941e-08, 1.1524e-08, 1.1042e-08,\n",
       "             9.7984e-09, 9.6867e-09, 9.3474e-09, 9.1447e-09, 8.8996e-09, 8.4158e-09,\n",
       "             8.3996e-09, 5.3876e-09, 4.9513e-09, 4.2444e-09, 4.1772e-09, 3.7363e-09,\n",
       "             3.4097e-09, 3.2546e-09, 2.9561e-09, 2.2005e-09, 1.8214e-09, 1.5845e-09,\n",
       "             1.5010e-09, 1.1694e-09, 1.1173e-09, 8.9433e-10, 8.8256e-10, 8.5948e-10,\n",
       "             8.5218e-10, 7.8654e-10, 7.1001e-10, 6.5226e-10, 5.9843e-10, 4.9850e-10,\n",
       "             4.5118e-10, 4.1799e-10, 3.9940e-10, 3.4257e-10, 3.1786e-10, 2.9726e-10,\n",
       "             2.8368e-10, 2.2069e-10, 2.1015e-10, 2.0713e-10, 2.0443e-10, 2.0400e-10,\n",
       "             1.8765e-10, 1.3487e-10, 1.2096e-10, 8.8842e-11, 8.2925e-11, 8.2289e-11,\n",
       "             7.1906e-11, 7.1142e-11, 5.4525e-11, 5.0830e-11, 2.9036e-11, 2.7332e-11,\n",
       "             2.1944e-11, 2.0110e-11, 1.6916e-11, 1.5131e-11, 1.3974e-11, 8.6244e-12,\n",
       "             8.4482e-12, 7.6893e-12, 4.5732e-12, 3.1500e-12, 2.6301e-12, 1.1981e-12,\n",
       "             1.1711e-12, 1.0194e-12, 6.3526e-13, 4.9402e-13, 4.3088e-13, 3.7284e-13,\n",
       "             2.6769e-13, 2.2911e-13, 2.2336e-13, 2.2317e-13, 8.1962e-14, 1.9042e-14,\n",
       "             1.0246e-14, 7.5566e-15, 1.4230e-15, 1.3032e-15, 8.3465e-16, 7.0545e-16,\n",
       "             4.6836e-16, 4.3261e-16, 3.3479e-16, 6.8222e-17, 5.6809e-17, 6.6368e-18,\n",
       "             4.0896e-19, 8.8062e-21, 3.9739e-24])}},\n",
       "   {'fpr': np.float64(0.20846905537459284),\n",
       "    'tpr': np.float64(0.9942528735632183),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0033, 0.0098, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
       "             0.0163, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228,\n",
       "             0.0228, 0.0228, 0.0228, 0.0228, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293,\n",
       "             0.0293, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0358, 0.0358,\n",
       "             0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0391, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0423, 0.0423, 0.0456,\n",
       "             0.0456, 0.0456, 0.0456, 0.0456, 0.0489, 0.0489, 0.0489, 0.0489, 0.0489,\n",
       "             0.0489, 0.0489, 0.0489, 0.0489, 0.0489, 0.0489, 0.0489, 0.0489, 0.0521,\n",
       "             0.0521, 0.0521, 0.0521, 0.0554, 0.0586, 0.0619, 0.0619, 0.0651, 0.0651,\n",
       "             0.0651, 0.0651, 0.0651, 0.0684, 0.0717, 0.0749, 0.0749, 0.0749, 0.0749,\n",
       "             0.0749, 0.0749, 0.0749, 0.0749, 0.0749, 0.0749, 0.0749, 0.0782, 0.0782,\n",
       "             0.0814, 0.0814, 0.0847, 0.0847, 0.0879, 0.0879, 0.0912, 0.0945, 0.0945,\n",
       "             0.0945, 0.0945, 0.0945, 0.0945, 0.0977, 0.1010, 0.1042, 0.1075, 0.1107,\n",
       "             0.1140, 0.1173, 0.1173, 0.1205, 0.1205, 0.1205, 0.1205, 0.1205, 0.1238,\n",
       "             0.1270, 0.1303, 0.1303, 0.1303, 0.1336, 0.1336, 0.1336, 0.1368, 0.1401,\n",
       "             0.1433, 0.1466, 0.1498, 0.1531, 0.1531, 0.1564, 0.1564, 0.1596, 0.1629,\n",
       "             0.1629, 0.1661, 0.1694, 0.1726, 0.1759, 0.1792, 0.1824, 0.1857, 0.1889,\n",
       "             0.1922, 0.1954, 0.1987, 0.2020, 0.2052, 0.2085, 0.2117, 0.2150, 0.2182,\n",
       "             0.2215, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345, 0.2378, 0.2410, 0.2443,\n",
       "             0.2476, 0.2508, 0.2541, 0.2573, 0.2606, 0.2638, 0.2671, 0.2704, 0.2736,\n",
       "             0.2769, 0.2801, 0.2834, 0.2866, 0.2899, 0.2932, 0.2964, 0.2997, 0.3029,\n",
       "             0.3062, 0.3094, 0.3127, 0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3290,\n",
       "             0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453, 0.3485, 0.3518, 0.3550,\n",
       "             0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746, 0.3779, 0.3811, 0.3844,\n",
       "             0.3876, 0.3909, 0.3941, 0.3974, 0.4007, 0.4039, 0.4072, 0.4104, 0.4137,\n",
       "             0.4169, 0.4202, 0.4235, 0.4267, 0.4300, 0.4332, 0.4365, 0.4397, 0.4430,\n",
       "             0.4463, 0.4495, 0.4528, 0.4560, 0.4593, 0.4593, 0.4625, 0.4658, 0.4691,\n",
       "             0.4723, 0.4756, 0.4788, 0.4821, 0.4853, 0.4886, 0.4919, 0.4951, 0.4984,\n",
       "             0.5016, 0.5049, 0.5081, 0.5114, 0.5147, 0.5179, 0.5212, 0.5244, 0.5277,\n",
       "             0.5309, 0.5342, 0.5375, 0.5407, 0.5440, 0.5472, 0.5505, 0.5537, 0.5570,\n",
       "             0.5603, 0.5635, 0.5668, 0.5700, 0.5733, 0.5765, 0.5798, 0.5831, 0.5831,\n",
       "             0.5863, 0.5896, 0.5928, 0.5961, 0.5993, 0.6026, 0.6059, 0.6091, 0.6124,\n",
       "             0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319, 0.6352, 0.6384, 0.6417,\n",
       "             0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612, 0.6645, 0.6678, 0.6710,\n",
       "             0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906, 0.6938, 0.6971, 0.7003,\n",
       "             0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199, 0.7231, 0.7264, 0.7296,\n",
       "             0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492, 0.7524, 0.7557, 0.7590,\n",
       "             0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785, 0.7818, 0.7850, 0.7883,\n",
       "             0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078, 0.8111, 0.8143, 0.8176,\n",
       "             0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371, 0.8404, 0.8436, 0.8469,\n",
       "             0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664, 0.8697, 0.8730, 0.8762,\n",
       "             0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958, 0.8990, 0.9023, 0.9055,\n",
       "             0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251, 0.9283, 0.9316, 0.9349,\n",
       "             0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544, 0.9577, 0.9609, 0.9642,\n",
       "             0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837, 0.9870, 0.9902, 0.9935,\n",
       "             0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.5621, 0.6356, 0.6736, 0.7000, 0.7115, 0.7253, 0.7345, 0.7379,\n",
       "             0.7460, 0.7552, 0.7563, 0.7598, 0.7655, 0.7678, 0.7690, 0.7713, 0.7724,\n",
       "             0.7736, 0.7782, 0.7816, 0.7851, 0.7874, 0.7897, 0.7920, 0.7966, 0.7977,\n",
       "             0.8000, 0.8023, 0.8069, 0.8080, 0.8103, 0.8126, 0.8138, 0.8149, 0.8161,\n",
       "             0.8207, 0.8218, 0.8230, 0.8264, 0.8287, 0.8299, 0.8310, 0.8322, 0.8345,\n",
       "             0.8356, 0.8379, 0.8391, 0.8402, 0.8414, 0.8425, 0.8460, 0.8483, 0.8494,\n",
       "             0.8506, 0.8506, 0.8517, 0.8540, 0.8552, 0.8563, 0.8575, 0.8586, 0.8598,\n",
       "             0.8609, 0.8621, 0.8632, 0.8655, 0.8655, 0.8678, 0.8690, 0.8701, 0.8724,\n",
       "             0.8736, 0.8759, 0.8770, 0.8782, 0.8805, 0.8816, 0.8828, 0.8839, 0.8851,\n",
       "             0.8862, 0.8862, 0.8874, 0.8885, 0.8897, 0.8908, 0.8920, 0.8931, 0.8943,\n",
       "             0.8954, 0.8954, 0.8966, 0.8977, 0.8989, 0.9000, 0.9011, 0.9023, 0.9034,\n",
       "             0.9046, 0.9057, 0.9069, 0.9080, 0.9092, 0.9103, 0.9126, 0.9126, 0.9138,\n",
       "             0.9149, 0.9161, 0.9172, 0.9184, 0.9195, 0.9207, 0.9218, 0.9218, 0.9230,\n",
       "             0.9241, 0.9253, 0.9264, 0.9276, 0.9287, 0.9299, 0.9299, 0.9310, 0.9310,\n",
       "             0.9322, 0.9333, 0.9345, 0.9356, 0.9356, 0.9368, 0.9379, 0.9391, 0.9402,\n",
       "             0.9414, 0.9425, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483, 0.9494, 0.9494,\n",
       "             0.9506, 0.9517, 0.9529, 0.9529, 0.9529, 0.9529, 0.9540, 0.9540, 0.9552,\n",
       "             0.9563, 0.9575, 0.9586, 0.9586, 0.9586, 0.9586, 0.9598, 0.9609, 0.9621,\n",
       "             0.9632, 0.9644, 0.9655, 0.9667, 0.9678, 0.9690, 0.9701, 0.9701, 0.9713,\n",
       "             0.9713, 0.9724, 0.9724, 0.9736, 0.9736, 0.9747, 0.9747, 0.9747, 0.9759,\n",
       "             0.9770, 0.9782, 0.9793, 0.9805, 0.9805, 0.9805, 0.9805, 0.9805, 0.9805,\n",
       "             0.9805, 0.9805, 0.9816, 0.9816, 0.9828, 0.9839, 0.9851, 0.9862, 0.9862,\n",
       "             0.9862, 0.9862, 0.9874, 0.9885, 0.9885, 0.9897, 0.9908, 0.9908, 0.9908,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9986e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9976e-01, 9.9976e-01, 9.9974e-01, 9.9972e-01, 9.9971e-01,\n",
       "             9.9970e-01, 9.9969e-01, 9.9966e-01, 9.9966e-01, 9.9964e-01, 9.9961e-01,\n",
       "             9.9961e-01, 9.9960e-01, 9.9959e-01, 9.9958e-01, 9.9956e-01, 9.9953e-01,\n",
       "             9.9952e-01, 9.9949e-01, 9.9946e-01, 9.9945e-01, 9.9943e-01, 9.9942e-01,\n",
       "             9.9933e-01, 9.9927e-01, 9.9917e-01, 9.9908e-01, 9.9907e-01, 9.9906e-01,\n",
       "             9.9903e-01, 9.9900e-01, 9.9896e-01, 9.9885e-01, 9.9884e-01, 9.9871e-01,\n",
       "             9.9833e-01, 9.9832e-01, 9.9814e-01, 9.9784e-01, 9.9775e-01, 9.9735e-01,\n",
       "             9.9699e-01, 9.9696e-01, 9.9688e-01, 9.9653e-01, 9.9647e-01, 9.9623e-01,\n",
       "             9.9593e-01, 9.9509e-01, 9.9495e-01, 9.9255e-01, 9.9150e-01, 9.9083e-01,\n",
       "             9.9082e-01, 9.8938e-01, 9.8925e-01, 9.8468e-01, 9.7918e-01, 9.7377e-01,\n",
       "             9.5849e-01, 9.5236e-01, 9.5005e-01, 9.4491e-01, 9.4386e-01, 9.3059e-01,\n",
       "             9.0952e-01, 8.8872e-01, 8.7954e-01, 8.7885e-01, 7.9442e-01, 7.8353e-01,\n",
       "             7.2923e-01, 6.8456e-01, 6.5707e-01, 6.3851e-01, 6.3454e-01, 6.3096e-01,\n",
       "             6.1735e-01, 5.9830e-01, 5.0963e-01, 4.5604e-01, 4.4624e-01, 3.7371e-01,\n",
       "             3.4022e-01, 3.4013e-01, 2.7035e-01, 2.6710e-01, 2.5744e-01, 2.2983e-01,\n",
       "             1.6923e-01, 1.5086e-01, 1.4258e-01, 1.3807e-01, 1.3772e-01, 1.3254e-01,\n",
       "             1.2611e-01, 9.6876e-02, 8.3695e-02, 7.4410e-02, 7.0988e-02, 7.0585e-02,\n",
       "             6.5298e-02, 6.4465e-02, 5.8346e-02, 5.7960e-02, 4.2553e-02, 3.7947e-02,\n",
       "             3.1184e-02, 3.0900e-02, 3.0783e-02, 2.8084e-02, 2.4839e-02, 2.2210e-02,\n",
       "             2.1511e-02, 1.6491e-02, 1.4746e-02, 1.2787e-02, 1.0912e-02, 6.9557e-03,\n",
       "             6.9308e-03, 6.0647e-03, 5.8777e-03, 5.4257e-03, 5.3070e-03, 5.1680e-03,\n",
       "             5.0790e-03, 5.0017e-03, 4.8503e-03, 4.7492e-03, 4.7235e-03, 4.3482e-03,\n",
       "             4.0264e-03, 3.3488e-03, 3.2814e-03, 3.2128e-03, 3.1788e-03, 2.9378e-03,\n",
       "             2.6830e-03, 1.9643e-03, 1.5205e-03, 1.2437e-03, 1.1977e-03, 1.1404e-03,\n",
       "             1.0681e-03, 1.0065e-03, 1.0049e-03, 9.1757e-04, 6.1062e-04, 5.7359e-04,\n",
       "             5.5720e-04, 3.9851e-04, 3.6792e-04, 3.4734e-04, 3.4660e-04, 2.7078e-04,\n",
       "             2.6870e-04, 2.6509e-04, 2.0467e-04, 1.9947e-04, 1.6277e-04, 1.5595e-04,\n",
       "             1.3315e-04, 1.2260e-04, 1.0730e-04, 1.0430e-04, 1.0353e-04, 9.9451e-05,\n",
       "             9.8087e-05, 9.6650e-05, 7.6931e-05, 7.4195e-05, 6.3591e-05, 6.0663e-05,\n",
       "             5.0502e-05, 4.4489e-05, 3.9446e-05, 3.6218e-05, 2.8952e-05, 2.6788e-05,\n",
       "             2.5811e-05, 2.5621e-05, 2.3994e-05, 2.3826e-05, 2.3634e-05, 2.2984e-05,\n",
       "             1.9851e-05, 1.9644e-05, 1.7758e-05, 1.6222e-05, 1.4801e-05, 1.4474e-05,\n",
       "             1.1878e-05, 1.1378e-05, 1.0080e-05, 7.2973e-06, 7.0820e-06, 6.5124e-06,\n",
       "             5.5078e-06, 5.0537e-06, 5.0033e-06, 4.8259e-06, 4.7052e-06, 4.6146e-06,\n",
       "             4.4585e-06, 4.3489e-06, 4.1851e-06, 4.0401e-06, 3.5837e-06, 2.8528e-06,\n",
       "             2.4528e-06, 2.2173e-06, 1.9498e-06, 1.8038e-06, 1.7024e-06, 1.6679e-06,\n",
       "             1.3182e-06, 1.3094e-06, 1.2541e-06, 1.1980e-06, 1.1728e-06, 1.1392e-06,\n",
       "             1.1380e-06, 1.0668e-06, 8.3981e-07, 8.0885e-07, 7.6587e-07, 7.2869e-07,\n",
       "             6.6392e-07, 6.0346e-07, 5.8463e-07, 3.7639e-07, 2.9783e-07, 2.7229e-07,\n",
       "             2.6104e-07, 2.0873e-07, 1.9805e-07, 1.8794e-07, 1.0403e-07, 9.8438e-08,\n",
       "             9.8103e-08, 9.4928e-08, 5.7001e-08, 4.1063e-08, 3.6202e-08, 3.1694e-08,\n",
       "             2.7171e-08, 2.2777e-08, 2.0737e-08, 2.0388e-08, 1.5952e-08, 1.5751e-08,\n",
       "             1.5673e-08, 1.4379e-08, 1.2790e-08, 1.0543e-08, 9.8614e-09, 8.8342e-09,\n",
       "             8.7384e-09, 6.5847e-09, 5.6997e-09, 5.0201e-09, 3.3569e-09, 3.3204e-09,\n",
       "             2.3249e-09, 1.8678e-09, 1.7243e-09, 1.6658e-09, 1.5647e-09, 1.5224e-09,\n",
       "             1.4592e-09, 1.3735e-09, 1.2087e-09, 1.1273e-09, 1.1071e-09, 1.0575e-09,\n",
       "             8.7010e-10, 8.0894e-10, 5.6920e-10, 5.4262e-10, 4.9698e-10, 4.1253e-10,\n",
       "             3.9268e-10, 3.8985e-10, 3.4921e-10, 3.0756e-10, 2.6552e-10, 1.6983e-10,\n",
       "             1.4866e-10, 1.1630e-10, 1.0039e-10, 9.5524e-11, 9.3483e-11, 8.2043e-11,\n",
       "             6.4554e-11, 3.4682e-11, 3.0827e-11, 2.7957e-11, 2.4481e-11, 1.7109e-11,\n",
       "             1.5954e-11, 1.2754e-11, 1.2174e-11, 1.1247e-11, 9.3912e-12, 3.8416e-12,\n",
       "             3.3687e-12, 2.4319e-12, 6.5457e-13, 3.4537e-13, 3.3186e-13, 2.8967e-13,\n",
       "             1.2895e-13, 6.5780e-14, 3.3433e-14, 2.1069e-14, 1.9780e-14, 1.6048e-14,\n",
       "             1.0202e-14, 4.5890e-15, 4.2446e-15, 3.6450e-15, 2.1319e-15, 7.0985e-16,\n",
       "             1.3808e-16, 2.7030e-17, 9.6205e-19, 1.1306e-19, 6.7570e-24])}},\n",
       "   {'fpr': np.float64(0.09446254071661238),\n",
       "    'tpr': np.float64(0.9850574712643678),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
       "             0.0098, 0.0098, 0.0130, 0.0163, 0.0163, 0.0163, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0195, 0.0195, 0.0195, 0.0228, 0.0228, 0.0228, 0.0228, 0.0228,\n",
       "             0.0261, 0.0293, 0.0326, 0.0326, 0.0326, 0.0358, 0.0391, 0.0391, 0.0423,\n",
       "             0.0456, 0.0456, 0.0456, 0.0489, 0.0521, 0.0521, 0.0521, 0.0521, 0.0554,\n",
       "             0.0554, 0.0586, 0.0619, 0.0619, 0.0619, 0.0651, 0.0651, 0.0684, 0.0717,\n",
       "             0.0749, 0.0749, 0.0749, 0.0782, 0.0782, 0.0814, 0.0847, 0.0879, 0.0912,\n",
       "             0.0945, 0.0977, 0.0977, 0.1010, 0.1042, 0.1075, 0.1107, 0.1140, 0.1173,\n",
       "             0.1173, 0.1173, 0.1205, 0.1238, 0.1270, 0.1303, 0.1336, 0.1368, 0.1401,\n",
       "             0.1433, 0.1466, 0.1498, 0.1531, 0.1531, 0.1564, 0.1596, 0.1629, 0.1661,\n",
       "             0.1661, 0.1694, 0.1726, 0.1759, 0.1759, 0.1792, 0.1792, 0.1824, 0.1857,\n",
       "             0.1857, 0.1889, 0.1922, 0.1954, 0.1987, 0.2020, 0.2020, 0.2052, 0.2085,\n",
       "             0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313, 0.2345, 0.2378,\n",
       "             0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2606, 0.2638, 0.2671,\n",
       "             0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899, 0.2899, 0.2932,\n",
       "             0.2932, 0.2964, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160, 0.3192,\n",
       "             0.3192, 0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420, 0.3453,\n",
       "             0.3485, 0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3681, 0.3713, 0.3746,\n",
       "             0.3746, 0.3779, 0.3811, 0.3844, 0.3876, 0.3909, 0.3941, 0.3974, 0.4007,\n",
       "             0.4039, 0.4072, 0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267, 0.4300,\n",
       "             0.4332, 0.4365, 0.4397, 0.4430, 0.4463, 0.4495, 0.4528, 0.4560, 0.4593,\n",
       "             0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788, 0.4821, 0.4853, 0.4886,\n",
       "             0.4919, 0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114, 0.5147, 0.5179,\n",
       "             0.5212, 0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407, 0.5440, 0.5472,\n",
       "             0.5505, 0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700, 0.5733, 0.5765,\n",
       "             0.5798, 0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5993, 0.6026, 0.6059,\n",
       "             0.6091, 0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319, 0.6352,\n",
       "             0.6384, 0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612, 0.6645,\n",
       "             0.6678, 0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906, 0.6938,\n",
       "             0.6971, 0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199, 0.7231,\n",
       "             0.7264, 0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492, 0.7524,\n",
       "             0.7557, 0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785, 0.7818,\n",
       "             0.7850, 0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078, 0.8111,\n",
       "             0.8143, 0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371, 0.8404,\n",
       "             0.8436, 0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664, 0.8697,\n",
       "             0.8730, 0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958, 0.8990,\n",
       "             0.9023, 0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251, 0.9283,\n",
       "             0.9316, 0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544, 0.9577,\n",
       "             0.9609, 0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837, 0.9870,\n",
       "             0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7356, 0.7931, 0.8161, 0.8333, 0.8414, 0.8494, 0.8540, 0.8586,\n",
       "             0.8644, 0.8690, 0.8701, 0.8747, 0.8793, 0.8816, 0.8828, 0.8839, 0.8862,\n",
       "             0.8874, 0.8920, 0.8931, 0.8966, 0.8989, 0.9011, 0.9023, 0.9034, 0.9046,\n",
       "             0.9057, 0.9069, 0.9080, 0.9115, 0.9149, 0.9161, 0.9172, 0.9184, 0.9195,\n",
       "             0.9207, 0.9218, 0.9241, 0.9253, 0.9264, 0.9276, 0.9287, 0.9299, 0.9310,\n",
       "             0.9322, 0.9333, 0.9345, 0.9356, 0.9368, 0.9379, 0.9391, 0.9402, 0.9414,\n",
       "             0.9425, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483, 0.9494, 0.9506, 0.9517,\n",
       "             0.9529, 0.9540, 0.9540, 0.9540, 0.9552, 0.9563, 0.9563, 0.9575, 0.9586,\n",
       "             0.9598, 0.9609, 0.9621, 0.9632, 0.9632, 0.9644, 0.9655, 0.9667, 0.9678,\n",
       "             0.9678, 0.9678, 0.9678, 0.9690, 0.9701, 0.9701, 0.9701, 0.9713, 0.9713,\n",
       "             0.9713, 0.9724, 0.9736, 0.9736, 0.9736, 0.9747, 0.9759, 0.9770, 0.9770,\n",
       "             0.9782, 0.9782, 0.9782, 0.9793, 0.9805, 0.9805, 0.9816, 0.9816, 0.9816,\n",
       "             0.9816, 0.9828, 0.9839, 0.9839, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851,\n",
       "             0.9851, 0.9851, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862,\n",
       "             0.9874, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9897, 0.9897, 0.9897, 0.9897, 0.9897,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9993e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9987e-01, 9.9985e-01, 9.9985e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9980e-01, 9.9979e-01, 9.9975e-01, 9.9959e-01,\n",
       "             9.9956e-01, 9.9949e-01, 9.9935e-01, 9.9925e-01, 9.9924e-01, 9.9917e-01,\n",
       "             9.9916e-01, 9.9853e-01, 9.9838e-01, 9.9764e-01, 9.9734e-01, 9.9712e-01,\n",
       "             9.9666e-01, 9.9641e-01, 9.9482e-01, 9.9478e-01, 9.9417e-01, 9.9357e-01,\n",
       "             9.9259e-01, 9.9152e-01, 9.8696e-01, 9.8611e-01, 9.8424e-01, 9.8289e-01,\n",
       "             9.8283e-01, 9.8115e-01, 9.5179e-01, 9.5145e-01, 9.4344e-01, 9.4127e-01,\n",
       "             9.2820e-01, 9.1886e-01, 8.9308e-01, 7.9808e-01, 6.2093e-01, 6.1160e-01,\n",
       "             5.6105e-01, 5.5338e-01, 5.2386e-01, 5.1969e-01, 4.3575e-01, 3.1544e-01,\n",
       "             3.0164e-01, 2.6431e-01, 2.2007e-01, 1.8099e-01, 1.6598e-01, 1.4001e-01,\n",
       "             1.2461e-01, 1.0268e-01, 1.0095e-01, 9.1635e-02, 9.0850e-02, 8.2867e-02,\n",
       "             5.5068e-02, 5.4999e-02, 3.0039e-02, 2.7696e-02, 2.2663e-02, 1.0594e-02,\n",
       "             7.3405e-03, 7.2553e-03, 2.8978e-03, 2.5010e-03, 2.1528e-03, 2.1473e-03,\n",
       "             1.2381e-03, 9.7286e-04, 4.7287e-04, 4.0888e-04, 3.7055e-04, 3.0720e-04,\n",
       "             2.4643e-04, 2.1792e-04, 2.1599e-04, 2.0821e-04, 1.9169e-04, 8.7627e-05,\n",
       "             8.3945e-05, 8.2009e-05, 6.0969e-05, 3.2406e-05, 3.0532e-05, 2.9065e-05,\n",
       "             2.0450e-05, 1.5365e-05, 1.5250e-05, 1.3238e-05, 6.1569e-06, 5.8838e-06,\n",
       "             3.7548e-06, 3.7525e-06, 2.7044e-06, 2.6369e-06, 2.1730e-06, 2.0836e-06,\n",
       "             1.4395e-06, 1.4294e-06, 1.3482e-06, 1.2987e-06, 1.2475e-06, 9.9127e-07,\n",
       "             9.1319e-07, 8.1599e-07, 8.0122e-07, 7.2886e-07, 5.9771e-07, 4.8109e-07,\n",
       "             4.1633e-07, 4.1191e-07, 4.0159e-07, 2.2885e-07, 2.1633e-07, 2.0908e-07,\n",
       "             1.8789e-07, 1.8095e-07, 1.4833e-07, 1.4174e-07, 1.4152e-07, 6.6249e-08,\n",
       "             6.2068e-08, 5.5187e-08, 5.1371e-08, 4.9582e-08, 4.7295e-08, 2.9183e-08,\n",
       "             2.6120e-08, 2.4226e-08, 2.2739e-08, 1.9921e-08, 1.7347e-08, 6.5988e-09,\n",
       "             6.3325e-09, 5.4389e-09, 4.1349e-09, 2.9335e-09, 2.6634e-09, 2.3236e-09,\n",
       "             2.1568e-09, 2.0742e-09, 1.8410e-09, 1.8238e-09, 1.3317e-09, 1.2450e-09,\n",
       "             1.1707e-09, 1.1430e-09, 1.1090e-09, 1.0630e-09, 9.7212e-10, 9.1859e-10,\n",
       "             7.0277e-10, 4.1891e-10, 2.8028e-10, 2.6980e-10, 2.5590e-10, 1.8438e-10,\n",
       "             1.1931e-10, 1.0814e-10, 1.0454e-10, 1.0396e-10, 1.0226e-10, 9.8571e-11,\n",
       "             9.1167e-11, 7.3830e-11, 5.9585e-11, 5.0946e-11, 4.7878e-11, 4.0557e-11,\n",
       "             3.3918e-11, 2.9224e-11, 2.7224e-11, 2.5045e-11, 1.9918e-11, 1.6303e-11,\n",
       "             1.5515e-11, 1.5329e-11, 1.2340e-11, 1.1793e-11, 1.0365e-11, 9.7660e-12,\n",
       "             7.7079e-12, 6.2304e-12, 4.8092e-12, 4.6527e-12, 4.4024e-12, 4.3152e-12,\n",
       "             4.1148e-12, 3.9991e-12, 3.4117e-12, 2.9936e-12, 2.3279e-12, 2.1243e-12,\n",
       "             2.0354e-12, 2.0215e-12, 1.7091e-12, 1.7035e-12, 1.4912e-12, 1.3344e-12,\n",
       "             1.2872e-12, 6.3213e-13, 5.1462e-13, 4.8842e-13, 4.4325e-13, 3.7336e-13,\n",
       "             3.2761e-13, 2.9487e-13, 2.5169e-13, 2.3418e-13, 2.1339e-13, 2.0958e-13,\n",
       "             1.6949e-13, 1.6710e-13, 1.6445e-13, 1.2224e-13, 1.0236e-13, 9.1409e-14,\n",
       "             9.0733e-14, 7.7185e-14, 7.1486e-14, 6.7730e-14, 6.6825e-14, 6.4102e-14,\n",
       "             5.6708e-14, 3.2112e-14, 2.7952e-14, 2.6493e-14, 2.3905e-14, 2.3463e-14,\n",
       "             1.9860e-14, 1.8544e-14, 1.7985e-14, 1.5269e-14, 1.0344e-14, 9.2552e-15,\n",
       "             6.5740e-15, 5.6427e-15, 5.5860e-15, 4.8980e-15, 4.4371e-15, 4.4017e-15,\n",
       "             3.2372e-15, 2.9514e-15, 2.6892e-15, 1.3195e-15, 1.1014e-15, 7.9386e-16,\n",
       "             7.6618e-16, 7.3825e-16, 1.7176e-16, 1.2831e-16, 8.7073e-17, 6.7629e-17,\n",
       "             6.2795e-17, 4.2479e-17, 3.1567e-17, 2.5905e-17, 2.1273e-17, 2.0894e-17,\n",
       "             1.6885e-17, 1.5113e-17, 1.3435e-17, 1.1439e-17, 8.0810e-18, 4.7424e-18,\n",
       "             4.2020e-18, 2.4571e-18, 2.1729e-18, 1.9755e-18, 1.6228e-18, 1.5842e-18,\n",
       "             1.5690e-18, 1.3528e-18, 1.0544e-18, 8.3467e-19, 7.7236e-19, 7.2577e-19,\n",
       "             5.4635e-19, 4.9469e-19, 3.1001e-19, 2.4634e-19, 2.0788e-19, 1.3943e-19,\n",
       "             1.1409e-19, 1.0841e-19, 9.3430e-20, 5.1271e-20, 4.0426e-20, 3.5253e-20,\n",
       "             3.2353e-20, 3.1462e-20, 2.7464e-20, 2.6640e-20, 1.8321e-20, 1.5883e-20,\n",
       "             1.3802e-20, 8.0419e-21, 5.5923e-21, 4.0539e-21, 2.0005e-21, 5.2275e-22,\n",
       "             2.6590e-22, 2.1600e-22, 1.7674e-22, 5.1101e-23, 4.3970e-23, 2.8455e-23,\n",
       "             2.8127e-23, 2.5190e-23, 2.1442e-23, 1.7988e-23, 7.5777e-24, 6.1469e-24,\n",
       "             3.3827e-24, 2.8136e-24, 2.2353e-24, 1.8796e-24, 4.3283e-25, 3.3768e-25,\n",
       "             1.5347e-25, 3.5331e-26, 1.3879e-26, 6.3339e-27, 4.2232e-27, 3.1270e-27,\n",
       "             7.3338e-28, 3.4301e-28, 8.3740e-29, 4.9000e-29, 2.5588e-33, 1.8654e-34,\n",
       "             0.0000e+00])}},\n",
       "   {'fpr': np.float64(0.06840390879478828),\n",
       "    'tpr': np.float64(0.9827586206896551),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
       "             0.0065, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098, 0.0130, 0.0130, 0.0130,\n",
       "             0.0130, 0.0130, 0.0130, 0.0130, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
       "             0.0163, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0195,\n",
       "             0.0195, 0.0228, 0.0228, 0.0228, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0293, 0.0293, 0.0326, 0.0358, 0.0358, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0423, 0.0423, 0.0456,\n",
       "             0.0489, 0.0521, 0.0554, 0.0586, 0.0619, 0.0619, 0.0651, 0.0684, 0.0684,\n",
       "             0.0684, 0.0717, 0.0717, 0.0749, 0.0749, 0.0782, 0.0814, 0.0814, 0.0847,\n",
       "             0.0879, 0.0912, 0.0945, 0.0945, 0.0977, 0.1010, 0.1042, 0.1075, 0.1075,\n",
       "             0.1075, 0.1107, 0.1107, 0.1140, 0.1140, 0.1173, 0.1205, 0.1238, 0.1270,\n",
       "             0.1303, 0.1336, 0.1368, 0.1368, 0.1368, 0.1401, 0.1433, 0.1466, 0.1498,\n",
       "             0.1531, 0.1564, 0.1564, 0.1596, 0.1629, 0.1661, 0.1661, 0.1694, 0.1726,\n",
       "             0.1759, 0.1792, 0.1824, 0.1857, 0.1889, 0.1889, 0.1922, 0.1922, 0.1954,\n",
       "             0.1987, 0.2020, 0.2052, 0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2248,\n",
       "             0.2280, 0.2313, 0.2345, 0.2378, 0.2410, 0.2443, 0.2476, 0.2508, 0.2541,\n",
       "             0.2573, 0.2606, 0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834,\n",
       "             0.2834, 0.2866, 0.2899, 0.2932, 0.2964, 0.2997, 0.3029, 0.3062, 0.3094,\n",
       "             0.3127, 0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388,\n",
       "             0.3420, 0.3453, 0.3485, 0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3681,\n",
       "             0.3713, 0.3746, 0.3779, 0.3811, 0.3844, 0.3876, 0.3909, 0.3941, 0.3974,\n",
       "             0.4007, 0.4039, 0.4072, 0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267,\n",
       "             0.4300, 0.4332, 0.4365, 0.4397, 0.4430, 0.4463, 0.4495, 0.4528, 0.4560,\n",
       "             0.4593, 0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788, 0.4821, 0.4853,\n",
       "             0.4886, 0.4919, 0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114, 0.5147,\n",
       "             0.5179, 0.5212, 0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407, 0.5440,\n",
       "             0.5472, 0.5505, 0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700, 0.5733,\n",
       "             0.5765, 0.5798, 0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5993, 0.6026,\n",
       "             0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287, 0.6319,\n",
       "             0.6352, 0.6384, 0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612,\n",
       "             0.6645, 0.6678, 0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906,\n",
       "             0.6938, 0.6971, 0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199,\n",
       "             0.7231, 0.7264, 0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492,\n",
       "             0.7524, 0.7557, 0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785,\n",
       "             0.7818, 0.7850, 0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078,\n",
       "             0.8111, 0.8143, 0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371,\n",
       "             0.8404, 0.8436, 0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664,\n",
       "             0.8697, 0.8730, 0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958,\n",
       "             0.8990, 0.9023, 0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251,\n",
       "             0.9283, 0.9316, 0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544,\n",
       "             0.9577, 0.9609, 0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837,\n",
       "             0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.6655, 0.7414, 0.7724, 0.7966, 0.8057, 0.8207, 0.8287, 0.8345,\n",
       "             0.8414, 0.8471, 0.8506, 0.8563, 0.8632, 0.8655, 0.8701, 0.8724, 0.8747,\n",
       "             0.8759, 0.8782, 0.8793, 0.8805, 0.8851, 0.8862, 0.8874, 0.8885, 0.8897,\n",
       "             0.8908, 0.8920, 0.8931, 0.8943, 0.8966, 0.8977, 0.8989, 0.9000, 0.9011,\n",
       "             0.9023, 0.9034, 0.9046, 0.9057, 0.9069, 0.9080, 0.9092, 0.9103, 0.9115,\n",
       "             0.9126, 0.9138, 0.9149, 0.9161, 0.9172, 0.9184, 0.9195, 0.9207, 0.9218,\n",
       "             0.9230, 0.9241, 0.9253, 0.9264, 0.9264, 0.9276, 0.9287, 0.9299, 0.9310,\n",
       "             0.9322, 0.9322, 0.9333, 0.9345, 0.9356, 0.9368, 0.9368, 0.9379, 0.9391,\n",
       "             0.9402, 0.9414, 0.9425, 0.9437, 0.9437, 0.9448, 0.9460, 0.9471, 0.9483,\n",
       "             0.9494, 0.9494, 0.9506, 0.9517, 0.9529, 0.9540, 0.9552, 0.9563, 0.9575,\n",
       "             0.9586, 0.9586, 0.9598, 0.9609, 0.9609, 0.9621, 0.9632, 0.9644, 0.9655,\n",
       "             0.9667, 0.9678, 0.9690, 0.9690, 0.9701, 0.9701, 0.9701, 0.9713, 0.9713,\n",
       "             0.9724, 0.9736, 0.9747, 0.9759, 0.9770, 0.9782, 0.9782, 0.9793, 0.9793,\n",
       "             0.9793, 0.9793, 0.9793, 0.9793, 0.9793, 0.9805, 0.9805, 0.9805, 0.9816,\n",
       "             0.9828, 0.9828, 0.9839, 0.9839, 0.9851, 0.9851, 0.9851, 0.9862, 0.9862,\n",
       "             0.9862, 0.9862, 0.9862, 0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9885,\n",
       "             0.9897, 0.9897, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9986e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9982e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9976e-01, 9.9968e-01, 9.9957e-01, 9.9955e-01,\n",
       "             9.9930e-01, 9.9890e-01, 9.9871e-01, 9.9864e-01, 9.9842e-01, 9.9840e-01,\n",
       "             9.9785e-01, 9.9767e-01, 9.9733e-01, 9.9695e-01, 9.9616e-01, 9.9612e-01,\n",
       "             9.9598e-01, 9.9483e-01, 9.9428e-01, 9.9407e-01, 9.9358e-01, 9.9289e-01,\n",
       "             9.9013e-01, 9.8932e-01, 9.8921e-01, 9.8820e-01, 9.8745e-01, 9.8638e-01,\n",
       "             9.8583e-01, 9.8538e-01, 9.6129e-01, 9.6004e-01, 9.4788e-01, 9.4302e-01,\n",
       "             9.3932e-01, 9.2895e-01, 9.2776e-01, 9.2110e-01, 9.1496e-01, 8.7573e-01,\n",
       "             8.6701e-01, 8.4449e-01, 8.4260e-01, 8.2154e-01, 7.6653e-01, 7.0869e-01,\n",
       "             7.0329e-01, 6.2059e-01, 5.9791e-01, 5.7379e-01, 5.2947e-01, 5.1992e-01,\n",
       "             5.1456e-01, 3.3774e-01, 2.9821e-01, 2.1233e-01, 1.5439e-01, 1.4770e-01,\n",
       "             1.3627e-01, 1.1635e-01, 1.1124e-01, 6.4294e-02, 5.4337e-02, 5.0501e-02,\n",
       "             3.3851e-02, 2.1637e-02, 2.0141e-02, 1.5159e-02, 1.2776e-02, 1.2122e-02,\n",
       "             1.1488e-02, 1.1167e-02, 6.5232e-03, 6.1918e-03, 3.8769e-03, 3.6271e-03,\n",
       "             3.5168e-03, 3.3544e-03, 3.2552e-03, 2.6778e-03, 2.6074e-03, 2.0163e-03,\n",
       "             5.2843e-04, 4.1341e-04, 4.1072e-04, 3.4129e-04, 2.2735e-04, 2.1528e-04,\n",
       "             2.1471e-04, 1.8346e-04, 1.5900e-04, 1.5055e-04, 1.2163e-04, 9.3927e-05,\n",
       "             8.6921e-05, 7.9783e-05, 6.6019e-05, 4.7074e-05, 4.5113e-05, 4.3993e-05,\n",
       "             3.4666e-05, 2.3408e-05, 1.3425e-05, 1.2318e-05, 9.3917e-06, 9.2275e-06,\n",
       "             8.7014e-06, 8.3585e-06, 7.3356e-06, 7.2509e-06, 5.5478e-06, 3.8667e-06,\n",
       "             3.4837e-06, 2.4438e-06, 2.0464e-06, 2.0311e-06, 1.8528e-06, 1.7409e-06,\n",
       "             1.3811e-06, 1.2344e-06, 1.2248e-06, 1.0384e-06, 6.5072e-07, 6.1570e-07,\n",
       "             5.7847e-07, 4.8344e-07, 3.4989e-07, 2.8710e-07, 1.8488e-07, 1.8320e-07,\n",
       "             1.7959e-07, 1.4572e-07, 1.3850e-07, 1.1931e-07, 1.0074e-07, 8.8557e-08,\n",
       "             7.1705e-08, 6.7372e-08, 5.4024e-08, 3.7527e-08, 3.6577e-08, 2.6859e-08,\n",
       "             2.1959e-08, 2.0383e-08, 1.9938e-08, 1.9460e-08, 1.8361e-08, 1.8298e-08,\n",
       "             1.7579e-08, 1.6302e-08, 9.3292e-09, 8.9891e-09, 7.3778e-09, 6.1539e-09,\n",
       "             5.0324e-09, 4.5596e-09, 4.1034e-09, 3.7560e-09, 3.4965e-09, 3.3299e-09,\n",
       "             3.1235e-09, 3.0079e-09, 3.0004e-09, 2.4804e-09, 2.4243e-09, 2.3877e-09,\n",
       "             2.2522e-09, 2.1100e-09, 2.0058e-09, 1.2313e-09, 1.1983e-09, 1.1494e-09,\n",
       "             1.0114e-09, 8.2670e-10, 7.5450e-10, 6.7028e-10, 5.8021e-10, 4.8983e-10,\n",
       "             4.5678e-10, 4.2670e-10, 3.0805e-10, 2.9643e-10, 2.8812e-10, 2.5987e-10,\n",
       "             2.1532e-10, 1.4824e-10, 1.4509e-10, 1.4428e-10, 1.0511e-10, 9.6492e-11,\n",
       "             9.0594e-11, 8.6170e-11, 7.1940e-11, 4.8938e-11, 4.6528e-11, 4.5179e-11,\n",
       "             4.4847e-11, 3.6926e-11, 3.5897e-11, 3.4540e-11, 3.2873e-11, 3.1008e-11,\n",
       "             2.3286e-11, 2.3035e-11, 2.1290e-11, 2.1219e-11, 1.9232e-11, 1.4501e-11,\n",
       "             1.2512e-11, 1.2248e-11, 1.1830e-11, 1.0997e-11, 1.0057e-11, 9.9713e-12,\n",
       "             9.7737e-12, 8.6813e-12, 5.2808e-12, 5.0710e-12, 4.7520e-12, 3.6144e-12,\n",
       "             3.1001e-12, 2.8417e-12, 2.7444e-12, 2.6992e-12, 2.2741e-12, 1.9977e-12,\n",
       "             1.2731e-12, 1.1400e-12, 1.0980e-12, 7.1180e-13, 6.8278e-13, 6.7797e-13,\n",
       "             5.1015e-13, 4.9449e-13, 4.7292e-13, 4.3120e-13, 4.0441e-13, 3.9146e-13,\n",
       "             3.6110e-13, 3.5790e-13, 3.2694e-13, 2.8417e-13, 2.7784e-13, 2.0879e-13,\n",
       "             1.9323e-13, 1.8742e-13, 1.3672e-13, 1.3370e-13, 1.0026e-13, 7.3266e-14,\n",
       "             7.1593e-14, 5.4078e-14, 4.7845e-14, 4.5167e-14, 4.2258e-14, 4.0426e-14,\n",
       "             4.0425e-14, 2.6634e-14, 2.1248e-14, 1.9981e-14, 1.4711e-14, 1.0989e-14,\n",
       "             1.0830e-14, 9.8509e-15, 8.6221e-15, 8.4930e-15, 8.0727e-15, 5.6963e-15,\n",
       "             5.2460e-15, 5.1935e-15, 4.7994e-15, 4.1154e-15, 2.7451e-15, 2.5113e-15,\n",
       "             2.2105e-15, 2.0769e-15, 2.0383e-15, 1.6802e-15, 1.4845e-15, 1.4255e-15,\n",
       "             1.4183e-15, 9.5505e-16, 9.0747e-16, 8.5272e-16, 7.6340e-16, 6.8301e-16,\n",
       "             5.4103e-16, 5.1624e-16, 5.1155e-16, 4.6368e-16, 4.5151e-16, 4.5089e-16,\n",
       "             4.3956e-16, 4.1056e-16, 3.4599e-16, 3.1967e-16, 1.9382e-16, 1.8607e-16,\n",
       "             1.4825e-16, 1.2409e-16, 1.1870e-16, 1.1023e-16, 8.7174e-17, 8.5597e-17,\n",
       "             7.9766e-17, 5.9819e-17, 5.1963e-17, 4.0572e-17, 3.5978e-17, 2.7378e-17,\n",
       "             2.3210e-17, 1.9177e-17, 1.4316e-17, 1.3378e-17, 1.0371e-17, 9.5646e-18,\n",
       "             7.9559e-18, 6.6291e-18, 5.3607e-18, 5.3301e-18, 4.9016e-18, 4.2437e-18,\n",
       "             2.3061e-18, 1.6202e-18, 6.2383e-19, 3.0402e-19, 2.4660e-19, 1.6475e-19,\n",
       "             1.5095e-19, 1.3178e-19, 8.1108e-20, 7.5320e-20, 6.3488e-20, 3.1774e-20,\n",
       "             2.9888e-20, 2.9109e-20, 2.4416e-20, 1.9905e-20, 1.9738e-20, 1.7946e-20,\n",
       "             7.5062e-21, 7.0796e-21, 6.0626e-21, 2.6736e-21, 7.5894e-22, 3.6925e-23,\n",
       "             1.9091e-23, 7.5897e-24, 5.4552e-24, 1.8391e-24, 4.1704e-27, 3.5881e-27,\n",
       "             2.1941e-27, 1.3004e-34])}},\n",
       "   {'fpr': np.float64(0.15960912052117263),\n",
       "    'tpr': np.float64(0.993103448275862),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0195, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
       "             0.0261, 0.0261, 0.0293, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "             0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358,\n",
       "             0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358, 0.0358,\n",
       "             0.0358, 0.0391, 0.0391, 0.0391, 0.0391, 0.0423, 0.0423, 0.0423, 0.0423,\n",
       "             0.0423, 0.0423, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456,\n",
       "             0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0456, 0.0489,\n",
       "             0.0521, 0.0521, 0.0554, 0.0554, 0.0554, 0.0554, 0.0586, 0.0586, 0.0586,\n",
       "             0.0586, 0.0586, 0.0619, 0.0619, 0.0651, 0.0684, 0.0717, 0.0717, 0.0717,\n",
       "             0.0749, 0.0782, 0.0814, 0.0814, 0.0814, 0.0814, 0.0814, 0.0847, 0.0879,\n",
       "             0.0912, 0.0945, 0.0945, 0.0977, 0.1010, 0.1010, 0.1010, 0.1010, 0.1010,\n",
       "             0.1010, 0.1042, 0.1075, 0.1107, 0.1140, 0.1140, 0.1173, 0.1173, 0.1205,\n",
       "             0.1238, 0.1270, 0.1303, 0.1336, 0.1368, 0.1401, 0.1433, 0.1466, 0.1498,\n",
       "             0.1531, 0.1564, 0.1596, 0.1629, 0.1629, 0.1661, 0.1694, 0.1726, 0.1759,\n",
       "             0.1792, 0.1824, 0.1857, 0.1889, 0.1922, 0.1954, 0.1954, 0.1987, 0.2020,\n",
       "             0.2052, 0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2248, 0.2280, 0.2313,\n",
       "             0.2345, 0.2378, 0.2410, 0.2443, 0.2476, 0.2508, 0.2541, 0.2573, 0.2606,\n",
       "             0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834, 0.2866, 0.2899,\n",
       "             0.2932, 0.2964, 0.2997, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127, 0.3160,\n",
       "             0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420,\n",
       "             0.3453, 0.3485, 0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3681, 0.3713,\n",
       "             0.3746, 0.3779, 0.3779, 0.3811, 0.3844, 0.3876, 0.3909, 0.3941, 0.3974,\n",
       "             0.4007, 0.4039, 0.4072, 0.4104, 0.4137, 0.4169, 0.4202, 0.4235, 0.4267,\n",
       "             0.4300, 0.4332, 0.4332, 0.4365, 0.4397, 0.4430, 0.4463, 0.4495, 0.4528,\n",
       "             0.4560, 0.4593, 0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788, 0.4821,\n",
       "             0.4853, 0.4886, 0.4919, 0.4951, 0.4984, 0.5016, 0.5049, 0.5081, 0.5114,\n",
       "             0.5147, 0.5179, 0.5212, 0.5244, 0.5277, 0.5309, 0.5342, 0.5375, 0.5407,\n",
       "             0.5440, 0.5472, 0.5505, 0.5537, 0.5570, 0.5603, 0.5635, 0.5668, 0.5700,\n",
       "             0.5733, 0.5765, 0.5798, 0.5831, 0.5863, 0.5896, 0.5928, 0.5961, 0.5993,\n",
       "             0.6026, 0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221, 0.6254, 0.6287,\n",
       "             0.6319, 0.6352, 0.6384, 0.6417, 0.6450, 0.6482, 0.6515, 0.6547, 0.6580,\n",
       "             0.6612, 0.6645, 0.6678, 0.6710, 0.6743, 0.6775, 0.6808, 0.6840, 0.6873,\n",
       "             0.6906, 0.6938, 0.6971, 0.7003, 0.7036, 0.7068, 0.7101, 0.7134, 0.7166,\n",
       "             0.7199, 0.7231, 0.7264, 0.7296, 0.7329, 0.7362, 0.7394, 0.7427, 0.7459,\n",
       "             0.7492, 0.7524, 0.7557, 0.7590, 0.7622, 0.7655, 0.7687, 0.7720, 0.7752,\n",
       "             0.7785, 0.7818, 0.7850, 0.7883, 0.7915, 0.7948, 0.7980, 0.8013, 0.8046,\n",
       "             0.8078, 0.8111, 0.8143, 0.8176, 0.8208, 0.8241, 0.8274, 0.8306, 0.8339,\n",
       "             0.8371, 0.8404, 0.8436, 0.8469, 0.8502, 0.8534, 0.8567, 0.8599, 0.8632,\n",
       "             0.8664, 0.8697, 0.8730, 0.8762, 0.8795, 0.8827, 0.8860, 0.8893, 0.8925,\n",
       "             0.8958, 0.8990, 0.9023, 0.9055, 0.9088, 0.9121, 0.9153, 0.9186, 0.9218,\n",
       "             0.9251, 0.9283, 0.9316, 0.9349, 0.9381, 0.9414, 0.9446, 0.9479, 0.9511,\n",
       "             0.9544, 0.9577, 0.9609, 0.9642, 0.9674, 0.9707, 0.9739, 0.9772, 0.9805,\n",
       "             0.9837, 0.9870, 0.9902, 0.9935, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7425, 0.7908, 0.8161, 0.8287, 0.8356, 0.8483, 0.8563, 0.8609,\n",
       "             0.8644, 0.8690, 0.8713, 0.8724, 0.8770, 0.8782, 0.8839, 0.8874, 0.8920,\n",
       "             0.8931, 0.8943, 0.8954, 0.8966, 0.8977, 0.8989, 0.9011, 0.9046, 0.9057,\n",
       "             0.9080, 0.9103, 0.9103, 0.9115, 0.9126, 0.9138, 0.9161, 0.9172, 0.9184,\n",
       "             0.9184, 0.9195, 0.9207, 0.9218, 0.9230, 0.9241, 0.9253, 0.9264, 0.9276,\n",
       "             0.9299, 0.9310, 0.9322, 0.9333, 0.9345, 0.9356, 0.9368, 0.9379, 0.9391,\n",
       "             0.9402, 0.9402, 0.9414, 0.9425, 0.9437, 0.9437, 0.9448, 0.9460, 0.9483,\n",
       "             0.9494, 0.9506, 0.9506, 0.9517, 0.9529, 0.9540, 0.9552, 0.9563, 0.9575,\n",
       "             0.9586, 0.9598, 0.9609, 0.9621, 0.9632, 0.9644, 0.9655, 0.9667, 0.9667,\n",
       "             0.9667, 0.9678, 0.9678, 0.9690, 0.9701, 0.9713, 0.9713, 0.9724, 0.9736,\n",
       "             0.9747, 0.9759, 0.9759, 0.9770, 0.9770, 0.9770, 0.9770, 0.9782, 0.9793,\n",
       "             0.9793, 0.9793, 0.9793, 0.9805, 0.9816, 0.9828, 0.9839, 0.9839, 0.9839,\n",
       "             0.9839, 0.9839, 0.9851, 0.9851, 0.9851, 0.9862, 0.9874, 0.9885, 0.9897,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9985e-01, 9.9982e-01, 9.9980e-01,\n",
       "             9.9977e-01, 9.9976e-01, 9.9974e-01, 9.9970e-01, 9.9968e-01, 9.9964e-01,\n",
       "             9.9953e-01, 9.9935e-01, 9.9925e-01, 9.9924e-01, 9.9922e-01, 9.9921e-01,\n",
       "             9.9918e-01, 9.9914e-01, 9.9908e-01, 9.9908e-01, 9.9895e-01, 9.9892e-01,\n",
       "             9.9879e-01, 9.9867e-01, 9.9847e-01, 9.9846e-01, 9.9805e-01, 9.9779e-01,\n",
       "             9.9779e-01, 9.9577e-01, 9.9441e-01, 9.9414e-01, 9.9337e-01, 9.9253e-01,\n",
       "             9.9169e-01, 9.9010e-01, 9.8885e-01, 9.8869e-01, 9.8795e-01, 9.8507e-01,\n",
       "             9.8384e-01, 9.7328e-01, 9.6303e-01, 9.5997e-01, 9.5856e-01, 9.5572e-01,\n",
       "             9.3265e-01, 8.5692e-01, 8.4325e-01, 8.2899e-01, 8.2545e-01, 7.9275e-01,\n",
       "             7.6400e-01, 7.5707e-01, 7.2108e-01, 7.1102e-01, 7.0883e-01, 6.7651e-01,\n",
       "             6.2695e-01, 6.0817e-01, 5.9350e-01, 5.7084e-01, 5.2183e-01, 5.0496e-01,\n",
       "             4.5393e-01, 3.8462e-01, 2.1205e-01, 1.8964e-01, 1.8762e-01, 1.1832e-01,\n",
       "             1.0680e-01, 6.7432e-02, 6.5909e-02, 6.3582e-02, 4.3201e-02, 3.9536e-02,\n",
       "             3.4225e-02, 3.3987e-02, 2.8810e-02, 2.5951e-02, 2.5095e-02, 2.4362e-02,\n",
       "             2.2662e-02, 2.2653e-02, 2.1948e-02, 9.4462e-03, 8.0601e-03, 7.1898e-03,\n",
       "             6.4778e-03, 4.6582e-03, 3.2193e-03, 3.1534e-03, 2.7494e-03, 2.3411e-03,\n",
       "             1.9837e-03, 1.5752e-03, 1.3174e-03, 1.1768e-03, 1.0687e-03, 9.8491e-04,\n",
       "             9.6335e-04, 5.6580e-04, 3.6130e-04, 3.1098e-04, 2.7902e-04, 2.7454e-04,\n",
       "             2.5719e-04, 2.2809e-04, 1.8952e-04, 1.7546e-04, 1.7001e-04, 1.6285e-04,\n",
       "             1.4447e-04, 1.3710e-04, 1.1860e-04, 9.1754e-05, 8.9395e-05, 8.7689e-05,\n",
       "             8.7185e-05, 8.4823e-05, 7.6336e-05, 7.1234e-05, 5.9460e-05, 5.4121e-05,\n",
       "             5.3539e-05, 4.4529e-05, 2.2845e-05, 2.2134e-05, 2.1774e-05, 1.8545e-05,\n",
       "             1.7888e-05, 1.5043e-05, 1.4775e-05, 1.2480e-05, 1.1340e-05, 1.0923e-05,\n",
       "             8.0282e-06, 7.1642e-06, 5.2478e-06, 4.1648e-06, 2.8990e-06, 2.2091e-06,\n",
       "             2.1258e-06, 1.9839e-06, 1.8253e-06, 1.8086e-06, 1.7536e-06, 1.6707e-06,\n",
       "             1.5561e-06, 1.4569e-06, 1.3938e-06, 1.2972e-06, 1.1090e-06, 1.0759e-06,\n",
       "             1.0436e-06, 8.5320e-07, 5.5240e-07, 5.2340e-07, 4.9782e-07, 3.8149e-07,\n",
       "             3.0043e-07, 2.7923e-07, 2.7690e-07, 2.6282e-07, 2.4495e-07, 1.9411e-07,\n",
       "             1.7465e-07, 1.6987e-07, 1.6394e-07, 1.4021e-07, 1.3958e-07, 1.3048e-07,\n",
       "             1.2994e-07, 9.6303e-08, 9.4302e-08, 8.9192e-08, 8.2356e-08, 7.9686e-08,\n",
       "             7.3251e-08, 6.0910e-08, 5.5149e-08, 4.4391e-08, 4.0560e-08, 4.0204e-08,\n",
       "             2.6459e-08, 2.4966e-08, 2.0831e-08, 1.6668e-08, 1.0609e-08, 1.0360e-08,\n",
       "             6.1072e-09, 5.8135e-09, 5.2381e-09, 5.1125e-09, 4.8441e-09, 4.8154e-09,\n",
       "             4.7863e-09, 3.4848e-09, 3.4602e-09, 3.4253e-09, 2.6125e-09, 2.4310e-09,\n",
       "             2.3434e-09, 1.8947e-09, 1.7804e-09, 1.7501e-09, 1.4000e-09, 1.3502e-09,\n",
       "             1.0932e-09, 1.0903e-09, 9.1380e-10, 9.0544e-10, 8.2607e-10, 7.7347e-10,\n",
       "             5.3994e-10, 5.1653e-10, 4.9575e-10, 4.6881e-10, 4.1377e-10, 3.6720e-10,\n",
       "             3.3597e-10, 2.6479e-10, 2.5414e-10, 2.5260e-10, 2.4447e-10, 2.4293e-10,\n",
       "             1.9787e-10, 1.8508e-10, 1.6813e-10, 1.3958e-10, 1.1860e-10, 9.4427e-11,\n",
       "             7.8205e-11, 7.7415e-11, 6.6333e-11, 6.5492e-11, 6.4304e-11, 6.1665e-11,\n",
       "             5.7720e-11, 4.8596e-11, 4.5117e-11, 3.4806e-11, 2.0648e-11, 1.9983e-11,\n",
       "             1.8525e-11, 1.7647e-11, 1.3070e-11, 1.2019e-11, 1.0430e-11, 8.3160e-12,\n",
       "             5.8564e-12, 5.7655e-12, 5.5720e-12, 4.4204e-12, 4.1323e-12, 4.1215e-12,\n",
       "             3.0328e-12, 2.9104e-12, 2.0840e-12, 1.9771e-12, 1.8563e-12, 1.7485e-12,\n",
       "             1.7022e-12, 1.5755e-12, 1.4707e-12, 9.5939e-13, 9.2879e-13, 8.8693e-13,\n",
       "             3.8595e-13, 3.7112e-13, 2.9221e-13, 2.4889e-13, 2.3442e-13, 2.1113e-13,\n",
       "             2.0448e-13, 1.6805e-13, 7.1670e-14, 6.9970e-14, 5.0736e-14, 3.1179e-14,\n",
       "             2.8935e-14, 2.8915e-14, 2.2269e-14, 1.1759e-14, 9.1897e-15, 8.2944e-15,\n",
       "             8.0583e-15, 7.3183e-15, 5.2710e-15, 1.9841e-15, 1.9160e-15, 1.4322e-15,\n",
       "             1.3398e-15, 1.1168e-15, 1.0929e-15, 9.1189e-16, 8.1115e-16, 7.9751e-16,\n",
       "             6.2058e-16, 5.5254e-16, 5.3329e-16, 3.7211e-16, 3.7116e-16, 8.8680e-17,\n",
       "             8.1609e-17, 7.1219e-17, 6.2794e-17, 5.4551e-17, 1.3357e-17, 1.0555e-17,\n",
       "             7.6948e-18, 1.6830e-18, 3.6178e-19, 2.6185e-19, 1.4097e-19, 6.8708e-20,\n",
       "             3.8379e-20, 3.7367e-20, 1.0676e-20, 9.4013e-21, 6.7412e-21, 5.1275e-21,\n",
       "             1.3166e-21, 7.6582e-22, 9.8185e-23, 4.7800e-23, 1.1491e-31, 5.5580e-32])}},\n",
       "   {'fpr': np.float64(0.40716612377850164),\n",
       "    'tpr': np.float64(0.9942528735632183),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0847, 0.0977, 0.0977, 0.0977, 0.1010, 0.1010, 0.1042, 0.1075,\n",
       "             0.1107, 0.1107, 0.1107, 0.1107, 0.1107, 0.1140, 0.1173, 0.1173, 0.1173,\n",
       "             0.1173, 0.1205, 0.1205, 0.1205, 0.1238, 0.1270, 0.1303, 0.1336, 0.1336,\n",
       "             0.1368, 0.1401, 0.1433, 0.1466, 0.1466, 0.1498, 0.1531, 0.1531, 0.1564,\n",
       "             0.1629, 0.1629, 0.1661, 0.1694, 0.1726, 0.1759, 0.1792, 0.1792, 0.1792,\n",
       "             0.1792, 0.1824, 0.1857, 0.1889, 0.1922, 0.1954, 0.1987, 0.2020, 0.2052,\n",
       "             0.2085, 0.2117, 0.2150, 0.2182, 0.2215, 0.2215, 0.2248, 0.2280, 0.2313,\n",
       "             0.2345, 0.2345, 0.2378, 0.2378, 0.2410, 0.2443, 0.2476, 0.2508, 0.2541,\n",
       "             0.2573, 0.2606, 0.2638, 0.2671, 0.2704, 0.2736, 0.2769, 0.2801, 0.2834,\n",
       "             0.2866, 0.2899, 0.2932, 0.2964, 0.2997, 0.3029, 0.3062, 0.3094, 0.3127,\n",
       "             0.3160, 0.3192, 0.3225, 0.3257, 0.3290, 0.3322, 0.3355, 0.3388, 0.3420,\n",
       "             0.3420, 0.3453, 0.3485, 0.3518, 0.3550, 0.3583, 0.3616, 0.3648, 0.3648,\n",
       "             0.3681, 0.3713, 0.3746, 0.3779, 0.3811, 0.3844, 0.3876, 0.3909, 0.3941,\n",
       "             0.3974, 0.4007, 0.4039, 0.4072, 0.4104, 0.4137, 0.4137, 0.4169, 0.4202,\n",
       "             0.4235, 0.4267, 0.4300, 0.4332, 0.4365, 0.4397, 0.4430, 0.4463, 0.4495,\n",
       "             0.4528, 0.4560, 0.4593, 0.4625, 0.4658, 0.4691, 0.4723, 0.4756, 0.4788,\n",
       "             0.4821, 0.4821, 0.4853, 0.4886, 0.4919, 0.4951, 0.4984, 0.5016, 0.5049,\n",
       "             0.5081, 0.5114, 0.5147, 0.5179, 0.5212, 0.5244, 0.5277, 0.5309, 0.5342,\n",
       "             0.5375, 0.5407, 0.5440, 0.5472, 0.5505, 0.5537, 0.5570, 0.5603, 0.5635,\n",
       "             0.5668, 0.5700, 0.5733, 0.5765, 0.5798, 0.5831, 0.5863, 0.5896, 0.5928,\n",
       "             0.5961, 0.5993, 0.6026, 0.6059, 0.6091, 0.6124, 0.6156, 0.6189, 0.6221,\n",
       "             0.6254, 0.6287, 0.6319, 0.6319, 0.6352, 0.6352, 0.6352, 0.6384, 0.6417,\n",
       "             0.6450, 0.6482, 0.6515, 0.6547, 0.6580, 0.6612, 0.6645, 0.6678, 0.6710,\n",
       "             0.6743, 0.6775, 0.6808, 0.6840, 0.6873, 0.6906, 0.6938, 0.6971, 0.7003,\n",
       "             0.7036, 0.7068, 0.7101, 0.7134, 0.7166, 0.7199, 0.7231, 0.7264, 0.7296,\n",
       "             0.7329, 0.7362, 0.7394, 0.7427, 0.7459, 0.7492, 0.7524, 0.7557, 0.7590,\n",
       "             0.7622, 0.7655, 0.7687, 0.7720, 0.7752, 0.7785, 0.7818, 0.7850, 0.7883,\n",
       "             0.7915, 0.7948, 0.7980, 0.8013, 0.8046, 0.8078, 0.8111, 0.8143, 0.8176,\n",
       "             0.8208, 0.8241, 0.8274, 0.8306, 0.8339, 0.8371, 0.8404, 0.8436, 0.8469,\n",
       "             0.8502, 0.8534, 0.8567, 0.8599, 0.8632, 0.8664, 0.8697, 0.8730, 0.8762,\n",
       "             0.8795, 0.8827, 0.8860, 0.8893, 0.8925, 0.8958, 0.8990, 0.9023, 0.9055,\n",
       "             0.9088, 0.9121, 0.9153, 0.9186, 0.9218, 0.9251, 0.9283, 0.9316, 0.9349,\n",
       "             0.9381, 0.9414, 0.9446, 0.9479, 0.9511, 0.9544, 0.9577, 0.9609, 0.9642,\n",
       "             0.9674, 0.9707, 0.9739, 0.9772, 0.9805, 0.9837, 0.9870, 0.9902, 0.9935,\n",
       "             0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9437, 0.9552, 0.9598, 0.9609, 0.9632, 0.9667, 0.9678, 0.9678,\n",
       "             0.9701, 0.9713, 0.9724, 0.9736, 0.9747, 0.9747, 0.9747, 0.9759, 0.9770,\n",
       "             0.9782, 0.9782, 0.9793, 0.9805, 0.9805, 0.9805, 0.9805, 0.9805, 0.9816,\n",
       "             0.9816, 0.9816, 0.9816, 0.9816, 0.9828, 0.9828, 0.9828, 0.9839, 0.9839,\n",
       "             0.9839, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9862, 0.9874,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9897, 0.9897, 0.9897, 0.9897,\n",
       "             0.9897, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9985e-01, 9.9984e-01, 9.9979e-01, 9.9979e-01, 9.9976e-01,\n",
       "             9.9973e-01, 9.9968e-01, 9.9967e-01, 9.9956e-01, 9.9952e-01, 9.9946e-01,\n",
       "             9.9937e-01, 9.9930e-01, 9.9928e-01, 9.9910e-01, 9.9883e-01, 9.9879e-01,\n",
       "             9.9842e-01, 9.9788e-01, 9.9752e-01, 9.9585e-01, 9.9456e-01, 9.9432e-01,\n",
       "             9.9391e-01, 9.9327e-01, 9.9277e-01, 9.9264e-01, 9.9155e-01, 9.9101e-01,\n",
       "             9.9100e-01, 9.9096e-01, 9.8926e-01, 9.8896e-01, 9.8718e-01, 9.8712e-01,\n",
       "             9.8687e-01, 9.8655e-01, 9.8584e-01, 9.8177e-01, 9.7785e-01, 9.7769e-01,\n",
       "             9.7617e-01, 9.7391e-01, 9.6645e-01, 9.6036e-01, 9.5843e-01, 9.5091e-01,\n",
       "             9.4750e-01, 9.4186e-01, 9.3837e-01, 9.3521e-01, 9.3325e-01, 9.2647e-01,\n",
       "             9.1827e-01, 9.1819e-01, 9.1018e-01, 8.7431e-01, 8.4068e-01, 8.3329e-01,\n",
       "             8.1714e-01, 8.0881e-01, 7.9513e-01, 7.8239e-01, 7.8050e-01, 7.3406e-01,\n",
       "             7.2542e-01, 7.2070e-01, 7.1849e-01, 6.6646e-01, 6.5411e-01, 6.3450e-01,\n",
       "             5.2118e-01, 4.8745e-01, 4.8398e-01, 4.7280e-01, 4.1788e-01, 3.7551e-01,\n",
       "             3.5434e-01, 3.4506e-01, 3.2689e-01, 3.1080e-01, 3.0322e-01, 3.0105e-01,\n",
       "             2.9921e-01, 2.4211e-01, 1.5474e-01, 1.5194e-01, 1.1454e-01, 1.1445e-01,\n",
       "             1.1013e-01, 9.4962e-02, 8.8094e-02, 8.3396e-02, 8.0044e-02, 7.3838e-02,\n",
       "             7.0535e-02, 5.8181e-02, 4.0192e-02, 3.1473e-02, 2.7407e-02, 1.2826e-02,\n",
       "             1.2694e-02, 1.1758e-02, 9.7902e-03, 9.4823e-03, 9.2022e-03, 8.7313e-03,\n",
       "             8.2645e-03, 7.3985e-03, 7.3048e-03, 7.1701e-03, 6.8347e-03, 6.6492e-03,\n",
       "             6.0072e-03, 5.2545e-03, 4.9417e-03, 4.2529e-03, 4.0833e-03, 4.0648e-03,\n",
       "             3.9032e-03, 3.7305e-03, 3.4819e-03, 2.9932e-03, 2.5155e-03, 2.4943e-03,\n",
       "             1.9667e-03, 1.2680e-03, 1.2318e-03, 9.9768e-04, 7.4822e-04, 7.4595e-04,\n",
       "             7.2377e-04, 6.5367e-04, 6.2651e-04, 6.0220e-04, 5.6899e-04, 5.3781e-04,\n",
       "             5.0099e-04, 4.2006e-04, 4.0219e-04, 3.7094e-04, 3.2791e-04, 3.1573e-04,\n",
       "             2.2279e-04, 2.0893e-04, 1.7960e-04, 1.6014e-04, 1.4052e-04, 1.3287e-04,\n",
       "             1.2861e-04, 1.2714e-04, 1.2502e-04, 9.9260e-05, 9.2601e-05, 8.5057e-05,\n",
       "             6.6717e-05, 4.8418e-05, 4.7421e-05, 4.2707e-05, 4.0536e-05, 3.8809e-05,\n",
       "             3.6525e-05, 3.3944e-05, 3.2164e-05, 3.1970e-05, 2.5503e-05, 2.4780e-05,\n",
       "             1.9836e-05, 1.7397e-05, 1.5594e-05, 1.1366e-05, 1.1112e-05, 9.4280e-06,\n",
       "             9.2554e-06, 9.1537e-06, 8.9136e-06, 8.1254e-06, 7.4271e-06, 7.2983e-06,\n",
       "             6.9246e-06, 6.8325e-06, 5.8510e-06, 5.6030e-06, 4.7942e-06, 4.5754e-06,\n",
       "             4.3928e-06, 3.8641e-06, 3.2708e-06, 3.0985e-06, 3.0677e-06, 2.3131e-06,\n",
       "             2.1758e-06, 2.1704e-06, 2.1527e-06, 1.5429e-06, 1.4189e-06, 1.3976e-06,\n",
       "             1.3714e-06, 1.3658e-06, 1.1497e-06, 7.2238e-07, 6.8555e-07, 6.5168e-07,\n",
       "             3.6477e-07, 2.1237e-07, 1.9015e-07, 1.6439e-07, 1.1797e-07, 8.9651e-08,\n",
       "             5.9813e-08, 5.3437e-08, 3.8448e-08, 3.7692e-08, 3.7374e-08, 3.1534e-08,\n",
       "             3.1251e-08, 1.3049e-08, 1.2371e-08, 1.2297e-08, 1.1574e-08, 9.2022e-09,\n",
       "             7.5066e-09, 7.3332e-09, 4.4305e-09, 4.0866e-09, 4.0268e-09, 2.5576e-09,\n",
       "             2.3644e-09, 1.2105e-09, 8.9201e-10, 8.7321e-10, 6.7055e-10, 5.9502e-10,\n",
       "             5.1302e-10, 2.6676e-10, 2.3212e-10, 1.9525e-10, 1.0652e-10, 2.2187e-11,\n",
       "             9.2430e-12, 3.7843e-12, 3.6776e-12, 1.5720e-12, 1.4861e-12, 1.3634e-12,\n",
       "             1.0913e-12, 9.7881e-13, 5.7826e-13, 5.0384e-13, 4.9256e-13, 3.3515e-13,\n",
       "             6.2697e-14, 2.5890e-14, 1.8232e-14, 3.2363e-15, 8.9638e-17, 6.3170e-17,\n",
       "             2.9467e-19, 2.9337e-19])}}],\n",
       "  [{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 3.9753e-01, 2.8787e-01,  ..., 2.6765e-11, 1.8997e-11,\n",
       "             1.5550e-11])}},\n",
       "   {'fpr': np.float64(0.0035587188612099642),\n",
       "    'tpr': np.float64(0.71875),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9122e-01, 9.8924e-01,  ..., 2.3982e-05, 1.1975e-05,\n",
       "             9.1302e-06])}},\n",
       "   {'fpr': np.float64(0.046263345195729534),\n",
       "    'tpr': np.float64(0.9453125),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9841e-01, 9.9770e-01,  ..., 1.2020e-04, 4.8357e-05,\n",
       "             4.8183e-05])}},\n",
       "   {'fpr': np.float64(0.017793594306049824),\n",
       "    'tpr': np.float64(0.9129464285714286),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0033,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9995e-01, 9.9993e-01,  ..., 1.0187e-07, 1.4640e-08,\n",
       "             5.6864e-09])}},\n",
       "   {'fpr': np.float64(0.028469750889679714),\n",
       "    'tpr': np.float64(0.9631696428571429),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0033, 0.0045,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 9.9999e-01,  ..., 1.0170e-08, 4.7741e-09,\n",
       "             2.5028e-09])}},\n",
       "   {'fpr': np.float64(0.02491103202846975),\n",
       "    'tpr': np.float64(0.9375),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 4.6949e-11, 1.8428e-11,\n",
       "             2.9440e-12])}},\n",
       "   {'fpr': np.float64(0.028469750889679714),\n",
       "    'tpr': np.float64(0.9665178571428571),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0033,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0119e-10, 2.8004e-10,\n",
       "             7.8818e-11])}},\n",
       "   {'fpr': np.float64(0.03558718861209965),\n",
       "    'tpr': np.float64(0.9709821428571429),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0033,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0863e-08, 3.7967e-08,\n",
       "             1.2946e-08])}},\n",
       "   {'fpr': np.float64(0.03558718861209965),\n",
       "    'tpr': np.float64(0.9821428571428571),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.3752e-08, 7.0870e-09,\n",
       "             4.9858e-09])}},\n",
       "   {'fpr': np.float64(0.021352313167259787),\n",
       "    'tpr': np.float64(0.9609375),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0022,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 7.5283e-11, 6.6510e-11,\n",
       "             2.9411e-11])}},\n",
       "   {'fpr': np.float64(0.0498220640569395),\n",
       "    'tpr': np.float64(0.9776785714285714),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0033, 0.0078,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 7.6604e-10, 7.2220e-10,\n",
       "             3.3287e-10])}},\n",
       "   {'fpr': np.float64(0.05693950177935943),\n",
       "    'tpr': np.float64(0.9899553571428571),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0285, 0.0285, 0.0285,\n",
       "             0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0320, 0.0356, 0.0391, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427,\n",
       "             0.0427, 0.0427, 0.0427, 0.0463, 0.0463, 0.0463, 0.0463, 0.0463, 0.0463,\n",
       "             0.0463, 0.0498, 0.0534, 0.0534, 0.0534, 0.0534, 0.0534, 0.0569, 0.0569,\n",
       "             0.0569, 0.0569, 0.0605, 0.0641, 0.0676, 0.0712, 0.0747, 0.0783, 0.0819,\n",
       "             0.0854, 0.0890, 0.0925, 0.0961, 0.0996, 0.0996, 0.1032, 0.1068, 0.1068,\n",
       "             0.1103, 0.1103, 0.1139, 0.1174, 0.1210, 0.1246, 0.1281, 0.1281, 0.1317,\n",
       "             0.1352, 0.1388, 0.1423, 0.1459, 0.1495, 0.1530, 0.1566, 0.1601, 0.1637,\n",
       "             0.1673, 0.1708, 0.1744, 0.1779, 0.1815, 0.1815, 0.1851, 0.1886, 0.1922,\n",
       "             0.1957, 0.1993, 0.2028, 0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242,\n",
       "             0.2278, 0.2313, 0.2349, 0.2384, 0.2420, 0.2456, 0.2491, 0.2527, 0.2562,\n",
       "             0.2598, 0.2633, 0.2669, 0.2705, 0.2740, 0.2776, 0.2811, 0.2847, 0.2883,\n",
       "             0.2918, 0.2954, 0.2989, 0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203,\n",
       "             0.3238, 0.3274, 0.3310, 0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523,\n",
       "             0.3559, 0.3559, 0.3594, 0.3630, 0.3665, 0.3701, 0.3737, 0.3772, 0.3808,\n",
       "             0.3843, 0.3879, 0.3915, 0.3950, 0.3986, 0.4021, 0.4057, 0.4093, 0.4128,\n",
       "             0.4164, 0.4199, 0.4235, 0.4270, 0.4306, 0.4342, 0.4377, 0.4377, 0.4413,\n",
       "             0.4448, 0.4484, 0.4520, 0.4555, 0.4591, 0.4626, 0.4662, 0.4698, 0.4733,\n",
       "             0.4769, 0.4804, 0.4840, 0.4875, 0.4911, 0.4947, 0.4982, 0.5018, 0.5053,\n",
       "             0.5089, 0.5125, 0.5160, 0.5196, 0.5231, 0.5267, 0.5302, 0.5338, 0.5374,\n",
       "             0.5409, 0.5445, 0.5480, 0.5516, 0.5552, 0.5587, 0.5623, 0.5658, 0.5694,\n",
       "             0.5730, 0.5765, 0.5801, 0.5836, 0.5872, 0.5907, 0.5943, 0.5979, 0.6014,\n",
       "             0.6050, 0.6085, 0.6121, 0.6157, 0.6192, 0.6228, 0.6263, 0.6299, 0.6335,\n",
       "             0.6370, 0.6406, 0.6441, 0.6477, 0.6512, 0.6548, 0.6584, 0.6619, 0.6655,\n",
       "             0.6690, 0.6726, 0.6762, 0.6797, 0.6833, 0.6868, 0.6904, 0.6940, 0.6975,\n",
       "             0.7011, 0.7046, 0.7082, 0.7117, 0.7153, 0.7189, 0.7224, 0.7260, 0.7295,\n",
       "             0.7331, 0.7367, 0.7402, 0.7438, 0.7473, 0.7509, 0.7544, 0.7580, 0.7616,\n",
       "             0.7651, 0.7687, 0.7722, 0.7758, 0.7794, 0.7829, 0.7865, 0.7900, 0.7936,\n",
       "             0.7972, 0.8007, 0.8043, 0.8078, 0.8114, 0.8149, 0.8185, 0.8221, 0.8256,\n",
       "             0.8292, 0.8327, 0.8363, 0.8399, 0.8434, 0.8470, 0.8505, 0.8541, 0.8577,\n",
       "             0.8612, 0.8648, 0.8683, 0.8719, 0.8754, 0.8790, 0.8826, 0.8861, 0.8897,\n",
       "             0.8932, 0.8968, 0.9004, 0.9039, 0.9075, 0.9110, 0.9146, 0.9181, 0.9217,\n",
       "             0.9253, 0.9288, 0.9324, 0.9359, 0.9395, 0.9431, 0.9466, 0.9502, 0.9537,\n",
       "             0.9573, 0.9609, 0.9644, 0.9680, 0.9715, 0.9751, 0.9786, 0.9822, 0.9858,\n",
       "             0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0446, 0.0792, 0.1049, 0.1295, 0.1462, 0.1618, 0.1730, 0.1819,\n",
       "             0.1853, 0.1897, 0.1964, 0.2076, 0.2109, 0.2188, 0.2310, 0.2388, 0.2444,\n",
       "             0.2511, 0.2545, 0.2589, 0.2701, 0.2779, 0.2812, 0.2835, 0.2846, 0.2913,\n",
       "             0.2935, 0.2969, 0.3013, 0.3047, 0.3092, 0.3136, 0.3147, 0.3170, 0.3181,\n",
       "             0.3214, 0.3225, 0.3248, 0.3259, 0.3304, 0.3348, 0.3382, 0.3415, 0.3438,\n",
       "             0.3449, 0.3471, 0.3504, 0.3538, 0.3560, 0.3605, 0.3616, 0.3638, 0.3661,\n",
       "             0.3672, 0.3683, 0.3694, 0.3717, 0.3750, 0.3761, 0.3772, 0.3783, 0.3795,\n",
       "             0.3806, 0.3828, 0.3850, 0.3873, 0.3884, 0.3917, 0.3929, 0.3940, 0.3951,\n",
       "             0.3962, 0.3984, 0.3996, 0.4007, 0.4018, 0.4029, 0.4051, 0.4062, 0.4096,\n",
       "             0.4118, 0.4129, 0.4141, 0.4163, 0.4185, 0.4230, 0.4252, 0.4263, 0.4286,\n",
       "             0.4297, 0.4308, 0.4342, 0.4375, 0.4420, 0.4442, 0.4475, 0.4487, 0.4498,\n",
       "             0.4509, 0.4520, 0.4542, 0.4554, 0.4609, 0.4621, 0.4654, 0.4665, 0.4676,\n",
       "             0.4688, 0.4699, 0.4710, 0.4732, 0.4754, 0.4766, 0.4788, 0.4810, 0.4821,\n",
       "             0.4833, 0.4855, 0.4866, 0.4888, 0.4911, 0.4922, 0.4933, 0.4944, 0.4955,\n",
       "             0.4978, 0.4989, 0.5000, 0.5011, 0.5033, 0.5045, 0.5056, 0.5067, 0.5078,\n",
       "             0.5089, 0.5100, 0.5123, 0.5134, 0.5145, 0.5156, 0.5167, 0.5190, 0.5201,\n",
       "             0.5212, 0.5223, 0.5246, 0.5257, 0.5268, 0.5279, 0.5290, 0.5301, 0.5312,\n",
       "             0.5324, 0.5335, 0.5346, 0.5357, 0.5368, 0.5379, 0.5391, 0.5402, 0.5413,\n",
       "             0.5424, 0.5435, 0.5446, 0.5458, 0.5469, 0.5480, 0.5513, 0.5525, 0.5536,\n",
       "             0.5558, 0.5569, 0.5580, 0.5592, 0.5603, 0.5625, 0.5636, 0.5647, 0.5658,\n",
       "             0.5670, 0.5681, 0.5703, 0.5714, 0.5725, 0.5737, 0.5748, 0.5759, 0.5770,\n",
       "             0.5781, 0.5792, 0.5804, 0.5815, 0.5826, 0.5837, 0.5848, 0.5859, 0.5871,\n",
       "             0.5882, 0.5893, 0.5904, 0.5915, 0.5938, 0.5949, 0.5960, 0.5971, 0.5982,\n",
       "             0.5993, 0.6004, 0.6038, 0.6049, 0.6060, 0.6071, 0.6083, 0.6094, 0.6105,\n",
       "             0.6116, 0.6127, 0.6138, 0.6150, 0.6194, 0.6205, 0.6217, 0.6228, 0.6239,\n",
       "             0.6250, 0.6261, 0.6272, 0.6283, 0.6295, 0.6306, 0.6317, 0.6328, 0.6339,\n",
       "             0.6350, 0.6362, 0.6373, 0.6384, 0.6395, 0.6406, 0.6417, 0.6429, 0.6440,\n",
       "             0.6451, 0.6462, 0.6473, 0.6496, 0.6518, 0.6529, 0.6540, 0.6551, 0.6562,\n",
       "             0.6574, 0.6585, 0.6596, 0.6607, 0.6618, 0.6629, 0.6641, 0.6652, 0.6663,\n",
       "             0.6674, 0.6685, 0.6708, 0.6719, 0.6730, 0.6741, 0.6752, 0.6763, 0.6775,\n",
       "             0.6786, 0.6797, 0.6808, 0.6819, 0.6830, 0.6842, 0.6853, 0.6864, 0.6875,\n",
       "             0.6886, 0.6897, 0.6908, 0.6920, 0.6931, 0.6942, 0.6953, 0.6964, 0.6975,\n",
       "             0.6987, 0.7009, 0.7020, 0.7031, 0.7042, 0.7065, 0.7076, 0.7087, 0.7098,\n",
       "             0.7109, 0.7121, 0.7132, 0.7143, 0.7154, 0.7165, 0.7176, 0.7188, 0.7199,\n",
       "             0.7210, 0.7221, 0.7232, 0.7254, 0.7266, 0.7277, 0.7288, 0.7299, 0.7310,\n",
       "             0.7321, 0.7333, 0.7344, 0.7355, 0.7366, 0.7377, 0.7388, 0.7411, 0.7422,\n",
       "             0.7433, 0.7444, 0.7455, 0.7467, 0.7478, 0.7489, 0.7500, 0.7511, 0.7522,\n",
       "             0.7533, 0.7556, 0.7567, 0.7578, 0.7589, 0.7600, 0.7612, 0.7623, 0.7634,\n",
       "             0.7645, 0.7656, 0.7667, 0.7679, 0.7690, 0.7701, 0.7712, 0.7723, 0.7734,\n",
       "             0.7746, 0.7757, 0.7768, 0.7779, 0.7790, 0.7801, 0.7812, 0.7824, 0.7835,\n",
       "             0.7846, 0.7857, 0.7868, 0.7879, 0.7891, 0.7902, 0.7913, 0.7924, 0.7935,\n",
       "             0.7946, 0.7958, 0.7969, 0.7980, 0.7991, 0.8002, 0.8013, 0.8025, 0.8036,\n",
       "             0.8047, 0.8058, 0.8069, 0.8069, 0.8080, 0.8092, 0.8103, 0.8114, 0.8125,\n",
       "             0.8136, 0.8147, 0.8147, 0.8158, 0.8170, 0.8181, 0.8192, 0.8203, 0.8214,\n",
       "             0.8225, 0.8237, 0.8248, 0.8259, 0.8270, 0.8281, 0.8292, 0.8304, 0.8315,\n",
       "             0.8326, 0.8337, 0.8348, 0.8359, 0.8371, 0.8382, 0.8393, 0.8404, 0.8415,\n",
       "             0.8426, 0.8438, 0.8438, 0.8449, 0.8460, 0.8471, 0.8482, 0.8493, 0.8504,\n",
       "             0.8516, 0.8527, 0.8538, 0.8549, 0.8560, 0.8571, 0.8583, 0.8594, 0.8605,\n",
       "             0.8616, 0.8627, 0.8638, 0.8650, 0.8661, 0.8672, 0.8683, 0.8694, 0.8705,\n",
       "             0.8717, 0.8728, 0.8739, 0.8750, 0.8761, 0.8772, 0.8783, 0.8795, 0.8806,\n",
       "             0.8817, 0.8828, 0.8839, 0.8850, 0.8862, 0.8873, 0.8884, 0.8895, 0.8906,\n",
       "             0.8917, 0.8929, 0.8940, 0.8962, 0.8973, 0.8984, 0.8996, 0.9007, 0.9018,\n",
       "             0.9029, 0.9040, 0.9051, 0.9062, 0.9074, 0.9085, 0.9096, 0.9107, 0.9118,\n",
       "             0.9129, 0.9141, 0.9152, 0.9163, 0.9174, 0.9185, 0.9196, 0.9208, 0.9219,\n",
       "             0.9219, 0.9230, 0.9241, 0.9252, 0.9263, 0.9275, 0.9286, 0.9297, 0.9308,\n",
       "             0.9319, 0.9330, 0.9342, 0.9353, 0.9364, 0.9375, 0.9386, 0.9397, 0.9408,\n",
       "             0.9420, 0.9431, 0.9442, 0.9453, 0.9464, 0.9464, 0.9464, 0.9475, 0.9487,\n",
       "             0.9487, 0.9498, 0.9509, 0.9520, 0.9531, 0.9542, 0.9554, 0.9565, 0.9576,\n",
       "             0.9587, 0.9598, 0.9609, 0.9621, 0.9632, 0.9643, 0.9654, 0.9665, 0.9676,\n",
       "             0.9688, 0.9699, 0.9699, 0.9699, 0.9699, 0.9710, 0.9721, 0.9732, 0.9743,\n",
       "             0.9754, 0.9766, 0.9777, 0.9777, 0.9788, 0.9799, 0.9810, 0.9821, 0.9833,\n",
       "             0.9844, 0.9844, 0.9844, 0.9855, 0.9866, 0.9877, 0.9888, 0.9888, 0.9900,\n",
       "             0.9911, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9944,\n",
       "             0.9944, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9964e-01, 9.9964e-01,\n",
       "             9.9963e-01, 9.9963e-01, 9.9962e-01, 9.9961e-01, 9.9961e-01, 9.9961e-01,\n",
       "             9.9961e-01, 9.9961e-01, 9.9959e-01, 9.9957e-01, 9.9956e-01, 9.9954e-01,\n",
       "             9.9954e-01, 9.9953e-01, 9.9952e-01, 9.9952e-01, 9.9951e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9948e-01, 9.9947e-01, 9.9947e-01, 9.9947e-01, 9.9946e-01,\n",
       "             9.9946e-01, 9.9944e-01, 9.9944e-01, 9.9940e-01, 9.9939e-01, 9.9937e-01,\n",
       "             9.9936e-01, 9.9936e-01, 9.9933e-01, 9.9932e-01, 9.9930e-01, 9.9930e-01,\n",
       "             9.9925e-01, 9.9924e-01, 9.9923e-01, 9.9923e-01, 9.9920e-01, 9.9918e-01,\n",
       "             9.9917e-01, 9.9917e-01, 9.9916e-01, 9.9915e-01, 9.9913e-01, 9.9912e-01,\n",
       "             9.9906e-01, 9.9906e-01, 9.9905e-01, 9.9904e-01, 9.9901e-01, 9.9900e-01,\n",
       "             9.9899e-01, 9.9898e-01, 9.9895e-01, 9.9889e-01, 9.9889e-01, 9.9887e-01,\n",
       "             9.9885e-01, 9.9884e-01, 9.9875e-01, 9.9875e-01, 9.9874e-01, 9.9847e-01,\n",
       "             9.9847e-01, 9.9845e-01, 9.9844e-01, 9.9843e-01, 9.9831e-01, 9.9829e-01,\n",
       "             9.9821e-01, 9.9821e-01, 9.9809e-01, 9.9800e-01, 9.9787e-01, 9.9771e-01,\n",
       "             9.9757e-01, 9.9756e-01, 9.9751e-01, 9.9730e-01, 9.9727e-01, 9.9695e-01,\n",
       "             9.9688e-01, 9.9684e-01, 9.9666e-01, 9.9652e-01, 9.9639e-01, 9.9615e-01,\n",
       "             9.9569e-01, 9.9544e-01, 9.9543e-01, 9.9534e-01, 9.9529e-01, 9.9510e-01,\n",
       "             9.9489e-01, 9.9489e-01, 9.9471e-01, 9.9464e-01, 9.9455e-01, 9.9446e-01,\n",
       "             9.9423e-01, 9.9374e-01, 9.9352e-01, 9.9317e-01, 9.9285e-01, 9.9217e-01,\n",
       "             9.9172e-01, 9.9088e-01, 9.8980e-01, 9.8777e-01, 9.8770e-01, 9.8743e-01,\n",
       "             9.8729e-01, 9.8718e-01, 9.8610e-01, 9.8582e-01, 9.8572e-01, 9.8470e-01,\n",
       "             9.8247e-01, 9.8031e-01, 9.7798e-01, 9.7521e-01, 9.7404e-01, 9.7397e-01,\n",
       "             9.7367e-01, 9.7282e-01, 9.7230e-01, 9.7076e-01, 9.6781e-01, 9.6694e-01,\n",
       "             9.6470e-01, 9.6263e-01, 9.5933e-01, 9.5530e-01, 9.5085e-01, 9.4995e-01,\n",
       "             9.4147e-01, 9.3622e-01, 9.2866e-01, 9.2268e-01, 8.9495e-01, 8.8847e-01,\n",
       "             8.5501e-01, 8.5170e-01, 8.5105e-01, 8.3001e-01, 8.2601e-01, 7.8591e-01,\n",
       "             7.3120e-01, 7.3036e-01, 6.3808e-01, 5.9710e-01, 5.9262e-01, 5.6508e-01,\n",
       "             4.9506e-01, 3.5115e-01, 3.0836e-01, 3.0418e-01, 2.2584e-01, 2.0051e-01,\n",
       "             1.7964e-01, 1.7302e-01, 1.6168e-01, 1.5529e-01, 1.2456e-01, 1.2096e-01,\n",
       "             1.2032e-01, 1.1621e-01, 1.1101e-01, 1.0146e-01, 7.1432e-02, 5.9772e-02,\n",
       "             5.8132e-02, 5.7956e-02, 5.7701e-02, 3.3361e-02, 3.0034e-02, 2.7951e-02,\n",
       "             2.7004e-02, 2.5749e-02, 2.1589e-02, 2.1055e-02, 1.8995e-02, 1.8836e-02,\n",
       "             1.7030e-02, 1.6517e-02, 1.6317e-02, 1.5461e-02, 1.5183e-02, 1.3790e-02,\n",
       "             1.2630e-02, 1.2552e-02, 1.2407e-02, 1.2309e-02, 1.0082e-02, 9.1798e-03,\n",
       "             9.1220e-03, 8.0858e-03, 7.6139e-03, 7.2260e-03, 6.9967e-03, 6.6574e-03,\n",
       "             6.2824e-03, 5.6758e-03, 5.5277e-03, 5.4479e-03, 5.1052e-03, 4.9340e-03,\n",
       "             4.7000e-03, 4.6015e-03, 4.5323e-03, 4.4871e-03, 4.3407e-03, 4.1451e-03,\n",
       "             3.9100e-03, 3.5917e-03, 3.4790e-03, 3.3713e-03, 2.6565e-03, 2.5778e-03,\n",
       "             2.4029e-03, 2.3849e-03, 2.2025e-03, 1.7084e-03, 1.6144e-03, 1.4174e-03,\n",
       "             1.3793e-03, 1.2429e-03, 1.2045e-03, 9.8737e-04, 9.2268e-04, 8.9023e-04,\n",
       "             8.8844e-04, 8.4873e-04, 8.4758e-04, 7.0348e-04, 7.0097e-04, 6.2866e-04,\n",
       "             6.1674e-04, 6.1042e-04, 5.8368e-04, 5.8002e-04, 5.7042e-04, 5.5587e-04,\n",
       "             5.4538e-04, 5.4103e-04, 4.8709e-04, 4.8642e-04, 4.4817e-04, 4.4123e-04,\n",
       "             4.3016e-04, 4.0792e-04, 3.9888e-04, 3.9828e-04, 3.8337e-04, 3.7456e-04,\n",
       "             3.6539e-04, 3.4558e-04, 3.4391e-04, 2.8562e-04, 2.7682e-04, 2.6637e-04,\n",
       "             2.6042e-04, 2.5885e-04, 2.5739e-04, 2.0711e-04, 2.0483e-04, 2.0040e-04,\n",
       "             1.8767e-04, 1.8074e-04, 1.7550e-04, 1.6472e-04, 1.6399e-04, 1.5622e-04,\n",
       "             1.5133e-04, 1.4487e-04, 1.3856e-04, 1.2545e-04, 1.0630e-04, 1.0591e-04,\n",
       "             9.0410e-05, 8.9851e-05, 8.1950e-05, 7.7077e-05, 7.3645e-05, 7.0910e-05,\n",
       "             6.4753e-05, 6.4301e-05, 6.4225e-05, 6.0526e-05, 6.0117e-05, 4.6414e-05,\n",
       "             4.5704e-05, 4.4221e-05, 4.4205e-05, 4.2753e-05, 4.1343e-05, 3.9303e-05,\n",
       "             3.7621e-05, 3.4260e-05, 3.3448e-05, 3.1697e-05, 3.0360e-05, 2.8907e-05,\n",
       "             2.5518e-05, 2.4836e-05, 2.4266e-05, 2.3381e-05, 2.3285e-05, 2.2058e-05,\n",
       "             2.1968e-05, 1.8505e-05, 1.7494e-05, 1.6023e-05, 1.5004e-05, 1.1135e-05,\n",
       "             9.6336e-06, 9.4074e-06, 8.0966e-06, 7.2299e-06, 6.4664e-06, 6.4087e-06,\n",
       "             5.4784e-06, 5.4364e-06, 4.8888e-06, 4.6227e-06, 4.5351e-06, 4.3235e-06,\n",
       "             4.2689e-06, 4.2662e-06, 4.1479e-06, 3.9404e-06, 3.3311e-06, 3.2247e-06,\n",
       "             3.2162e-06, 2.8805e-06, 2.8088e-06, 2.3955e-06, 2.3256e-06, 2.1454e-06,\n",
       "             2.1039e-06, 2.0919e-06, 1.9683e-06, 1.9500e-06, 1.8032e-06, 1.7983e-06,\n",
       "             1.6808e-06, 1.6302e-06, 1.5985e-06, 1.5844e-06, 1.5824e-06, 1.5220e-06,\n",
       "             1.4555e-06, 1.4198e-06, 1.3067e-06, 1.2103e-06, 1.1178e-06, 1.1064e-06,\n",
       "             1.0706e-06, 1.0704e-06, 1.0278e-06, 9.4874e-07, 9.1204e-07, 8.8956e-07,\n",
       "             8.3841e-07, 8.3421e-07, 7.7957e-07, 6.6041e-07, 6.0924e-07, 5.4980e-07,\n",
       "             4.9452e-07, 4.8771e-07, 4.4973e-07, 4.4590e-07, 4.1380e-07, 3.8732e-07,\n",
       "             3.7590e-07, 3.4452e-07, 3.2878e-07, 3.2732e-07, 3.2504e-07, 3.1473e-07,\n",
       "             2.9113e-07, 2.2668e-07, 2.2613e-07, 2.0863e-07, 2.0457e-07, 1.8473e-07,\n",
       "             1.8415e-07, 1.7335e-07, 1.6324e-07, 1.5566e-07, 1.5153e-07, 1.3750e-07,\n",
       "             1.3250e-07, 1.3162e-07, 1.2141e-07, 8.4341e-08, 8.1199e-08, 6.8081e-08,\n",
       "             6.2675e-08, 5.4418e-08, 5.3043e-08, 5.1257e-08, 5.0921e-08, 4.7812e-08,\n",
       "             4.7470e-08, 2.7615e-08, 2.3060e-08, 2.1740e-08, 2.0753e-08, 1.4812e-08,\n",
       "             1.0997e-08, 9.7883e-09, 9.5975e-09, 8.0418e-09, 6.2846e-09, 5.0905e-09,\n",
       "             2.5797e-09, 1.6325e-09, 1.3171e-09, 1.2174e-09, 1.2068e-09, 1.2061e-09,\n",
       "             1.0669e-09, 9.7288e-10, 4.9730e-10, 1.6262e-10])}},\n",
       "   {'fpr': np.float64(0.03558718861209965),\n",
       "    'tpr': np.float64(0.9720982142857143),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0056, 0.0100,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 7.3463e-12, 5.5101e-12,\n",
       "             1.6504e-12])}},\n",
       "   {'fpr': np.float64(0.05693950177935943),\n",
       "    'tpr': np.float64(0.9799107142857143),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0285, 0.0285,\n",
       "             0.0285, 0.0285, 0.0285, 0.0285, 0.0320, 0.0320, 0.0320, 0.0320, 0.0356,\n",
       "             0.0356, 0.0356, 0.0356, 0.0391, 0.0427, 0.0463, 0.0463, 0.0463, 0.0463,\n",
       "             0.0463, 0.0463, 0.0463, 0.0498, 0.0498, 0.0498, 0.0534, 0.0569, 0.0569,\n",
       "             0.0605, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641,\n",
       "             0.0641, 0.0676, 0.0712, 0.0712, 0.0712, 0.0747, 0.0783, 0.0819, 0.0854,\n",
       "             0.0890, 0.0925, 0.0925, 0.0961, 0.0996, 0.1032, 0.1068, 0.1103, 0.1103,\n",
       "             0.1139, 0.1174, 0.1210, 0.1246, 0.1281, 0.1317, 0.1352, 0.1388, 0.1423,\n",
       "             0.1423, 0.1459, 0.1495, 0.1495, 0.1530, 0.1530, 0.1566, 0.1601, 0.1637,\n",
       "             0.1673, 0.1708, 0.1744, 0.1779, 0.1815, 0.1851, 0.1886, 0.1922, 0.1957,\n",
       "             0.1957, 0.1993, 0.2028, 0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242,\n",
       "             0.2278, 0.2313, 0.2349, 0.2384, 0.2420, 0.2456, 0.2491, 0.2527, 0.2562,\n",
       "             0.2598, 0.2633, 0.2669, 0.2705, 0.2740, 0.2776, 0.2811, 0.2847, 0.2883,\n",
       "             0.2918, 0.2954, 0.2989, 0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203,\n",
       "             0.3238, 0.3274, 0.3310, 0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523,\n",
       "             0.3559, 0.3594, 0.3630, 0.3665, 0.3701, 0.3737, 0.3772, 0.3808, 0.3843,\n",
       "             0.3879, 0.3915, 0.3950, 0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164,\n",
       "             0.4199, 0.4235, 0.4270, 0.4306, 0.4342, 0.4377, 0.4413, 0.4448, 0.4484,\n",
       "             0.4520, 0.4555, 0.4591, 0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804,\n",
       "             0.4840, 0.4875, 0.4911, 0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5089,\n",
       "             0.5125, 0.5160, 0.5196, 0.5231, 0.5267, 0.5302, 0.5338, 0.5374, 0.5409,\n",
       "             0.5445, 0.5480, 0.5516, 0.5552, 0.5587, 0.5623, 0.5658, 0.5694, 0.5730,\n",
       "             0.5765, 0.5801, 0.5836, 0.5872, 0.5907, 0.5943, 0.5979, 0.6014, 0.6050,\n",
       "             0.6085, 0.6121, 0.6157, 0.6192, 0.6228, 0.6263, 0.6299, 0.6335, 0.6370,\n",
       "             0.6406, 0.6441, 0.6477, 0.6512, 0.6548, 0.6584, 0.6619, 0.6655, 0.6690,\n",
       "             0.6726, 0.6762, 0.6797, 0.6833, 0.6868, 0.6904, 0.6940, 0.6975, 0.7011,\n",
       "             0.7046, 0.7082, 0.7117, 0.7153, 0.7189, 0.7224, 0.7260, 0.7295, 0.7331,\n",
       "             0.7367, 0.7402, 0.7438, 0.7473, 0.7509, 0.7544, 0.7580, 0.7616, 0.7651,\n",
       "             0.7687, 0.7722, 0.7758, 0.7794, 0.7829, 0.7865, 0.7900, 0.7936, 0.7972,\n",
       "             0.8007, 0.8043, 0.8078, 0.8114, 0.8149, 0.8185, 0.8221, 0.8256, 0.8292,\n",
       "             0.8327, 0.8363, 0.8399, 0.8434, 0.8470, 0.8505, 0.8541, 0.8577, 0.8612,\n",
       "             0.8648, 0.8683, 0.8719, 0.8754, 0.8790, 0.8826, 0.8861, 0.8897, 0.8932,\n",
       "             0.8968, 0.9004, 0.9039, 0.9075, 0.9110, 0.9146, 0.9181, 0.9217, 0.9253,\n",
       "             0.9288, 0.9324, 0.9359, 0.9395, 0.9431, 0.9466, 0.9502, 0.9537, 0.9573,\n",
       "             0.9609, 0.9644, 0.9680, 0.9715, 0.9751, 0.9786, 0.9822, 0.9858, 0.9893,\n",
       "             0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0179, 0.0279, 0.0491, 0.0636, 0.0792, 0.0904, 0.0993, 0.1071,\n",
       "             0.1138, 0.1205, 0.1272, 0.1395, 0.1440, 0.1551, 0.1596, 0.1652, 0.1730,\n",
       "             0.1786, 0.1808, 0.1886, 0.1920, 0.1987, 0.2031, 0.2087, 0.2132, 0.2176,\n",
       "             0.2188, 0.2199, 0.2221, 0.2266, 0.2299, 0.2344, 0.2366, 0.2411, 0.2444,\n",
       "             0.2478, 0.2500, 0.2511, 0.2533, 0.2545, 0.2556, 0.2578, 0.2589, 0.2600,\n",
       "             0.2612, 0.2645, 0.2690, 0.2712, 0.2734, 0.2790, 0.2812, 0.2846, 0.2913,\n",
       "             0.2924, 0.2935, 0.2958, 0.2969, 0.3002, 0.3036, 0.3058, 0.3069, 0.3080,\n",
       "             0.3114, 0.3125, 0.3147, 0.3181, 0.3203, 0.3237, 0.3248, 0.3259, 0.3304,\n",
       "             0.3315, 0.3337, 0.3359, 0.3382, 0.3393, 0.3404, 0.3415, 0.3438, 0.3449,\n",
       "             0.3460, 0.3482, 0.3493, 0.3516, 0.3538, 0.3560, 0.3571, 0.3583, 0.3616,\n",
       "             0.3661, 0.3672, 0.3683, 0.3694, 0.3705, 0.3728, 0.3739, 0.3750, 0.3761,\n",
       "             0.3772, 0.3795, 0.3817, 0.3828, 0.3850, 0.3884, 0.3906, 0.3929, 0.3973,\n",
       "             0.4007, 0.4029, 0.4051, 0.4074, 0.4085, 0.4096, 0.4107, 0.4118, 0.4129,\n",
       "             0.4141, 0.4152, 0.4174, 0.4185, 0.4219, 0.4230, 0.4241, 0.4263, 0.4275,\n",
       "             0.4286, 0.4308, 0.4319, 0.4342, 0.4353, 0.4364, 0.4375, 0.4386, 0.4420,\n",
       "             0.4431, 0.4442, 0.4453, 0.4464, 0.4487, 0.4498, 0.4509, 0.4520, 0.4542,\n",
       "             0.4554, 0.4565, 0.4576, 0.4598, 0.4609, 0.4621, 0.4643, 0.4676, 0.4688,\n",
       "             0.4699, 0.4710, 0.4721, 0.4743, 0.4754, 0.4766, 0.4777, 0.4788, 0.4799,\n",
       "             0.4821, 0.4833, 0.4844, 0.4855, 0.4866, 0.4888, 0.4900, 0.4911, 0.4922,\n",
       "             0.4944, 0.4955, 0.4967, 0.4978, 0.4989, 0.5000, 0.5011, 0.5022, 0.5033,\n",
       "             0.5045, 0.5056, 0.5078, 0.5089, 0.5100, 0.5112, 0.5123, 0.5134, 0.5145,\n",
       "             0.5156, 0.5167, 0.5179, 0.5190, 0.5212, 0.5234, 0.5246, 0.5257, 0.5279,\n",
       "             0.5290, 0.5301, 0.5312, 0.5324, 0.5346, 0.5357, 0.5368, 0.5379, 0.5391,\n",
       "             0.5413, 0.5424, 0.5435, 0.5458, 0.5469, 0.5480, 0.5502, 0.5513, 0.5536,\n",
       "             0.5558, 0.5569, 0.5580, 0.5592, 0.5603, 0.5614, 0.5625, 0.5636, 0.5647,\n",
       "             0.5658, 0.5670, 0.5681, 0.5692, 0.5703, 0.5714, 0.5725, 0.5737, 0.5748,\n",
       "             0.5759, 0.5770, 0.5781, 0.5792, 0.5804, 0.5815, 0.5826, 0.5837, 0.5848,\n",
       "             0.5859, 0.5871, 0.5882, 0.5893, 0.5904, 0.5915, 0.5926, 0.5938, 0.5949,\n",
       "             0.5960, 0.5971, 0.5982, 0.5993, 0.6004, 0.6016, 0.6027, 0.6038, 0.6049,\n",
       "             0.6060, 0.6071, 0.6083, 0.6094, 0.6105, 0.6116, 0.6127, 0.6138, 0.6150,\n",
       "             0.6161, 0.6194, 0.6205, 0.6217, 0.6228, 0.6239, 0.6250, 0.6272, 0.6295,\n",
       "             0.6306, 0.6317, 0.6328, 0.6339, 0.6350, 0.6362, 0.6373, 0.6384, 0.6395,\n",
       "             0.6406, 0.6417, 0.6429, 0.6440, 0.6462, 0.6473, 0.6484, 0.6496, 0.6507,\n",
       "             0.6518, 0.6529, 0.6540, 0.6551, 0.6562, 0.6574, 0.6585, 0.6596, 0.6607,\n",
       "             0.6618, 0.6629, 0.6641, 0.6652, 0.6663, 0.6674, 0.6685, 0.6696, 0.6708,\n",
       "             0.6719, 0.6730, 0.6752, 0.6763, 0.6775, 0.6786, 0.6797, 0.6808, 0.6819,\n",
       "             0.6830, 0.6842, 0.6853, 0.6864, 0.6875, 0.6886, 0.6897, 0.6920, 0.6931,\n",
       "             0.6942, 0.6964, 0.6975, 0.6987, 0.6998, 0.7009, 0.7020, 0.7031, 0.7042,\n",
       "             0.7054, 0.7065, 0.7076, 0.7087, 0.7098, 0.7109, 0.7121, 0.7132, 0.7143,\n",
       "             0.7154, 0.7165, 0.7176, 0.7188, 0.7199, 0.7210, 0.7221, 0.7232, 0.7243,\n",
       "             0.7254, 0.7266, 0.7277, 0.7288, 0.7299, 0.7310, 0.7321, 0.7333, 0.7355,\n",
       "             0.7366, 0.7377, 0.7388, 0.7400, 0.7411, 0.7422, 0.7433, 0.7444, 0.7455,\n",
       "             0.7467, 0.7478, 0.7489, 0.7500, 0.7500, 0.7511, 0.7522, 0.7533, 0.7545,\n",
       "             0.7556, 0.7567, 0.7578, 0.7589, 0.7600, 0.7612, 0.7623, 0.7634, 0.7645,\n",
       "             0.7656, 0.7667, 0.7679, 0.7690, 0.7701, 0.7712, 0.7723, 0.7734, 0.7746,\n",
       "             0.7757, 0.7768, 0.7779, 0.7790, 0.7801, 0.7812, 0.7824, 0.7835, 0.7846,\n",
       "             0.7857, 0.7879, 0.7891, 0.7902, 0.7913, 0.7924, 0.7935, 0.7946, 0.7958,\n",
       "             0.7969, 0.7980, 0.7991, 0.8002, 0.8013, 0.8013, 0.8025, 0.8036, 0.8047,\n",
       "             0.8058, 0.8069, 0.8080, 0.8092, 0.8103, 0.8114, 0.8125, 0.8136, 0.8147,\n",
       "             0.8158, 0.8170, 0.8181, 0.8192, 0.8203, 0.8214, 0.8225, 0.8237, 0.8248,\n",
       "             0.8259, 0.8270, 0.8281, 0.8292, 0.8304, 0.8315, 0.8326, 0.8337, 0.8348,\n",
       "             0.8359, 0.8371, 0.8382, 0.8393, 0.8404, 0.8415, 0.8426, 0.8438, 0.8449,\n",
       "             0.8460, 0.8471, 0.8471, 0.8482, 0.8493, 0.8504, 0.8516, 0.8527, 0.8538,\n",
       "             0.8549, 0.8560, 0.8571, 0.8583, 0.8594, 0.8605, 0.8616, 0.8627, 0.8638,\n",
       "             0.8650, 0.8661, 0.8672, 0.8683, 0.8694, 0.8705, 0.8717, 0.8728, 0.8739,\n",
       "             0.8750, 0.8761, 0.8772, 0.8783, 0.8795, 0.8806, 0.8817, 0.8828, 0.8839,\n",
       "             0.8850, 0.8862, 0.8873, 0.8884, 0.8895, 0.8906, 0.8917, 0.8929, 0.8940,\n",
       "             0.8951, 0.8962, 0.8973, 0.8984, 0.8996, 0.9007, 0.9018, 0.9029, 0.9040,\n",
       "             0.9051, 0.9062, 0.9074, 0.9074, 0.9085, 0.9096, 0.9107, 0.9118, 0.9129,\n",
       "             0.9141, 0.9152, 0.9163, 0.9174, 0.9185, 0.9196, 0.9208, 0.9219, 0.9230,\n",
       "             0.9241, 0.9252, 0.9263, 0.9263, 0.9275, 0.9286, 0.9297, 0.9308, 0.9319,\n",
       "             0.9330, 0.9342, 0.9353, 0.9364, 0.9375, 0.9375, 0.9386, 0.9397, 0.9408,\n",
       "             0.9420, 0.9431, 0.9442, 0.9453, 0.9464, 0.9475, 0.9487, 0.9498, 0.9509,\n",
       "             0.9520, 0.9531, 0.9542, 0.9554, 0.9565, 0.9576, 0.9587, 0.9587, 0.9598,\n",
       "             0.9609, 0.9621, 0.9632, 0.9643, 0.9643, 0.9654, 0.9665, 0.9676, 0.9676,\n",
       "             0.9688, 0.9699, 0.9710, 0.9710, 0.9710, 0.9710, 0.9721, 0.9732, 0.9743,\n",
       "             0.9754, 0.9766, 0.9777, 0.9777, 0.9788, 0.9799, 0.9799, 0.9799, 0.9810,\n",
       "             0.9810, 0.9810, 0.9821, 0.9833, 0.9844, 0.9855, 0.9866, 0.9877, 0.9888,\n",
       "             0.9900, 0.9900, 0.9900, 0.9911, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9944,\n",
       "             0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944,\n",
       "             0.9955, 0.9955, 0.9955, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01,\n",
       "             9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9971e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9970e-01, 9.9970e-01, 9.9968e-01,\n",
       "             9.9967e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01, 9.9965e-01, 9.9965e-01,\n",
       "             9.9965e-01, 9.9964e-01, 9.9963e-01, 9.9961e-01, 9.9961e-01, 9.9961e-01,\n",
       "             9.9961e-01, 9.9960e-01, 9.9960e-01, 9.9960e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9959e-01, 9.9959e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9956e-01, 9.9956e-01, 9.9954e-01, 9.9953e-01, 9.9953e-01, 9.9952e-01,\n",
       "             9.9951e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01, 9.9948e-01, 9.9948e-01,\n",
       "             9.9947e-01, 9.9947e-01, 9.9947e-01, 9.9946e-01, 9.9945e-01, 9.9944e-01,\n",
       "             9.9944e-01, 9.9943e-01, 9.9943e-01, 9.9943e-01, 9.9943e-01, 9.9942e-01,\n",
       "             9.9942e-01, 9.9940e-01, 9.9940e-01, 9.9940e-01, 9.9939e-01, 9.9939e-01,\n",
       "             9.9938e-01, 9.9937e-01, 9.9934e-01, 9.9934e-01, 9.9933e-01, 9.9931e-01,\n",
       "             9.9930e-01, 9.9928e-01, 9.9927e-01, 9.9922e-01, 9.9922e-01, 9.9921e-01,\n",
       "             9.9920e-01, 9.9920e-01, 9.9916e-01, 9.9915e-01, 9.9915e-01, 9.9912e-01,\n",
       "             9.9912e-01, 9.9911e-01, 9.9911e-01, 9.9911e-01, 9.9908e-01, 9.9907e-01,\n",
       "             9.9905e-01, 9.9903e-01, 9.9902e-01, 9.9899e-01, 9.9899e-01, 9.9894e-01,\n",
       "             9.9889e-01, 9.9887e-01, 9.9886e-01, 9.9884e-01, 9.9880e-01, 9.9879e-01,\n",
       "             9.9877e-01, 9.9876e-01, 9.9875e-01, 9.9873e-01, 9.9871e-01, 9.9864e-01,\n",
       "             9.9863e-01, 9.9862e-01, 9.9858e-01, 9.9853e-01, 9.9853e-01, 9.9852e-01,\n",
       "             9.9849e-01, 9.9848e-01, 9.9848e-01, 9.9848e-01, 9.9846e-01, 9.9845e-01,\n",
       "             9.9835e-01, 9.9834e-01, 9.9834e-01, 9.9834e-01, 9.9825e-01, 9.9825e-01,\n",
       "             9.9824e-01, 9.9815e-01, 9.9800e-01, 9.9798e-01, 9.9789e-01, 9.9788e-01,\n",
       "             9.9776e-01, 9.9772e-01, 9.9767e-01, 9.9761e-01, 9.9753e-01, 9.9753e-01,\n",
       "             9.9743e-01, 9.9718e-01, 9.9705e-01, 9.9703e-01, 9.9684e-01, 9.9671e-01,\n",
       "             9.9667e-01, 9.9652e-01, 9.9651e-01, 9.9650e-01, 9.9642e-01, 9.9595e-01,\n",
       "             9.9568e-01, 9.9567e-01, 9.9549e-01, 9.9549e-01, 9.9521e-01, 9.9501e-01,\n",
       "             9.9475e-01, 9.9470e-01, 9.9456e-01, 9.9443e-01, 9.9435e-01, 9.9434e-01,\n",
       "             9.9424e-01, 9.9422e-01, 9.9420e-01, 9.9418e-01, 9.9415e-01, 9.9402e-01,\n",
       "             9.9351e-01, 9.9342e-01, 9.9302e-01, 9.9274e-01, 9.9247e-01, 9.9242e-01,\n",
       "             9.9232e-01, 9.9188e-01, 9.9187e-01, 9.9164e-01, 9.9123e-01, 9.9069e-01,\n",
       "             9.9019e-01, 9.8972e-01, 9.8971e-01, 9.8935e-01, 9.8932e-01, 9.8929e-01,\n",
       "             9.8840e-01, 9.8740e-01, 9.8530e-01, 9.8453e-01, 9.8396e-01, 9.8344e-01,\n",
       "             9.8317e-01, 9.8213e-01, 9.7797e-01, 9.7796e-01, 9.7589e-01, 9.7395e-01,\n",
       "             9.7394e-01, 9.6431e-01, 9.6380e-01, 9.6102e-01, 9.5854e-01, 9.5710e-01,\n",
       "             9.5500e-01, 9.4909e-01, 9.4655e-01, 9.4325e-01, 9.4148e-01, 9.3695e-01,\n",
       "             9.2460e-01, 8.9581e-01, 8.8534e-01, 8.7613e-01, 8.4503e-01, 8.3395e-01,\n",
       "             8.2245e-01, 7.6875e-01, 7.5139e-01, 7.4658e-01, 7.4053e-01, 7.3798e-01,\n",
       "             6.9622e-01, 6.0380e-01, 6.0364e-01, 5.9104e-01, 5.8257e-01, 5.7359e-01,\n",
       "             5.5925e-01, 5.3822e-01, 4.8627e-01, 4.8487e-01, 4.1136e-01, 3.8424e-01,\n",
       "             3.4876e-01, 3.4390e-01, 3.1722e-01, 2.6418e-01, 2.3702e-01, 2.0173e-01,\n",
       "             1.3004e-01, 1.2351e-01, 1.2328e-01, 1.1651e-01, 9.4792e-02, 7.0036e-02,\n",
       "             5.2995e-02, 5.2134e-02, 3.6987e-02, 3.4729e-02, 3.1888e-02, 2.7691e-02,\n",
       "             1.7092e-02, 1.4087e-02, 1.3343e-02, 1.2619e-02, 1.1707e-02, 1.1590e-02,\n",
       "             1.0369e-02, 9.5719e-03, 9.3892e-03, 9.2111e-03, 8.7817e-03, 7.6489e-03,\n",
       "             7.2862e-03, 7.2419e-03, 6.9020e-03, 6.8702e-03, 6.7275e-03, 6.5801e-03,\n",
       "             6.4738e-03, 6.2286e-03, 5.3580e-03, 5.0736e-03, 4.8544e-03, 3.8306e-03,\n",
       "             3.2660e-03, 3.2640e-03, 2.9120e-03, 2.7369e-03, 2.4704e-03, 2.4267e-03,\n",
       "             2.1006e-03, 1.7828e-03, 1.6749e-03, 1.6557e-03, 1.5858e-03, 1.5841e-03,\n",
       "             1.4145e-03, 1.3223e-03, 1.2616e-03, 1.1022e-03, 8.0994e-04, 7.8246e-04,\n",
       "             6.8569e-04, 6.6971e-04, 5.7425e-04, 5.2396e-04, 5.1656e-04, 5.1413e-04,\n",
       "             5.1197e-04, 4.9322e-04, 4.9137e-04, 3.5890e-04, 3.2288e-04, 3.0996e-04,\n",
       "             2.7661e-04, 2.6708e-04, 2.5900e-04, 2.5175e-04, 2.3969e-04, 2.3939e-04,\n",
       "             2.3701e-04, 2.3237e-04, 2.0824e-04, 2.0113e-04, 1.4846e-04, 1.4475e-04,\n",
       "             1.3371e-04, 1.0772e-04, 1.0238e-04, 1.0112e-04, 9.3707e-05, 9.3673e-05,\n",
       "             8.9034e-05, 8.8358e-05, 7.9826e-05, 7.6712e-05, 7.4100e-05, 7.0946e-05,\n",
       "             6.5352e-05, 6.0653e-05, 5.9624e-05, 5.9365e-05, 5.7373e-05, 4.7390e-05,\n",
       "             4.4094e-05, 3.7209e-05, 3.1778e-05, 3.1576e-05, 3.1280e-05, 3.0553e-05,\n",
       "             2.7711e-05, 2.6725e-05, 2.2674e-05, 2.0759e-05, 1.9580e-05, 1.9449e-05,\n",
       "             1.8951e-05, 1.8345e-05, 1.8112e-05, 1.7702e-05, 1.7542e-05, 1.7055e-05,\n",
       "             1.5951e-05, 1.4569e-05, 1.4087e-05, 1.3195e-05, 1.2765e-05, 1.1773e-05,\n",
       "             1.0094e-05, 9.9348e-06, 9.5622e-06, 8.5332e-06, 8.3831e-06, 8.1951e-06,\n",
       "             8.1653e-06, 7.6242e-06, 7.4143e-06, 6.7475e-06, 6.1508e-06, 6.1433e-06,\n",
       "             5.3126e-06, 4.7562e-06, 4.5006e-06, 4.1896e-06, 4.0960e-06, 3.9322e-06,\n",
       "             3.7814e-06, 3.4008e-06, 2.8056e-06, 2.8014e-06, 2.5957e-06, 2.4203e-06,\n",
       "             2.0607e-06, 2.0598e-06, 2.0116e-06, 1.9203e-06, 1.8634e-06, 1.7681e-06,\n",
       "             1.5266e-06, 1.5118e-06, 1.4823e-06, 1.4526e-06, 1.3921e-06, 1.2921e-06,\n",
       "             1.1655e-06, 1.1407e-06, 1.0630e-06, 1.0525e-06, 9.9961e-07, 8.9678e-07,\n",
       "             7.8475e-07, 7.3636e-07, 7.2421e-07, 6.7257e-07, 6.4921e-07, 6.0842e-07,\n",
       "             5.7708e-07, 5.5123e-07, 5.2408e-07, 4.8374e-07, 4.0363e-07, 3.9253e-07,\n",
       "             3.9123e-07, 3.8846e-07, 3.8024e-07, 3.4687e-07, 3.4177e-07, 3.3099e-07,\n",
       "             2.7617e-07, 2.3165e-07, 1.9897e-07, 1.9553e-07, 1.7260e-07, 1.6412e-07,\n",
       "             1.5567e-07, 1.5558e-07, 1.4061e-07, 1.3772e-07, 1.3354e-07, 1.1111e-07,\n",
       "             1.0376e-07, 9.6838e-08, 9.3698e-08, 8.8930e-08, 8.7942e-08, 8.7110e-08,\n",
       "             8.2905e-08, 7.6368e-08, 7.6358e-08, 7.0507e-08, 6.9439e-08, 6.6706e-08,\n",
       "             6.4490e-08, 5.7859e-08, 5.3877e-08, 5.0678e-08, 4.9370e-08, 4.1159e-08,\n",
       "             4.0864e-08, 3.8283e-08, 3.6907e-08, 3.5810e-08, 3.5480e-08, 3.4494e-08,\n",
       "             3.3592e-08, 3.2835e-08, 3.1595e-08, 3.1211e-08, 2.3034e-08, 2.2843e-08,\n",
       "             2.1797e-08, 1.8073e-08, 1.5134e-08, 1.4870e-08, 1.4787e-08, 1.3985e-08,\n",
       "             1.2981e-08, 1.2894e-08, 1.2351e-08, 1.2039e-08, 1.2014e-08, 9.4368e-09,\n",
       "             9.4029e-09, 8.2015e-09, 8.1949e-09, 7.8216e-09, 5.6345e-09, 4.2971e-09,\n",
       "             2.8678e-09, 2.7007e-09, 2.6332e-09, 2.5034e-09, 2.1411e-09, 2.0251e-09,\n",
       "             1.9507e-09, 1.9243e-09, 1.4620e-09, 1.1050e-09, 9.9953e-10, 9.9481e-10,\n",
       "             9.3797e-10, 9.0629e-10, 8.9517e-10, 8.8209e-10, 6.4048e-10, 5.7835e-10,\n",
       "             5.3831e-10, 4.8296e-10, 4.7401e-10, 3.7255e-10, 2.1437e-10, 1.8772e-10,\n",
       "             1.4998e-10, 1.2983e-10, 1.2515e-10, 9.8088e-11, 5.5438e-11, 1.3972e-11,\n",
       "             9.4565e-12, 7.9558e-12, 6.9756e-12])}},\n",
       "   {'fpr': np.float64(0.2313167259786477),\n",
       "    'tpr': np.float64(0.9966517857142857),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0285, 0.0285, 0.0285,\n",
       "             0.0285, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0391, 0.0427, 0.0463,\n",
       "             0.0463, 0.0463, 0.0498, 0.0534, 0.0534, 0.0569, 0.0569, 0.0569, 0.0605,\n",
       "             0.0641, 0.0641, 0.0641, 0.0641, 0.0676, 0.0712, 0.0712, 0.0747, 0.0747,\n",
       "             0.0783, 0.0819, 0.0819, 0.0854, 0.0854, 0.0890, 0.0890, 0.0925, 0.0961,\n",
       "             0.0996, 0.1032, 0.1068, 0.1103, 0.1139, 0.1174, 0.1210, 0.1210, 0.1246,\n",
       "             0.1281, 0.1317, 0.1352, 0.1388, 0.1423, 0.1459, 0.1495, 0.1530, 0.1566,\n",
       "             0.1566, 0.1601, 0.1637, 0.1673, 0.1708, 0.1744, 0.1779, 0.1815, 0.1851,\n",
       "             0.1886, 0.1922, 0.1957, 0.1993, 0.2028, 0.2064, 0.2100, 0.2135, 0.2171,\n",
       "             0.2206, 0.2206, 0.2242, 0.2278, 0.2313, 0.2313, 0.2349, 0.2384, 0.2420,\n",
       "             0.2456, 0.2491, 0.2527, 0.2562, 0.2598, 0.2633, 0.2669, 0.2705, 0.2740,\n",
       "             0.2776, 0.2811, 0.2847, 0.2883, 0.2918, 0.2954, 0.2989, 0.3025, 0.3060,\n",
       "             0.3096, 0.3132, 0.3167, 0.3203, 0.3238, 0.3274, 0.3310, 0.3345, 0.3381,\n",
       "             0.3416, 0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630, 0.3665, 0.3665,\n",
       "             0.3701, 0.3737, 0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950,\n",
       "             0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270,\n",
       "             0.4306, 0.4342, 0.4377, 0.4413, 0.4448, 0.4484, 0.4520, 0.4555, 0.4591,\n",
       "             0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804, 0.4840, 0.4875, 0.4911,\n",
       "             0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5125, 0.5160, 0.5196, 0.5231,\n",
       "             0.5267, 0.5302, 0.5338, 0.5374, 0.5409, 0.5445, 0.5480, 0.5516, 0.5552,\n",
       "             0.5587, 0.5623, 0.5658, 0.5694, 0.5730, 0.5765, 0.5801, 0.5836, 0.5872,\n",
       "             0.5907, 0.5943, 0.5979, 0.6014, 0.6050, 0.6085, 0.6121, 0.6157, 0.6192,\n",
       "             0.6228, 0.6263, 0.6299, 0.6335, 0.6370, 0.6406, 0.6441, 0.6477, 0.6512,\n",
       "             0.6548, 0.6584, 0.6619, 0.6655, 0.6690, 0.6726, 0.6762, 0.6797, 0.6833,\n",
       "             0.6868, 0.6904, 0.6940, 0.6975, 0.7011, 0.7046, 0.7082, 0.7117, 0.7153,\n",
       "             0.7189, 0.7224, 0.7260, 0.7295, 0.7331, 0.7367, 0.7402, 0.7438, 0.7473,\n",
       "             0.7509, 0.7544, 0.7580, 0.7616, 0.7651, 0.7687, 0.7722, 0.7758, 0.7794,\n",
       "             0.7829, 0.7865, 0.7900, 0.7936, 0.7972, 0.8007, 0.8043, 0.8078, 0.8114,\n",
       "             0.8149, 0.8185, 0.8221, 0.8256, 0.8292, 0.8327, 0.8363, 0.8399, 0.8434,\n",
       "             0.8470, 0.8505, 0.8541, 0.8577, 0.8612, 0.8648, 0.8683, 0.8719, 0.8754,\n",
       "             0.8790, 0.8826, 0.8861, 0.8897, 0.8932, 0.8968, 0.9004, 0.9039, 0.9075,\n",
       "             0.9110, 0.9146, 0.9181, 0.9217, 0.9253, 0.9288, 0.9324, 0.9359, 0.9395,\n",
       "             0.9431, 0.9466, 0.9502, 0.9537, 0.9573, 0.9609, 0.9644, 0.9680, 0.9715,\n",
       "             0.9751, 0.9786, 0.9822, 0.9858, 0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8549, 0.8962, 0.9129, 0.9185, 0.9230, 0.9252, 0.9275, 0.9297,\n",
       "             0.9330, 0.9342, 0.9375, 0.9431, 0.9475, 0.9487, 0.9498, 0.9509, 0.9520,\n",
       "             0.9531, 0.9542, 0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621,\n",
       "             0.9632, 0.9643, 0.9654, 0.9665, 0.9676, 0.9688, 0.9699, 0.9710, 0.9721,\n",
       "             0.9732, 0.9743, 0.9754, 0.9766, 0.9777, 0.9788, 0.9788, 0.9788, 0.9788,\n",
       "             0.9799, 0.9810, 0.9810, 0.9810, 0.9821, 0.9821, 0.9833, 0.9844, 0.9844,\n",
       "             0.9844, 0.9855, 0.9866, 0.9877, 0.9877, 0.9877, 0.9888, 0.9888, 0.9900,\n",
       "             0.9900, 0.9900, 0.9911, 0.9911, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9944, 0.9944,\n",
       "             0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944, 0.9944,\n",
       "             0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9967, 0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9989,\n",
       "             0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9996e-01, 9.9996e-01, 9.9994e-01, 9.9994e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9988e-01, 9.9986e-01, 9.9982e-01,\n",
       "             9.9976e-01, 9.9974e-01, 9.9967e-01, 9.9966e-01, 9.9952e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9948e-01, 9.9928e-01, 9.9901e-01, 9.9877e-01, 9.9875e-01,\n",
       "             9.9825e-01, 9.9814e-01, 9.9764e-01, 9.9745e-01, 9.9580e-01, 9.9497e-01,\n",
       "             9.9378e-01, 9.9307e-01, 9.9252e-01, 9.9125e-01, 9.8900e-01, 9.8150e-01,\n",
       "             9.7888e-01, 9.7711e-01, 9.7551e-01, 9.7354e-01, 9.7202e-01, 9.7196e-01,\n",
       "             9.7121e-01, 9.6634e-01, 9.5645e-01, 9.1742e-01, 9.1031e-01, 9.0957e-01,\n",
       "             9.0661e-01, 9.0555e-01, 8.9268e-01, 8.8166e-01, 8.7929e-01, 8.5653e-01,\n",
       "             8.4727e-01, 8.4162e-01, 8.4037e-01, 8.3606e-01, 8.3121e-01, 8.1233e-01,\n",
       "             7.8604e-01, 7.7971e-01, 7.7501e-01, 7.5315e-01, 7.2394e-01, 7.1645e-01,\n",
       "             7.0262e-01, 6.9814e-01, 6.6713e-01, 6.5668e-01, 6.5310e-01, 6.4439e-01,\n",
       "             6.1031e-01, 5.7906e-01, 5.6998e-01, 5.2466e-01, 5.0994e-01, 4.8181e-01,\n",
       "             4.7295e-01, 4.2509e-01, 4.1910e-01, 3.3856e-01, 3.3761e-01, 3.2492e-01,\n",
       "             3.1462e-01, 2.9947e-01, 2.8069e-01, 2.7623e-01, 2.6231e-01, 2.5362e-01,\n",
       "             2.4934e-01, 2.3959e-01, 2.3473e-01, 2.1979e-01, 2.1303e-01, 2.0922e-01,\n",
       "             1.7248e-01, 1.4689e-01, 1.3831e-01, 1.1043e-01, 1.0240e-01, 9.8970e-02,\n",
       "             8.3644e-02, 8.1900e-02, 7.2692e-02, 6.6644e-02, 6.5383e-02, 6.2118e-02,\n",
       "             6.0112e-02, 5.4865e-02, 5.4428e-02, 4.9011e-02, 3.8518e-02, 3.8184e-02,\n",
       "             3.7486e-02, 3.5213e-02, 3.4614e-02, 2.2248e-02, 2.0778e-02, 1.7788e-02,\n",
       "             1.5773e-02, 1.4181e-02, 1.1745e-02, 1.1349e-02, 9.0859e-03, 8.7898e-03,\n",
       "             8.2561e-03, 7.5034e-03, 7.4250e-03, 6.1986e-03, 4.7698e-03, 4.6570e-03,\n",
       "             3.7227e-03, 3.4676e-03, 3.1373e-03, 2.8702e-03, 2.8245e-03, 2.4397e-03,\n",
       "             2.3183e-03, 2.2191e-03, 1.9428e-03, 1.8337e-03, 1.8081e-03, 1.7117e-03,\n",
       "             1.5110e-03, 1.5030e-03, 1.3401e-03, 1.2668e-03, 1.2651e-03, 1.2414e-03,\n",
       "             1.0726e-03, 1.0670e-03, 1.0383e-03, 8.0124e-04, 7.4977e-04, 6.5812e-04,\n",
       "             5.1535e-04, 5.1265e-04, 5.0602e-04, 4.7710e-04, 3.8067e-04, 3.6833e-04,\n",
       "             3.6043e-04, 2.6830e-04, 2.1626e-04, 2.0819e-04, 1.8934e-04, 1.8504e-04,\n",
       "             1.5312e-04, 9.6528e-05, 8.9718e-05, 8.1991e-05, 7.9810e-05, 7.8717e-05,\n",
       "             7.8602e-05, 7.8276e-05, 7.2916e-05, 7.1761e-05, 6.0197e-05, 5.2068e-05,\n",
       "             5.2038e-05, 4.4904e-05, 4.1607e-05, 4.0186e-05, 3.9405e-05, 3.8641e-05,\n",
       "             3.7633e-05, 3.1405e-05, 2.6547e-05, 2.5159e-05, 2.3756e-05, 2.2419e-05,\n",
       "             2.1704e-05, 1.6654e-05, 1.5067e-05, 1.4753e-05, 1.3160e-05, 1.2181e-05,\n",
       "             1.2009e-05, 1.0727e-05, 8.3315e-06, 7.9355e-06, 7.6220e-06, 7.5671e-06,\n",
       "             7.0692e-06, 6.6659e-06, 5.1732e-06, 5.1381e-06, 4.9377e-06, 4.6716e-06,\n",
       "             4.0942e-06, 3.9605e-06, 3.3897e-06, 3.1993e-06, 3.0392e-06, 2.8914e-06,\n",
       "             2.6322e-06, 2.5202e-06, 2.3770e-06, 2.1494e-06, 2.0150e-06, 1.7343e-06,\n",
       "             1.5434e-06, 1.5358e-06, 1.4956e-06, 1.4835e-06, 1.4615e-06, 1.3322e-06,\n",
       "             1.0610e-06, 1.0366e-06, 9.4503e-07, 9.0657e-07, 8.1625e-07, 8.0022e-07,\n",
       "             7.4468e-07, 7.2307e-07, 6.2909e-07, 6.0262e-07, 5.8139e-07, 3.1670e-07,\n",
       "             2.9202e-07, 2.4793e-07, 2.3612e-07, 2.1186e-07, 1.6600e-07, 1.5664e-07,\n",
       "             1.5228e-07, 1.2933e-07, 1.0943e-07, 1.0605e-07, 1.0459e-07, 1.0333e-07,\n",
       "             9.9596e-08, 7.6636e-08, 6.8763e-08, 6.7599e-08, 6.3918e-08, 6.2889e-08,\n",
       "             5.0493e-08, 4.5320e-08, 3.6431e-08, 2.9711e-08, 2.8610e-08, 2.6829e-08,\n",
       "             2.3602e-08, 2.0717e-08, 1.9106e-08, 1.8158e-08, 6.6971e-09, 6.4210e-09,\n",
       "             3.7667e-09, 3.7492e-09, 3.2286e-09, 2.9436e-09, 2.8975e-09, 1.9028e-09,\n",
       "             5.2192e-10, 4.5123e-10, 4.3307e-10, 3.2349e-10, 1.4923e-10, 1.1739e-10,\n",
       "             9.8953e-11, 8.9832e-11, 8.5752e-11, 5.7683e-11, 4.8827e-11, 4.6093e-11,\n",
       "             2.5136e-11, 2.3040e-11, 2.2330e-11, 2.0540e-11, 1.9224e-11, 1.8475e-11,\n",
       "             5.5155e-12, 4.8816e-12])}},\n",
       "   {'fpr': np.float64(0.042704626334519574),\n",
       "    'tpr': np.float64(0.9787946428571429),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0142, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249,\n",
       "             0.0285, 0.0285, 0.0285, 0.0285, 0.0320, 0.0320, 0.0320, 0.0320, 0.0320,\n",
       "             0.0320, 0.0320, 0.0356, 0.0356, 0.0356, 0.0356, 0.0391, 0.0391, 0.0391,\n",
       "             0.0427, 0.0427, 0.0463, 0.0463, 0.0498, 0.0534, 0.0534, 0.0534, 0.0534,\n",
       "             0.0534, 0.0534, 0.0534, 0.0534, 0.0569, 0.0569, 0.0605, 0.0641, 0.0676,\n",
       "             0.0712, 0.0747, 0.0783, 0.0819, 0.0819, 0.0854, 0.0854, 0.0890, 0.0890,\n",
       "             0.0925, 0.0961, 0.0996, 0.1032, 0.1068, 0.1103, 0.1139, 0.1174, 0.1210,\n",
       "             0.1246, 0.1246, 0.1281, 0.1317, 0.1352, 0.1352, 0.1388, 0.1423, 0.1459,\n",
       "             0.1495, 0.1530, 0.1566, 0.1601, 0.1601, 0.1637, 0.1673, 0.1708, 0.1744,\n",
       "             0.1779, 0.1815, 0.1851, 0.1886, 0.1922, 0.1922, 0.1957, 0.1993, 0.2028,\n",
       "             0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242, 0.2278, 0.2313, 0.2349,\n",
       "             0.2384, 0.2420, 0.2456, 0.2491, 0.2527, 0.2562, 0.2598, 0.2633, 0.2669,\n",
       "             0.2705, 0.2740, 0.2776, 0.2811, 0.2847, 0.2883, 0.2918, 0.2954, 0.2989,\n",
       "             0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203, 0.3238, 0.3274, 0.3310,\n",
       "             0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630,\n",
       "             0.3665, 0.3701, 0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950,\n",
       "             0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270,\n",
       "             0.4306, 0.4342, 0.4342, 0.4377, 0.4413, 0.4448, 0.4448, 0.4484, 0.4520,\n",
       "             0.4555, 0.4591, 0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804, 0.4840,\n",
       "             0.4875, 0.4911, 0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5125, 0.5160,\n",
       "             0.5196, 0.5231, 0.5267, 0.5302, 0.5338, 0.5374, 0.5409, 0.5445, 0.5480,\n",
       "             0.5516, 0.5552, 0.5587, 0.5623, 0.5658, 0.5694, 0.5730, 0.5765, 0.5801,\n",
       "             0.5836, 0.5872, 0.5907, 0.5943, 0.5979, 0.6014, 0.6050, 0.6085, 0.6121,\n",
       "             0.6157, 0.6192, 0.6228, 0.6263, 0.6299, 0.6335, 0.6370, 0.6406, 0.6441,\n",
       "             0.6477, 0.6512, 0.6548, 0.6584, 0.6619, 0.6655, 0.6690, 0.6726, 0.6762,\n",
       "             0.6797, 0.6833, 0.6868, 0.6904, 0.6940, 0.6975, 0.7011, 0.7046, 0.7082,\n",
       "             0.7117, 0.7153, 0.7189, 0.7224, 0.7260, 0.7295, 0.7331, 0.7367, 0.7402,\n",
       "             0.7438, 0.7473, 0.7509, 0.7544, 0.7580, 0.7616, 0.7651, 0.7687, 0.7722,\n",
       "             0.7758, 0.7794, 0.7829, 0.7865, 0.7900, 0.7936, 0.7972, 0.8007, 0.8043,\n",
       "             0.8078, 0.8114, 0.8149, 0.8185, 0.8221, 0.8256, 0.8292, 0.8327, 0.8363,\n",
       "             0.8399, 0.8434, 0.8470, 0.8505, 0.8541, 0.8577, 0.8612, 0.8648, 0.8683,\n",
       "             0.8719, 0.8754, 0.8790, 0.8826, 0.8861, 0.8897, 0.8932, 0.8968, 0.9004,\n",
       "             0.9039, 0.9075, 0.9110, 0.9146, 0.9181, 0.9217, 0.9253, 0.9288, 0.9324,\n",
       "             0.9359, 0.9395, 0.9431, 0.9466, 0.9502, 0.9537, 0.9573, 0.9609, 0.9644,\n",
       "             0.9680, 0.9715, 0.9751, 0.9786, 0.9822, 0.9858, 0.9893, 0.9929, 0.9964,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.3817, 0.4754, 0.5190, 0.5413, 0.5603, 0.5737, 0.5837, 0.5904,\n",
       "             0.6004, 0.6071, 0.6150, 0.6217, 0.6295, 0.6373, 0.6406, 0.6484, 0.6507,\n",
       "             0.6529, 0.6562, 0.6596, 0.6629, 0.6674, 0.6708, 0.6730, 0.6763, 0.6775,\n",
       "             0.6830, 0.6864, 0.6875, 0.6920, 0.6942, 0.6953, 0.6964, 0.6998, 0.7031,\n",
       "             0.7042, 0.7065, 0.7098, 0.7109, 0.7132, 0.7165, 0.7188, 0.7199, 0.7210,\n",
       "             0.7232, 0.7243, 0.7254, 0.7277, 0.7321, 0.7355, 0.7377, 0.7388, 0.7411,\n",
       "             0.7433, 0.7444, 0.7455, 0.7478, 0.7489, 0.7500, 0.7511, 0.7522, 0.7545,\n",
       "             0.7567, 0.7589, 0.7600, 0.7623, 0.7634, 0.7645, 0.7656, 0.7667, 0.7679,\n",
       "             0.7690, 0.7701, 0.7712, 0.7712, 0.7723, 0.7734, 0.7746, 0.7757, 0.7768,\n",
       "             0.7779, 0.7790, 0.7801, 0.7824, 0.7846, 0.7857, 0.7868, 0.7879, 0.7891,\n",
       "             0.7902, 0.7913, 0.7924, 0.7935, 0.7946, 0.7958, 0.7969, 0.7980, 0.7991,\n",
       "             0.8002, 0.8013, 0.8025, 0.8036, 0.8047, 0.8058, 0.8069, 0.8092, 0.8103,\n",
       "             0.8114, 0.8125, 0.8136, 0.8147, 0.8158, 0.8170, 0.8181, 0.8192, 0.8237,\n",
       "             0.8248, 0.8259, 0.8270, 0.8281, 0.8292, 0.8304, 0.8315, 0.8326, 0.8337,\n",
       "             0.8348, 0.8359, 0.8371, 0.8382, 0.8393, 0.8404, 0.8415, 0.8426, 0.8438,\n",
       "             0.8449, 0.8460, 0.8471, 0.8482, 0.8493, 0.8504, 0.8516, 0.8527, 0.8538,\n",
       "             0.8549, 0.8560, 0.8571, 0.8583, 0.8594, 0.8616, 0.8627, 0.8661, 0.8672,\n",
       "             0.8683, 0.8694, 0.8705, 0.8717, 0.8728, 0.8739, 0.8750, 0.8761, 0.8772,\n",
       "             0.8783, 0.8795, 0.8806, 0.8817, 0.8828, 0.8839, 0.8850, 0.8862, 0.8873,\n",
       "             0.8884, 0.8895, 0.8906, 0.8917, 0.8929, 0.8940, 0.8951, 0.8962, 0.8973,\n",
       "             0.8984, 0.8996, 0.9007, 0.9018, 0.9029, 0.9040, 0.9051, 0.9051, 0.9051,\n",
       "             0.9062, 0.9074, 0.9085, 0.9096, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152,\n",
       "             0.9163, 0.9174, 0.9174, 0.9185, 0.9196, 0.9208, 0.9219, 0.9230, 0.9241,\n",
       "             0.9252, 0.9263, 0.9275, 0.9286, 0.9297, 0.9308, 0.9319, 0.9330, 0.9342,\n",
       "             0.9353, 0.9364, 0.9375, 0.9386, 0.9397, 0.9408, 0.9420, 0.9431, 0.9442,\n",
       "             0.9453, 0.9464, 0.9475, 0.9487, 0.9498, 0.9509, 0.9520, 0.9531, 0.9542,\n",
       "             0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621, 0.9632, 0.9632,\n",
       "             0.9632, 0.9643, 0.9654, 0.9665, 0.9665, 0.9676, 0.9688, 0.9699, 0.9710,\n",
       "             0.9721, 0.9732, 0.9732, 0.9743, 0.9754, 0.9766, 0.9766, 0.9777, 0.9788,\n",
       "             0.9788, 0.9799, 0.9799, 0.9810, 0.9810, 0.9810, 0.9821, 0.9833, 0.9844,\n",
       "             0.9855, 0.9866, 0.9877, 0.9888, 0.9888, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9911, 0.9911, 0.9922, 0.9922, 0.9933,\n",
       "             0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9944, 0.9944, 0.9944, 0.9944, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9955, 0.9955, 0.9955, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9975e-01, 9.9974e-01, 9.9973e-01, 9.9964e-01,\n",
       "             9.9957e-01, 9.9957e-01, 9.9957e-01, 9.9956e-01, 9.9953e-01, 9.9953e-01,\n",
       "             9.9952e-01, 9.9945e-01, 9.9940e-01, 9.9939e-01, 9.9936e-01, 9.9932e-01,\n",
       "             9.9926e-01, 9.9926e-01, 9.9925e-01, 9.9920e-01, 9.9916e-01, 9.9912e-01,\n",
       "             9.9910e-01, 9.9908e-01, 9.9907e-01, 9.9906e-01, 9.9890e-01, 9.9889e-01,\n",
       "             9.9867e-01, 9.9866e-01, 9.9858e-01, 9.9843e-01, 9.9842e-01, 9.9833e-01,\n",
       "             9.9832e-01, 9.9825e-01, 9.9822e-01, 9.9809e-01, 9.9804e-01, 9.9791e-01,\n",
       "             9.9783e-01, 9.9707e-01, 9.9697e-01, 9.9680e-01, 9.9610e-01, 9.9557e-01,\n",
       "             9.9553e-01, 9.9546e-01, 9.9538e-01, 9.9517e-01, 9.9497e-01, 9.9480e-01,\n",
       "             9.9345e-01, 9.9265e-01, 9.9259e-01, 9.9221e-01, 9.9132e-01, 9.9064e-01,\n",
       "             9.9012e-01, 9.8675e-01, 9.8608e-01, 9.8559e-01, 9.8338e-01, 9.7554e-01,\n",
       "             9.7376e-01, 9.6778e-01, 9.6768e-01, 9.6104e-01, 9.6035e-01, 9.5183e-01,\n",
       "             9.4876e-01, 9.4142e-01, 9.3591e-01, 9.2680e-01, 9.2305e-01, 9.2164e-01,\n",
       "             8.8036e-01, 8.7898e-01, 8.5866e-01, 8.5567e-01, 7.6692e-01, 7.1936e-01,\n",
       "             6.4511e-01, 6.3631e-01, 5.7232e-01, 5.4027e-01, 4.1508e-01, 3.2391e-01,\n",
       "             3.0701e-01, 2.5037e-01, 2.4007e-01, 2.3098e-01, 1.2085e-01, 1.1932e-01,\n",
       "             9.1048e-02, 7.2720e-02, 7.1578e-02, 6.7341e-02, 5.4102e-02, 3.0249e-02,\n",
       "             2.8995e-02, 2.7830e-02, 2.7248e-02, 2.6495e-02, 2.5855e-02, 2.2845e-02,\n",
       "             1.8907e-02, 1.7634e-02, 1.7302e-02, 1.5947e-02, 1.3594e-02, 1.3412e-02,\n",
       "             1.2307e-02, 1.1468e-02, 9.9335e-03, 8.3273e-03, 7.3189e-03, 6.2117e-03,\n",
       "             5.7084e-03, 5.6714e-03, 5.4588e-03, 5.3091e-03, 3.2172e-03, 2.1634e-03,\n",
       "             2.1579e-03, 2.1178e-03, 2.1155e-03, 2.0016e-03, 1.9372e-03, 1.9290e-03,\n",
       "             1.9100e-03, 1.8026e-03, 1.3995e-03, 1.3400e-03, 1.1055e-03, 8.4562e-04,\n",
       "             7.4759e-04, 7.3670e-04, 7.0074e-04, 6.9210e-04, 6.6141e-04, 6.0797e-04,\n",
       "             6.0727e-04, 5.5854e-04, 5.5478e-04, 5.1828e-04, 4.1504e-04, 3.8806e-04,\n",
       "             2.4530e-04, 2.3077e-04, 2.3032e-04, 2.0248e-04, 1.9812e-04, 1.7315e-04,\n",
       "             1.6545e-04, 1.6233e-04, 1.2441e-04, 1.2261e-04, 1.1516e-04, 8.7824e-05,\n",
       "             8.3112e-05, 7.9984e-05, 7.2236e-05, 6.5948e-05, 6.1308e-05, 5.8819e-05,\n",
       "             5.8726e-05, 5.2317e-05, 5.0612e-05, 4.9505e-05, 4.3197e-05, 3.6787e-05,\n",
       "             3.6219e-05, 3.6077e-05, 3.3230e-05, 2.7638e-05, 2.7240e-05, 2.4943e-05,\n",
       "             2.4581e-05, 2.3472e-05, 2.1463e-05, 2.1039e-05, 1.6584e-05, 1.2748e-05,\n",
       "             1.2741e-05, 1.2315e-05, 1.1729e-05, 1.0693e-05, 1.0599e-05, 1.0297e-05,\n",
       "             8.6067e-06, 7.7985e-06, 7.6081e-06, 5.9411e-06, 5.8406e-06, 5.2771e-06,\n",
       "             5.0187e-06, 4.0395e-06, 3.9131e-06, 3.7764e-06, 3.4720e-06, 2.3065e-06,\n",
       "             2.2757e-06, 2.2555e-06, 2.0253e-06, 1.9835e-06, 1.8223e-06, 1.7715e-06,\n",
       "             1.4748e-06, 1.3245e-06, 1.3076e-06, 1.1640e-06, 1.1012e-06, 9.6044e-07,\n",
       "             9.5721e-07, 9.2382e-07, 8.9217e-07, 8.8118e-07, 8.1316e-07, 6.0371e-07,\n",
       "             3.9930e-07, 3.9879e-07, 3.5937e-07, 3.5000e-07, 3.4967e-07, 3.3571e-07,\n",
       "             3.2862e-07, 2.8354e-07, 2.5555e-07, 2.5216e-07, 2.4209e-07, 1.8852e-07,\n",
       "             1.6187e-07, 1.5266e-07, 1.4261e-07, 1.3041e-07, 8.7543e-08, 8.5090e-08,\n",
       "             7.8767e-08, 7.5763e-08, 7.2874e-08, 5.9204e-08, 5.6689e-08, 5.3054e-08,\n",
       "             4.2453e-08, 4.0702e-08, 4.0642e-08, 3.9574e-08, 3.8756e-08, 3.5536e-08,\n",
       "             3.4664e-08, 3.0754e-08, 2.2829e-08, 2.2482e-08, 2.0753e-08, 2.0524e-08,\n",
       "             2.0447e-08, 1.9428e-08, 1.9242e-08, 1.8917e-08, 1.7104e-08, 1.6823e-08,\n",
       "             1.6798e-08, 1.5290e-08, 1.5015e-08, 1.4911e-08, 1.4125e-08, 1.3394e-08,\n",
       "             1.2874e-08, 1.1611e-08, 1.0733e-08, 1.0578e-08, 1.0153e-08, 8.5777e-09,\n",
       "             8.1778e-09, 7.9371e-09, 5.8756e-09, 5.7634e-09, 5.7231e-09, 5.4351e-09,\n",
       "             5.3618e-09, 4.7260e-09, 4.6232e-09, 4.1607e-09, 3.3170e-09, 2.5594e-09,\n",
       "             2.4922e-09, 2.1618e-09, 2.1524e-09, 2.1424e-09, 2.0408e-09, 1.9187e-09,\n",
       "             1.7710e-09, 1.7291e-09, 1.4325e-09, 9.9919e-10, 9.8778e-10, 7.8716e-10,\n",
       "             6.8616e-10, 6.1237e-10, 5.4864e-10, 5.0847e-10, 5.0275e-10, 4.7809e-10,\n",
       "             4.5455e-10, 4.5275e-10, 4.2343e-10, 2.8229e-10, 2.3594e-10, 1.9926e-10,\n",
       "             1.8084e-10, 1.4112e-10, 1.3892e-10, 1.3887e-10, 1.1744e-10, 1.1424e-10,\n",
       "             1.1264e-10, 1.0456e-10, 9.3968e-11, 8.0036e-11, 6.6435e-11, 6.3152e-11,\n",
       "             5.4090e-11, 5.3912e-11, 4.9935e-11, 3.9537e-11, 3.6193e-11, 3.4491e-11,\n",
       "             2.7553e-11, 2.5801e-11, 2.1771e-11, 1.6165e-11, 1.3985e-11, 1.3894e-11,\n",
       "             1.2946e-11, 1.0370e-11, 7.3585e-12, 7.3211e-12, 6.9926e-12, 6.5477e-12,\n",
       "             6.4413e-12, 6.0222e-12, 3.8625e-12, 3.3143e-12, 2.5487e-12, 2.1488e-12,\n",
       "             1.7084e-12, 1.6242e-12, 1.5163e-12, 1.4308e-12, 1.2801e-12, 1.2687e-12,\n",
       "             1.2266e-12, 6.3438e-13, 6.0637e-13, 5.9832e-13, 5.5934e-13, 5.5305e-13,\n",
       "             4.9415e-13, 4.6200e-13, 3.5873e-13, 3.5762e-13, 3.4720e-13, 3.0918e-13,\n",
       "             2.1158e-13, 1.4853e-13, 5.3404e-14, 4.7319e-14, 2.6954e-14, 2.5783e-14,\n",
       "             1.9910e-14, 1.1615e-14, 8.0487e-16, 3.8938e-16])}},\n",
       "   {'fpr': np.float64(0.07829181494661921),\n",
       "    'tpr': np.float64(0.9832589285714286),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0142, 0.0142,\n",
       "             0.0142, 0.0178, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0285, 0.0285, 0.0285, 0.0285, 0.0285, 0.0285, 0.0285, 0.0320,\n",
       "             0.0356, 0.0356, 0.0356, 0.0391, 0.0391, 0.0391, 0.0391, 0.0427, 0.0463,\n",
       "             0.0498, 0.0534, 0.0534, 0.0534, 0.0569, 0.0569, 0.0569, 0.0569, 0.0605,\n",
       "             0.0605, 0.0605, 0.0641, 0.0641, 0.0641, 0.0676, 0.0676, 0.0676, 0.0676,\n",
       "             0.0676, 0.0676, 0.0712, 0.0747, 0.0783, 0.0819, 0.0819, 0.0854, 0.0890,\n",
       "             0.0890, 0.0890, 0.0925, 0.0961, 0.0961, 0.0961, 0.0961, 0.0996, 0.1032,\n",
       "             0.1068, 0.1103, 0.1139, 0.1174, 0.1210, 0.1210, 0.1246, 0.1281, 0.1281,\n",
       "             0.1317, 0.1352, 0.1388, 0.1423, 0.1459, 0.1495, 0.1530, 0.1566, 0.1601,\n",
       "             0.1637, 0.1673, 0.1708, 0.1708, 0.1744, 0.1779, 0.1815, 0.1851, 0.1886,\n",
       "             0.1922, 0.1957, 0.1993, 0.2028, 0.2064, 0.2100, 0.2100, 0.2135, 0.2135,\n",
       "             0.2135, 0.2171, 0.2206, 0.2242, 0.2278, 0.2313, 0.2349, 0.2384, 0.2420,\n",
       "             0.2456, 0.2491, 0.2527, 0.2527, 0.2562, 0.2598, 0.2633, 0.2669, 0.2705,\n",
       "             0.2740, 0.2776, 0.2811, 0.2847, 0.2883, 0.2883, 0.2918, 0.2954, 0.2989,\n",
       "             0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203, 0.3238, 0.3274, 0.3310,\n",
       "             0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630,\n",
       "             0.3665, 0.3701, 0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950,\n",
       "             0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270,\n",
       "             0.4306, 0.4342, 0.4377, 0.4413, 0.4448, 0.4484, 0.4520, 0.4555, 0.4591,\n",
       "             0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804, 0.4840, 0.4875, 0.4911,\n",
       "             0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5125, 0.5160, 0.5196, 0.5231,\n",
       "             0.5267, 0.5302, 0.5338, 0.5338, 0.5374, 0.5409, 0.5445, 0.5480, 0.5516,\n",
       "             0.5552, 0.5587, 0.5623, 0.5658, 0.5694, 0.5730, 0.5765, 0.5801, 0.5836,\n",
       "             0.5872, 0.5907, 0.5943, 0.5979, 0.6014, 0.6050, 0.6085, 0.6121, 0.6157,\n",
       "             0.6192, 0.6228, 0.6263, 0.6299, 0.6335, 0.6370, 0.6406, 0.6441, 0.6477,\n",
       "             0.6512, 0.6548, 0.6584, 0.6619, 0.6655, 0.6690, 0.6726, 0.6762, 0.6797,\n",
       "             0.6833, 0.6868, 0.6904, 0.6940, 0.6975, 0.7011, 0.7046, 0.7082, 0.7117,\n",
       "             0.7153, 0.7189, 0.7224, 0.7260, 0.7295, 0.7331, 0.7367, 0.7402, 0.7438,\n",
       "             0.7473, 0.7509, 0.7544, 0.7580, 0.7616, 0.7651, 0.7687, 0.7722, 0.7758,\n",
       "             0.7794, 0.7829, 0.7865, 0.7900, 0.7936, 0.7972, 0.8007, 0.8043, 0.8078,\n",
       "             0.8114, 0.8149, 0.8185, 0.8221, 0.8256, 0.8292, 0.8327, 0.8363, 0.8399,\n",
       "             0.8434, 0.8470, 0.8505, 0.8541, 0.8577, 0.8612, 0.8648, 0.8683, 0.8719,\n",
       "             0.8754, 0.8790, 0.8826, 0.8861, 0.8897, 0.8932, 0.8968, 0.9004, 0.9039,\n",
       "             0.9075, 0.9110, 0.9146, 0.9181, 0.9217, 0.9253, 0.9288, 0.9324, 0.9359,\n",
       "             0.9395, 0.9431, 0.9466, 0.9502, 0.9537, 0.9573, 0.9609, 0.9644, 0.9680,\n",
       "             0.9715, 0.9751, 0.9786, 0.9822, 0.9858, 0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.4141, 0.4833, 0.5223, 0.5513, 0.5647, 0.5837, 0.5926, 0.6049,\n",
       "             0.6150, 0.6239, 0.6339, 0.6417, 0.6484, 0.6529, 0.6551, 0.6585, 0.6652,\n",
       "             0.6696, 0.6708, 0.6752, 0.6775, 0.6808, 0.6842, 0.6853, 0.6864, 0.6886,\n",
       "             0.6897, 0.6908, 0.6953, 0.6987, 0.7009, 0.7020, 0.7054, 0.7065, 0.7087,\n",
       "             0.7109, 0.7121, 0.7143, 0.7154, 0.7176, 0.7199, 0.7232, 0.7254, 0.7266,\n",
       "             0.7299, 0.7310, 0.7321, 0.7333, 0.7344, 0.7355, 0.7388, 0.7400, 0.7411,\n",
       "             0.7433, 0.7455, 0.7467, 0.7478, 0.7489, 0.7511, 0.7522, 0.7533, 0.7545,\n",
       "             0.7556, 0.7567, 0.7589, 0.7600, 0.7612, 0.7623, 0.7634, 0.7645, 0.7656,\n",
       "             0.7679, 0.7690, 0.7701, 0.7712, 0.7723, 0.7734, 0.7746, 0.7757, 0.7768,\n",
       "             0.7790, 0.7801, 0.7812, 0.7824, 0.7835, 0.7857, 0.7868, 0.7879, 0.7891,\n",
       "             0.7902, 0.7913, 0.7924, 0.7935, 0.7946, 0.7980, 0.8002, 0.8025, 0.8036,\n",
       "             0.8047, 0.8058, 0.8069, 0.8080, 0.8092, 0.8103, 0.8114, 0.8125, 0.8136,\n",
       "             0.8147, 0.8158, 0.8170, 0.8181, 0.8192, 0.8203, 0.8214, 0.8237, 0.8248,\n",
       "             0.8259, 0.8270, 0.8281, 0.8304, 0.8315, 0.8326, 0.8337, 0.8348, 0.8359,\n",
       "             0.8371, 0.8382, 0.8393, 0.8404, 0.8415, 0.8426, 0.8449, 0.8460, 0.8471,\n",
       "             0.8482, 0.8493, 0.8504, 0.8516, 0.8527, 0.8527, 0.8538, 0.8549, 0.8560,\n",
       "             0.8583, 0.8594, 0.8605, 0.8616, 0.8627, 0.8638, 0.8650, 0.8661, 0.8672,\n",
       "             0.8683, 0.8694, 0.8717, 0.8728, 0.8739, 0.8750, 0.8761, 0.8772, 0.8783,\n",
       "             0.8795, 0.8806, 0.8817, 0.8828, 0.8839, 0.8850, 0.8862, 0.8873, 0.8884,\n",
       "             0.8895, 0.8906, 0.8917, 0.8929, 0.8940, 0.8951, 0.8962, 0.8973, 0.8996,\n",
       "             0.9007, 0.9018, 0.9029, 0.9040, 0.9051, 0.9062, 0.9074, 0.9085, 0.9096,\n",
       "             0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9163, 0.9174, 0.9185, 0.9196,\n",
       "             0.9208, 0.9219, 0.9230, 0.9241, 0.9252, 0.9263, 0.9275, 0.9286, 0.9297,\n",
       "             0.9308, 0.9319, 0.9330, 0.9342, 0.9353, 0.9364, 0.9364, 0.9375, 0.9386,\n",
       "             0.9397, 0.9408, 0.9420, 0.9431, 0.9442, 0.9453, 0.9464, 0.9464, 0.9475,\n",
       "             0.9487, 0.9487, 0.9487, 0.9498, 0.9509, 0.9520, 0.9520, 0.9531, 0.9542,\n",
       "             0.9554, 0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621, 0.9621,\n",
       "             0.9621, 0.9632, 0.9643, 0.9643, 0.9654, 0.9665, 0.9676, 0.9676, 0.9676,\n",
       "             0.9676, 0.9676, 0.9688, 0.9699, 0.9699, 0.9710, 0.9721, 0.9732, 0.9732,\n",
       "             0.9743, 0.9754, 0.9754, 0.9766, 0.9777, 0.9777, 0.9788, 0.9799, 0.9810,\n",
       "             0.9821, 0.9833, 0.9833, 0.9833, 0.9833, 0.9833, 0.9844, 0.9844, 0.9844,\n",
       "             0.9855, 0.9866, 0.9866, 0.9866, 0.9877, 0.9888, 0.9900, 0.9900, 0.9900,\n",
       "             0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9911, 0.9911, 0.9911, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9944, 0.9944, 0.9955,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9981e-01, 9.9979e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9974e-01, 9.9971e-01, 9.9969e-01, 9.9969e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9964e-01, 9.9963e-01, 9.9960e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9958e-01, 9.9958e-01, 9.9958e-01, 9.9957e-01, 9.9955e-01,\n",
       "             9.9953e-01, 9.9952e-01, 9.9947e-01, 9.9944e-01, 9.9944e-01, 9.9941e-01,\n",
       "             9.9937e-01, 9.9934e-01, 9.9931e-01, 9.9928e-01, 9.9928e-01, 9.9923e-01,\n",
       "             9.9917e-01, 9.9916e-01, 9.9900e-01, 9.9899e-01, 9.9897e-01, 9.9852e-01,\n",
       "             9.9843e-01, 9.9840e-01, 9.9837e-01, 9.9831e-01, 9.9830e-01, 9.9825e-01,\n",
       "             9.9816e-01, 9.9796e-01, 9.9783e-01, 9.9781e-01, 9.9720e-01, 9.9700e-01,\n",
       "             9.9683e-01, 9.9659e-01, 9.9655e-01, 9.9634e-01, 9.9632e-01, 9.9614e-01,\n",
       "             9.9335e-01, 9.9147e-01, 9.9069e-01, 9.8681e-01, 9.8467e-01, 9.8460e-01,\n",
       "             9.8459e-01, 9.7757e-01, 9.7520e-01, 9.7143e-01, 9.6636e-01, 9.6037e-01,\n",
       "             9.5441e-01, 9.5428e-01, 9.4873e-01, 9.4756e-01, 9.2870e-01, 9.2098e-01,\n",
       "             8.6452e-01, 8.5500e-01, 8.4323e-01, 8.2757e-01, 8.1286e-01, 7.9469e-01,\n",
       "             7.8378e-01, 7.7188e-01, 7.4889e-01, 7.4877e-01, 6.8998e-01, 6.2035e-01,\n",
       "             6.0685e-01, 5.8848e-01, 5.6128e-01, 5.4222e-01, 5.0248e-01, 4.8245e-01,\n",
       "             4.4884e-01, 3.8721e-01, 3.7902e-01, 3.2041e-01, 3.1218e-01, 2.9044e-01,\n",
       "             2.8536e-01, 2.8012e-01, 2.7446e-01, 2.3439e-01, 1.9659e-01, 1.7435e-01,\n",
       "             1.5222e-01, 1.3891e-01, 1.3764e-01, 1.2941e-01, 1.1917e-01, 9.2031e-02,\n",
       "             6.6481e-02, 5.6081e-02, 5.1734e-02, 4.4063e-02, 4.3980e-02, 3.6864e-02,\n",
       "             3.5539e-02, 3.4548e-02, 3.1900e-02, 3.1418e-02, 2.7080e-02, 2.5935e-02,\n",
       "             2.5753e-02, 2.3082e-02, 2.2935e-02, 1.8566e-02, 1.7941e-02, 1.6091e-02,\n",
       "             1.4651e-02, 1.2613e-02, 1.1541e-02, 1.1381e-02, 6.1783e-03, 6.1370e-03,\n",
       "             5.9530e-03, 5.4872e-03, 5.2805e-03, 5.1670e-03, 5.0007e-03, 4.9204e-03,\n",
       "             4.5091e-03, 4.0074e-03, 3.4756e-03, 3.2355e-03, 3.1580e-03, 2.9083e-03,\n",
       "             2.6258e-03, 1.8286e-03, 1.3842e-03, 1.3526e-03, 1.1634e-03, 1.0713e-03,\n",
       "             8.8174e-04, 6.4945e-04, 6.2617e-04, 5.8310e-04, 5.4971e-04, 5.3634e-04,\n",
       "             5.3504e-04, 5.3100e-04, 4.8737e-04, 4.6849e-04, 4.6791e-04, 4.6327e-04,\n",
       "             3.4941e-04, 3.3921e-04, 3.3724e-04, 3.3542e-04, 3.0613e-04, 2.1783e-04,\n",
       "             2.1454e-04, 1.9503e-04, 1.9022e-04, 1.8943e-04, 1.8491e-04, 1.7439e-04,\n",
       "             1.6259e-04, 1.5496e-04, 1.5448e-04, 1.4514e-04, 8.7399e-05, 8.7365e-05,\n",
       "             8.2339e-05, 7.9166e-05, 7.6424e-05, 6.3809e-05, 6.1403e-05, 5.2052e-05,\n",
       "             4.1597e-05, 3.7614e-05, 2.8106e-05, 2.4154e-05, 1.9949e-05, 1.9290e-05,\n",
       "             1.7429e-05, 1.6199e-05, 1.4236e-05, 1.3486e-05, 1.1091e-05, 1.0378e-05,\n",
       "             1.0091e-05, 9.5001e-06, 7.4999e-06, 7.3464e-06, 7.0484e-06, 6.9194e-06,\n",
       "             6.8357e-06, 4.7002e-06, 4.2845e-06, 4.0054e-06, 3.6653e-06, 3.3172e-06,\n",
       "             2.2832e-06, 2.2257e-06, 2.1243e-06, 1.9986e-06, 1.8449e-06, 1.7822e-06,\n",
       "             1.5809e-06, 1.1601e-06, 1.1390e-06, 1.1052e-06, 9.4334e-07, 8.9781e-07,\n",
       "             8.6089e-07, 6.7234e-07, 5.8673e-07, 5.7657e-07, 5.1524e-07, 4.9632e-07,\n",
       "             4.4427e-07, 4.3433e-07, 3.5486e-07, 3.2356e-07, 2.8006e-07, 2.3175e-07,\n",
       "             2.0937e-07, 1.9210e-07, 1.6726e-07, 1.5251e-07, 1.4234e-07, 1.2045e-07,\n",
       "             1.1032e-07, 1.0209e-07, 9.8514e-08, 9.5466e-08, 7.8512e-08, 7.4156e-08,\n",
       "             7.1818e-08, 5.6246e-08, 5.5589e-08, 5.3019e-08, 5.1702e-08, 4.3706e-08,\n",
       "             4.2830e-08, 3.8241e-08, 3.6682e-08, 3.0450e-08, 2.6859e-08, 2.1024e-08,\n",
       "             1.8384e-08, 1.7157e-08, 1.6729e-08, 1.6255e-08, 1.6137e-08, 1.6106e-08,\n",
       "             1.6065e-08, 1.5628e-08, 1.5551e-08, 1.5325e-08, 1.4761e-08, 1.3564e-08,\n",
       "             1.3556e-08, 1.2871e-08, 1.2268e-08, 1.2108e-08, 1.1748e-08, 1.1217e-08,\n",
       "             1.1193e-08, 1.0958e-08, 1.0851e-08, 8.6418e-09, 8.5567e-09, 8.4900e-09,\n",
       "             8.0302e-09, 7.1378e-09, 6.9341e-09, 5.4070e-09, 3.3752e-09, 2.8763e-09,\n",
       "             2.5767e-09, 2.4221e-09, 2.3118e-09, 2.1158e-09, 2.0332e-09, 2.0165e-09,\n",
       "             1.9183e-09, 1.7020e-09, 1.5808e-09, 1.4416e-09, 1.2836e-09, 1.2258e-09,\n",
       "             1.1802e-09, 1.1152e-09, 7.7341e-10, 7.4509e-10, 7.3410e-10, 7.1358e-10,\n",
       "             7.0534e-10, 4.4439e-10, 4.0136e-10, 3.6803e-10, 3.0678e-10, 2.7046e-10,\n",
       "             2.4914e-10, 2.4003e-10, 2.3256e-10, 2.2007e-10, 1.8650e-10, 1.8521e-10,\n",
       "             1.6289e-10, 1.5857e-10, 1.4260e-10, 1.3582e-10, 1.0826e-10, 1.0681e-10,\n",
       "             8.5769e-11, 7.4266e-11, 6.8492e-11, 4.0895e-11, 3.9116e-11, 2.8749e-11,\n",
       "             2.4662e-11, 2.4291e-11, 1.6073e-11, 1.5931e-11, 1.5857e-11, 1.3728e-11,\n",
       "             1.1934e-11, 1.0778e-11, 8.7170e-12, 7.4139e-12, 6.6662e-12, 5.4806e-12,\n",
       "             3.4489e-12, 3.3290e-12, 3.0981e-12, 2.8969e-12, 1.9446e-12, 1.3098e-12,\n",
       "             9.0781e-13, 6.9787e-13, 5.4031e-13, 4.1198e-13, 3.2969e-13, 1.9495e-13,\n",
       "             1.7565e-13, 1.6897e-13, 1.5913e-13, 1.0997e-13, 7.1927e-14, 3.7110e-14,\n",
       "             3.2308e-14, 1.8884e-14, 1.7129e-14])}},\n",
       "   {'fpr': np.float64(0.099644128113879),\n",
       "    'tpr': np.float64(0.9933035714285714),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "             0.0036, 0.0036, 0.0036, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
       "             0.0107, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0249, 0.0285, 0.0285, 0.0285, 0.0320, 0.0356, 0.0356, 0.0356,\n",
       "             0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356,\n",
       "             0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0391, 0.0391, 0.0391, 0.0391,\n",
       "             0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0391, 0.0427, 0.0427, 0.0427,\n",
       "             0.0427, 0.0463, 0.0463, 0.0463, 0.0463, 0.0498, 0.0498, 0.0534, 0.0534,\n",
       "             0.0569, 0.0605, 0.0605, 0.0605, 0.0605, 0.0641, 0.0676, 0.0712, 0.0747,\n",
       "             0.0783, 0.0819, 0.0854, 0.0854, 0.0890, 0.0925, 0.0961, 0.0996, 0.1032,\n",
       "             0.1068, 0.1068, 0.1103, 0.1139, 0.1139, 0.1174, 0.1210, 0.1246, 0.1281,\n",
       "             0.1317, 0.1352, 0.1352, 0.1388, 0.1423, 0.1459, 0.1495, 0.1530, 0.1566,\n",
       "             0.1601, 0.1637, 0.1673, 0.1708, 0.1744, 0.1779, 0.1815, 0.1851, 0.1886,\n",
       "             0.1922, 0.1957, 0.1993, 0.2028, 0.2028, 0.2064, 0.2100, 0.2135, 0.2171,\n",
       "             0.2206, 0.2242, 0.2278, 0.2313, 0.2349, 0.2384, 0.2420, 0.2420, 0.2420,\n",
       "             0.2456, 0.2491, 0.2527, 0.2562, 0.2598, 0.2633, 0.2669, 0.2705, 0.2740,\n",
       "             0.2776, 0.2811, 0.2847, 0.2883, 0.2918, 0.2954, 0.2989, 0.3025, 0.3060,\n",
       "             0.3096, 0.3132, 0.3167, 0.3203, 0.3238, 0.3274, 0.3310, 0.3345, 0.3381,\n",
       "             0.3416, 0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630, 0.3665, 0.3701,\n",
       "             0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950, 0.3986, 0.4021,\n",
       "             0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270, 0.4306, 0.4342,\n",
       "             0.4377, 0.4413, 0.4448, 0.4484, 0.4520, 0.4555, 0.4591, 0.4626, 0.4662,\n",
       "             0.4698, 0.4733, 0.4769, 0.4804, 0.4840, 0.4875, 0.4911, 0.4947, 0.4982,\n",
       "             0.5018, 0.5053, 0.5089, 0.5125, 0.5160, 0.5196, 0.5231, 0.5267, 0.5302,\n",
       "             0.5338, 0.5374, 0.5409, 0.5445, 0.5480, 0.5516, 0.5552, 0.5587, 0.5623,\n",
       "             0.5658, 0.5694, 0.5730, 0.5765, 0.5801, 0.5836, 0.5872, 0.5907, 0.5943,\n",
       "             0.5979, 0.6014, 0.6050, 0.6085, 0.6121, 0.6157, 0.6192, 0.6228, 0.6263,\n",
       "             0.6299, 0.6335, 0.6370, 0.6406, 0.6441, 0.6477, 0.6512, 0.6548, 0.6584,\n",
       "             0.6619, 0.6655, 0.6690, 0.6726, 0.6762, 0.6797, 0.6833, 0.6868, 0.6904,\n",
       "             0.6940, 0.6975, 0.7011, 0.7046, 0.7082, 0.7117, 0.7153, 0.7189, 0.7224,\n",
       "             0.7260, 0.7295, 0.7331, 0.7367, 0.7402, 0.7438, 0.7473, 0.7509, 0.7544,\n",
       "             0.7580, 0.7616, 0.7651, 0.7687, 0.7722, 0.7758, 0.7794, 0.7829, 0.7865,\n",
       "             0.7900, 0.7936, 0.7972, 0.8007, 0.8043, 0.8078, 0.8114, 0.8149, 0.8185,\n",
       "             0.8221, 0.8256, 0.8292, 0.8327, 0.8363, 0.8399, 0.8434, 0.8470, 0.8505,\n",
       "             0.8541, 0.8577, 0.8612, 0.8648, 0.8683, 0.8719, 0.8754, 0.8790, 0.8826,\n",
       "             0.8861, 0.8897, 0.8932, 0.8968, 0.9004, 0.9039, 0.9075, 0.9110, 0.9146,\n",
       "             0.9181, 0.9217, 0.9253, 0.9288, 0.9324, 0.9359, 0.9395, 0.9431, 0.9466,\n",
       "             0.9502, 0.9537, 0.9573, 0.9609, 0.9644, 0.9680, 0.9715, 0.9751, 0.9786,\n",
       "             0.9822, 0.9858, 0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7299, 0.7768, 0.8025, 0.8170, 0.8292, 0.8371, 0.8438, 0.8471,\n",
       "             0.8527, 0.8549, 0.8583, 0.8616, 0.8650, 0.8661, 0.8683, 0.8694, 0.8739,\n",
       "             0.8750, 0.8761, 0.8772, 0.8783, 0.8795, 0.8806, 0.8817, 0.8839, 0.8862,\n",
       "             0.8862, 0.8873, 0.8884, 0.8895, 0.8906, 0.8917, 0.8929, 0.8940, 0.8951,\n",
       "             0.8962, 0.8973, 0.8996, 0.9007, 0.9018, 0.9029, 0.9051, 0.9074, 0.9085,\n",
       "             0.9096, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9163, 0.9174, 0.9185,\n",
       "             0.9196, 0.9208, 0.9219, 0.9230, 0.9241, 0.9252, 0.9263, 0.9275, 0.9286,\n",
       "             0.9297, 0.9308, 0.9319, 0.9330, 0.9342, 0.9353, 0.9364, 0.9386, 0.9397,\n",
       "             0.9408, 0.9420, 0.9420, 0.9431, 0.9442, 0.9453, 0.9453, 0.9464, 0.9475,\n",
       "             0.9487, 0.9498, 0.9498, 0.9509, 0.9520, 0.9520, 0.9520, 0.9531, 0.9542,\n",
       "             0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621, 0.9632, 0.9643,\n",
       "             0.9654, 0.9665, 0.9676, 0.9688, 0.9699, 0.9699, 0.9710, 0.9721, 0.9732,\n",
       "             0.9743, 0.9754, 0.9766, 0.9777, 0.9788, 0.9799, 0.9799, 0.9810, 0.9821,\n",
       "             0.9833, 0.9833, 0.9844, 0.9855, 0.9866, 0.9866, 0.9877, 0.9877, 0.9888,\n",
       "             0.9888, 0.9888, 0.9900, 0.9911, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9944, 0.9944, 0.9944, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9955, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9989, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9982e-01, 9.9979e-01, 9.9977e-01,\n",
       "             9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9972e-01, 9.9970e-01, 9.9966e-01,\n",
       "             9.9966e-01, 9.9963e-01, 9.9961e-01, 9.9959e-01, 9.9958e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9949e-01, 9.9944e-01, 9.9938e-01, 9.9933e-01, 9.9911e-01,\n",
       "             9.9909e-01, 9.9901e-01, 9.9899e-01, 9.9892e-01, 9.9892e-01, 9.9885e-01,\n",
       "             9.9879e-01, 9.9873e-01, 9.9845e-01, 9.9820e-01, 9.9756e-01, 9.9744e-01,\n",
       "             9.9667e-01, 9.9567e-01, 9.9476e-01, 9.9444e-01, 9.9296e-01, 9.9202e-01,\n",
       "             9.8857e-01, 9.8722e-01, 9.8604e-01, 9.7985e-01, 9.7276e-01, 9.5373e-01,\n",
       "             9.5143e-01, 9.1923e-01, 9.0979e-01, 8.9715e-01, 8.6410e-01, 8.4439e-01,\n",
       "             8.3411e-01, 8.2958e-01, 7.9270e-01, 7.6907e-01, 7.5204e-01, 7.5062e-01,\n",
       "             6.8792e-01, 6.8426e-01, 6.0468e-01, 6.0464e-01, 5.7906e-01, 4.7775e-01,\n",
       "             4.4365e-01, 4.1016e-01, 3.9996e-01, 3.8674e-01, 3.2070e-01, 2.9235e-01,\n",
       "             2.5317e-01, 2.4550e-01, 2.2853e-01, 2.1995e-01, 2.0608e-01, 1.6625e-01,\n",
       "             1.5709e-01, 1.5161e-01, 1.3884e-01, 1.3695e-01, 1.3509e-01, 1.3055e-01,\n",
       "             1.1216e-01, 1.1164e-01, 1.1056e-01, 8.5030e-02, 7.3192e-02, 6.7197e-02,\n",
       "             6.6481e-02, 6.2310e-02, 6.1779e-02, 5.9346e-02, 5.8948e-02, 4.5715e-02,\n",
       "             4.0289e-02, 3.6578e-02, 3.0765e-02, 2.9502e-02, 2.1485e-02, 2.0812e-02,\n",
       "             1.5100e-02, 1.4571e-02, 1.4157e-02, 1.3451e-02, 1.1659e-02, 1.0919e-02,\n",
       "             8.3600e-03, 8.0971e-03, 7.8570e-03, 7.1660e-03, 6.0562e-03, 5.6479e-03,\n",
       "             5.3734e-03, 4.5346e-03, 4.1503e-03, 3.9906e-03, 3.7237e-03, 3.5517e-03,\n",
       "             3.5048e-03, 2.9383e-03, 2.9304e-03, 2.8083e-03, 2.5153e-03, 2.5033e-03,\n",
       "             2.5027e-03, 2.4648e-03, 2.2668e-03, 2.2228e-03, 2.0294e-03, 1.5551e-03,\n",
       "             1.5235e-03, 1.4714e-03, 1.4563e-03, 1.3093e-03, 1.1737e-03, 1.0255e-03,\n",
       "             9.3592e-04, 9.1356e-04, 8.1547e-04, 7.9920e-04, 7.3963e-04, 7.3619e-04,\n",
       "             7.1282e-04, 6.8635e-04, 6.8393e-04, 6.2635e-04, 5.9011e-04, 5.8607e-04,\n",
       "             5.8049e-04, 5.5098e-04, 4.9909e-04, 4.7940e-04, 4.4477e-04, 4.1826e-04,\n",
       "             3.8766e-04, 3.7350e-04, 3.4031e-04, 2.8342e-04, 2.6534e-04, 2.4171e-04,\n",
       "             2.3756e-04, 2.2166e-04, 2.1045e-04, 1.8122e-04, 1.7474e-04, 1.7208e-04,\n",
       "             1.4708e-04, 1.3240e-04, 1.1471e-04, 1.1424e-04, 9.8320e-05, 9.6339e-05,\n",
       "             8.0069e-05, 7.8818e-05, 7.7996e-05, 6.7477e-05, 6.4631e-05, 5.8832e-05,\n",
       "             5.2950e-05, 5.1906e-05, 5.0942e-05, 5.0633e-05, 4.9381e-05, 4.7492e-05,\n",
       "             4.4276e-05, 4.3320e-05, 3.7542e-05, 2.7659e-05, 2.4369e-05, 2.4297e-05,\n",
       "             2.2601e-05, 2.1915e-05, 2.1775e-05, 1.8309e-05, 1.7707e-05, 1.5813e-05,\n",
       "             1.4705e-05, 1.4705e-05, 1.4025e-05, 1.2771e-05, 1.2504e-05, 1.2404e-05,\n",
       "             1.2255e-05, 1.1685e-05, 1.0952e-05, 1.0890e-05, 9.6605e-06, 8.8195e-06,\n",
       "             8.2179e-06, 6.3390e-06, 5.3661e-06, 5.3205e-06, 5.0804e-06, 4.7831e-06,\n",
       "             4.3769e-06, 3.1220e-06, 2.7151e-06, 2.6644e-06, 2.2197e-06, 1.9329e-06,\n",
       "             1.8131e-06, 1.5685e-06, 1.5457e-06, 1.4629e-06, 1.3034e-06, 1.1171e-06,\n",
       "             1.1130e-06, 1.0598e-06, 1.0217e-06, 1.0115e-06, 8.9334e-07, 8.6634e-07,\n",
       "             8.4665e-07, 7.8121e-07, 7.6955e-07, 7.4662e-07, 7.4001e-07, 6.1791e-07,\n",
       "             5.5905e-07, 5.4017e-07, 5.0463e-07, 4.7430e-07, 4.3230e-07, 3.8817e-07,\n",
       "             3.7870e-07, 3.6764e-07, 3.2005e-07, 3.0673e-07, 2.9666e-07, 2.7470e-07,\n",
       "             2.4947e-07, 2.4128e-07, 2.3432e-07, 2.0962e-07, 1.7587e-07, 1.7398e-07,\n",
       "             1.6100e-07, 1.3960e-07, 1.3651e-07, 1.3382e-07, 1.1879e-07, 1.0962e-07,\n",
       "             1.0446e-07, 9.1028e-08, 8.9906e-08, 8.6539e-08, 8.1118e-08, 7.7244e-08,\n",
       "             7.6462e-08, 6.4917e-08, 5.5445e-08, 5.3301e-08, 3.6710e-08, 3.1910e-08,\n",
       "             2.8405e-08, 2.7480e-08, 2.7238e-08, 2.6293e-08, 2.4106e-08, 1.7218e-08,\n",
       "             1.3455e-08, 1.2757e-08, 1.2322e-08, 1.2252e-08, 1.1397e-08, 1.1349e-08,\n",
       "             8.6507e-09, 6.9985e-09, 5.1889e-09, 5.1273e-09, 4.6295e-09, 4.5754e-09,\n",
       "             4.2680e-09, 3.3343e-09, 3.2624e-09, 3.2214e-09, 2.9855e-09, 1.8133e-09,\n",
       "             1.3882e-09, 1.3603e-09, 1.1994e-09, 8.4195e-10, 7.4035e-10, 6.0859e-10,\n",
       "             4.8852e-10, 4.5045e-10, 2.2562e-10, 2.1561e-10, 1.5155e-10, 1.4167e-10,\n",
       "             1.2668e-10, 8.1325e-11, 7.1520e-11, 6.4784e-11, 5.9243e-11, 5.4520e-11,\n",
       "             2.8787e-11, 1.2123e-11, 9.5886e-12, 4.1634e-12, 9.9026e-13, 5.4309e-13])}},\n",
       "   {'fpr': np.float64(0.06761565836298933),\n",
       "    'tpr': np.float64(0.9921875),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0071, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107,\n",
       "             0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142,\n",
       "             0.0142, 0.0142, 0.0142, 0.0142, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
       "             0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178, 0.0214, 0.0214, 0.0214,\n",
       "             0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0214, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249, 0.0249,\n",
       "             0.0249, 0.0285, 0.0320, 0.0320, 0.0320, 0.0356, 0.0356, 0.0356, 0.0356,\n",
       "             0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0356, 0.0391, 0.0391,\n",
       "             0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0463, 0.0463, 0.0463, 0.0463,\n",
       "             0.0463, 0.0463, 0.0498, 0.0534, 0.0534, 0.0534, 0.0534, 0.0569, 0.0569,\n",
       "             0.0605, 0.0641, 0.0676, 0.0676, 0.0712, 0.0712, 0.0747, 0.0783, 0.0783,\n",
       "             0.0819, 0.0854, 0.0890, 0.0890, 0.0925, 0.0961, 0.0996, 0.1032, 0.1068,\n",
       "             0.1103, 0.1139, 0.1174, 0.1210, 0.1246, 0.1281, 0.1317, 0.1352, 0.1388,\n",
       "             0.1423, 0.1459, 0.1495, 0.1530, 0.1566, 0.1601, 0.1637, 0.1673, 0.1708,\n",
       "             0.1744, 0.1779, 0.1815, 0.1815, 0.1851, 0.1886, 0.1922, 0.1957, 0.1993,\n",
       "             0.2028, 0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242, 0.2278, 0.2313,\n",
       "             0.2349, 0.2384, 0.2420, 0.2456, 0.2456, 0.2491, 0.2527, 0.2562, 0.2598,\n",
       "             0.2633, 0.2669, 0.2705, 0.2740, 0.2776, 0.2811, 0.2847, 0.2883, 0.2918,\n",
       "             0.2918, 0.2954, 0.2989, 0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203,\n",
       "             0.3238, 0.3274, 0.3310, 0.3345, 0.3381, 0.3416, 0.3452, 0.3488, 0.3523,\n",
       "             0.3559, 0.3594, 0.3630, 0.3665, 0.3701, 0.3737, 0.3772, 0.3808, 0.3843,\n",
       "             0.3879, 0.3915, 0.3950, 0.3986, 0.4021, 0.4057, 0.4093, 0.4128, 0.4164,\n",
       "             0.4199, 0.4235, 0.4270, 0.4306, 0.4342, 0.4377, 0.4413, 0.4448, 0.4484,\n",
       "             0.4520, 0.4555, 0.4591, 0.4626, 0.4662, 0.4698, 0.4733, 0.4769, 0.4804,\n",
       "             0.4840, 0.4875, 0.4911, 0.4947, 0.4982, 0.5018, 0.5053, 0.5089, 0.5125,\n",
       "             0.5160, 0.5196, 0.5231, 0.5267, 0.5302, 0.5338, 0.5374, 0.5409, 0.5445,\n",
       "             0.5480, 0.5516, 0.5552, 0.5587, 0.5623, 0.5658, 0.5694, 0.5730, 0.5765,\n",
       "             0.5801, 0.5836, 0.5872, 0.5907, 0.5943, 0.5979, 0.6014, 0.6050, 0.6085,\n",
       "             0.6121, 0.6157, 0.6192, 0.6228, 0.6263, 0.6299, 0.6335, 0.6370, 0.6406,\n",
       "             0.6441, 0.6477, 0.6512, 0.6548, 0.6584, 0.6619, 0.6655, 0.6690, 0.6726,\n",
       "             0.6762, 0.6797, 0.6833, 0.6868, 0.6904, 0.6940, 0.6975, 0.7011, 0.7046,\n",
       "             0.7082, 0.7117, 0.7153, 0.7189, 0.7224, 0.7260, 0.7295, 0.7331, 0.7367,\n",
       "             0.7402, 0.7438, 0.7473, 0.7509, 0.7544, 0.7580, 0.7616, 0.7651, 0.7687,\n",
       "             0.7722, 0.7758, 0.7794, 0.7829, 0.7865, 0.7900, 0.7936, 0.7972, 0.8007,\n",
       "             0.8043, 0.8078, 0.8114, 0.8149, 0.8185, 0.8221, 0.8256, 0.8292, 0.8327,\n",
       "             0.8363, 0.8399, 0.8434, 0.8470, 0.8505, 0.8541, 0.8577, 0.8612, 0.8648,\n",
       "             0.8683, 0.8719, 0.8754, 0.8790, 0.8826, 0.8861, 0.8897, 0.8932, 0.8968,\n",
       "             0.9004, 0.9039, 0.9075, 0.9110, 0.9146, 0.9181, 0.9217, 0.9253, 0.9288,\n",
       "             0.9324, 0.9359, 0.9395, 0.9431, 0.9466, 0.9502, 0.9537, 0.9573, 0.9609,\n",
       "             0.9644, 0.9680, 0.9715, 0.9751, 0.9786, 0.9822, 0.9858, 0.9893, 0.9929,\n",
       "             0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.8393, 0.8728, 0.8862, 0.8906, 0.8962, 0.8973, 0.9007, 0.9018,\n",
       "             0.9040, 0.9062, 0.9074, 0.9085, 0.9096, 0.9107, 0.9118, 0.9141, 0.9152,\n",
       "             0.9163, 0.9174, 0.9185, 0.9196, 0.9208, 0.9219, 0.9230, 0.9241, 0.9252,\n",
       "             0.9263, 0.9275, 0.9286, 0.9297, 0.9297, 0.9308, 0.9319, 0.9330, 0.9342,\n",
       "             0.9353, 0.9364, 0.9375, 0.9386, 0.9397, 0.9408, 0.9408, 0.9420, 0.9431,\n",
       "             0.9442, 0.9453, 0.9464, 0.9475, 0.9487, 0.9498, 0.9498, 0.9509, 0.9520,\n",
       "             0.9531, 0.9542, 0.9554, 0.9565, 0.9576, 0.9587, 0.9598, 0.9609, 0.9621,\n",
       "             0.9632, 0.9632, 0.9632, 0.9643, 0.9654, 0.9654, 0.9665, 0.9676, 0.9688,\n",
       "             0.9699, 0.9710, 0.9721, 0.9732, 0.9743, 0.9754, 0.9766, 0.9766, 0.9777,\n",
       "             0.9777, 0.9788, 0.9799, 0.9810, 0.9821, 0.9821, 0.9833, 0.9844, 0.9855,\n",
       "             0.9866, 0.9877, 0.9877, 0.9877, 0.9888, 0.9900, 0.9911, 0.9911, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9944, 0.9944, 0.9944, 0.9955,\n",
       "             0.9955, 0.9955, 0.9955, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
       "             0.9967, 0.9967, 0.9967, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978, 0.9978,\n",
       "             0.9978, 0.9978, 0.9978, 0.9978, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9988e-01, 9.9985e-01, 9.9984e-01, 9.9981e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9976e-01, 9.9973e-01, 9.9966e-01, 9.9965e-01,\n",
       "             9.9964e-01, 9.9961e-01, 9.9960e-01, 9.9948e-01, 9.9929e-01, 9.9889e-01,\n",
       "             9.9885e-01, 9.9834e-01, 9.9819e-01, 9.9759e-01, 9.9741e-01, 9.9739e-01,\n",
       "             9.9721e-01, 9.9705e-01, 9.9588e-01, 9.9584e-01, 9.9554e-01, 9.9513e-01,\n",
       "             9.9225e-01, 9.9210e-01, 9.8872e-01, 9.8803e-01, 9.8750e-01, 9.7528e-01,\n",
       "             9.7506e-01, 9.7446e-01, 9.6800e-01, 9.4797e-01, 9.4611e-01, 9.4354e-01,\n",
       "             8.9899e-01, 8.5323e-01, 8.5167e-01, 8.4835e-01, 8.2074e-01, 7.9407e-01,\n",
       "             7.8106e-01, 7.7790e-01, 7.1565e-01, 5.4377e-01, 5.2941e-01, 5.2747e-01,\n",
       "             4.3850e-01, 4.1524e-01, 4.0947e-01, 3.2049e-01, 2.3114e-01, 2.2235e-01,\n",
       "             2.1034e-01, 1.6445e-01, 1.6245e-01, 1.1131e-01, 7.4833e-02, 6.0509e-02,\n",
       "             5.5004e-02, 5.1802e-02, 5.0628e-02, 3.6660e-02, 3.0725e-02, 2.5452e-02,\n",
       "             2.5215e-02, 1.9891e-02, 1.6463e-02, 1.6101e-02, 1.5892e-02, 1.5175e-02,\n",
       "             1.4758e-02, 1.4467e-02, 1.3644e-02, 1.3564e-02, 1.2880e-02, 1.1131e-02,\n",
       "             7.5194e-03, 6.5584e-03, 6.4919e-03, 6.2146e-03, 4.8574e-03, 4.4371e-03,\n",
       "             4.4352e-03, 3.9441e-03, 3.7191e-03, 3.4653e-03, 3.3733e-03, 2.9925e-03,\n",
       "             2.9619e-03, 2.8318e-03, 2.3956e-03, 2.0231e-03, 1.9463e-03, 1.6526e-03,\n",
       "             1.3242e-03, 1.2655e-03, 1.1367e-03, 9.8847e-04, 9.0283e-04, 8.1680e-04,\n",
       "             8.0307e-04, 7.2512e-04, 6.7601e-04, 5.4176e-04, 5.3934e-04, 5.3739e-04,\n",
       "             5.1690e-04, 4.2794e-04, 3.7047e-04, 3.5008e-04, 3.0009e-04, 2.5774e-04,\n",
       "             2.2687e-04, 2.0474e-04, 2.0237e-04, 1.8650e-04, 1.8626e-04, 1.6888e-04,\n",
       "             1.2769e-04, 1.2090e-04, 9.9388e-05, 9.7242e-05, 7.4821e-05, 6.3476e-05,\n",
       "             5.1957e-05, 4.7976e-05, 4.3549e-05, 4.1425e-05, 4.0384e-05, 3.0159e-05,\n",
       "             2.9774e-05, 2.9701e-05, 2.8575e-05, 2.0429e-05, 1.8226e-05, 1.4261e-05,\n",
       "             1.2823e-05, 1.2707e-05, 1.2161e-05, 1.1264e-05, 1.0659e-05, 9.9420e-06,\n",
       "             9.4895e-06, 8.6675e-06, 8.4280e-06, 7.1343e-06, 6.9322e-06, 6.6454e-06,\n",
       "             6.5162e-06, 5.2423e-06, 5.1697e-06, 4.7835e-06, 3.8524e-06, 3.6878e-06,\n",
       "             3.5237e-06, 3.3502e-06, 3.2563e-06, 3.1516e-06, 3.1382e-06, 2.9479e-06,\n",
       "             2.9471e-06, 2.8600e-06, 2.6746e-06, 2.6603e-06, 2.3931e-06, 2.2948e-06,\n",
       "             2.0041e-06, 1.9936e-06, 1.9136e-06, 1.6292e-06, 1.5271e-06, 1.4803e-06,\n",
       "             1.4288e-06, 1.4094e-06, 1.0438e-06, 9.7109e-07, 9.1782e-07, 8.9236e-07,\n",
       "             6.0549e-07, 5.9324e-07, 4.5640e-07, 4.5183e-07, 4.2574e-07, 4.1962e-07,\n",
       "             4.1101e-07, 3.9242e-07, 3.8545e-07, 3.3410e-07, 3.1601e-07, 2.7726e-07,\n",
       "             2.3709e-07, 2.3402e-07, 2.3242e-07, 2.0674e-07, 2.0316e-07, 2.0165e-07,\n",
       "             2.0119e-07, 1.9809e-07, 1.8969e-07, 1.8790e-07, 1.7199e-07, 1.3100e-07,\n",
       "             1.2751e-07, 1.1691e-07, 1.1312e-07, 1.1102e-07, 9.1275e-08, 8.3158e-08,\n",
       "             8.2134e-08, 7.4629e-08, 7.3570e-08, 7.2013e-08, 5.6697e-08, 5.5657e-08,\n",
       "             3.9180e-08, 3.6141e-08, 3.3463e-08, 3.2793e-08, 3.2035e-08, 3.1234e-08,\n",
       "             3.0768e-08, 3.0135e-08, 2.4744e-08, 2.4255e-08, 2.3933e-08, 2.2975e-08,\n",
       "             2.2357e-08, 2.2141e-08, 2.1316e-08, 1.9431e-08, 1.9425e-08, 1.5908e-08,\n",
       "             1.5820e-08, 1.5367e-08, 1.5348e-08, 1.3131e-08, 1.1375e-08, 1.0727e-08,\n",
       "             9.8369e-09, 9.7120e-09, 8.4718e-09, 7.3659e-09, 7.0932e-09, 7.0474e-09,\n",
       "             6.7790e-09, 5.8151e-09, 5.4428e-09, 5.4244e-09, 4.9645e-09, 4.8525e-09,\n",
       "             3.1498e-09, 2.6880e-09, 2.6060e-09, 2.6040e-09, 2.5949e-09, 2.5597e-09,\n",
       "             2.5338e-09, 2.4959e-09, 2.2729e-09, 2.2151e-09, 2.0504e-09, 1.9414e-09,\n",
       "             1.6868e-09, 1.6052e-09, 1.1015e-09, 9.7773e-10, 8.6945e-10, 8.2059e-10,\n",
       "             7.3824e-10, 7.2438e-10, 5.5949e-10, 4.8434e-10, 4.6159e-10, 4.2842e-10,\n",
       "             4.1002e-10, 2.6979e-10, 2.5551e-10, 2.5050e-10, 2.4430e-10, 2.2022e-10,\n",
       "             2.1302e-10, 1.6927e-10, 1.6095e-10, 1.5002e-10, 1.4421e-10, 1.1266e-10,\n",
       "             9.7093e-11, 9.6175e-11, 7.0201e-11, 4.6668e-11, 4.5933e-11, 3.9893e-11,\n",
       "             3.4936e-11, 3.0265e-11, 2.4582e-11, 2.3553e-11, 1.7789e-11, 1.2356e-11,\n",
       "             8.1497e-12, 7.5343e-12, 6.4765e-12, 4.3964e-12, 4.0174e-12, 2.3911e-12,\n",
       "             1.5034e-12, 1.0568e-12, 6.7893e-13, 6.7487e-13, 5.8081e-13, 3.2341e-13,\n",
       "             2.1888e-13, 1.7376e-13, 1.0976e-13, 3.9428e-14, 2.8520e-16])}},\n",
       "   {'fpr': np.float64(0.3487544483985765),\n",
       "    'tpr': np.float64(0.9988839285714286),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0747, 0.0961, 0.1068, 0.1103, 0.1103, 0.1139, 0.1174, 0.1210,\n",
       "             0.1246, 0.1281, 0.1317, 0.1352, 0.1352, 0.1352, 0.1352, 0.1388, 0.1423,\n",
       "             0.1423, 0.1459, 0.1495, 0.1530, 0.1530, 0.1566, 0.1566, 0.1601, 0.1637,\n",
       "             0.1637, 0.1673, 0.1708, 0.1708, 0.1708, 0.1744, 0.1744, 0.1779, 0.1815,\n",
       "             0.1815, 0.1815, 0.1851, 0.1886, 0.1922, 0.1957, 0.1993, 0.2028, 0.2064,\n",
       "             0.2064, 0.2100, 0.2135, 0.2171, 0.2206, 0.2242, 0.2278, 0.2313, 0.2349,\n",
       "             0.2384, 0.2420, 0.2456, 0.2491, 0.2527, 0.2527, 0.2562, 0.2598, 0.2633,\n",
       "             0.2669, 0.2705, 0.2740, 0.2776, 0.2776, 0.2811, 0.2811, 0.2847, 0.2883,\n",
       "             0.2918, 0.2954, 0.2989, 0.3025, 0.3060, 0.3096, 0.3132, 0.3167, 0.3203,\n",
       "             0.3238, 0.3238, 0.3238, 0.3274, 0.3310, 0.3310, 0.3345, 0.3381, 0.3416,\n",
       "             0.3452, 0.3488, 0.3523, 0.3559, 0.3594, 0.3630, 0.3665, 0.3701, 0.3701,\n",
       "             0.3737, 0.3772, 0.3808, 0.3843, 0.3879, 0.3915, 0.3950, 0.3986, 0.4021,\n",
       "             0.4057, 0.4093, 0.4128, 0.4164, 0.4199, 0.4235, 0.4270, 0.4306, 0.4342,\n",
       "             0.4377, 0.4413, 0.4448, 0.4484, 0.4520, 0.4555, 0.4591, 0.4626, 0.4662,\n",
       "             0.4698, 0.4733, 0.4769, 0.4804, 0.4840, 0.4875, 0.4911, 0.4947, 0.4982,\n",
       "             0.5018, 0.5053, 0.5089, 0.5125, 0.5160, 0.5196, 0.5231, 0.5267, 0.5302,\n",
       "             0.5338, 0.5374, 0.5409, 0.5445, 0.5480, 0.5516, 0.5552, 0.5587, 0.5623,\n",
       "             0.5658, 0.5694, 0.5730, 0.5765, 0.5801, 0.5836, 0.5872, 0.5907, 0.5943,\n",
       "             0.5979, 0.6014, 0.6050, 0.6085, 0.6121, 0.6157, 0.6192, 0.6228, 0.6263,\n",
       "             0.6299, 0.6335, 0.6370, 0.6406, 0.6441, 0.6477, 0.6512, 0.6548, 0.6584,\n",
       "             0.6619, 0.6655, 0.6690, 0.6726, 0.6762, 0.6797, 0.6833, 0.6868, 0.6904,\n",
       "             0.6940, 0.6975, 0.7011, 0.7046, 0.7082, 0.7117, 0.7153, 0.7189, 0.7224,\n",
       "             0.7260, 0.7295, 0.7331, 0.7367, 0.7402, 0.7438, 0.7473, 0.7509, 0.7544,\n",
       "             0.7580, 0.7616, 0.7651, 0.7687, 0.7722, 0.7758, 0.7794, 0.7829, 0.7865,\n",
       "             0.7900, 0.7936, 0.7972, 0.8007, 0.8043, 0.8078, 0.8114, 0.8149, 0.8185,\n",
       "             0.8221, 0.8256, 0.8292, 0.8327, 0.8363, 0.8399, 0.8434, 0.8470, 0.8505,\n",
       "             0.8541, 0.8577, 0.8612, 0.8648, 0.8683, 0.8719, 0.8754, 0.8790, 0.8826,\n",
       "             0.8861, 0.8897, 0.8932, 0.8968, 0.9004, 0.9039, 0.9075, 0.9110, 0.9146,\n",
       "             0.9181, 0.9217, 0.9253, 0.9288, 0.9324, 0.9359, 0.9395, 0.9431, 0.9466,\n",
       "             0.9502, 0.9537, 0.9573, 0.9609, 0.9644, 0.9680, 0.9715, 0.9751, 0.9786,\n",
       "             0.9822, 0.9858, 0.9893, 0.9929, 0.9964, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.9621, 0.9676, 0.9699, 0.9699, 0.9721, 0.9754, 0.9754, 0.9766,\n",
       "             0.9777, 0.9777, 0.9777, 0.9777, 0.9788, 0.9799, 0.9810, 0.9810, 0.9810,\n",
       "             0.9821, 0.9821, 0.9821, 0.9821, 0.9833, 0.9833, 0.9844, 0.9844, 0.9844,\n",
       "             0.9855, 0.9855, 0.9855, 0.9866, 0.9877, 0.9877, 0.9888, 0.9888, 0.9888,\n",
       "             0.9900, 0.9911, 0.9911, 0.9911, 0.9911, 0.9911, 0.9911, 0.9911, 0.9911,\n",
       "             0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "             0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9933, 0.9933, 0.9933, 0.9933,\n",
       "             0.9933, 0.9933, 0.9933, 0.9933, 0.9944, 0.9944, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n",
       "             0.9955, 0.9967, 0.9978, 0.9978, 0.9978, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9989e-01, 9.9988e-01, 9.9987e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9981e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9978e-01, 9.9966e-01, 9.9956e-01, 9.9944e-01, 9.9937e-01, 9.9923e-01,\n",
       "             9.9895e-01, 9.9864e-01, 9.9850e-01, 9.9834e-01, 9.9827e-01, 9.9820e-01,\n",
       "             9.9816e-01, 9.9747e-01, 9.9689e-01, 9.9674e-01, 9.9659e-01, 9.9652e-01,\n",
       "             9.9489e-01, 9.9417e-01, 9.9394e-01, 9.9147e-01, 9.8986e-01, 9.8984e-01,\n",
       "             9.8921e-01, 9.8026e-01, 9.7823e-01, 9.7207e-01, 9.6284e-01, 9.5060e-01,\n",
       "             9.4275e-01, 9.3372e-01, 9.3115e-01, 9.0355e-01, 9.0129e-01, 8.9088e-01,\n",
       "             8.8861e-01, 8.7861e-01, 8.4611e-01, 8.1317e-01, 8.0788e-01, 7.6028e-01,\n",
       "             7.4414e-01, 7.3981e-01, 7.1177e-01, 6.7983e-01, 6.4332e-01, 5.6686e-01,\n",
       "             5.2748e-01, 5.2077e-01, 4.2976e-01, 4.2691e-01, 4.1544e-01, 4.0746e-01,\n",
       "             3.6096e-01, 3.4445e-01, 3.2238e-01, 2.4552e-01, 2.1151e-01, 1.9415e-01,\n",
       "             1.4016e-01, 1.1343e-01, 8.4925e-02, 6.1973e-02, 6.0276e-02, 5.6347e-02,\n",
       "             5.3604e-02, 4.0857e-02, 3.7127e-02, 3.6289e-02, 3.5563e-02, 3.5493e-02,\n",
       "             2.9515e-02, 2.6721e-02, 2.6610e-02, 2.1457e-02, 2.1073e-02, 2.0511e-02,\n",
       "             1.9421e-02, 1.3207e-02, 1.3133e-02, 1.0398e-02, 6.0361e-03, 5.9832e-03,\n",
       "             4.4668e-03, 3.5514e-03, 3.1322e-03, 2.4984e-03, 2.1506e-03, 1.4297e-03,\n",
       "             1.3307e-03, 1.0156e-03, 9.4453e-04, 7.5755e-04, 6.7953e-04, 6.1205e-04,\n",
       "             5.2175e-04, 4.5752e-04, 4.0111e-04, 1.3920e-04, 1.3006e-04, 1.2096e-04,\n",
       "             1.1430e-04, 1.1211e-04, 9.3480e-05, 9.1914e-05, 9.1150e-05, 7.8020e-05,\n",
       "             7.7055e-05, 7.5508e-05, 7.4092e-05, 4.5456e-05, 3.8370e-05, 3.6621e-05,\n",
       "             3.5887e-05, 3.4630e-05, 3.1330e-05, 2.4830e-05, 1.6187e-05, 1.4843e-05,\n",
       "             1.2899e-05, 1.1239e-05, 1.1096e-05, 9.1164e-06, 7.4277e-06, 7.1645e-06,\n",
       "             6.9969e-06, 6.2391e-06, 5.0205e-06, 4.1712e-06, 3.8449e-06, 3.1822e-06,\n",
       "             1.9569e-06, 1.8042e-06, 1.6187e-06, 1.1986e-06, 1.0809e-06, 1.0655e-06,\n",
       "             1.0059e-06, 9.8727e-07, 6.7897e-07, 6.4817e-07, 4.8999e-07, 4.7061e-07,\n",
       "             4.6884e-07, 4.2542e-07, 4.1116e-07, 3.4793e-07, 2.7347e-07, 2.3554e-07,\n",
       "             1.8988e-07, 1.7530e-07, 1.4282e-07, 1.3699e-07, 1.0809e-07, 1.0511e-07,\n",
       "             1.0388e-07, 9.4249e-08, 8.1110e-08, 7.7922e-08, 5.3562e-08, 5.1319e-08,\n",
       "             5.0232e-08, 4.8396e-08, 3.7407e-08, 3.4793e-08, 2.6312e-08, 1.6956e-08,\n",
       "             1.3090e-08, 9.8565e-09, 9.7055e-09, 8.9802e-09, 8.5319e-09, 7.5162e-09,\n",
       "             7.2331e-09, 6.0528e-09, 5.9505e-09, 4.7186e-09, 4.4765e-09, 3.7791e-09,\n",
       "             3.4924e-09, 2.8208e-09, 2.7065e-09, 2.6235e-09, 2.5295e-09, 2.4915e-09,\n",
       "             1.7329e-09, 1.5877e-09, 1.5503e-09, 1.1955e-09, 9.9062e-10, 9.1316e-10,\n",
       "             7.0347e-10, 6.1767e-10, 4.5955e-10, 4.5214e-10, 1.3543e-10, 1.2749e-10,\n",
       "             1.1837e-10, 8.4878e-11, 7.1897e-11, 6.9795e-11, 5.1699e-11, 4.3964e-11,\n",
       "             4.2394e-11, 3.6696e-11, 2.5772e-11, 2.5189e-11, 2.3024e-11, 2.2720e-11,\n",
       "             1.1148e-11, 8.0878e-12, 4.0528e-12, 2.2524e-12, 1.1423e-12, 3.2898e-13,\n",
       "             1.7344e-13, 8.1380e-14, 2.1365e-14, 1.5519e-14, 7.7312e-15, 5.2853e-15,\n",
       "             4.0320e-15, 1.8459e-15, 9.8677e-16, 8.3382e-16, 6.5143e-16, 6.1229e-16,\n",
       "             6.1066e-16, 2.3135e-16, 1.5034e-16, 1.4943e-16, 8.6705e-17, 5.2950e-21])}}],\n",
       "  [{'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.5166093928980527),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9613e-01, 9.9480e-01,  ..., 1.1939e-09, 6.3452e-10,\n",
       "             4.0871e-10])}},\n",
       "   {'fpr': np.float64(0.006578947368421052),\n",
       "    'tpr': np.float64(0.8075601374570447),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9539e-01, 9.9523e-01,  ..., 9.4897e-06, 5.1119e-06,\n",
       "             2.9281e-06])}},\n",
       "   {'fpr': np.float64(0.03618421052631579),\n",
       "    'tpr': np.float64(0.9415807560137457),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9929e-01, 9.9902e-01,  ..., 3.2747e-05, 2.6414e-05,\n",
       "             2.5067e-05])}},\n",
       "   {'fpr': np.float64(0.039473684210526314),\n",
       "    'tpr': np.float64(0.9599083619702177),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9954e-01, 9.9927e-01,  ..., 1.4751e-05, 1.4493e-05,\n",
       "             8.1029e-06])}},\n",
       "   {'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.865979381443299),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9971e-01, 9.9953e-01,  ..., 6.4662e-08, 5.4990e-08,\n",
       "             3.8530e-08])}},\n",
       "   {'fpr': np.float64(0.04276315789473684),\n",
       "    'tpr': np.float64(0.97709049255441),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9911e-01, 9.9910e-01,  ..., 3.2590e-05, 2.0359e-05,\n",
       "             1.8481e-05])}},\n",
       "   {'fpr': np.float64(0.003289473684210526),\n",
       "    'tpr': np.float64(0.9392898052691867),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.7436e-09, 1.7701e-09,\n",
       "             9.2837e-10])}},\n",
       "   {'fpr': np.float64(0.06578947368421052),\n",
       "    'tpr': np.float64(0.9873997709049256),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9997e-01, 9.9996e-01,  ..., 7.5083e-07, 6.3422e-07,\n",
       "             4.0186e-07])}},\n",
       "   {'fpr': np.float64(0.12171052631578948),\n",
       "    'tpr': np.float64(0.9965635738831615),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.1348e-06, 8.1142e-07,\n",
       "             2.3587e-07])}},\n",
       "   {'fpr': np.float64(0.006578947368421052),\n",
       "    'tpr': np.float64(0.9656357388316151),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 9.9998e-01, 9.9998e-01,  ..., 4.4753e-08, 9.5980e-09,\n",
       "             2.2391e-09])}},\n",
       "   {'fpr': np.float64(0.046052631578947366),\n",
       "    'tpr': np.float64(0.97709049255441),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000,  ..., 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0011, 0.0023,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.3811e-10, 5.3819e-11,\n",
       "             2.1742e-11])}},\n",
       "   {'fpr': np.float64(0.06907894736842106),\n",
       "    'tpr': np.float64(0.9885452462772051),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099,\n",
       "             0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0197, 0.0230, 0.0263, 0.0263, 0.0263, 0.0263, 0.0296,\n",
       "             0.0329, 0.0362, 0.0395, 0.0428, 0.0428, 0.0428, 0.0461, 0.0461, 0.0461,\n",
       "             0.0493, 0.0493, 0.0526, 0.0559, 0.0592, 0.0625, 0.0658, 0.0658, 0.0658,\n",
       "             0.0691, 0.0691, 0.0724, 0.0724, 0.0724, 0.0757, 0.0789, 0.0789, 0.0789,\n",
       "             0.0822, 0.0855, 0.0888, 0.0921, 0.0954, 0.0987, 0.1020, 0.1053, 0.1086,\n",
       "             0.1086, 0.1086, 0.1118, 0.1151, 0.1184, 0.1217, 0.1250, 0.1283, 0.1316,\n",
       "             0.1316, 0.1349, 0.1382, 0.1414, 0.1447, 0.1480, 0.1513, 0.1546, 0.1546,\n",
       "             0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1776, 0.1809, 0.1842,\n",
       "             0.1875, 0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138,\n",
       "             0.2138, 0.2171, 0.2204, 0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401,\n",
       "             0.2434, 0.2467, 0.2500, 0.2533, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697,\n",
       "             0.2730, 0.2763, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993,\n",
       "             0.3026, 0.3059, 0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289,\n",
       "             0.3322, 0.3355, 0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586,\n",
       "             0.3618, 0.3651, 0.3684, 0.3717, 0.3750, 0.3783, 0.3783, 0.3816, 0.3849,\n",
       "             0.3882, 0.3914, 0.3947, 0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145,\n",
       "             0.4178, 0.4211, 0.4243, 0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441,\n",
       "             0.4474, 0.4507, 0.4539, 0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737,\n",
       "             0.4770, 0.4803, 0.4836, 0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033,\n",
       "             0.5066, 0.5099, 0.5132, 0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329,\n",
       "             0.5362, 0.5395, 0.5428, 0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625,\n",
       "             0.5658, 0.5691, 0.5724, 0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921,\n",
       "             0.5954, 0.5987, 0.6020, 0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217,\n",
       "             0.6250, 0.6283, 0.6316, 0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513,\n",
       "             0.6546, 0.6579, 0.6612, 0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809,\n",
       "             0.6842, 0.6875, 0.6908, 0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105,\n",
       "             0.7138, 0.7171, 0.7204, 0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401,\n",
       "             0.7434, 0.7467, 0.7500, 0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697,\n",
       "             0.7730, 0.7763, 0.7796, 0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993,\n",
       "             0.8026, 0.8059, 0.8092, 0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289,\n",
       "             0.8322, 0.8355, 0.8388, 0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586,\n",
       "             0.8618, 0.8651, 0.8684, 0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882,\n",
       "             0.8914, 0.8947, 0.8980, 0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178,\n",
       "             0.9211, 0.9243, 0.9276, 0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474,\n",
       "             0.9507, 0.9539, 0.9572, 0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770,\n",
       "             0.9803, 0.9836, 0.9868, 0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0069, 0.0229, 0.0367, 0.0504, 0.0641, 0.0733, 0.0848, 0.0905,\n",
       "             0.0951, 0.0974, 0.1100, 0.1134, 0.1191, 0.1271, 0.1363, 0.1409, 0.1455,\n",
       "             0.1512, 0.1569, 0.1592, 0.1638, 0.1661, 0.1707, 0.1730, 0.1821, 0.1856,\n",
       "             0.1936, 0.1982, 0.2027, 0.2050, 0.2096, 0.2131, 0.2188, 0.2199, 0.2245,\n",
       "             0.2257, 0.2279, 0.2314, 0.2325, 0.2348, 0.2383, 0.2417, 0.2440, 0.2463,\n",
       "             0.2509, 0.2532, 0.2554, 0.2612, 0.2646, 0.2658, 0.2680, 0.2692, 0.2703,\n",
       "             0.2726, 0.2761, 0.2795, 0.2818, 0.2841, 0.2864, 0.2875, 0.2887, 0.2910,\n",
       "             0.2932, 0.2944, 0.2967, 0.2978, 0.3013, 0.3024, 0.3036, 0.3058, 0.3081,\n",
       "             0.3093, 0.3104, 0.3127, 0.3139, 0.3150, 0.3173, 0.3196, 0.3219, 0.3230,\n",
       "             0.3242, 0.3253, 0.3288, 0.3310, 0.3345, 0.3368, 0.3379, 0.3402, 0.3414,\n",
       "             0.3448, 0.3459, 0.3471, 0.3482, 0.3494, 0.3517, 0.3528, 0.3551, 0.3574,\n",
       "             0.3608, 0.3620, 0.3631, 0.3654, 0.3677, 0.3688, 0.3711, 0.3723, 0.3734,\n",
       "             0.3757, 0.3780, 0.3792, 0.3803, 0.3837, 0.3860, 0.3872, 0.3883, 0.3895,\n",
       "             0.3906, 0.3918, 0.3952, 0.3963, 0.3986, 0.3998, 0.4009, 0.4021, 0.4032,\n",
       "             0.4055, 0.4066, 0.4078, 0.4112, 0.4124, 0.4135, 0.4147, 0.4158, 0.4181,\n",
       "             0.4204, 0.4227, 0.4238, 0.4261, 0.4284, 0.4296, 0.4307, 0.4318, 0.4330,\n",
       "             0.4341, 0.4353, 0.4364, 0.4387, 0.4399, 0.4410, 0.4433, 0.4456, 0.4467,\n",
       "             0.4479, 0.4490, 0.4502, 0.4513, 0.4525, 0.4536, 0.4559, 0.4570, 0.4605,\n",
       "             0.4616, 0.4639, 0.4651, 0.4662, 0.4685, 0.4696, 0.4719, 0.4731, 0.4742,\n",
       "             0.4777, 0.4788, 0.4800, 0.4811, 0.4822, 0.4834, 0.4845, 0.4857, 0.4868,\n",
       "             0.4880, 0.4891, 0.4903, 0.4914, 0.4926, 0.4937, 0.4960, 0.4971, 0.4994,\n",
       "             0.5006, 0.5017, 0.5040, 0.5052, 0.5074, 0.5097, 0.5109, 0.5120, 0.5132,\n",
       "             0.5143, 0.5166, 0.5189, 0.5200, 0.5212, 0.5223, 0.5235, 0.5246, 0.5258,\n",
       "             0.5269, 0.5281, 0.5292, 0.5304, 0.5315, 0.5326, 0.5338, 0.5349, 0.5361,\n",
       "             0.5384, 0.5395, 0.5407, 0.5418, 0.5430, 0.5441, 0.5452, 0.5464, 0.5475,\n",
       "             0.5487, 0.5498, 0.5510, 0.5521, 0.5533, 0.5544, 0.5567, 0.5578, 0.5590,\n",
       "             0.5601, 0.5613, 0.5624, 0.5636, 0.5647, 0.5659, 0.5670, 0.5682, 0.5693,\n",
       "             0.5704, 0.5716, 0.5727, 0.5739, 0.5750, 0.5762, 0.5773, 0.5785, 0.5796,\n",
       "             0.5808, 0.5819, 0.5830, 0.5842, 0.5853, 0.5865, 0.5876, 0.5888, 0.5899,\n",
       "             0.5911, 0.5922, 0.5934, 0.5945, 0.5956, 0.5968, 0.5979, 0.5991, 0.6002,\n",
       "             0.6014, 0.6025, 0.6037, 0.6048, 0.6060, 0.6071, 0.6082, 0.6094, 0.6105,\n",
       "             0.6117, 0.6128, 0.6140, 0.6151, 0.6163, 0.6174, 0.6186, 0.6197, 0.6208,\n",
       "             0.6220, 0.6243, 0.6254, 0.6266, 0.6277, 0.6289, 0.6300, 0.6312, 0.6323,\n",
       "             0.6334, 0.6346, 0.6357, 0.6369, 0.6380, 0.6392, 0.6403, 0.6415, 0.6438,\n",
       "             0.6449, 0.6460, 0.6472, 0.6483, 0.6495, 0.6506, 0.6529, 0.6541, 0.6552,\n",
       "             0.6575, 0.6586, 0.6598, 0.6609, 0.6621, 0.6632, 0.6644, 0.6667, 0.6678,\n",
       "             0.6690, 0.6701, 0.6712, 0.6724, 0.6735, 0.6747, 0.6758, 0.6770, 0.6781,\n",
       "             0.6793, 0.6804, 0.6816, 0.6827, 0.6838, 0.6850, 0.6861, 0.6873, 0.6884,\n",
       "             0.6896, 0.6907, 0.6919, 0.6930, 0.6942, 0.6953, 0.6964, 0.6976, 0.6987,\n",
       "             0.6999, 0.7010, 0.7022, 0.7033, 0.7045, 0.7056, 0.7068, 0.7079, 0.7090,\n",
       "             0.7102, 0.7113, 0.7125, 0.7136, 0.7148, 0.7159, 0.7171, 0.7182, 0.7194,\n",
       "             0.7205, 0.7216, 0.7228, 0.7239, 0.7251, 0.7262, 0.7274, 0.7285, 0.7297,\n",
       "             0.7308, 0.7320, 0.7331, 0.7342, 0.7354, 0.7365, 0.7377, 0.7388, 0.7400,\n",
       "             0.7411, 0.7423, 0.7434, 0.7446, 0.7457, 0.7468, 0.7480, 0.7491, 0.7503,\n",
       "             0.7514, 0.7526, 0.7537, 0.7549, 0.7560, 0.7572, 0.7583, 0.7595, 0.7606,\n",
       "             0.7617, 0.7629, 0.7640, 0.7652, 0.7663, 0.7675, 0.7686, 0.7698, 0.7709,\n",
       "             0.7721, 0.7732, 0.7743, 0.7755, 0.7766, 0.7778, 0.7789, 0.7801, 0.7812,\n",
       "             0.7824, 0.7835, 0.7847, 0.7858, 0.7869, 0.7881, 0.7892, 0.7904, 0.7915,\n",
       "             0.7927, 0.7938, 0.7950, 0.7961, 0.7973, 0.7984, 0.7995, 0.8007, 0.8018,\n",
       "             0.8030, 0.8041, 0.8053, 0.8076, 0.8087, 0.8099, 0.8110, 0.8121, 0.8133,\n",
       "             0.8144, 0.8156, 0.8167, 0.8179, 0.8190, 0.8202, 0.8213, 0.8225, 0.8236,\n",
       "             0.8247, 0.8259, 0.8270, 0.8282, 0.8293, 0.8305, 0.8316, 0.8328, 0.8339,\n",
       "             0.8351, 0.8362, 0.8373, 0.8385, 0.8396, 0.8408, 0.8419, 0.8431, 0.8442,\n",
       "             0.8454, 0.8465, 0.8477, 0.8488, 0.8499, 0.8511, 0.8522, 0.8534, 0.8545,\n",
       "             0.8557, 0.8568, 0.8580, 0.8591, 0.8603, 0.8614, 0.8625, 0.8637, 0.8648,\n",
       "             0.8660, 0.8671, 0.8683, 0.8694, 0.8706, 0.8717, 0.8729, 0.8740, 0.8751,\n",
       "             0.8763, 0.8774, 0.8786, 0.8797, 0.8809, 0.8820, 0.8832, 0.8843, 0.8855,\n",
       "             0.8866, 0.8877, 0.8889, 0.8900, 0.8912, 0.8923, 0.8935, 0.8946, 0.8946,\n",
       "             0.8958, 0.8969, 0.8992, 0.9003, 0.9015, 0.9026, 0.9038, 0.9049, 0.9061,\n",
       "             0.9072, 0.9084, 0.9095, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9164,\n",
       "             0.9175, 0.9187, 0.9198, 0.9210, 0.9221, 0.9233, 0.9244, 0.9255, 0.9267,\n",
       "             0.9278, 0.9290, 0.9301, 0.9313, 0.9324, 0.9336, 0.9347, 0.9359, 0.9370,\n",
       "             0.9381, 0.9393, 0.9404, 0.9416, 0.9416, 0.9427, 0.9439, 0.9450, 0.9450,\n",
       "             0.9462, 0.9473, 0.9473, 0.9485, 0.9496, 0.9507, 0.9519, 0.9530, 0.9542,\n",
       "             0.9553, 0.9565, 0.9576, 0.9588, 0.9599, 0.9611, 0.9611, 0.9622, 0.9633,\n",
       "             0.9645, 0.9656, 0.9668, 0.9679, 0.9691, 0.9702, 0.9714, 0.9725, 0.9737,\n",
       "             0.9748, 0.9759, 0.9759, 0.9759, 0.9759, 0.9771, 0.9782, 0.9794, 0.9794,\n",
       "             0.9794, 0.9794, 0.9794, 0.9794, 0.9805, 0.9817, 0.9817, 0.9828, 0.9840,\n",
       "             0.9840, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9863, 0.9874,\n",
       "             0.9874, 0.9885, 0.9885, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01, 9.9974e-01,\n",
       "             9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9970e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01,\n",
       "             9.9966e-01, 9.9966e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9963e-01,\n",
       "             9.9963e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01, 9.9962e-01, 9.9961e-01,\n",
       "             9.9961e-01, 9.9961e-01, 9.9961e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9959e-01, 9.9959e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9955e-01, 9.9954e-01, 9.9954e-01, 9.9954e-01, 9.9954e-01, 9.9953e-01,\n",
       "             9.9953e-01, 9.9952e-01, 9.9951e-01, 9.9951e-01, 9.9948e-01, 9.9948e-01,\n",
       "             9.9946e-01, 9.9946e-01, 9.9943e-01, 9.9939e-01, 9.9939e-01, 9.9938e-01,\n",
       "             9.9938e-01, 9.9937e-01, 9.9936e-01, 9.9934e-01, 9.9933e-01, 9.9933e-01,\n",
       "             9.9926e-01, 9.9926e-01, 9.9926e-01, 9.9924e-01, 9.9921e-01, 9.9920e-01,\n",
       "             9.9920e-01, 9.9919e-01, 9.9918e-01, 9.9913e-01, 9.9911e-01, 9.9910e-01,\n",
       "             9.9909e-01, 9.9908e-01, 9.9908e-01, 9.9907e-01, 9.9905e-01, 9.9903e-01,\n",
       "             9.9896e-01, 9.9894e-01, 9.9892e-01, 9.9890e-01, 9.9889e-01, 9.9885e-01,\n",
       "             9.9884e-01, 9.9883e-01, 9.9882e-01, 9.9877e-01, 9.9875e-01, 9.9873e-01,\n",
       "             9.9870e-01, 9.9862e-01, 9.9862e-01, 9.9860e-01, 9.9858e-01, 9.9856e-01,\n",
       "             9.9856e-01, 9.9849e-01, 9.9849e-01, 9.9848e-01, 9.9845e-01, 9.9842e-01,\n",
       "             9.9840e-01, 9.9835e-01, 9.9834e-01, 9.9834e-01, 9.9828e-01, 9.9827e-01,\n",
       "             9.9827e-01, 9.9825e-01, 9.9815e-01, 9.9807e-01, 9.9797e-01, 9.9789e-01,\n",
       "             9.9787e-01, 9.9785e-01, 9.9783e-01, 9.9778e-01, 9.9774e-01, 9.9749e-01,\n",
       "             9.9747e-01, 9.9745e-01, 9.9742e-01, 9.9738e-01, 9.9729e-01, 9.9725e-01,\n",
       "             9.9721e-01, 9.9719e-01, 9.9712e-01, 9.9709e-01, 9.9689e-01, 9.9683e-01,\n",
       "             9.9665e-01, 9.9651e-01, 9.9647e-01, 9.9647e-01, 9.9645e-01, 9.9634e-01,\n",
       "             9.9630e-01, 9.9622e-01, 9.9614e-01, 9.9564e-01, 9.9552e-01, 9.9530e-01,\n",
       "             9.9524e-01, 9.9521e-01, 9.9520e-01, 9.9495e-01, 9.9491e-01, 9.9487e-01,\n",
       "             9.9484e-01, 9.9396e-01, 9.9361e-01, 9.9359e-01, 9.9359e-01, 9.9333e-01,\n",
       "             9.9287e-01, 9.9232e-01, 9.9223e-01, 9.9204e-01, 9.9201e-01, 9.9147e-01,\n",
       "             9.9088e-01, 9.9073e-01, 9.9004e-01, 9.8988e-01, 9.8964e-01, 9.8938e-01,\n",
       "             9.8897e-01, 9.8877e-01, 9.8820e-01, 9.8689e-01, 9.8663e-01, 9.8657e-01,\n",
       "             9.8547e-01, 9.8507e-01, 9.8319e-01, 9.8318e-01, 9.8317e-01, 9.8243e-01,\n",
       "             9.8094e-01, 9.7775e-01, 9.7642e-01, 9.7535e-01, 9.7521e-01, 9.7327e-01,\n",
       "             9.7178e-01, 9.6797e-01, 9.6648e-01, 9.6486e-01, 9.6328e-01, 9.6283e-01,\n",
       "             9.6026e-01, 9.5978e-01, 9.5957e-01, 9.5807e-01, 9.4911e-01, 9.4264e-01,\n",
       "             9.4168e-01, 9.3170e-01, 9.2405e-01, 8.9711e-01, 8.9490e-01, 8.7687e-01,\n",
       "             8.7508e-01, 8.3980e-01, 8.2747e-01, 7.8543e-01, 7.8521e-01, 7.8188e-01,\n",
       "             7.8133e-01, 7.7481e-01, 7.7140e-01, 7.4328e-01, 7.3345e-01, 7.2804e-01,\n",
       "             7.2746e-01, 7.2559e-01, 6.9863e-01, 6.8947e-01, 6.6287e-01, 6.4442e-01,\n",
       "             6.3965e-01, 6.1435e-01, 5.7295e-01, 5.4386e-01, 5.2212e-01, 4.9323e-01,\n",
       "             4.5311e-01, 4.4312e-01, 3.8432e-01, 2.9583e-01, 2.9255e-01, 2.7722e-01,\n",
       "             2.6071e-01, 2.4245e-01, 1.9914e-01, 1.9064e-01, 1.4612e-01, 1.4573e-01,\n",
       "             1.3007e-01, 1.2331e-01, 1.1158e-01, 1.1068e-01, 7.9803e-02, 7.8859e-02,\n",
       "             7.8376e-02, 7.4642e-02, 6.8343e-02, 6.5441e-02, 6.4254e-02, 5.4179e-02,\n",
       "             5.1158e-02, 5.0219e-02, 4.5704e-02, 4.4529e-02, 4.1932e-02, 4.1320e-02,\n",
       "             3.9651e-02, 3.6182e-02, 3.2276e-02, 3.2165e-02, 3.1022e-02, 2.6524e-02,\n",
       "             2.5364e-02, 2.4172e-02, 1.9421e-02, 1.9400e-02, 1.9149e-02, 1.8994e-02,\n",
       "             1.8690e-02, 1.7845e-02, 1.6922e-02, 1.6316e-02, 1.5419e-02, 1.4806e-02,\n",
       "             1.3832e-02, 1.2909e-02, 1.2828e-02, 1.2773e-02, 1.2313e-02, 1.2241e-02,\n",
       "             9.7571e-03, 7.9543e-03, 7.8635e-03, 7.1079e-03, 6.7822e-03, 6.6166e-03,\n",
       "             6.3433e-03, 6.1058e-03, 5.7953e-03, 5.7443e-03, 5.7155e-03, 5.5428e-03,\n",
       "             5.5338e-03, 5.4749e-03, 5.3267e-03, 5.1172e-03, 4.7432e-03, 4.4948e-03,\n",
       "             4.4403e-03, 4.0044e-03, 3.8266e-03, 3.7742e-03, 3.7351e-03, 3.5498e-03,\n",
       "             3.3085e-03, 3.2899e-03, 3.2213e-03, 3.2151e-03, 3.1355e-03, 2.7991e-03,\n",
       "             2.7095e-03, 2.5321e-03, 2.4869e-03, 2.4611e-03, 2.2439e-03, 2.2157e-03,\n",
       "             2.1640e-03, 2.0976e-03, 2.0791e-03, 2.0515e-03, 2.0475e-03, 1.8702e-03,\n",
       "             1.8008e-03, 1.5186e-03, 1.5114e-03, 1.4978e-03, 1.4525e-03, 1.2441e-03,\n",
       "             1.2439e-03, 1.1461e-03, 1.1278e-03, 1.1248e-03, 1.0957e-03, 1.0442e-03,\n",
       "             1.0265e-03, 9.9555e-04, 8.3854e-04, 7.9560e-04, 6.2250e-04, 6.2226e-04,\n",
       "             5.9866e-04, 5.8874e-04, 5.7844e-04, 5.6126e-04, 5.3311e-04, 5.1989e-04,\n",
       "             4.6351e-04, 4.3084e-04, 4.2004e-04, 3.8069e-04, 3.8056e-04, 3.7269e-04,\n",
       "             3.6283e-04, 3.4353e-04, 2.8376e-04, 2.5799e-04, 2.2871e-04, 2.2786e-04,\n",
       "             2.2334e-04, 2.2260e-04, 2.1962e-04, 2.1782e-04, 2.0829e-04, 2.0642e-04,\n",
       "             2.0078e-04, 1.9875e-04, 1.9137e-04, 1.8321e-04, 1.7770e-04, 1.6899e-04,\n",
       "             1.6619e-04, 1.5862e-04, 1.4721e-04, 1.4327e-04, 1.3405e-04, 1.0692e-04,\n",
       "             1.0383e-04, 9.1328e-05, 8.6115e-05, 7.9680e-05, 7.6850e-05, 7.4613e-05,\n",
       "             7.1281e-05, 6.8412e-05, 6.6844e-05, 6.5533e-05, 6.4643e-05, 6.3582e-05,\n",
       "             6.3295e-05, 6.2300e-05, 6.2229e-05, 6.0678e-05, 5.8061e-05, 5.6323e-05,\n",
       "             5.5378e-05, 5.3648e-05, 5.3376e-05, 5.2235e-05, 5.1645e-05, 4.3321e-05,\n",
       "             4.2587e-05, 3.9726e-05, 3.8821e-05, 3.8756e-05, 3.8258e-05, 3.7332e-05,\n",
       "             3.6614e-05, 3.5466e-05, 2.9816e-05, 2.9723e-05, 2.9259e-05, 2.9114e-05,\n",
       "             2.7997e-05, 2.7399e-05, 2.6628e-05, 2.6261e-05, 2.6253e-05, 2.4901e-05,\n",
       "             2.3749e-05, 2.3020e-05, 2.0848e-05, 2.0476e-05, 2.0334e-05, 1.9316e-05,\n",
       "             1.8216e-05, 1.5373e-05, 1.5051e-05, 1.4994e-05, 1.3936e-05, 1.3032e-05,\n",
       "             1.2935e-05, 1.2639e-05, 1.1402e-05, 1.1287e-05, 1.0302e-05, 1.0218e-05,\n",
       "             8.8336e-06, 8.7398e-06, 8.7053e-06, 8.2012e-06, 8.1368e-06, 7.7428e-06,\n",
       "             7.4454e-06, 6.7392e-06, 6.4426e-06, 6.2439e-06, 5.6845e-06, 5.0643e-06,\n",
       "             4.8473e-06, 4.2525e-06, 4.2139e-06, 4.2073e-06, 3.9802e-06, 3.7324e-06,\n",
       "             3.4514e-06, 3.2606e-06, 3.1674e-06, 3.1206e-06, 3.0645e-06, 2.9240e-06,\n",
       "             2.6053e-06, 2.5455e-06, 2.4104e-06, 2.2027e-06, 2.1172e-06, 2.0984e-06,\n",
       "             1.9502e-06, 1.7999e-06, 1.5928e-06, 1.2226e-06, 1.1045e-06, 1.0522e-06,\n",
       "             1.0288e-06, 9.8380e-07, 8.4461e-07, 7.7413e-07, 7.7195e-07, 6.5044e-07,\n",
       "             6.4220e-07, 6.1785e-07, 6.1424e-07, 5.7765e-07, 5.7072e-07, 5.2319e-07,\n",
       "             4.6906e-07, 4.4940e-07, 4.4753e-07, 4.2360e-07, 3.9055e-07, 3.7973e-07,\n",
       "             3.6209e-07, 3.5923e-07, 2.1635e-07, 1.8441e-07, 1.7259e-07, 1.3369e-07,\n",
       "             1.0980e-07, 5.7502e-08, 4.5071e-08, 4.1612e-08, 4.0214e-08, 3.3063e-08,\n",
       "             3.0197e-08, 2.9940e-08, 2.1662e-08, 1.8913e-08, 1.7388e-08, 1.5393e-08,\n",
       "             1.3987e-08, 1.1156e-08, 2.9825e-09, 2.8368e-09, 2.1659e-09, 1.1309e-09,\n",
       "             4.9948e-10, 4.2765e-10, 3.4285e-10, 3.5979e-11])}},\n",
       "   {'fpr': np.float64(0.0756578947368421),\n",
       "    'tpr': np.float64(0.9885452462772051),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0230, 0.0263, 0.0296, 0.0329, 0.0329,\n",
       "             0.0362, 0.0362, 0.0395, 0.0428, 0.0428, 0.0461, 0.0493, 0.0526, 0.0559,\n",
       "             0.0559, 0.0559, 0.0559, 0.0592, 0.0625, 0.0625, 0.0625, 0.0658, 0.0691,\n",
       "             0.0724, 0.0757, 0.0789, 0.0822, 0.0855, 0.0888, 0.0888, 0.0921, 0.0954,\n",
       "             0.0987, 0.1020, 0.1053, 0.1086, 0.1086, 0.1118, 0.1151, 0.1184, 0.1184,\n",
       "             0.1217, 0.1250, 0.1250, 0.1283, 0.1316, 0.1349, 0.1382, 0.1414, 0.1414,\n",
       "             0.1414, 0.1447, 0.1480, 0.1513, 0.1546, 0.1579, 0.1612, 0.1645, 0.1678,\n",
       "             0.1678, 0.1711, 0.1711, 0.1743, 0.1776, 0.1809, 0.1842, 0.1875, 0.1908,\n",
       "             0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138, 0.2171, 0.2204,\n",
       "             0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2500,\n",
       "             0.2500, 0.2533, 0.2566, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697, 0.2730,\n",
       "             0.2763, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026,\n",
       "             0.3059, 0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322,\n",
       "             0.3355, 0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618,\n",
       "             0.3651, 0.3684, 0.3717, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914,\n",
       "             0.3947, 0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211,\n",
       "             0.4243, 0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507,\n",
       "             0.4539, 0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803,\n",
       "             0.4836, 0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099,\n",
       "             0.5132, 0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395,\n",
       "             0.5428, 0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691,\n",
       "             0.5724, 0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987,\n",
       "             0.6020, 0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283,\n",
       "             0.6316, 0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579,\n",
       "             0.6612, 0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875,\n",
       "             0.6908, 0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171,\n",
       "             0.7204, 0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467,\n",
       "             0.7500, 0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763,\n",
       "             0.7796, 0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059,\n",
       "             0.8092, 0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355,\n",
       "             0.8388, 0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651,\n",
       "             0.8684, 0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947,\n",
       "             0.8980, 0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243,\n",
       "             0.9276, 0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539,\n",
       "             0.9572, 0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836,\n",
       "             0.9868, 0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0034, 0.0092, 0.0160, 0.0206, 0.0263, 0.0298, 0.0355, 0.0435,\n",
       "             0.0481, 0.0550, 0.0584, 0.0619, 0.0722, 0.0779, 0.0825, 0.0859, 0.0905,\n",
       "             0.0916, 0.0928, 0.0997, 0.1054, 0.1111, 0.1157, 0.1168, 0.1191, 0.1203,\n",
       "             0.1226, 0.1260, 0.1271, 0.1283, 0.1294, 0.1317, 0.1352, 0.1397, 0.1432,\n",
       "             0.1501, 0.1558, 0.1569, 0.1604, 0.1615, 0.1627, 0.1638, 0.1672, 0.1695,\n",
       "             0.1707, 0.1753, 0.1775, 0.1798, 0.1821, 0.1833, 0.1856, 0.1879, 0.1901,\n",
       "             0.1936, 0.1970, 0.1993, 0.2005, 0.2016, 0.2027, 0.2062, 0.2073, 0.2085,\n",
       "             0.2108, 0.2131, 0.2142, 0.2153, 0.2176, 0.2199, 0.2222, 0.2257, 0.2279,\n",
       "             0.2302, 0.2314, 0.2325, 0.2348, 0.2360, 0.2383, 0.2394, 0.2405, 0.2417,\n",
       "             0.2428, 0.2440, 0.2451, 0.2474, 0.2486, 0.2509, 0.2532, 0.2543, 0.2554,\n",
       "             0.2566, 0.2577, 0.2612, 0.2635, 0.2658, 0.2669, 0.2680, 0.2692, 0.2703,\n",
       "             0.2715, 0.2738, 0.2772, 0.2795, 0.2806, 0.2818, 0.2829, 0.2841, 0.2864,\n",
       "             0.2887, 0.2898, 0.2910, 0.2921, 0.2932, 0.2944, 0.2967, 0.2990, 0.3001,\n",
       "             0.3013, 0.3024, 0.3058, 0.3081, 0.3093, 0.3116, 0.3127, 0.3162, 0.3173,\n",
       "             0.3184, 0.3196, 0.3207, 0.3230, 0.3242, 0.3253, 0.3265, 0.3288, 0.3310,\n",
       "             0.3322, 0.3333, 0.3345, 0.3356, 0.3368, 0.3379, 0.3391, 0.3402, 0.3414,\n",
       "             0.3425, 0.3436, 0.3471, 0.3494, 0.3517, 0.3528, 0.3562, 0.3574, 0.3585,\n",
       "             0.3608, 0.3631, 0.3643, 0.3654, 0.3666, 0.3688, 0.3700, 0.3734, 0.3757,\n",
       "             0.3769, 0.3780, 0.3792, 0.3803, 0.3814, 0.3826, 0.3837, 0.3849, 0.3860,\n",
       "             0.3872, 0.3883, 0.3906, 0.3929, 0.3952, 0.3963, 0.3975, 0.4009, 0.4021,\n",
       "             0.4032, 0.4044, 0.4055, 0.4066, 0.4078, 0.4089, 0.4101, 0.4112, 0.4124,\n",
       "             0.4147, 0.4158, 0.4181, 0.4192, 0.4215, 0.4227, 0.4238, 0.4250, 0.4261,\n",
       "             0.4273, 0.4284, 0.4296, 0.4318, 0.4330, 0.4341, 0.4353, 0.4364, 0.4376,\n",
       "             0.4387, 0.4399, 0.4410, 0.4422, 0.4433, 0.4444, 0.4456, 0.4467, 0.4490,\n",
       "             0.4502, 0.4525, 0.4536, 0.4548, 0.4559, 0.4570, 0.4582, 0.4593, 0.4605,\n",
       "             0.4616, 0.4628, 0.4639, 0.4651, 0.4662, 0.4674, 0.4696, 0.4708, 0.4719,\n",
       "             0.4742, 0.4754, 0.4777, 0.4788, 0.4800, 0.4811, 0.4822, 0.4834, 0.4845,\n",
       "             0.4857, 0.4868, 0.4880, 0.4891, 0.4903, 0.4914, 0.4926, 0.4937, 0.4948,\n",
       "             0.4960, 0.4971, 0.4983, 0.4994, 0.5006, 0.5029, 0.5040, 0.5052, 0.5063,\n",
       "             0.5074, 0.5086, 0.5097, 0.5109, 0.5120, 0.5132, 0.5143, 0.5155, 0.5166,\n",
       "             0.5178, 0.5189, 0.5200, 0.5212, 0.5223, 0.5258, 0.5269, 0.5281, 0.5292,\n",
       "             0.5304, 0.5326, 0.5338, 0.5349, 0.5361, 0.5372, 0.5395, 0.5407, 0.5418,\n",
       "             0.5430, 0.5441, 0.5464, 0.5475, 0.5487, 0.5498, 0.5510, 0.5521, 0.5544,\n",
       "             0.5556, 0.5567, 0.5578, 0.5590, 0.5601, 0.5613, 0.5624, 0.5636, 0.5647,\n",
       "             0.5659, 0.5670, 0.5682, 0.5693, 0.5704, 0.5716, 0.5727, 0.5739, 0.5750,\n",
       "             0.5762, 0.5773, 0.5785, 0.5796, 0.5808, 0.5819, 0.5830, 0.5842, 0.5853,\n",
       "             0.5865, 0.5876, 0.5888, 0.5899, 0.5911, 0.5922, 0.5934, 0.5945, 0.5956,\n",
       "             0.5968, 0.5979, 0.5991, 0.6002, 0.6014, 0.6025, 0.6037, 0.6048, 0.6060,\n",
       "             0.6071, 0.6082, 0.6094, 0.6105, 0.6117, 0.6128, 0.6140, 0.6151, 0.6163,\n",
       "             0.6174, 0.6186, 0.6197, 0.6208, 0.6220, 0.6231, 0.6243, 0.6254, 0.6266,\n",
       "             0.6277, 0.6289, 0.6300, 0.6312, 0.6323, 0.6334, 0.6346, 0.6357, 0.6369,\n",
       "             0.6380, 0.6392, 0.6403, 0.6415, 0.6426, 0.6438, 0.6449, 0.6460, 0.6483,\n",
       "             0.6495, 0.6506, 0.6518, 0.6529, 0.6541, 0.6552, 0.6564, 0.6575, 0.6586,\n",
       "             0.6598, 0.6609, 0.6621, 0.6632, 0.6644, 0.6655, 0.6667, 0.6678, 0.6690,\n",
       "             0.6701, 0.6712, 0.6724, 0.6735, 0.6747, 0.6758, 0.6770, 0.6781, 0.6793,\n",
       "             0.6804, 0.6816, 0.6827, 0.6838, 0.6850, 0.6861, 0.6873, 0.6884, 0.6896,\n",
       "             0.6907, 0.6919, 0.6930, 0.6942, 0.6953, 0.6964, 0.6976, 0.6987, 0.6999,\n",
       "             0.7022, 0.7045, 0.7056, 0.7068, 0.7079, 0.7090, 0.7102, 0.7113, 0.7125,\n",
       "             0.7136, 0.7148, 0.7159, 0.7171, 0.7182, 0.7194, 0.7205, 0.7216, 0.7228,\n",
       "             0.7239, 0.7251, 0.7262, 0.7274, 0.7285, 0.7297, 0.7308, 0.7320, 0.7331,\n",
       "             0.7342, 0.7365, 0.7377, 0.7388, 0.7400, 0.7411, 0.7411, 0.7423, 0.7434,\n",
       "             0.7446, 0.7457, 0.7468, 0.7480, 0.7491, 0.7503, 0.7514, 0.7526, 0.7537,\n",
       "             0.7549, 0.7560, 0.7572, 0.7583, 0.7595, 0.7606, 0.7617, 0.7629, 0.7640,\n",
       "             0.7652, 0.7663, 0.7675, 0.7686, 0.7698, 0.7709, 0.7721, 0.7732, 0.7743,\n",
       "             0.7755, 0.7766, 0.7778, 0.7789, 0.7801, 0.7812, 0.7824, 0.7835, 0.7847,\n",
       "             0.7858, 0.7869, 0.7881, 0.7892, 0.7904, 0.7915, 0.7927, 0.7938, 0.7950,\n",
       "             0.7961, 0.7973, 0.7984, 0.7995, 0.8007, 0.8018, 0.8030, 0.8041, 0.8053,\n",
       "             0.8064, 0.8076, 0.8087, 0.8099, 0.8110, 0.8121, 0.8133, 0.8144, 0.8156,\n",
       "             0.8167, 0.8179, 0.8190, 0.8202, 0.8213, 0.8225, 0.8236, 0.8247, 0.8259,\n",
       "             0.8270, 0.8282, 0.8293, 0.8305, 0.8316, 0.8328, 0.8339, 0.8351, 0.8362,\n",
       "             0.8373, 0.8385, 0.8396, 0.8408, 0.8419, 0.8431, 0.8442, 0.8454, 0.8465,\n",
       "             0.8477, 0.8488, 0.8499, 0.8511, 0.8522, 0.8534, 0.8545, 0.8557, 0.8568,\n",
       "             0.8580, 0.8591, 0.8603, 0.8614, 0.8625, 0.8637, 0.8648, 0.8660, 0.8671,\n",
       "             0.8683, 0.8694, 0.8706, 0.8717, 0.8729, 0.8740, 0.8751, 0.8763, 0.8774,\n",
       "             0.8786, 0.8797, 0.8809, 0.8820, 0.8832, 0.8843, 0.8855, 0.8866, 0.8877,\n",
       "             0.8889, 0.8900, 0.8912, 0.8923, 0.8935, 0.8946, 0.8958, 0.8969, 0.8981,\n",
       "             0.8992, 0.9003, 0.9015, 0.9026, 0.9038, 0.9049, 0.9061, 0.9072, 0.9084,\n",
       "             0.9095, 0.9107, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9164, 0.9175,\n",
       "             0.9187, 0.9198, 0.9210, 0.9221, 0.9233, 0.9244, 0.9255, 0.9267, 0.9278,\n",
       "             0.9290, 0.9301, 0.9313, 0.9324, 0.9336, 0.9347, 0.9359, 0.9370, 0.9381,\n",
       "             0.9393, 0.9404, 0.9416, 0.9427, 0.9439, 0.9450, 0.9462, 0.9473, 0.9485,\n",
       "             0.9496, 0.9507, 0.9507, 0.9519, 0.9519, 0.9530, 0.9542, 0.9553, 0.9565,\n",
       "             0.9576, 0.9588, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668,\n",
       "             0.9679, 0.9691, 0.9702, 0.9702, 0.9714, 0.9725, 0.9725, 0.9737, 0.9748,\n",
       "             0.9759, 0.9771, 0.9782, 0.9794, 0.9794, 0.9794, 0.9794, 0.9794, 0.9805,\n",
       "             0.9805, 0.9817, 0.9817, 0.9817, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828,\n",
       "             0.9840, 0.9851, 0.9863, 0.9863, 0.9863, 0.9874, 0.9885, 0.9885, 0.9885,\n",
       "             0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9885, 0.9897, 0.9897, 0.9897,\n",
       "             0.9897, 0.9897, 0.9897, 0.9897, 0.9908, 0.9908, 0.9908, 0.9908, 0.9920,\n",
       "             0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9943,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9966, 0.9966, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01, 9.9975e-01,\n",
       "             9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9971e-01,\n",
       "             9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9968e-01, 9.9968e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9968e-01, 9.9967e-01, 9.9967e-01, 9.9967e-01, 9.9967e-01,\n",
       "             9.9967e-01, 9.9967e-01, 9.9967e-01, 9.9966e-01, 9.9966e-01, 9.9966e-01,\n",
       "             9.9965e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01, 9.9961e-01, 9.9961e-01,\n",
       "             9.9958e-01, 9.9958e-01, 9.9958e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9957e-01, 9.9957e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9955e-01,\n",
       "             9.9955e-01, 9.9955e-01, 9.9954e-01, 9.9953e-01, 9.9952e-01, 9.9952e-01,\n",
       "             9.9952e-01, 9.9948e-01, 9.9948e-01, 9.9948e-01, 9.9947e-01, 9.9944e-01,\n",
       "             9.9944e-01, 9.9944e-01, 9.9944e-01, 9.9944e-01, 9.9944e-01, 9.9943e-01,\n",
       "             9.9942e-01, 9.9941e-01, 9.9941e-01, 9.9938e-01, 9.9938e-01, 9.9937e-01,\n",
       "             9.9937e-01, 9.9935e-01, 9.9935e-01, 9.9934e-01, 9.9934e-01, 9.9934e-01,\n",
       "             9.9932e-01, 9.9932e-01, 9.9930e-01, 9.9930e-01, 9.9927e-01, 9.9926e-01,\n",
       "             9.9926e-01, 9.9925e-01, 9.9924e-01, 9.9923e-01, 9.9923e-01, 9.9921e-01,\n",
       "             9.9919e-01, 9.9919e-01, 9.9917e-01, 9.9917e-01, 9.9916e-01, 9.9916e-01,\n",
       "             9.9916e-01, 9.9916e-01, 9.9915e-01, 9.9913e-01, 9.9910e-01, 9.9910e-01,\n",
       "             9.9908e-01, 9.9908e-01, 9.9906e-01, 9.9905e-01, 9.9904e-01, 9.9900e-01,\n",
       "             9.9900e-01, 9.9898e-01, 9.9898e-01, 9.9896e-01, 9.9895e-01, 9.9893e-01,\n",
       "             9.9892e-01, 9.9884e-01, 9.9881e-01, 9.9881e-01, 9.9880e-01, 9.9876e-01,\n",
       "             9.9873e-01, 9.9873e-01, 9.9872e-01, 9.9871e-01, 9.9870e-01, 9.9868e-01,\n",
       "             9.9865e-01, 9.9864e-01, 9.9862e-01, 9.9857e-01, 9.9856e-01, 9.9853e-01,\n",
       "             9.9853e-01, 9.9853e-01, 9.9850e-01, 9.9848e-01, 9.9843e-01, 9.9843e-01,\n",
       "             9.9842e-01, 9.9842e-01, 9.9838e-01, 9.9838e-01, 9.9834e-01, 9.9834e-01,\n",
       "             9.9831e-01, 9.9829e-01, 9.9824e-01, 9.9823e-01, 9.9820e-01, 9.9817e-01,\n",
       "             9.9817e-01, 9.9817e-01, 9.9816e-01, 9.9810e-01, 9.9803e-01, 9.9798e-01,\n",
       "             9.9798e-01, 9.9797e-01, 9.9793e-01, 9.9788e-01, 9.9785e-01, 9.9783e-01,\n",
       "             9.9778e-01, 9.9778e-01, 9.9778e-01, 9.9774e-01, 9.9773e-01, 9.9773e-01,\n",
       "             9.9772e-01, 9.9764e-01, 9.9745e-01, 9.9744e-01, 9.9738e-01, 9.9735e-01,\n",
       "             9.9730e-01, 9.9727e-01, 9.9679e-01, 9.9679e-01, 9.9667e-01, 9.9655e-01,\n",
       "             9.9646e-01, 9.9636e-01, 9.9633e-01, 9.9593e-01, 9.9591e-01, 9.9590e-01,\n",
       "             9.9566e-01, 9.9537e-01, 9.9518e-01, 9.9516e-01, 9.9514e-01, 9.9493e-01,\n",
       "             9.9488e-01, 9.9478e-01, 9.9443e-01, 9.9425e-01, 9.9406e-01, 9.9386e-01,\n",
       "             9.9364e-01, 9.9347e-01, 9.9340e-01, 9.9307e-01, 9.9293e-01, 9.9277e-01,\n",
       "             9.9276e-01, 9.9162e-01, 9.9141e-01, 9.9128e-01, 9.9113e-01, 9.9108e-01,\n",
       "             9.9100e-01, 9.9058e-01, 9.9054e-01, 9.8871e-01, 9.8828e-01, 9.8784e-01,\n",
       "             9.8740e-01, 9.8561e-01, 9.8354e-01, 9.8333e-01, 9.8268e-01, 9.8249e-01,\n",
       "             9.8169e-01, 9.7927e-01, 9.7628e-01, 9.7128e-01, 9.6721e-01, 9.6683e-01,\n",
       "             9.6505e-01, 9.6261e-01, 9.6075e-01, 9.5993e-01, 9.5769e-01, 9.5653e-01,\n",
       "             9.5544e-01, 9.5506e-01, 9.5449e-01, 9.5163e-01, 9.5041e-01, 9.4819e-01,\n",
       "             9.4627e-01, 9.4543e-01, 9.4502e-01, 9.4441e-01, 9.4132e-01, 9.3877e-01,\n",
       "             9.3612e-01, 9.2813e-01, 9.1363e-01, 9.0942e-01, 9.0591e-01, 8.9901e-01,\n",
       "             8.9643e-01, 8.9259e-01, 8.8322e-01, 8.8067e-01, 8.7687e-01, 8.5220e-01,\n",
       "             8.4678e-01, 8.4455e-01, 8.3252e-01, 8.2329e-01, 8.2216e-01, 8.1959e-01,\n",
       "             8.1889e-01, 8.0920e-01, 7.8950e-01, 7.8103e-01, 7.3829e-01, 7.3735e-01,\n",
       "             7.2703e-01, 7.0247e-01, 6.9002e-01, 5.9925e-01, 5.0087e-01, 4.4741e-01,\n",
       "             4.4641e-01, 4.3707e-01, 4.2658e-01, 3.8659e-01, 3.8251e-01, 3.4542e-01,\n",
       "             3.4488e-01, 3.1040e-01, 2.8407e-01, 2.1414e-01, 1.9727e-01, 1.9181e-01,\n",
       "             1.8754e-01, 1.8480e-01, 1.7794e-01, 1.7297e-01, 1.6540e-01, 1.6465e-01,\n",
       "             1.3688e-01, 1.3023e-01, 1.2266e-01, 1.2144e-01, 1.0831e-01, 1.0829e-01,\n",
       "             1.0066e-01, 1.0002e-01, 9.7849e-02, 9.1787e-02, 8.7234e-02, 8.4198e-02,\n",
       "             8.1685e-02, 7.8333e-02, 6.9576e-02, 6.7585e-02, 6.3788e-02, 6.0175e-02,\n",
       "             5.4506e-02, 5.4072e-02, 5.3733e-02, 5.3046e-02, 5.0182e-02, 5.0017e-02,\n",
       "             4.4648e-02, 4.2984e-02, 4.2268e-02, 3.8757e-02, 3.8554e-02, 3.4950e-02,\n",
       "             3.3015e-02, 3.2502e-02, 2.4192e-02, 2.3039e-02, 2.1245e-02, 1.7077e-02,\n",
       "             1.6549e-02, 1.3021e-02, 1.2969e-02, 1.2707e-02, 1.2696e-02, 1.2547e-02,\n",
       "             1.2530e-02, 1.2147e-02, 1.1164e-02, 9.3260e-03, 8.7908e-03, 8.5821e-03,\n",
       "             8.3293e-03, 8.2250e-03, 7.9162e-03, 6.7349e-03, 6.5002e-03, 6.4062e-03,\n",
       "             6.2558e-03, 5.7851e-03, 4.3222e-03, 3.9839e-03, 3.9476e-03, 3.7782e-03,\n",
       "             3.6185e-03, 3.4812e-03, 3.4742e-03, 3.4561e-03, 3.3989e-03, 3.3915e-03,\n",
       "             3.0189e-03, 2.7354e-03, 2.7329e-03, 2.7255e-03, 2.6855e-03, 2.3266e-03,\n",
       "             2.2664e-03, 2.1492e-03, 2.1126e-03, 2.0983e-03, 2.0908e-03, 2.0527e-03,\n",
       "             2.0354e-03, 1.9300e-03, 1.7097e-03, 1.5319e-03, 1.4727e-03, 1.4690e-03,\n",
       "             1.2069e-03, 1.1002e-03, 1.0028e-03, 8.2583e-04, 8.0942e-04, 8.0139e-04,\n",
       "             7.6732e-04, 7.5439e-04, 7.0459e-04, 6.3387e-04, 6.2621e-04, 6.1586e-04,\n",
       "             5.9989e-04, 5.9955e-04, 5.6116e-04, 5.5396e-04, 5.4854e-04, 5.4584e-04,\n",
       "             5.2289e-04, 5.1343e-04, 4.9699e-04, 4.5865e-04, 4.1337e-04, 4.0783e-04,\n",
       "             3.9763e-04, 3.6771e-04, 3.4736e-04, 3.4406e-04, 3.2177e-04, 2.9119e-04,\n",
       "             2.5550e-04, 2.4847e-04, 2.2670e-04, 1.9610e-04, 1.8402e-04, 1.7418e-04,\n",
       "             1.5390e-04, 1.2928e-04, 1.2743e-04, 1.2039e-04, 1.1517e-04, 1.0461e-04,\n",
       "             1.0265e-04, 9.3433e-05, 7.6179e-05, 7.5775e-05, 7.2162e-05, 6.8055e-05,\n",
       "             6.6720e-05, 6.4426e-05, 6.3914e-05, 6.2899e-05, 6.1659e-05, 5.9742e-05,\n",
       "             5.7830e-05, 5.7666e-05, 5.5740e-05, 5.4529e-05, 5.4078e-05, 5.3887e-05,\n",
       "             5.1506e-05, 5.0589e-05, 5.0369e-05, 4.9422e-05, 4.5250e-05, 4.4499e-05,\n",
       "             4.3885e-05, 4.3575e-05, 4.2608e-05, 4.0517e-05, 3.6961e-05, 3.6186e-05,\n",
       "             3.5464e-05, 3.5433e-05, 2.7854e-05, 2.7726e-05, 2.6156e-05, 2.5685e-05,\n",
       "             2.4607e-05, 2.3508e-05, 2.1863e-05, 2.1697e-05, 2.1227e-05, 2.0769e-05,\n",
       "             2.0514e-05, 1.9925e-05, 1.8772e-05, 1.8562e-05, 1.7184e-05, 1.6708e-05,\n",
       "             1.6334e-05, 1.5941e-05, 1.5543e-05, 1.3510e-05, 1.3453e-05, 1.2961e-05,\n",
       "             1.1690e-05, 1.1280e-05, 1.0545e-05, 9.4593e-06, 8.2024e-06, 8.1712e-06,\n",
       "             7.6349e-06, 7.1514e-06, 6.6396e-06, 6.2620e-06, 5.2105e-06, 5.1551e-06,\n",
       "             5.1451e-06, 5.0419e-06, 4.6538e-06, 4.3477e-06, 4.2924e-06, 4.0031e-06,\n",
       "             3.5308e-06, 3.4885e-06, 3.3502e-06, 3.2549e-06, 3.2181e-06, 2.9062e-06,\n",
       "             2.4250e-06, 2.4191e-06, 2.3987e-06, 2.3335e-06, 2.1850e-06, 2.1820e-06,\n",
       "             2.0395e-06, 1.9305e-06, 1.8508e-06, 1.7212e-06, 1.6770e-06, 1.5436e-06,\n",
       "             1.5289e-06, 1.4532e-06, 1.3192e-06, 1.0909e-06, 1.0740e-06, 9.4223e-07,\n",
       "             8.2203e-07, 8.0272e-07, 7.0777e-07, 6.7663e-07, 6.5870e-07, 5.9748e-07,\n",
       "             4.3699e-07, 4.3565e-07, 4.2319e-07, 4.1971e-07, 3.7315e-07, 3.7075e-07,\n",
       "             3.6501e-07, 3.0923e-07, 2.9375e-07, 2.7710e-07, 2.5875e-07, 2.3380e-07,\n",
       "             2.0817e-07, 1.8307e-07, 1.4645e-07, 1.2867e-07, 1.0938e-07, 1.0104e-07,\n",
       "             7.6695e-08, 5.7352e-08, 3.5388e-08, 3.4265e-08, 3.2773e-08, 3.1812e-08,\n",
       "             2.5705e-08, 2.4316e-08, 2.0855e-08, 1.8554e-08, 1.5984e-08, 1.5111e-08,\n",
       "             1.2905e-08, 8.5651e-09, 7.9152e-09, 6.1324e-09, 6.0513e-09, 5.4353e-09,\n",
       "             2.3807e-09, 1.4317e-09, 9.2194e-10, 9.2015e-10, 5.7000e-10, 4.7494e-11,\n",
       "             7.3970e-12, 3.0041e-12])}},\n",
       "   {'fpr': np.float64(0.10526315789473684),\n",
       "    'tpr': np.float64(0.9919816723940436),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0132, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0230, 0.0230, 0.0230, 0.0263, 0.0296,\n",
       "             0.0296, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0362, 0.0362, 0.0362,\n",
       "             0.0395, 0.0428, 0.0461, 0.0493, 0.0493, 0.0493, 0.0526, 0.0526, 0.0559,\n",
       "             0.0592, 0.0625, 0.0658, 0.0691, 0.0724, 0.0757, 0.0789, 0.0822, 0.0855,\n",
       "             0.0855, 0.0855, 0.0855, 0.0888, 0.0888, 0.0921, 0.0954, 0.0954, 0.0987,\n",
       "             0.1020, 0.1053, 0.1053, 0.1053, 0.1086, 0.1118, 0.1151, 0.1151, 0.1184,\n",
       "             0.1217, 0.1250, 0.1283, 0.1283, 0.1316, 0.1316, 0.1349, 0.1382, 0.1414,\n",
       "             0.1447, 0.1480, 0.1513, 0.1546, 0.1546, 0.1579, 0.1612, 0.1645, 0.1678,\n",
       "             0.1711, 0.1743, 0.1776, 0.1809, 0.1842, 0.1875, 0.1908, 0.1941, 0.1974,\n",
       "             0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138, 0.2171, 0.2204, 0.2237,\n",
       "             0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2467, 0.2500,\n",
       "             0.2533, 0.2566, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697, 0.2730, 0.2763,\n",
       "             0.2796, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026, 0.3059,\n",
       "             0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322, 0.3355,\n",
       "             0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618, 0.3651,\n",
       "             0.3684, 0.3717, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914, 0.3947,\n",
       "             0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243,\n",
       "             0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539,\n",
       "             0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836,\n",
       "             0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132,\n",
       "             0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428,\n",
       "             0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724,\n",
       "             0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020,\n",
       "             0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316,\n",
       "             0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612,\n",
       "             0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908,\n",
       "             0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204,\n",
       "             0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500,\n",
       "             0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796,\n",
       "             0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092,\n",
       "             0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388,\n",
       "             0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684,\n",
       "             0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980,\n",
       "             0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276,\n",
       "             0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572,\n",
       "             0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868,\n",
       "             0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.2497, 0.3711, 0.4101, 0.4502, 0.4777, 0.4983, 0.5155, 0.5292,\n",
       "             0.5407, 0.5487, 0.5578, 0.5682, 0.5808, 0.5899, 0.5968, 0.6002, 0.6025,\n",
       "             0.6071, 0.6140, 0.6197, 0.6231, 0.6312, 0.6403, 0.6426, 0.6460, 0.6483,\n",
       "             0.6495, 0.6529, 0.6564, 0.6598, 0.6632, 0.6701, 0.6747, 0.6781, 0.6804,\n",
       "             0.6816, 0.6827, 0.6838, 0.6873, 0.6907, 0.6942, 0.6964, 0.6987, 0.6999,\n",
       "             0.7022, 0.7068, 0.7102, 0.7113, 0.7136, 0.7148, 0.7205, 0.7216, 0.7228,\n",
       "             0.7239, 0.7251, 0.7262, 0.7274, 0.7285, 0.7297, 0.7320, 0.7365, 0.7377,\n",
       "             0.7400, 0.7411, 0.7423, 0.7434, 0.7446, 0.7457, 0.7491, 0.7514, 0.7526,\n",
       "             0.7537, 0.7560, 0.7572, 0.7583, 0.7595, 0.7606, 0.7617, 0.7629, 0.7640,\n",
       "             0.7652, 0.7663, 0.7675, 0.7686, 0.7698, 0.7709, 0.7721, 0.7743, 0.7755,\n",
       "             0.7766, 0.7789, 0.7801, 0.7812, 0.7824, 0.7835, 0.7847, 0.7881, 0.7892,\n",
       "             0.7904, 0.7915, 0.7915, 0.7927, 0.7938, 0.7961, 0.7973, 0.7984, 0.7995,\n",
       "             0.8007, 0.8018, 0.8030, 0.8041, 0.8053, 0.8064, 0.8076, 0.8087, 0.8099,\n",
       "             0.8121, 0.8133, 0.8144, 0.8156, 0.8179, 0.8190, 0.8202, 0.8213, 0.8225,\n",
       "             0.8236, 0.8247, 0.8259, 0.8270, 0.8282, 0.8305, 0.8316, 0.8328, 0.8339,\n",
       "             0.8351, 0.8362, 0.8373, 0.8396, 0.8408, 0.8419, 0.8431, 0.8442, 0.8454,\n",
       "             0.8465, 0.8477, 0.8488, 0.8499, 0.8511, 0.8522, 0.8534, 0.8557, 0.8568,\n",
       "             0.8580, 0.8591, 0.8614, 0.8625, 0.8637, 0.8648, 0.8660, 0.8671, 0.8683,\n",
       "             0.8694, 0.8706, 0.8717, 0.8729, 0.8740, 0.8751, 0.8763, 0.8774, 0.8786,\n",
       "             0.8797, 0.8809, 0.8820, 0.8832, 0.8843, 0.8855, 0.8866, 0.8877, 0.8889,\n",
       "             0.8900, 0.8912, 0.8923, 0.8935, 0.8946, 0.8958, 0.8969, 0.8981, 0.8992,\n",
       "             0.9003, 0.9015, 0.9026, 0.9038, 0.9049, 0.9061, 0.9072, 0.9084, 0.9095,\n",
       "             0.9107, 0.9118, 0.9129, 0.9141, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198,\n",
       "             0.9210, 0.9221, 0.9233, 0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301,\n",
       "             0.9313, 0.9324, 0.9336, 0.9347, 0.9347, 0.9359, 0.9370, 0.9381, 0.9381,\n",
       "             0.9393, 0.9404, 0.9416, 0.9427, 0.9439, 0.9450, 0.9462, 0.9473, 0.9485,\n",
       "             0.9496, 0.9507, 0.9519, 0.9530, 0.9542, 0.9553, 0.9565, 0.9565, 0.9565,\n",
       "             0.9576, 0.9588, 0.9599, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656,\n",
       "             0.9668, 0.9679, 0.9691, 0.9702, 0.9702, 0.9714, 0.9725, 0.9725, 0.9725,\n",
       "             0.9737, 0.9737, 0.9748, 0.9759, 0.9771, 0.9782, 0.9782, 0.9794, 0.9805,\n",
       "             0.9805, 0.9805, 0.9805, 0.9805, 0.9817, 0.9828, 0.9828, 0.9840, 0.9840,\n",
       "             0.9840, 0.9840, 0.9840, 0.9840, 0.9840, 0.9840, 0.9840, 0.9840, 0.9840,\n",
       "             0.9851, 0.9863, 0.9874, 0.9874, 0.9885, 0.9885, 0.9885, 0.9897, 0.9897,\n",
       "             0.9897, 0.9897, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9987e-01, 9.9986e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9974e-01,\n",
       "             9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9966e-01, 9.9965e-01, 9.9962e-01,\n",
       "             9.9961e-01, 9.9961e-01, 9.9960e-01, 9.9957e-01, 9.9955e-01, 9.9955e-01,\n",
       "             9.9955e-01, 9.9952e-01, 9.9945e-01, 9.9942e-01, 9.9936e-01, 9.9935e-01,\n",
       "             9.9933e-01, 9.9930e-01, 9.9919e-01, 9.9906e-01, 9.9904e-01, 9.9896e-01,\n",
       "             9.9892e-01, 9.9889e-01, 9.9882e-01, 9.9877e-01, 9.9864e-01, 9.9862e-01,\n",
       "             9.9859e-01, 9.9851e-01, 9.9848e-01, 9.9831e-01, 9.9819e-01, 9.9814e-01,\n",
       "             9.9795e-01, 9.9791e-01, 9.9755e-01, 9.9746e-01, 9.9730e-01, 9.9719e-01,\n",
       "             9.9717e-01, 9.9707e-01, 9.9706e-01, 9.9655e-01, 9.9617e-01, 9.9599e-01,\n",
       "             9.9475e-01, 9.9459e-01, 9.9438e-01, 9.9388e-01, 9.9353e-01, 9.9346e-01,\n",
       "             9.9228e-01, 9.9142e-01, 9.9122e-01, 9.9045e-01, 9.8882e-01, 9.8600e-01,\n",
       "             9.8552e-01, 9.8285e-01, 9.8256e-01, 9.8189e-01, 9.8101e-01, 9.8017e-01,\n",
       "             9.7700e-01, 9.7368e-01, 9.6804e-01, 9.6353e-01, 9.6261e-01, 9.6092e-01,\n",
       "             9.5720e-01, 9.5233e-01, 9.4528e-01, 9.2755e-01, 9.1185e-01, 9.0641e-01,\n",
       "             9.0525e-01, 8.9906e-01, 8.3950e-01, 8.3220e-01, 8.3074e-01, 8.1675e-01,\n",
       "             8.1652e-01, 8.1080e-01, 7.9984e-01, 7.9771e-01, 7.5717e-01, 7.5503e-01,\n",
       "             7.3565e-01, 6.9259e-01, 6.8831e-01, 6.3797e-01, 6.3651e-01, 6.3087e-01,\n",
       "             5.1717e-01, 4.8424e-01, 4.5465e-01, 4.5158e-01, 4.4263e-01, 4.1548e-01,\n",
       "             3.2289e-01, 3.1626e-01, 3.0371e-01, 2.7897e-01, 2.6159e-01, 2.5678e-01,\n",
       "             1.6482e-01, 1.5742e-01, 1.2019e-01, 1.1969e-01, 1.1379e-01, 1.1202e-01,\n",
       "             9.2865e-02, 8.8185e-02, 8.6287e-02, 8.0619e-02, 7.7396e-02, 7.5477e-02,\n",
       "             7.0966e-02, 7.0065e-02, 6.7434e-02, 6.6578e-02, 6.3319e-02, 6.3145e-02,\n",
       "             5.6688e-02, 5.5543e-02, 5.3653e-02, 5.2371e-02, 5.1577e-02, 5.1318e-02,\n",
       "             5.1137e-02, 4.0743e-02, 3.8210e-02, 3.6985e-02, 3.3629e-02, 3.3186e-02,\n",
       "             3.2244e-02, 3.1828e-02, 3.0611e-02, 2.7223e-02, 2.3797e-02, 2.3328e-02,\n",
       "             2.1163e-02, 1.9164e-02, 1.7072e-02, 1.6697e-02, 1.6659e-02, 1.6522e-02,\n",
       "             1.4949e-02, 1.4488e-02, 9.0709e-03, 8.2386e-03, 8.0792e-03, 7.8448e-03,\n",
       "             6.3775e-03, 5.5260e-03, 5.4137e-03, 4.1751e-03, 4.0317e-03, 3.8583e-03,\n",
       "             3.7409e-03, 3.5942e-03, 3.3777e-03, 3.2559e-03, 3.0674e-03, 2.8009e-03,\n",
       "             2.6485e-03, 2.3649e-03, 2.1509e-03, 1.9388e-03, 1.8398e-03, 1.7598e-03,\n",
       "             1.6536e-03, 1.5923e-03, 1.5240e-03, 1.4706e-03, 1.4136e-03, 1.2905e-03,\n",
       "             1.2635e-03, 1.2501e-03, 1.2479e-03, 1.0189e-03, 9.9103e-04, 9.8113e-04,\n",
       "             7.7206e-04, 7.5995e-04, 7.4145e-04, 7.1789e-04, 6.9258e-04, 6.8750e-04,\n",
       "             6.7387e-04, 5.7518e-04, 5.6328e-04, 5.5004e-04, 5.2346e-04, 5.1098e-04,\n",
       "             5.1052e-04, 3.8693e-04, 3.3347e-04, 2.8470e-04, 2.5672e-04, 2.5597e-04,\n",
       "             2.4221e-04, 2.3132e-04, 2.2657e-04, 2.0620e-04, 1.9478e-04, 1.9254e-04,\n",
       "             1.8869e-04, 1.8088e-04, 1.3726e-04, 1.3359e-04, 1.1874e-04, 1.1857e-04,\n",
       "             1.0829e-04, 1.0563e-04, 8.5611e-05, 8.3532e-05, 8.1967e-05, 8.0567e-05,\n",
       "             7.1625e-05, 6.6461e-05, 6.3024e-05, 5.3616e-05, 5.2677e-05, 4.8883e-05,\n",
       "             4.2477e-05, 4.1771e-05, 4.1470e-05, 3.8923e-05, 3.7439e-05, 3.3015e-05,\n",
       "             3.2922e-05, 3.2206e-05, 2.8504e-05, 2.5858e-05, 2.5260e-05, 2.4763e-05,\n",
       "             2.2553e-05, 2.0970e-05, 1.7501e-05, 1.7489e-05, 1.5867e-05, 1.5182e-05,\n",
       "             1.5114e-05, 1.4411e-05, 1.4205e-05, 1.3722e-05, 1.3465e-05, 1.1323e-05,\n",
       "             1.1124e-05, 1.0391e-05, 9.8176e-06, 9.0025e-06, 8.9602e-06, 8.9176e-06,\n",
       "             8.8805e-06, 8.0085e-06, 7.9860e-06, 7.7319e-06, 6.4408e-06, 6.1480e-06,\n",
       "             5.3839e-06, 5.1910e-06, 4.3956e-06, 4.0332e-06, 3.9955e-06, 3.3405e-06,\n",
       "             3.2848e-06, 3.0437e-06, 2.8860e-06, 2.8415e-06, 2.5824e-06, 2.5091e-06,\n",
       "             2.4740e-06, 2.3874e-06, 2.3201e-06, 2.1793e-06, 2.0887e-06, 2.0623e-06,\n",
       "             1.7740e-06, 1.6775e-06, 1.6551e-06, 1.5460e-06, 1.5053e-06, 1.4424e-06,\n",
       "             1.3070e-06, 1.2564e-06, 1.2185e-06, 1.0556e-06, 8.5242e-07, 7.1337e-07,\n",
       "             6.3953e-07, 6.1347e-07, 5.9839e-07, 4.5876e-07, 4.2885e-07, 4.2113e-07,\n",
       "             4.1808e-07, 4.0314e-07, 3.7698e-07, 3.5898e-07, 3.5028e-07, 2.3492e-07,\n",
       "             2.3092e-07, 2.2953e-07, 2.2108e-07, 2.1330e-07, 2.1194e-07, 2.0788e-07,\n",
       "             1.8224e-07, 1.7703e-07, 1.7618e-07, 1.7375e-07, 1.6701e-07, 1.6320e-07,\n",
       "             1.5863e-07, 1.0521e-07, 1.0041e-07, 7.7449e-08, 6.1182e-08, 5.7356e-08,\n",
       "             5.7076e-08, 4.6657e-08, 4.2617e-08, 4.1327e-08, 3.6651e-08, 3.4400e-08,\n",
       "             3.3158e-08, 3.2577e-08, 3.0370e-08, 2.6239e-08, 2.4744e-08, 1.8493e-08,\n",
       "             1.6279e-08, 1.1791e-08, 8.9203e-09, 8.8781e-09, 7.5771e-09, 7.0958e-09,\n",
       "             6.7694e-09, 5.4109e-09, 5.0627e-09, 4.3012e-09, 3.6560e-09, 3.2895e-09,\n",
       "             2.9830e-09, 2.3060e-09, 1.7889e-09, 1.3455e-09, 1.1781e-09, 1.0263e-09,\n",
       "             8.7538e-10, 5.1188e-10, 4.7239e-10, 4.0932e-10, 3.8569e-10, 3.2266e-10,\n",
       "             1.6512e-10, 9.7684e-11, 8.3809e-11, 8.1484e-11, 7.8272e-11, 3.5224e-11,\n",
       "             2.3285e-11, 1.5775e-11, 1.1052e-11, 1.0411e-11, 6.8361e-12, 9.8251e-13,\n",
       "             9.5887e-13, 5.1288e-14, 1.7178e-14, 1.6255e-14])}},\n",
       "   {'fpr': np.float64(0.049342105263157895),\n",
       "    'tpr': np.float64(0.981672394043528),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0197, 0.0197, 0.0197, 0.0197, 0.0230, 0.0230, 0.0263, 0.0263,\n",
       "             0.0296, 0.0296, 0.0329, 0.0362, 0.0395, 0.0428, 0.0461, 0.0461, 0.0461,\n",
       "             0.0461, 0.0493, 0.0493, 0.0526, 0.0559, 0.0559, 0.0559, 0.0592, 0.0592,\n",
       "             0.0625, 0.0625, 0.0658, 0.0658, 0.0691, 0.0691, 0.0724, 0.0724, 0.0724,\n",
       "             0.0757, 0.0789, 0.0789, 0.0822, 0.0855, 0.0888, 0.0921, 0.0954, 0.0987,\n",
       "             0.1020, 0.1053, 0.1086, 0.1118, 0.1151, 0.1184, 0.1217, 0.1250, 0.1283,\n",
       "             0.1316, 0.1349, 0.1382, 0.1414, 0.1414, 0.1447, 0.1480, 0.1513, 0.1513,\n",
       "             0.1546, 0.1546, 0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1776,\n",
       "             0.1809, 0.1842, 0.1875, 0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072,\n",
       "             0.2072, 0.2105, 0.2138, 0.2171, 0.2204, 0.2237, 0.2270, 0.2303, 0.2336,\n",
       "             0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2500, 0.2533, 0.2566, 0.2599,\n",
       "             0.2599, 0.2632, 0.2664, 0.2697, 0.2697, 0.2730, 0.2763, 0.2796, 0.2829,\n",
       "             0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026, 0.3059, 0.3092, 0.3125,\n",
       "             0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322, 0.3355, 0.3388, 0.3421,\n",
       "             0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618, 0.3651, 0.3684, 0.3717,\n",
       "             0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914, 0.3947, 0.3980, 0.4013,\n",
       "             0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243, 0.4276, 0.4309,\n",
       "             0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539, 0.4572, 0.4605,\n",
       "             0.4638, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836, 0.4868, 0.4901,\n",
       "             0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132, 0.5164, 0.5197,\n",
       "             0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428, 0.5461, 0.5493,\n",
       "             0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724, 0.5757, 0.5789,\n",
       "             0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020, 0.6053, 0.6086,\n",
       "             0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316, 0.6349, 0.6382,\n",
       "             0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612, 0.6645, 0.6678,\n",
       "             0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908, 0.6941, 0.6974,\n",
       "             0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204, 0.7237, 0.7270,\n",
       "             0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500, 0.7533, 0.7566,\n",
       "             0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796, 0.7829, 0.7862,\n",
       "             0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092, 0.8125, 0.8158,\n",
       "             0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388, 0.8421, 0.8454,\n",
       "             0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684, 0.8717, 0.8750,\n",
       "             0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980, 0.9013, 0.9046,\n",
       "             0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276, 0.9309, 0.9342,\n",
       "             0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572, 0.9605, 0.9638,\n",
       "             0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868, 0.9901, 0.9934,\n",
       "             0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.1077, 0.1856, 0.2199, 0.2417, 0.2623, 0.2806, 0.3070, 0.3265,\n",
       "             0.3322, 0.3414, 0.3436, 0.3608, 0.3666, 0.3746, 0.3803, 0.3872, 0.3895,\n",
       "             0.3918, 0.3986, 0.4009, 0.4032, 0.4101, 0.4147, 0.4181, 0.4204, 0.4238,\n",
       "             0.4318, 0.4341, 0.4422, 0.4456, 0.4490, 0.4513, 0.4548, 0.4570, 0.4616,\n",
       "             0.4639, 0.4662, 0.4685, 0.4731, 0.4742, 0.4788, 0.4800, 0.4822, 0.4834,\n",
       "             0.4857, 0.4868, 0.4880, 0.4914, 0.4937, 0.4983, 0.5006, 0.5040, 0.5063,\n",
       "             0.5074, 0.5097, 0.5109, 0.5132, 0.5189, 0.5200, 0.5212, 0.5246, 0.5269,\n",
       "             0.5292, 0.5304, 0.5338, 0.5372, 0.5395, 0.5407, 0.5418, 0.5475, 0.5487,\n",
       "             0.5510, 0.5533, 0.5544, 0.5556, 0.5590, 0.5601, 0.5624, 0.5636, 0.5647,\n",
       "             0.5659, 0.5682, 0.5693, 0.5704, 0.5716, 0.5727, 0.5750, 0.5762, 0.5785,\n",
       "             0.5796, 0.5808, 0.5830, 0.5842, 0.5853, 0.5865, 0.5876, 0.5888, 0.5911,\n",
       "             0.5922, 0.5945, 0.5956, 0.5979, 0.5991, 0.6002, 0.6025, 0.6037, 0.6060,\n",
       "             0.6071, 0.6082, 0.6094, 0.6105, 0.6117, 0.6128, 0.6140, 0.6151, 0.6174,\n",
       "             0.6186, 0.6197, 0.6208, 0.6231, 0.6254, 0.6266, 0.6277, 0.6300, 0.6312,\n",
       "             0.6323, 0.6334, 0.6357, 0.6369, 0.6380, 0.6392, 0.6403, 0.6415, 0.6426,\n",
       "             0.6438, 0.6449, 0.6460, 0.6472, 0.6483, 0.6495, 0.6506, 0.6518, 0.6529,\n",
       "             0.6541, 0.6552, 0.6564, 0.6575, 0.6586, 0.6598, 0.6609, 0.6621, 0.6632,\n",
       "             0.6644, 0.6655, 0.6667, 0.6678, 0.6690, 0.6701, 0.6712, 0.6724, 0.6747,\n",
       "             0.6758, 0.6770, 0.6781, 0.6793, 0.6804, 0.6816, 0.6827, 0.6838, 0.6850,\n",
       "             0.6861, 0.6873, 0.6884, 0.6896, 0.6907, 0.6919, 0.6930, 0.6942, 0.6953,\n",
       "             0.6964, 0.6976, 0.6987, 0.6999, 0.7010, 0.7022, 0.7033, 0.7045, 0.7068,\n",
       "             0.7079, 0.7090, 0.7102, 0.7113, 0.7125, 0.7136, 0.7148, 0.7159, 0.7171,\n",
       "             0.7182, 0.7194, 0.7205, 0.7216, 0.7228, 0.7239, 0.7251, 0.7262, 0.7274,\n",
       "             0.7285, 0.7297, 0.7308, 0.7320, 0.7331, 0.7342, 0.7354, 0.7365, 0.7377,\n",
       "             0.7388, 0.7400, 0.7411, 0.7423, 0.7434, 0.7446, 0.7457, 0.7468, 0.7480,\n",
       "             0.7491, 0.7503, 0.7514, 0.7526, 0.7537, 0.7549, 0.7560, 0.7572, 0.7583,\n",
       "             0.7595, 0.7606, 0.7617, 0.7629, 0.7640, 0.7652, 0.7663, 0.7675, 0.7686,\n",
       "             0.7698, 0.7709, 0.7721, 0.7732, 0.7743, 0.7755, 0.7766, 0.7778, 0.7789,\n",
       "             0.7801, 0.7812, 0.7824, 0.7835, 0.7847, 0.7858, 0.7869, 0.7881, 0.7892,\n",
       "             0.7904, 0.7915, 0.7927, 0.7938, 0.7950, 0.7961, 0.7973, 0.7984, 0.7995,\n",
       "             0.8007, 0.8018, 0.8030, 0.8041, 0.8053, 0.8064, 0.8076, 0.8087, 0.8099,\n",
       "             0.8110, 0.8121, 0.8133, 0.8144, 0.8156, 0.8167, 0.8179, 0.8190, 0.8202,\n",
       "             0.8213, 0.8225, 0.8236, 0.8247, 0.8259, 0.8270, 0.8282, 0.8293, 0.8305,\n",
       "             0.8316, 0.8328, 0.8339, 0.8351, 0.8362, 0.8373, 0.8385, 0.8396, 0.8408,\n",
       "             0.8419, 0.8431, 0.8442, 0.8454, 0.8465, 0.8477, 0.8488, 0.8499, 0.8499,\n",
       "             0.8511, 0.8522, 0.8534, 0.8545, 0.8557, 0.8568, 0.8580, 0.8591, 0.8603,\n",
       "             0.8614, 0.8625, 0.8637, 0.8648, 0.8660, 0.8671, 0.8683, 0.8694, 0.8706,\n",
       "             0.8717, 0.8729, 0.8740, 0.8751, 0.8763, 0.8774, 0.8786, 0.8797, 0.8809,\n",
       "             0.8820, 0.8832, 0.8843, 0.8855, 0.8866, 0.8877, 0.8889, 0.8900, 0.8912,\n",
       "             0.8923, 0.8935, 0.8946, 0.8958, 0.8969, 0.8981, 0.8992, 0.9003, 0.9015,\n",
       "             0.9026, 0.9038, 0.9049, 0.9061, 0.9072, 0.9084, 0.9095, 0.9107, 0.9118,\n",
       "             0.9129, 0.9141, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198, 0.9210, 0.9221,\n",
       "             0.9233, 0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324,\n",
       "             0.9336, 0.9347, 0.9359, 0.9370, 0.9381, 0.9393, 0.9393, 0.9404, 0.9416,\n",
       "             0.9416, 0.9427, 0.9439, 0.9450, 0.9450, 0.9462, 0.9473, 0.9485, 0.9496,\n",
       "             0.9507, 0.9519, 0.9530, 0.9542, 0.9553, 0.9565, 0.9576, 0.9588, 0.9599,\n",
       "             0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668, 0.9668, 0.9679, 0.9691,\n",
       "             0.9702, 0.9702, 0.9714, 0.9725, 0.9737, 0.9737, 0.9748, 0.9748, 0.9759,\n",
       "             0.9759, 0.9771, 0.9771, 0.9771, 0.9771, 0.9771, 0.9771, 0.9782, 0.9794,\n",
       "             0.9805, 0.9805, 0.9817, 0.9817, 0.9817, 0.9828, 0.9840, 0.9840, 0.9851,\n",
       "             0.9851, 0.9863, 0.9863, 0.9874, 0.9874, 0.9885, 0.9885, 0.9897, 0.9908,\n",
       "             0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931, 0.9943,\n",
       "             0.9943, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9976e-01, 9.9976e-01, 9.9973e-01, 9.9973e-01, 9.9973e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9969e-01, 9.9968e-01,\n",
       "             9.9968e-01, 9.9966e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9963e-01,\n",
       "             9.9962e-01, 9.9961e-01, 9.9960e-01, 9.9960e-01, 9.9960e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9955e-01, 9.9955e-01, 9.9953e-01, 9.9951e-01, 9.9949e-01,\n",
       "             9.9948e-01, 9.9946e-01, 9.9945e-01, 9.9944e-01, 9.9942e-01, 9.9941e-01,\n",
       "             9.9940e-01, 9.9937e-01, 9.9934e-01, 9.9933e-01, 9.9932e-01, 9.9931e-01,\n",
       "             9.9931e-01, 9.9930e-01, 9.9930e-01, 9.9929e-01, 9.9929e-01, 9.9928e-01,\n",
       "             9.9927e-01, 9.9926e-01, 9.9925e-01, 9.9925e-01, 9.9924e-01, 9.9920e-01,\n",
       "             9.9919e-01, 9.9918e-01, 9.9916e-01, 9.9912e-01, 9.9906e-01, 9.9898e-01,\n",
       "             9.9891e-01, 9.9888e-01, 9.9886e-01, 9.9879e-01, 9.9877e-01, 9.9875e-01,\n",
       "             9.9869e-01, 9.9869e-01, 9.9860e-01, 9.9859e-01, 9.9859e-01, 9.9856e-01,\n",
       "             9.9855e-01, 9.9851e-01, 9.9843e-01, 9.9834e-01, 9.9833e-01, 9.9832e-01,\n",
       "             9.9830e-01, 9.9825e-01, 9.9818e-01, 9.9814e-01, 9.9813e-01, 9.9813e-01,\n",
       "             9.9811e-01, 9.9806e-01, 9.9802e-01, 9.9774e-01, 9.9720e-01, 9.9709e-01,\n",
       "             9.9690e-01, 9.9687e-01, 9.9670e-01, 9.9662e-01, 9.9653e-01, 9.9618e-01,\n",
       "             9.9597e-01, 9.9574e-01, 9.9572e-01, 9.9563e-01, 9.9528e-01, 9.9507e-01,\n",
       "             9.9493e-01, 9.9478e-01, 9.9439e-01, 9.9363e-01, 9.9362e-01, 9.9340e-01,\n",
       "             9.9287e-01, 9.9275e-01, 9.9268e-01, 9.9206e-01, 9.9200e-01, 9.9154e-01,\n",
       "             9.9072e-01, 9.9044e-01, 9.8752e-01, 9.8705e-01, 9.8664e-01, 9.8510e-01,\n",
       "             9.8140e-01, 9.8088e-01, 9.8084e-01, 9.7988e-01, 9.7820e-01, 9.7766e-01,\n",
       "             9.7528e-01, 9.7121e-01, 9.7025e-01, 9.6982e-01, 9.6899e-01, 9.6866e-01,\n",
       "             9.6717e-01, 9.6247e-01, 9.5439e-01, 9.5298e-01, 9.5221e-01, 9.4905e-01,\n",
       "             9.4518e-01, 9.3821e-01, 9.2837e-01, 9.2810e-01, 9.0525e-01, 8.8398e-01,\n",
       "             8.8369e-01, 8.7534e-01, 8.7155e-01, 8.6867e-01, 8.6481e-01, 8.6463e-01,\n",
       "             8.5655e-01, 8.5206e-01, 8.3221e-01, 8.2422e-01, 8.0415e-01, 7.9699e-01,\n",
       "             7.6636e-01, 7.6464e-01, 7.4188e-01, 7.3786e-01, 7.1352e-01, 7.0603e-01,\n",
       "             6.6563e-01, 6.6370e-01, 6.4747e-01, 6.0317e-01, 6.0216e-01, 5.0151e-01,\n",
       "             4.6140e-01, 3.8589e-01, 3.6710e-01, 3.5735e-01, 3.2599e-01, 3.2330e-01,\n",
       "             3.1150e-01, 2.7950e-01, 2.6481e-01, 2.2524e-01, 2.1402e-01, 2.1233e-01,\n",
       "             1.9197e-01, 1.7803e-01, 1.7642e-01, 1.0791e-01, 1.0719e-01, 1.0712e-01,\n",
       "             1.0610e-01, 1.0258e-01, 8.3002e-02, 7.6448e-02, 6.9508e-02, 5.4249e-02,\n",
       "             4.5557e-02, 4.0340e-02, 3.7628e-02, 3.7401e-02, 2.8668e-02, 2.3868e-02,\n",
       "             2.3317e-02, 2.0493e-02, 1.8973e-02, 1.6644e-02, 1.1236e-02, 1.0455e-02,\n",
       "             8.6985e-03, 8.4549e-03, 7.5070e-03, 6.6889e-03, 5.9801e-03, 5.2941e-03,\n",
       "             4.8270e-03, 4.7555e-03, 4.4074e-03, 4.2275e-03, 4.2018e-03, 3.9321e-03,\n",
       "             3.8443e-03, 3.5746e-03, 3.5504e-03, 2.7064e-03, 2.3896e-03, 2.2051e-03,\n",
       "             1.9132e-03, 1.8103e-03, 1.3004e-03, 1.0812e-03, 1.0190e-03, 9.0102e-04,\n",
       "             7.9684e-04, 7.6610e-04, 6.6773e-04, 6.6279e-04, 5.9287e-04, 5.5542e-04,\n",
       "             5.1379e-04, 4.9782e-04, 4.5318e-04, 4.4945e-04, 4.2479e-04, 4.0313e-04,\n",
       "             3.0259e-04, 2.9518e-04, 2.9192e-04, 2.8600e-04, 2.6364e-04, 2.4559e-04,\n",
       "             2.1914e-04, 2.1604e-04, 2.0115e-04, 1.9420e-04, 1.6080e-04, 1.4650e-04,\n",
       "             1.3416e-04, 1.2986e-04, 9.2047e-05, 7.8536e-05, 6.4052e-05, 5.0863e-05,\n",
       "             4.7513e-05, 4.6049e-05, 3.8774e-05, 3.7749e-05, 3.6635e-05, 3.3727e-05,\n",
       "             2.7488e-05, 2.6769e-05, 2.6319e-05, 2.5770e-05, 2.2360e-05, 2.0696e-05,\n",
       "             1.9488e-05, 1.6409e-05, 1.4595e-05, 1.3269e-05, 1.3034e-05, 1.1069e-05,\n",
       "             8.6189e-06, 8.0111e-06, 7.7792e-06, 7.5838e-06, 7.3957e-06, 7.0149e-06,\n",
       "             6.7846e-06, 6.2461e-06, 6.2127e-06, 6.0534e-06, 5.9439e-06, 5.7609e-06,\n",
       "             5.4422e-06, 5.0328e-06, 5.0067e-06, 4.4186e-06, 4.4121e-06, 4.4102e-06,\n",
       "             4.2901e-06, 4.0472e-06, 3.8927e-06, 3.4570e-06, 3.4041e-06, 2.9949e-06,\n",
       "             2.9637e-06, 2.7276e-06, 2.6525e-06, 2.4433e-06, 2.1427e-06, 2.1258e-06,\n",
       "             2.1208e-06, 2.0026e-06, 1.6887e-06, 1.6251e-06, 1.5281e-06, 1.4530e-06,\n",
       "             1.2268e-06, 1.2180e-06, 1.0843e-06, 1.0841e-06, 9.4386e-07, 8.7102e-07,\n",
       "             8.5610e-07, 8.5566e-07, 8.3575e-07, 7.0523e-07, 6.7483e-07, 6.3803e-07,\n",
       "             6.2043e-07, 6.2034e-07, 5.3296e-07, 5.1005e-07, 5.0249e-07, 4.9057e-07,\n",
       "             4.1375e-07, 3.9659e-07, 3.8164e-07, 3.6717e-07, 3.2694e-07, 2.9255e-07,\n",
       "             2.8720e-07, 2.8140e-07, 2.0966e-07, 2.0957e-07, 1.9789e-07, 1.9477e-07,\n",
       "             1.8306e-07, 1.7435e-07, 1.6946e-07, 1.6678e-07, 1.6484e-07, 1.5485e-07,\n",
       "             1.4964e-07, 1.2582e-07, 1.1550e-07, 1.1383e-07, 1.0716e-07, 9.8929e-08,\n",
       "             8.2843e-08, 7.5974e-08, 7.1185e-08, 7.0672e-08, 6.9270e-08, 5.3432e-08,\n",
       "             5.1814e-08, 4.8543e-08, 4.8513e-08, 4.3989e-08, 4.2732e-08, 3.9492e-08,\n",
       "             3.7963e-08, 3.6951e-08, 3.0730e-08, 2.7589e-08, 2.6982e-08, 2.6253e-08,\n",
       "             2.4798e-08, 1.7557e-08, 1.7335e-08, 1.5875e-08, 1.5145e-08, 1.3645e-08,\n",
       "             1.3215e-08, 1.2218e-08, 1.1356e-08, 1.0182e-08, 7.3953e-09, 7.2848e-09,\n",
       "             7.2715e-09, 6.7532e-09, 6.6545e-09, 6.0561e-09, 5.9527e-09, 5.7739e-09,\n",
       "             4.8553e-09, 4.7879e-09, 2.9944e-09, 2.9934e-09, 2.7888e-09, 2.5106e-09,\n",
       "             2.3904e-09, 1.8635e-09, 1.6982e-09, 1.5383e-09, 1.5290e-09, 1.4046e-09,\n",
       "             1.1390e-09, 1.1239e-09, 1.0089e-09, 6.6397e-10, 5.2450e-10, 4.9098e-10,\n",
       "             4.2537e-10, 4.1661e-10, 3.7274e-10, 3.4807e-10, 3.2385e-10, 2.9861e-10,\n",
       "             2.9855e-10, 2.9087e-10, 2.3921e-10, 1.9887e-10, 1.9804e-10, 1.4786e-10,\n",
       "             1.4746e-10, 1.4709e-10, 1.2961e-10, 8.3088e-11, 7.7755e-11, 7.5289e-11,\n",
       "             5.8596e-11, 5.0007e-11, 4.5757e-11, 4.2918e-11, 4.1637e-11, 3.3148e-11,\n",
       "             2.9717e-11, 2.9503e-11, 2.1619e-11, 2.0539e-11, 1.8607e-11, 1.7061e-11,\n",
       "             1.4158e-11, 1.2694e-11, 8.9376e-12, 7.4927e-12, 5.7324e-12, 4.7303e-12,\n",
       "             4.3949e-12, 4.0012e-12, 3.9319e-12, 3.7803e-12, 2.6589e-12, 2.5035e-12,\n",
       "             2.1480e-12, 2.1030e-12, 1.7713e-12, 1.0771e-12, 7.6127e-13, 7.4178e-13,\n",
       "             7.0571e-13, 7.0514e-13, 3.3256e-13, 2.9313e-13, 9.8435e-14, 8.2988e-14,\n",
       "             7.4610e-14, 6.7413e-14, 6.1653e-14, 5.8183e-14, 2.8348e-14, 1.4037e-14,\n",
       "             8.9286e-16, 1.5281e-16, 1.2609e-16, 7.7686e-17, 1.8496e-17])}},\n",
       "   {'fpr': np.float64(0.03618421052631579),\n",
       "    'tpr': np.float64(0.9564719358533792),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0197, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230,\n",
       "             0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0263, 0.0263,\n",
       "             0.0263, 0.0263, 0.0263, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296,\n",
       "             0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296,\n",
       "             0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0329, 0.0329, 0.0362, 0.0362,\n",
       "             0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362,\n",
       "             0.0362, 0.0362, 0.0395, 0.0428, 0.0428, 0.0461, 0.0461, 0.0461, 0.0461,\n",
       "             0.0461, 0.0461, 0.0493, 0.0493, 0.0493, 0.0526, 0.0559, 0.0592, 0.0625,\n",
       "             0.0625, 0.0625, 0.0658, 0.0658, 0.0658, 0.0658, 0.0658, 0.0658, 0.0691,\n",
       "             0.0724, 0.0757, 0.0789, 0.0822, 0.0855, 0.0855, 0.0855, 0.0888, 0.0888,\n",
       "             0.0921, 0.0954, 0.0987, 0.1020, 0.1020, 0.1053, 0.1086, 0.1118, 0.1151,\n",
       "             0.1151, 0.1184, 0.1217, 0.1250, 0.1283, 0.1316, 0.1349, 0.1382, 0.1414,\n",
       "             0.1447, 0.1480, 0.1513, 0.1513, 0.1546, 0.1546, 0.1579, 0.1612, 0.1645,\n",
       "             0.1678, 0.1711, 0.1743, 0.1776, 0.1776, 0.1809, 0.1842, 0.1875, 0.1908,\n",
       "             0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138, 0.2171, 0.2204,\n",
       "             0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2500,\n",
       "             0.2533, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697, 0.2730, 0.2763, 0.2796,\n",
       "             0.2829, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026, 0.3059,\n",
       "             0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3257, 0.3289, 0.3322,\n",
       "             0.3355, 0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618,\n",
       "             0.3651, 0.3684, 0.3717, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914,\n",
       "             0.3947, 0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211,\n",
       "             0.4243, 0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507,\n",
       "             0.4539, 0.4572, 0.4605, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770,\n",
       "             0.4803, 0.4836, 0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066,\n",
       "             0.5099, 0.5132, 0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362,\n",
       "             0.5395, 0.5428, 0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658,\n",
       "             0.5691, 0.5724, 0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954,\n",
       "             0.5987, 0.6020, 0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250,\n",
       "             0.6283, 0.6316, 0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546,\n",
       "             0.6579, 0.6612, 0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842,\n",
       "             0.6875, 0.6908, 0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138,\n",
       "             0.7171, 0.7204, 0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434,\n",
       "             0.7467, 0.7500, 0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730,\n",
       "             0.7763, 0.7796, 0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026,\n",
       "             0.8059, 0.8092, 0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322,\n",
       "             0.8355, 0.8388, 0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618,\n",
       "             0.8651, 0.8684, 0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914,\n",
       "             0.8947, 0.8980, 0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211,\n",
       "             0.9243, 0.9276, 0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507,\n",
       "             0.9539, 0.9572, 0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803,\n",
       "             0.9836, 0.9868, 0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.0309, 0.0527, 0.0836, 0.0905, 0.1008, 0.1157, 0.1260, 0.1317,\n",
       "             0.1397, 0.1466, 0.1535, 0.1615, 0.1707, 0.1741, 0.1775, 0.1833, 0.1844,\n",
       "             0.1879, 0.1947, 0.1970, 0.1993, 0.2005, 0.2039, 0.2085, 0.2119, 0.2176,\n",
       "             0.2199, 0.2211, 0.2245, 0.2257, 0.2291, 0.2348, 0.2371, 0.2383, 0.2394,\n",
       "             0.2440, 0.2463, 0.2486, 0.2543, 0.2577, 0.2612, 0.2623, 0.2646, 0.2658,\n",
       "             0.2669, 0.2692, 0.2715, 0.2738, 0.2772, 0.2795, 0.2841, 0.2864, 0.2887,\n",
       "             0.2921, 0.2955, 0.2990, 0.3013, 0.3036, 0.3058, 0.3081, 0.3093, 0.3127,\n",
       "             0.3162, 0.3173, 0.3184, 0.3196, 0.3230, 0.3253, 0.3265, 0.3276, 0.3288,\n",
       "             0.3299, 0.3310, 0.3322, 0.3333, 0.3345, 0.3368, 0.3391, 0.3414, 0.3448,\n",
       "             0.3459, 0.3471, 0.3505, 0.3517, 0.3540, 0.3562, 0.3597, 0.3608, 0.3631,\n",
       "             0.3654, 0.3666, 0.3677, 0.3688, 0.3700, 0.3711, 0.3723, 0.3746, 0.3757,\n",
       "             0.3780, 0.3814, 0.3826, 0.3849, 0.3860, 0.3883, 0.3895, 0.3906, 0.3918,\n",
       "             0.3929, 0.3952, 0.3975, 0.3986, 0.4009, 0.4021, 0.4032, 0.4044, 0.4055,\n",
       "             0.4066, 0.4078, 0.4101, 0.4112, 0.4124, 0.4158, 0.4170, 0.4181, 0.4192,\n",
       "             0.4204, 0.4215, 0.4227, 0.4238, 0.4250, 0.4261, 0.4273, 0.4284, 0.4296,\n",
       "             0.4307, 0.4318, 0.4330, 0.4353, 0.4376, 0.4387, 0.4399, 0.4410, 0.4422,\n",
       "             0.4433, 0.4444, 0.4456, 0.4467, 0.4479, 0.4490, 0.4502, 0.4525, 0.4536,\n",
       "             0.4548, 0.4559, 0.4582, 0.4593, 0.4605, 0.4616, 0.4628, 0.4639, 0.4651,\n",
       "             0.4662, 0.4674, 0.4685, 0.4696, 0.4708, 0.4719, 0.4731, 0.4742, 0.4754,\n",
       "             0.4765, 0.4777, 0.4788, 0.4811, 0.4822, 0.4834, 0.4845, 0.4857, 0.4868,\n",
       "             0.4880, 0.4903, 0.4914, 0.4926, 0.4937, 0.4948, 0.4960, 0.4971, 0.4983,\n",
       "             0.4994, 0.5006, 0.5017, 0.5029, 0.5040, 0.5052, 0.5063, 0.5074, 0.5086,\n",
       "             0.5097, 0.5109, 0.5120, 0.5132, 0.5155, 0.5166, 0.5178, 0.5200, 0.5212,\n",
       "             0.5223, 0.5235, 0.5246, 0.5258, 0.5269, 0.5281, 0.5292, 0.5315, 0.5326,\n",
       "             0.5349, 0.5372, 0.5384, 0.5395, 0.5407, 0.5418, 0.5464, 0.5475, 0.5487,\n",
       "             0.5510, 0.5521, 0.5533, 0.5544, 0.5556, 0.5567, 0.5578, 0.5590, 0.5601,\n",
       "             0.5613, 0.5636, 0.5647, 0.5659, 0.5670, 0.5682, 0.5693, 0.5716, 0.5727,\n",
       "             0.5739, 0.5750, 0.5762, 0.5773, 0.5785, 0.5796, 0.5808, 0.5819, 0.5830,\n",
       "             0.5842, 0.5853, 0.5865, 0.5876, 0.5888, 0.5899, 0.5911, 0.5934, 0.5945,\n",
       "             0.5956, 0.5968, 0.5979, 0.5991, 0.6002, 0.6014, 0.6025, 0.6037, 0.6048,\n",
       "             0.6060, 0.6071, 0.6082, 0.6094, 0.6105, 0.6117, 0.6128, 0.6140, 0.6151,\n",
       "             0.6163, 0.6174, 0.6186, 0.6197, 0.6208, 0.6220, 0.6231, 0.6243, 0.6254,\n",
       "             0.6266, 0.6277, 0.6289, 0.6300, 0.6312, 0.6323, 0.6334, 0.6346, 0.6357,\n",
       "             0.6369, 0.6380, 0.6392, 0.6403, 0.6415, 0.6426, 0.6438, 0.6449, 0.6460,\n",
       "             0.6472, 0.6483, 0.6495, 0.6506, 0.6518, 0.6529, 0.6541, 0.6552, 0.6564,\n",
       "             0.6575, 0.6586, 0.6598, 0.6609, 0.6621, 0.6632, 0.6644, 0.6655, 0.6667,\n",
       "             0.6678, 0.6690, 0.6701, 0.6712, 0.6724, 0.6735, 0.6747, 0.6758, 0.6770,\n",
       "             0.6781, 0.6793, 0.6804, 0.6816, 0.6827, 0.6838, 0.6850, 0.6861, 0.6873,\n",
       "             0.6884, 0.6896, 0.6907, 0.6919, 0.6942, 0.6953, 0.6964, 0.6976, 0.6987,\n",
       "             0.6999, 0.7010, 0.7022, 0.7033, 0.7056, 0.7068, 0.7079, 0.7090, 0.7102,\n",
       "             0.7113, 0.7125, 0.7136, 0.7148, 0.7159, 0.7171, 0.7182, 0.7194, 0.7205,\n",
       "             0.7216, 0.7228, 0.7239, 0.7251, 0.7262, 0.7274, 0.7285, 0.7297, 0.7308,\n",
       "             0.7320, 0.7331, 0.7342, 0.7354, 0.7365, 0.7377, 0.7388, 0.7400, 0.7411,\n",
       "             0.7423, 0.7446, 0.7457, 0.7468, 0.7480, 0.7491, 0.7503, 0.7514, 0.7526,\n",
       "             0.7537, 0.7549, 0.7560, 0.7572, 0.7583, 0.7595, 0.7606, 0.7617, 0.7629,\n",
       "             0.7640, 0.7652, 0.7663, 0.7675, 0.7686, 0.7698, 0.7709, 0.7721, 0.7732,\n",
       "             0.7743, 0.7766, 0.7778, 0.7789, 0.7801, 0.7812, 0.7824, 0.7835, 0.7847,\n",
       "             0.7858, 0.7869, 0.7881, 0.7892, 0.7904, 0.7915, 0.7927, 0.7938, 0.7950,\n",
       "             0.7961, 0.7973, 0.7984, 0.7995, 0.8007, 0.8018, 0.8030, 0.8041, 0.8053,\n",
       "             0.8064, 0.8076, 0.8087, 0.8099, 0.8110, 0.8121, 0.8133, 0.8144, 0.8156,\n",
       "             0.8167, 0.8179, 0.8190, 0.8202, 0.8202, 0.8213, 0.8225, 0.8236, 0.8247,\n",
       "             0.8259, 0.8270, 0.8282, 0.8293, 0.8305, 0.8316, 0.8328, 0.8328, 0.8339,\n",
       "             0.8351, 0.8362, 0.8373, 0.8385, 0.8396, 0.8408, 0.8419, 0.8431, 0.8442,\n",
       "             0.8454, 0.8465, 0.8477, 0.8488, 0.8499, 0.8511, 0.8522, 0.8534, 0.8545,\n",
       "             0.8557, 0.8568, 0.8580, 0.8591, 0.8603, 0.8614, 0.8614, 0.8625, 0.8637,\n",
       "             0.8648, 0.8660, 0.8671, 0.8683, 0.8694, 0.8706, 0.8717, 0.8729, 0.8740,\n",
       "             0.8751, 0.8763, 0.8774, 0.8786, 0.8797, 0.8809, 0.8820, 0.8832, 0.8843,\n",
       "             0.8855, 0.8866, 0.8877, 0.8889, 0.8900, 0.8912, 0.8923, 0.8935, 0.8946,\n",
       "             0.8958, 0.8969, 0.8981, 0.8992, 0.9003, 0.9015, 0.9026, 0.9038, 0.9049,\n",
       "             0.9061, 0.9061, 0.9072, 0.9072, 0.9084, 0.9095, 0.9107, 0.9118, 0.9129,\n",
       "             0.9141, 0.9152, 0.9152, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198, 0.9210,\n",
       "             0.9221, 0.9233, 0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9290, 0.9301,\n",
       "             0.9313, 0.9324, 0.9336, 0.9336, 0.9347, 0.9359, 0.9370, 0.9381, 0.9393,\n",
       "             0.9404, 0.9416, 0.9427, 0.9439, 0.9450, 0.9462, 0.9473, 0.9485, 0.9496,\n",
       "             0.9507, 0.9519, 0.9530, 0.9542, 0.9553, 0.9553, 0.9565, 0.9565, 0.9576,\n",
       "             0.9588, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668, 0.9679,\n",
       "             0.9691, 0.9702, 0.9702, 0.9702, 0.9714, 0.9714, 0.9725, 0.9737, 0.9748,\n",
       "             0.9759, 0.9771, 0.9771, 0.9782, 0.9794, 0.9794, 0.9794, 0.9794, 0.9794,\n",
       "             0.9805, 0.9817, 0.9817, 0.9828, 0.9840, 0.9851, 0.9863, 0.9874, 0.9874,\n",
       "             0.9874, 0.9874, 0.9874, 0.9874, 0.9874, 0.9885, 0.9897, 0.9897, 0.9908,\n",
       "             0.9908, 0.9908, 0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9920,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01,\n",
       "             9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01,\n",
       "             9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9983e-01, 9.9983e-01, 9.9983e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01,\n",
       "             9.9981e-01, 9.9981e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01, 9.9980e-01,\n",
       "             9.9980e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01, 9.9979e-01,\n",
       "             9.9979e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01, 9.9978e-01,\n",
       "             9.9977e-01, 9.9977e-01, 9.9976e-01, 9.9976e-01, 9.9976e-01, 9.9975e-01,\n",
       "             9.9975e-01, 9.9975e-01, 9.9974e-01, 9.9974e-01, 9.9973e-01, 9.9973e-01,\n",
       "             9.9973e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01, 9.9972e-01,\n",
       "             9.9972e-01, 9.9971e-01, 9.9971e-01, 9.9970e-01, 9.9970e-01, 9.9970e-01,\n",
       "             9.9969e-01, 9.9969e-01, 9.9969e-01, 9.9968e-01, 9.9968e-01, 9.9968e-01,\n",
       "             9.9967e-01, 9.9967e-01, 9.9966e-01, 9.9965e-01, 9.9964e-01, 9.9964e-01,\n",
       "             9.9964e-01, 9.9962e-01, 9.9961e-01, 9.9961e-01, 9.9960e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9958e-01, 9.9958e-01, 9.9957e-01, 9.9957e-01, 9.9957e-01,\n",
       "             9.9956e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9954e-01, 9.9953e-01,\n",
       "             9.9952e-01, 9.9951e-01, 9.9951e-01, 9.9950e-01, 9.9949e-01, 9.9948e-01,\n",
       "             9.9946e-01, 9.9946e-01, 9.9946e-01, 9.9945e-01, 9.9944e-01, 9.9941e-01,\n",
       "             9.9940e-01, 9.9939e-01, 9.9939e-01, 9.9939e-01, 9.9931e-01, 9.9931e-01,\n",
       "             9.9928e-01, 9.9928e-01, 9.9927e-01, 9.9927e-01, 9.9926e-01, 9.9925e-01,\n",
       "             9.9925e-01, 9.9925e-01, 9.9925e-01, 9.9924e-01, 9.9924e-01, 9.9922e-01,\n",
       "             9.9922e-01, 9.9919e-01, 9.9918e-01, 9.9918e-01, 9.9918e-01, 9.9914e-01,\n",
       "             9.9913e-01, 9.9911e-01, 9.9907e-01, 9.9901e-01, 9.9901e-01, 9.9897e-01,\n",
       "             9.9896e-01, 9.9896e-01, 9.9895e-01, 9.9893e-01, 9.9891e-01, 9.9888e-01,\n",
       "             9.9887e-01, 9.9879e-01, 9.9876e-01, 9.9871e-01, 9.9866e-01, 9.9862e-01,\n",
       "             9.9861e-01, 9.9861e-01, 9.9859e-01, 9.9857e-01, 9.9857e-01, 9.9854e-01,\n",
       "             9.9854e-01, 9.9849e-01, 9.9842e-01, 9.9841e-01, 9.9837e-01, 9.9830e-01,\n",
       "             9.9824e-01, 9.9818e-01, 9.9818e-01, 9.9815e-01, 9.9815e-01, 9.9810e-01,\n",
       "             9.9803e-01, 9.9802e-01, 9.9797e-01, 9.9793e-01, 9.9792e-01, 9.9792e-01,\n",
       "             9.9784e-01, 9.9783e-01, 9.9782e-01, 9.9776e-01, 9.9771e-01, 9.9754e-01,\n",
       "             9.9748e-01, 9.9745e-01, 9.9743e-01, 9.9737e-01, 9.9727e-01, 9.9721e-01,\n",
       "             9.9718e-01, 9.9715e-01, 9.9712e-01, 9.9708e-01, 9.9703e-01, 9.9683e-01,\n",
       "             9.9682e-01, 9.9680e-01, 9.9680e-01, 9.9677e-01, 9.9673e-01, 9.9657e-01,\n",
       "             9.9654e-01, 9.9647e-01, 9.9604e-01, 9.9602e-01, 9.9596e-01, 9.9594e-01,\n",
       "             9.9590e-01, 9.9570e-01, 9.9546e-01, 9.9534e-01, 9.9516e-01, 9.9515e-01,\n",
       "             9.9507e-01, 9.9506e-01, 9.9505e-01, 9.9503e-01, 9.9491e-01, 9.9474e-01,\n",
       "             9.9466e-01, 9.9459e-01, 9.9436e-01, 9.9429e-01, 9.9426e-01, 9.9416e-01,\n",
       "             9.9371e-01, 9.9357e-01, 9.9310e-01, 9.9299e-01, 9.9294e-01, 9.9293e-01,\n",
       "             9.9230e-01, 9.9226e-01, 9.9203e-01, 9.9146e-01, 9.9106e-01, 9.9080e-01,\n",
       "             9.9006e-01, 9.9005e-01, 9.8983e-01, 9.8919e-01, 9.8882e-01, 9.8854e-01,\n",
       "             9.8828e-01, 9.8820e-01, 9.8786e-01, 9.8766e-01, 9.8754e-01, 9.8676e-01,\n",
       "             9.8589e-01, 9.8561e-01, 9.8544e-01, 9.8335e-01, 9.8312e-01, 9.7876e-01,\n",
       "             9.7830e-01, 9.7728e-01, 9.7673e-01, 9.7665e-01, 9.7621e-01, 9.7610e-01,\n",
       "             9.7610e-01, 9.7598e-01, 9.7163e-01, 9.6974e-01, 9.6583e-01, 9.6431e-01,\n",
       "             9.6419e-01, 9.6193e-01, 9.5557e-01, 9.5480e-01, 9.5371e-01, 9.5308e-01,\n",
       "             9.4610e-01, 9.4387e-01, 9.4381e-01, 9.4007e-01, 9.3896e-01, 9.3383e-01,\n",
       "             9.3283e-01, 9.3229e-01, 9.2235e-01, 9.2164e-01, 9.1803e-01, 9.1489e-01,\n",
       "             9.1421e-01, 9.0872e-01, 9.0469e-01, 9.0322e-01, 9.0076e-01, 8.9617e-01,\n",
       "             8.8800e-01, 8.7857e-01, 8.7772e-01, 8.7355e-01, 8.7160e-01, 8.6294e-01,\n",
       "             8.6016e-01, 8.5858e-01, 8.5274e-01, 8.5089e-01, 8.4457e-01, 8.4295e-01,\n",
       "             8.3280e-01, 8.0424e-01, 7.9656e-01, 7.8992e-01, 7.8588e-01, 7.7981e-01,\n",
       "             7.7943e-01, 7.4586e-01, 7.3795e-01, 7.3361e-01, 7.2153e-01, 7.1139e-01,\n",
       "             6.9803e-01, 6.8810e-01, 6.5645e-01, 6.5161e-01, 6.2381e-01, 6.0512e-01,\n",
       "             5.3345e-01, 5.3146e-01, 4.6645e-01, 4.6333e-01, 4.5199e-01, 4.3762e-01,\n",
       "             4.2507e-01, 4.0605e-01, 3.8075e-01, 3.5866e-01, 3.5536e-01, 3.3503e-01,\n",
       "             3.2924e-01, 3.1695e-01, 2.8631e-01, 2.8271e-01, 2.3662e-01, 1.9457e-01,\n",
       "             1.7278e-01, 1.6295e-01, 1.5324e-01, 1.4338e-01, 1.2822e-01, 1.0145e-01,\n",
       "             9.7433e-02, 9.4173e-02, 8.6957e-02, 8.5922e-02, 7.8949e-02, 5.0562e-02,\n",
       "             4.6784e-02, 4.4337e-02, 4.0589e-02, 3.7886e-02, 2.7994e-02, 2.3795e-02,\n",
       "             2.2988e-02, 2.0790e-02, 1.9983e-02, 1.6375e-02, 1.6030e-02, 1.5837e-02,\n",
       "             1.5643e-02, 1.4171e-02, 1.4142e-02, 1.4126e-02, 1.0632e-02, 1.0128e-02,\n",
       "             9.0810e-03, 5.9239e-03, 5.6874e-03, 5.4128e-03, 5.2093e-03, 4.7632e-03,\n",
       "             4.4102e-03, 4.3252e-03, 3.6165e-03, 3.2196e-03, 3.1896e-03, 2.7557e-03,\n",
       "             2.1838e-03, 1.9480e-03, 1.3849e-03, 1.3549e-03, 1.3204e-03, 1.1300e-03,\n",
       "             8.6812e-04, 7.9779e-04, 7.7377e-04, 7.5459e-04, 7.3081e-04, 5.6027e-04,\n",
       "             5.0734e-04, 4.9630e-04, 4.2553e-04, 4.1818e-04, 4.1802e-04, 3.9452e-04,\n",
       "             3.7640e-04, 3.5529e-04, 3.0110e-04, 2.8152e-04, 2.7303e-04, 2.5332e-04,\n",
       "             2.4363e-04, 2.1365e-04, 2.0676e-04, 1.9696e-04, 1.9645e-04, 1.9445e-04,\n",
       "             1.5851e-04, 1.0732e-04, 9.1700e-05, 8.0627e-05, 7.8550e-05, 7.2854e-05,\n",
       "             6.7743e-05, 6.6559e-05, 6.5662e-05, 6.2714e-05, 5.7026e-05, 5.6540e-05,\n",
       "             5.4416e-05, 5.1764e-05, 5.1653e-05, 4.3822e-05, 4.3217e-05, 4.0282e-05,\n",
       "             4.0143e-05, 3.8705e-05, 3.6185e-05, 3.1860e-05, 3.0776e-05, 3.0768e-05,\n",
       "             2.6746e-05, 2.1386e-05, 2.1255e-05, 2.0725e-05, 1.5908e-05, 1.4885e-05,\n",
       "             1.1998e-05, 1.1450e-05, 8.7671e-06, 8.6473e-06, 6.9632e-06, 6.8846e-06,\n",
       "             5.7867e-06, 5.7728e-06, 5.3317e-06, 5.2324e-06, 4.6447e-06, 4.3743e-06,\n",
       "             4.2061e-06, 3.7276e-06, 3.5916e-06, 3.3356e-06, 2.8338e-06, 2.7356e-06,\n",
       "             2.0827e-06, 1.8387e-06, 1.6847e-06, 1.6846e-06, 1.5657e-06, 1.5022e-06,\n",
       "             1.3334e-06, 1.2212e-06, 1.1947e-06, 1.1558e-06, 1.0486e-06, 9.9898e-07,\n",
       "             9.4153e-07, 9.3008e-07, 8.3481e-07, 7.3618e-07, 6.4906e-07, 6.3542e-07,\n",
       "             6.0591e-07, 5.2360e-07, 5.1652e-07, 4.8154e-07, 4.0920e-07, 4.0815e-07,\n",
       "             4.0514e-07, 3.9651e-07, 3.7816e-07, 3.3580e-07, 3.2345e-07, 3.2084e-07,\n",
       "             3.0854e-07, 2.7202e-07, 2.4675e-07, 2.4296e-07, 2.1445e-07, 1.9242e-07,\n",
       "             1.8537e-07, 1.6447e-07, 1.4704e-07, 1.4120e-07, 1.2955e-07, 1.2601e-07,\n",
       "             1.2180e-07, 1.0937e-07, 1.0585e-07, 1.0079e-07, 1.0048e-07, 9.3812e-08,\n",
       "             8.5364e-08, 7.7547e-08, 5.8413e-08, 5.4521e-08, 4.6256e-08, 3.8808e-08,\n",
       "             3.6594e-08, 3.4644e-08, 3.3455e-08, 3.1882e-08, 3.1260e-08, 3.1251e-08,\n",
       "             3.0263e-08, 3.0035e-08, 2.9449e-08, 2.8257e-08, 2.5788e-08, 2.5019e-08,\n",
       "             2.2414e-08, 2.1940e-08, 2.1867e-08, 2.0251e-08, 1.9904e-08, 1.8338e-08,\n",
       "             1.7426e-08, 1.7074e-08, 1.6412e-08, 1.5933e-08, 1.4704e-08, 1.4669e-08,\n",
       "             1.4173e-08, 1.3487e-08, 1.3397e-08, 1.3062e-08, 1.2247e-08, 1.2188e-08,\n",
       "             1.1746e-08, 1.1588e-08, 8.5660e-09, 8.3193e-09, 7.8164e-09, 7.7013e-09,\n",
       "             7.2282e-09, 6.7770e-09, 6.0327e-09, 5.8922e-09, 5.7412e-09, 5.5671e-09,\n",
       "             4.5926e-09, 4.2086e-09, 4.0125e-09, 3.9902e-09, 3.9638e-09, 3.4346e-09,\n",
       "             2.6997e-09, 1.9250e-09, 1.8942e-09, 1.7919e-09, 1.5926e-09, 1.4977e-09,\n",
       "             1.4702e-09, 1.4224e-09, 1.3019e-09, 1.2616e-09, 1.2028e-09, 1.0601e-09,\n",
       "             9.7472e-10, 7.8834e-10, 6.0645e-10, 5.2016e-10, 5.0030e-10, 4.8243e-10,\n",
       "             4.8171e-10, 4.4065e-10, 3.8819e-10, 3.8728e-10, 3.4059e-10, 3.3160e-10,\n",
       "             3.1075e-10, 2.7066e-10, 2.2307e-10, 2.1464e-10, 2.0366e-10, 1.9908e-10,\n",
       "             1.8234e-10, 1.6848e-10, 1.6340e-10, 1.5712e-10, 1.5250e-10, 1.3773e-10,\n",
       "             1.2210e-10, 6.3110e-11, 5.7505e-11, 5.0799e-11, 4.5405e-11, 4.2957e-11,\n",
       "             3.8216e-11, 3.1046e-11, 2.9229e-11, 2.2810e-11, 1.9484e-11, 1.6525e-11,\n",
       "             1.5679e-11, 1.5246e-11, 1.4071e-11, 1.3862e-11, 1.3837e-11, 1.3290e-11,\n",
       "             1.2849e-11, 1.0046e-11, 8.0215e-12, 6.1473e-12, 5.3988e-12, 4.9760e-12,\n",
       "             4.5201e-12, 3.7011e-12, 3.4819e-12, 2.8136e-12, 1.4772e-12, 1.3611e-12,\n",
       "             1.1941e-12, 1.1344e-12, 9.2561e-13, 8.8814e-13, 7.4492e-13, 5.3608e-13,\n",
       "             4.8168e-13, 4.2222e-13, 3.6857e-13, 2.5131e-13, 2.3343e-13, 2.2263e-13,\n",
       "             2.0654e-13, 1.9867e-13, 1.7982e-13, 1.2747e-13, 1.1565e-13, 9.7425e-14,\n",
       "             8.6041e-14, 6.2443e-14, 2.6796e-14, 2.2198e-14, 5.9215e-15, 1.9099e-15,\n",
       "             4.6763e-16, 2.0310e-16, 7.5636e-19])}},\n",
       "   {'fpr': np.float64(0.05921052631578947),\n",
       "    'tpr': np.float64(0.983963344788087),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099,\n",
       "             0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164, 0.0197,\n",
       "             0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230,\n",
       "             0.0230, 0.0263, 0.0263, 0.0296, 0.0329, 0.0362, 0.0362, 0.0362, 0.0395,\n",
       "             0.0428, 0.0428, 0.0428, 0.0428, 0.0428, 0.0428, 0.0428, 0.0461, 0.0493,\n",
       "             0.0493, 0.0526, 0.0559, 0.0592, 0.0592, 0.0592, 0.0592, 0.0592, 0.0592,\n",
       "             0.0592, 0.0625, 0.0625, 0.0658, 0.0658, 0.0658, 0.0691, 0.0724, 0.0724,\n",
       "             0.0724, 0.0757, 0.0789, 0.0822, 0.0855, 0.0888, 0.0921, 0.0954, 0.0987,\n",
       "             0.1020, 0.1053, 0.1086, 0.1118, 0.1151, 0.1184, 0.1217, 0.1250, 0.1283,\n",
       "             0.1283, 0.1316, 0.1349, 0.1382, 0.1414, 0.1447, 0.1480, 0.1513, 0.1546,\n",
       "             0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1776, 0.1809, 0.1842,\n",
       "             0.1875, 0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138,\n",
       "             0.2171, 0.2204, 0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2401,\n",
       "             0.2434, 0.2467, 0.2467, 0.2500, 0.2533, 0.2566, 0.2599, 0.2599, 0.2632,\n",
       "             0.2664, 0.2697, 0.2730, 0.2763, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928,\n",
       "             0.2961, 0.2993, 0.3026, 0.3059, 0.3092, 0.3125, 0.3158, 0.3191, 0.3224,\n",
       "             0.3257, 0.3257, 0.3289, 0.3322, 0.3355, 0.3355, 0.3388, 0.3421, 0.3454,\n",
       "             0.3487, 0.3520, 0.3553, 0.3586, 0.3618, 0.3651, 0.3684, 0.3717, 0.3750,\n",
       "             0.3783, 0.3816, 0.3849, 0.3882, 0.3914, 0.3947, 0.3980, 0.4013, 0.4046,\n",
       "             0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243, 0.4276, 0.4309, 0.4342,\n",
       "             0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539, 0.4572, 0.4605, 0.4638,\n",
       "             0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836, 0.4868, 0.4901, 0.4934,\n",
       "             0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132, 0.5164, 0.5197, 0.5230,\n",
       "             0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428, 0.5461, 0.5493, 0.5526,\n",
       "             0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724, 0.5757, 0.5789, 0.5822,\n",
       "             0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020, 0.6053, 0.6086, 0.6118,\n",
       "             0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316, 0.6349, 0.6382, 0.6414,\n",
       "             0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612, 0.6645, 0.6678, 0.6711,\n",
       "             0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908, 0.6941, 0.6974, 0.7007,\n",
       "             0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204, 0.7237, 0.7270, 0.7303,\n",
       "             0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500, 0.7533, 0.7566, 0.7599,\n",
       "             0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796, 0.7829, 0.7862, 0.7895,\n",
       "             0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092, 0.8125, 0.8158, 0.8191,\n",
       "             0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388, 0.8421, 0.8454, 0.8487,\n",
       "             0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684, 0.8717, 0.8750, 0.8783,\n",
       "             0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980, 0.9013, 0.9046, 0.9079,\n",
       "             0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276, 0.9309, 0.9342, 0.9375,\n",
       "             0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572, 0.9605, 0.9638, 0.9671,\n",
       "             0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868, 0.9901, 0.9934, 0.9967,\n",
       "             1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.2360, 0.3242, 0.3688, 0.3998, 0.4147, 0.4307, 0.4467, 0.4605,\n",
       "             0.4742, 0.4822, 0.4948, 0.4994, 0.5052, 0.5109, 0.5166, 0.5200, 0.5246,\n",
       "             0.5315, 0.5361, 0.5395, 0.5418, 0.5441, 0.5487, 0.5521, 0.5533, 0.5601,\n",
       "             0.5624, 0.5659, 0.5670, 0.5727, 0.5739, 0.5750, 0.5762, 0.5773, 0.5796,\n",
       "             0.5842, 0.5853, 0.5865, 0.5876, 0.5888, 0.5899, 0.5945, 0.5979, 0.6037,\n",
       "             0.6060, 0.6071, 0.6082, 0.6117, 0.6140, 0.6151, 0.6174, 0.6186, 0.6208,\n",
       "             0.6220, 0.6243, 0.6254, 0.6266, 0.6277, 0.6289, 0.6300, 0.6312, 0.6323,\n",
       "             0.6346, 0.6357, 0.6380, 0.6403, 0.6415, 0.6426, 0.6438, 0.6460, 0.6472,\n",
       "             0.6495, 0.6506, 0.6518, 0.6541, 0.6552, 0.6564, 0.6575, 0.6586, 0.6598,\n",
       "             0.6609, 0.6621, 0.6632, 0.6644, 0.6655, 0.6667, 0.6678, 0.6690, 0.6701,\n",
       "             0.6712, 0.6724, 0.6735, 0.6747, 0.6758, 0.6770, 0.6793, 0.6804, 0.6816,\n",
       "             0.6827, 0.6850, 0.6861, 0.6873, 0.6884, 0.6907, 0.6919, 0.6930, 0.6964,\n",
       "             0.6976, 0.6987, 0.6999, 0.7010, 0.7022, 0.7033, 0.7045, 0.7056, 0.7068,\n",
       "             0.7079, 0.7090, 0.7102, 0.7113, 0.7125, 0.7136, 0.7148, 0.7159, 0.7171,\n",
       "             0.7182, 0.7194, 0.7205, 0.7216, 0.7228, 0.7239, 0.7251, 0.7262, 0.7274,\n",
       "             0.7285, 0.7297, 0.7308, 0.7320, 0.7331, 0.7342, 0.7354, 0.7365, 0.7377,\n",
       "             0.7388, 0.7400, 0.7411, 0.7423, 0.7434, 0.7446, 0.7457, 0.7468, 0.7480,\n",
       "             0.7491, 0.7503, 0.7514, 0.7526, 0.7537, 0.7549, 0.7560, 0.7572, 0.7583,\n",
       "             0.7595, 0.7606, 0.7617, 0.7629, 0.7640, 0.7652, 0.7663, 0.7675, 0.7686,\n",
       "             0.7698, 0.7709, 0.7721, 0.7732, 0.7743, 0.7755, 0.7766, 0.7778, 0.7789,\n",
       "             0.7801, 0.7812, 0.7824, 0.7835, 0.7847, 0.7858, 0.7858, 0.7869, 0.7881,\n",
       "             0.7892, 0.7904, 0.7915, 0.7927, 0.7938, 0.7950, 0.7961, 0.7973, 0.7984,\n",
       "             0.7995, 0.8007, 0.8018, 0.8030, 0.8041, 0.8053, 0.8064, 0.8076, 0.8087,\n",
       "             0.8099, 0.8110, 0.8121, 0.8133, 0.8144, 0.8167, 0.8179, 0.8190, 0.8202,\n",
       "             0.8213, 0.8225, 0.8236, 0.8247, 0.8259, 0.8270, 0.8282, 0.8293, 0.8305,\n",
       "             0.8316, 0.8328, 0.8339, 0.8351, 0.8362, 0.8373, 0.8385, 0.8396, 0.8408,\n",
       "             0.8419, 0.8431, 0.8442, 0.8454, 0.8465, 0.8477, 0.8488, 0.8499, 0.8511,\n",
       "             0.8522, 0.8534, 0.8545, 0.8557, 0.8568, 0.8580, 0.8591, 0.8603, 0.8614,\n",
       "             0.8625, 0.8637, 0.8648, 0.8660, 0.8671, 0.8683, 0.8694, 0.8706, 0.8717,\n",
       "             0.8729, 0.8740, 0.8751, 0.8763, 0.8774, 0.8786, 0.8797, 0.8809, 0.8820,\n",
       "             0.8832, 0.8843, 0.8855, 0.8866, 0.8877, 0.8889, 0.8900, 0.8912, 0.8923,\n",
       "             0.8935, 0.8946, 0.8958, 0.8969, 0.8981, 0.8992, 0.9003, 0.9015, 0.9026,\n",
       "             0.9038, 0.9049, 0.9061, 0.9072, 0.9072, 0.9084, 0.9095, 0.9107, 0.9118,\n",
       "             0.9129, 0.9141, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198, 0.9210, 0.9221,\n",
       "             0.9233, 0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324,\n",
       "             0.9336, 0.9347, 0.9359, 0.9370, 0.9381, 0.9393, 0.9404, 0.9416, 0.9416,\n",
       "             0.9427, 0.9439, 0.9439, 0.9450, 0.9462, 0.9473, 0.9485, 0.9496, 0.9507,\n",
       "             0.9519, 0.9530, 0.9542, 0.9553, 0.9565, 0.9565, 0.9576, 0.9588, 0.9588,\n",
       "             0.9588, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668, 0.9679,\n",
       "             0.9691, 0.9691, 0.9702, 0.9702, 0.9702, 0.9702, 0.9714, 0.9725, 0.9725,\n",
       "             0.9725, 0.9737, 0.9748, 0.9759, 0.9771, 0.9782, 0.9794, 0.9794, 0.9794,\n",
       "             0.9805, 0.9805, 0.9805, 0.9805, 0.9817, 0.9828, 0.9840, 0.9851, 0.9863,\n",
       "             0.9874, 0.9874, 0.9885, 0.9885, 0.9897, 0.9908, 0.9908, 0.9908, 0.9920,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954,\n",
       "             0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9996e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9992e-01,\n",
       "             9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01,\n",
       "             9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "             9.9991e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9990e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9989e-01, 9.9989e-01,\n",
       "             9.9989e-01, 9.9989e-01, 9.9989e-01, 9.9988e-01, 9.9988e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9988e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01, 9.9987e-01,\n",
       "             9.9987e-01, 9.9987e-01, 9.9986e-01, 9.9986e-01, 9.9986e-01, 9.9985e-01,\n",
       "             9.9985e-01, 9.9984e-01, 9.9984e-01, 9.9983e-01, 9.9983e-01, 9.9983e-01,\n",
       "             9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9982e-01, 9.9981e-01, 9.9981e-01,\n",
       "             9.9979e-01, 9.9979e-01, 9.9978e-01, 9.9977e-01, 9.9977e-01, 9.9977e-01,\n",
       "             9.9975e-01, 9.9973e-01, 9.9971e-01, 9.9970e-01, 9.9969e-01, 9.9969e-01,\n",
       "             9.9967e-01, 9.9963e-01, 9.9962e-01, 9.9962e-01, 9.9961e-01, 9.9960e-01,\n",
       "             9.9960e-01, 9.9960e-01, 9.9959e-01, 9.9959e-01, 9.9959e-01, 9.9958e-01,\n",
       "             9.9958e-01, 9.9957e-01, 9.9956e-01, 9.9955e-01, 9.9955e-01, 9.9954e-01,\n",
       "             9.9953e-01, 9.9947e-01, 9.9945e-01, 9.9944e-01, 9.9943e-01, 9.9943e-01,\n",
       "             9.9938e-01, 9.9937e-01, 9.9935e-01, 9.9933e-01, 9.9932e-01, 9.9930e-01,\n",
       "             9.9929e-01, 9.9927e-01, 9.9927e-01, 9.9920e-01, 9.9917e-01, 9.9917e-01,\n",
       "             9.9916e-01, 9.9909e-01, 9.9904e-01, 9.9896e-01, 9.9886e-01, 9.9872e-01,\n",
       "             9.9870e-01, 9.9869e-01, 9.9858e-01, 9.9850e-01, 9.9849e-01, 9.9844e-01,\n",
       "             9.9843e-01, 9.9842e-01, 9.9836e-01, 9.9828e-01, 9.9828e-01, 9.9828e-01,\n",
       "             9.9807e-01, 9.9791e-01, 9.9785e-01, 9.9780e-01, 9.9752e-01, 9.9748e-01,\n",
       "             9.9737e-01, 9.9725e-01, 9.9687e-01, 9.9684e-01, 9.9621e-01, 9.9607e-01,\n",
       "             9.9578e-01, 9.9572e-01, 9.9556e-01, 9.9550e-01, 9.9499e-01, 9.9470e-01,\n",
       "             9.9463e-01, 9.9450e-01, 9.9412e-01, 9.9403e-01, 9.9325e-01, 9.9271e-01,\n",
       "             9.9187e-01, 9.9171e-01, 9.9046e-01, 9.8971e-01, 9.8921e-01, 9.8668e-01,\n",
       "             9.8597e-01, 9.8315e-01, 9.8125e-01, 9.8025e-01, 9.7916e-01, 9.7827e-01,\n",
       "             9.7665e-01, 9.7531e-01, 9.7489e-01, 9.7112e-01, 9.7070e-01, 9.6940e-01,\n",
       "             9.6566e-01, 9.6247e-01, 9.6227e-01, 9.6084e-01, 9.5897e-01, 9.5707e-01,\n",
       "             9.4645e-01, 9.4608e-01, 9.3581e-01, 9.3006e-01, 9.2517e-01, 9.1069e-01,\n",
       "             9.0388e-01, 8.9452e-01, 8.9276e-01, 8.8220e-01, 8.7619e-01, 8.6546e-01,\n",
       "             8.6052e-01, 8.3821e-01, 8.3480e-01, 8.1681e-01, 7.9201e-01, 7.3492e-01,\n",
       "             7.1196e-01, 6.5833e-01, 6.5746e-01, 6.5190e-01, 6.1924e-01, 5.9635e-01,\n",
       "             5.3256e-01, 5.0160e-01, 5.0084e-01, 5.0003e-01, 4.9893e-01, 4.9005e-01,\n",
       "             4.7971e-01, 4.7782e-01, 4.6317e-01, 4.5157e-01, 3.5226e-01, 2.5127e-01,\n",
       "             2.4789e-01, 2.3625e-01, 1.7506e-01, 1.6516e-01, 1.6182e-01, 1.2735e-01,\n",
       "             1.1565e-01, 1.1514e-01, 1.1144e-01, 8.8570e-02, 7.9579e-02, 7.4363e-02,\n",
       "             6.4933e-02, 6.2796e-02, 5.9519e-02, 5.6016e-02, 5.0828e-02, 4.5854e-02,\n",
       "             4.2999e-02, 4.2122e-02, 4.1954e-02, 3.9727e-02, 3.8677e-02, 3.7560e-02,\n",
       "             3.6349e-02, 2.9827e-02, 2.2701e-02, 2.1597e-02, 2.1433e-02, 2.0716e-02,\n",
       "             1.8205e-02, 1.7681e-02, 1.4652e-02, 1.4383e-02, 1.3926e-02, 1.3689e-02,\n",
       "             1.3380e-02, 1.2492e-02, 1.2055e-02, 9.5773e-03, 8.6885e-03, 7.3943e-03,\n",
       "             6.8251e-03, 5.9153e-03, 5.8718e-03, 4.7451e-03, 4.5839e-03, 4.4682e-03,\n",
       "             4.2955e-03, 3.7166e-03, 2.9871e-03, 2.8405e-03, 2.6581e-03, 2.2919e-03,\n",
       "             2.2261e-03, 1.9477e-03, 1.6877e-03, 1.6641e-03, 1.3879e-03, 1.0746e-03,\n",
       "             1.0581e-03, 1.0082e-03, 9.6571e-04, 9.6040e-04, 7.6030e-04, 7.5373e-04,\n",
       "             7.3473e-04, 7.2279e-04, 6.7790e-04, 6.0765e-04, 5.0008e-04, 4.9608e-04,\n",
       "             3.2592e-04, 3.0470e-04, 2.9243e-04, 2.5713e-04, 2.3429e-04, 2.2750e-04,\n",
       "             1.9729e-04, 1.9297e-04, 1.7808e-04, 1.6145e-04, 1.4855e-04, 1.3590e-04,\n",
       "             1.2585e-04, 1.0611e-04, 1.0187e-04, 6.2303e-05, 5.6543e-05, 5.5891e-05,\n",
       "             5.4893e-05, 4.7838e-05, 4.7387e-05, 4.1708e-05, 3.9505e-05, 3.7716e-05,\n",
       "             3.7668e-05, 3.7280e-05, 3.6860e-05, 3.5279e-05, 3.3799e-05, 3.1570e-05,\n",
       "             3.1468e-05, 2.5427e-05, 2.5389e-05, 2.3811e-05, 2.0394e-05, 1.9678e-05,\n",
       "             1.8837e-05, 1.5251e-05, 1.5183e-05, 1.2800e-05, 1.2405e-05, 1.2171e-05,\n",
       "             1.0602e-05, 1.0317e-05, 9.9718e-06, 9.9672e-06, 9.6241e-06, 9.2730e-06,\n",
       "             8.6842e-06, 8.3247e-06, 8.2703e-06, 8.0856e-06, 6.7341e-06, 6.6518e-06,\n",
       "             6.6345e-06, 6.5716e-06, 6.3593e-06, 6.2648e-06, 5.9867e-06, 5.8363e-06,\n",
       "             5.4465e-06, 4.9838e-06, 4.9042e-06, 4.6100e-06, 4.5266e-06, 3.5231e-06,\n",
       "             3.1941e-06, 3.0782e-06, 2.9079e-06, 2.7403e-06, 2.5611e-06, 2.5136e-06,\n",
       "             2.5091e-06, 2.2554e-06, 2.2174e-06, 1.8663e-06, 1.8152e-06, 1.8047e-06,\n",
       "             1.7207e-06, 1.6995e-06, 1.5385e-06, 1.4259e-06, 1.3301e-06, 1.2852e-06,\n",
       "             1.1326e-06, 9.6685e-07, 8.6824e-07, 7.2698e-07, 6.8295e-07, 6.2947e-07,\n",
       "             5.6224e-07, 5.0684e-07, 4.9416e-07, 4.9299e-07, 4.8941e-07, 4.8762e-07,\n",
       "             3.9055e-07, 3.3427e-07, 3.2868e-07, 3.1100e-07, 2.6795e-07, 2.5452e-07,\n",
       "             2.2294e-07, 2.2153e-07, 1.9314e-07, 1.6427e-07, 1.5416e-07, 1.4673e-07,\n",
       "             1.4209e-07, 1.3085e-07, 1.2305e-07, 1.1030e-07, 1.1019e-07, 9.9458e-08,\n",
       "             8.6880e-08, 7.7733e-08, 7.7374e-08, 5.6347e-08, 5.6337e-08, 4.8381e-08,\n",
       "             4.6587e-08, 4.4023e-08, 4.0838e-08, 4.0355e-08, 3.9290e-08, 3.8665e-08,\n",
       "             3.8310e-08, 3.1931e-08, 2.2106e-08, 2.1744e-08, 2.1678e-08, 2.1004e-08,\n",
       "             1.8506e-08, 1.5613e-08, 1.3810e-08, 1.0046e-08, 9.4018e-09, 9.0396e-09,\n",
       "             8.0778e-09, 7.9612e-09, 5.9543e-09, 4.8740e-09, 4.5500e-09, 4.1817e-09,\n",
       "             3.5493e-09, 3.3467e-09, 3.3405e-09, 2.7731e-09, 2.4205e-09, 2.3752e-09,\n",
       "             2.3087e-09, 1.7363e-09, 1.1839e-09, 1.1556e-09, 1.1051e-09, 1.0298e-09,\n",
       "             7.4833e-10, 6.9390e-10, 5.3526e-10, 4.3625e-10, 4.0805e-10, 3.1073e-10,\n",
       "             3.0146e-10, 2.8699e-10, 2.8692e-10, 2.0208e-10, 1.5592e-10, 1.4076e-10,\n",
       "             1.4059e-10, 1.1696e-10, 1.1466e-10, 1.0570e-10, 1.0009e-10, 8.8120e-11,\n",
       "             6.5045e-11, 5.8678e-11, 5.4898e-11, 5.3409e-11, 4.8837e-11, 4.0286e-11,\n",
       "             3.8155e-11, 2.9495e-11, 2.3820e-11, 2.0933e-11, 1.8918e-11, 1.7547e-11,\n",
       "             1.6541e-11, 9.3959e-12, 8.2467e-12, 3.2920e-12, 2.6369e-12, 2.5194e-12,\n",
       "             2.3893e-12, 2.3136e-12, 2.0601e-12, 1.5210e-12, 7.2334e-13, 6.9275e-13,\n",
       "             6.4818e-13, 5.0680e-13, 4.6295e-13, 4.3799e-13, 4.3565e-13, 2.8098e-13,\n",
       "             2.3741e-13, 2.3487e-13, 1.9022e-13, 7.5560e-14, 3.9188e-14, 1.1966e-14,\n",
       "             1.0215e-14, 7.2041e-15, 2.9108e-15, 2.8382e-15, 9.9976e-16, 4.2557e-16,\n",
       "             3.2816e-16, 8.0560e-17, 8.5218e-18, 3.1177e-18])}},\n",
       "   {'fpr': np.float64(0.17763157894736842),\n",
       "    'tpr': np.float64(0.995418098510882),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0033, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0197,\n",
       "             0.0197, 0.0197, 0.0230, 0.0230, 0.0263, 0.0296, 0.0296, 0.0296, 0.0296,\n",
       "             0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296,\n",
       "             0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0296, 0.0329,\n",
       "             0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329, 0.0329,\n",
       "             0.0329, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362,\n",
       "             0.0362, 0.0362, 0.0362, 0.0395, 0.0395, 0.0395, 0.0395, 0.0395, 0.0428,\n",
       "             0.0428, 0.0461, 0.0461, 0.0461, 0.0461, 0.0493, 0.0526, 0.0526, 0.0559,\n",
       "             0.0592, 0.0592, 0.0625, 0.0625, 0.0658, 0.0658, 0.0658, 0.0658, 0.0691,\n",
       "             0.0691, 0.0691, 0.0691, 0.0724, 0.0757, 0.0789, 0.0822, 0.0855, 0.0888,\n",
       "             0.0888, 0.0921, 0.0921, 0.0954, 0.0954, 0.0954, 0.0987, 0.1020, 0.1053,\n",
       "             0.1053, 0.1053, 0.1086, 0.1118, 0.1151, 0.1151, 0.1184, 0.1217, 0.1250,\n",
       "             0.1283, 0.1316, 0.1349, 0.1349, 0.1382, 0.1414, 0.1447, 0.1480, 0.1513,\n",
       "             0.1546, 0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1743, 0.1776,\n",
       "             0.1809, 0.1842, 0.1875, 0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072,\n",
       "             0.2105, 0.2138, 0.2171, 0.2204, 0.2237, 0.2270, 0.2270, 0.2303, 0.2336,\n",
       "             0.2368, 0.2401, 0.2434, 0.2467, 0.2500, 0.2533, 0.2566, 0.2599, 0.2632,\n",
       "             0.2664, 0.2697, 0.2730, 0.2763, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928,\n",
       "             0.2961, 0.2993, 0.3026, 0.3059, 0.3092, 0.3092, 0.3125, 0.3158, 0.3191,\n",
       "             0.3224, 0.3257, 0.3289, 0.3322, 0.3355, 0.3388, 0.3421, 0.3454, 0.3487,\n",
       "             0.3520, 0.3553, 0.3586, 0.3618, 0.3651, 0.3684, 0.3717, 0.3750, 0.3783,\n",
       "             0.3816, 0.3849, 0.3882, 0.3914, 0.3947, 0.3947, 0.3980, 0.4013, 0.4046,\n",
       "             0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243, 0.4276, 0.4309, 0.4342,\n",
       "             0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539, 0.4572, 0.4605, 0.4638,\n",
       "             0.4671, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836, 0.4868, 0.4901,\n",
       "             0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132, 0.5164, 0.5197,\n",
       "             0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428, 0.5461, 0.5493,\n",
       "             0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724, 0.5757, 0.5789,\n",
       "             0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020, 0.6053, 0.6086,\n",
       "             0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316, 0.6349, 0.6382,\n",
       "             0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612, 0.6645, 0.6678,\n",
       "             0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908, 0.6941, 0.6974,\n",
       "             0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204, 0.7237, 0.7270,\n",
       "             0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500, 0.7533, 0.7566,\n",
       "             0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796, 0.7829, 0.7862,\n",
       "             0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092, 0.8125, 0.8158,\n",
       "             0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388, 0.8421, 0.8454,\n",
       "             0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684, 0.8717, 0.8750,\n",
       "             0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980, 0.9013, 0.9046,\n",
       "             0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276, 0.9309, 0.9342,\n",
       "             0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572, 0.9605, 0.9638,\n",
       "             0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868, 0.9901, 0.9934,\n",
       "             0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.6976, 0.7709, 0.7984, 0.8190, 0.8351, 0.8419, 0.8545, 0.8580,\n",
       "             0.8648, 0.8694, 0.8751, 0.8774, 0.8809, 0.8820, 0.8843, 0.8866, 0.8877,\n",
       "             0.8889, 0.8900, 0.8923, 0.8935, 0.8946, 0.8958, 0.8969, 0.8981, 0.8992,\n",
       "             0.9015, 0.9038, 0.9049, 0.9061, 0.9072, 0.9084, 0.9095, 0.9118, 0.9118,\n",
       "             0.9129, 0.9152, 0.9164, 0.9175, 0.9187, 0.9187, 0.9198, 0.9221, 0.9233,\n",
       "             0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324, 0.9336,\n",
       "             0.9347, 0.9359, 0.9370, 0.9381, 0.9393, 0.9404, 0.9416, 0.9427, 0.9427,\n",
       "             0.9439, 0.9450, 0.9462, 0.9473, 0.9485, 0.9496, 0.9507, 0.9519, 0.9530,\n",
       "             0.9542, 0.9542, 0.9553, 0.9565, 0.9576, 0.9588, 0.9599, 0.9611, 0.9622,\n",
       "             0.9633, 0.9645, 0.9656, 0.9656, 0.9668, 0.9679, 0.9691, 0.9702, 0.9702,\n",
       "             0.9714, 0.9714, 0.9725, 0.9737, 0.9748, 0.9748, 0.9748, 0.9759, 0.9759,\n",
       "             0.9759, 0.9771, 0.9771, 0.9782, 0.9782, 0.9794, 0.9805, 0.9817, 0.9817,\n",
       "             0.9828, 0.9840, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851, 0.9851,\n",
       "             0.9863, 0.9863, 0.9874, 0.9874, 0.9885, 0.9897, 0.9897, 0.9897, 0.9897,\n",
       "             0.9908, 0.9920, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01, 9.9997e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9995e-01,\n",
       "             9.9995e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9992e-01, 9.9991e-01,\n",
       "             9.9990e-01, 9.9990e-01, 9.9989e-01, 9.9987e-01, 9.9987e-01, 9.9986e-01,\n",
       "             9.9982e-01, 9.9977e-01, 9.9976e-01, 9.9964e-01, 9.9961e-01, 9.9959e-01,\n",
       "             9.9959e-01, 9.9957e-01, 9.9954e-01, 9.9948e-01, 9.9947e-01, 9.9944e-01,\n",
       "             9.9934e-01, 9.9932e-01, 9.9917e-01, 9.9909e-01, 9.9909e-01, 9.9896e-01,\n",
       "             9.9894e-01, 9.9893e-01, 9.9885e-01, 9.9880e-01, 9.9864e-01, 9.9844e-01,\n",
       "             9.9826e-01, 9.9817e-01, 9.9798e-01, 9.9765e-01, 9.9721e-01, 9.9691e-01,\n",
       "             9.9578e-01, 9.9569e-01, 9.9409e-01, 9.9367e-01, 9.9283e-01, 9.9182e-01,\n",
       "             9.9160e-01, 9.8883e-01, 9.8622e-01, 9.8500e-01, 9.8168e-01, 9.7575e-01,\n",
       "             9.7484e-01, 9.7146e-01, 9.7013e-01, 9.6977e-01, 9.6651e-01, 9.6627e-01,\n",
       "             9.5161e-01, 9.3841e-01, 9.3549e-01, 9.2404e-01, 8.8920e-01, 8.7282e-01,\n",
       "             8.6762e-01, 8.4946e-01, 8.1768e-01, 7.9154e-01, 6.7849e-01, 6.7836e-01,\n",
       "             6.7544e-01, 6.6965e-01, 6.3907e-01, 6.3793e-01, 6.3178e-01, 6.0931e-01,\n",
       "             5.6452e-01, 5.2369e-01, 5.1456e-01, 3.6444e-01, 2.7300e-01, 2.2936e-01,\n",
       "             2.1551e-01, 1.9631e-01, 1.6401e-01, 1.6240e-01, 1.5754e-01, 1.5373e-01,\n",
       "             1.4891e-01, 1.4305e-01, 1.3626e-01, 1.2417e-01, 1.1138e-01, 1.0466e-01,\n",
       "             1.0132e-01, 8.4525e-02, 7.7086e-02, 7.3245e-02, 7.0051e-02, 5.5668e-02,\n",
       "             4.3810e-02, 4.0063e-02, 4.0026e-02, 3.5353e-02, 2.9336e-02, 2.8931e-02,\n",
       "             2.7630e-02, 2.6995e-02, 2.6353e-02, 2.1657e-02, 2.1613e-02, 2.0801e-02,\n",
       "             1.8661e-02, 1.4554e-02, 1.2250e-02, 1.1615e-02, 1.0772e-02, 1.0470e-02,\n",
       "             9.1324e-03, 8.6019e-03, 7.4008e-03, 5.9201e-03, 5.5595e-03, 5.4560e-03,\n",
       "             5.1302e-03, 3.8553e-03, 3.3107e-03, 3.0854e-03, 2.9916e-03, 2.3762e-03,\n",
       "             2.1386e-03, 2.0350e-03, 1.9745e-03, 1.9638e-03, 1.7847e-03, 1.2395e-03,\n",
       "             1.1996e-03, 9.1967e-04, 9.0755e-04, 8.5835e-04, 8.5585e-04, 7.2024e-04,\n",
       "             6.6341e-04, 6.2517e-04, 6.2282e-04, 5.9627e-04, 5.1199e-04, 4.9121e-04,\n",
       "             4.8410e-04, 4.2461e-04, 4.1513e-04, 4.1136e-04, 4.0817e-04, 4.0380e-04,\n",
       "             3.6554e-04, 2.8119e-04, 2.5048e-04, 2.4316e-04, 1.9898e-04, 1.7218e-04,\n",
       "             1.4106e-04, 1.3682e-04, 1.2167e-04, 1.1666e-04, 1.1217e-04, 8.5247e-05,\n",
       "             7.5743e-05, 6.7267e-05, 5.6394e-05, 5.3400e-05, 4.3567e-05, 3.8956e-05,\n",
       "             3.8225e-05, 3.8156e-05, 3.6494e-05, 3.6192e-05, 3.3967e-05, 3.1056e-05,\n",
       "             2.9470e-05, 2.3413e-05, 2.2875e-05, 2.2448e-05, 2.1889e-05, 2.0838e-05,\n",
       "             2.0727e-05, 1.9019e-05, 1.7213e-05, 1.6679e-05, 1.4648e-05, 1.4597e-05,\n",
       "             1.3537e-05, 1.3464e-05, 1.1701e-05, 1.1236e-05, 1.0813e-05, 1.0502e-05,\n",
       "             9.0898e-06, 8.8345e-06, 7.1619e-06, 5.7696e-06, 5.6213e-06, 5.1020e-06,\n",
       "             4.4070e-06, 4.1961e-06, 4.1686e-06, 3.5947e-06, 3.4242e-06, 2.9045e-06,\n",
       "             2.8752e-06, 2.7840e-06, 2.7233e-06, 2.6858e-06, 2.5552e-06, 2.2109e-06,\n",
       "             2.1371e-06, 2.0015e-06, 1.7796e-06, 1.7335e-06, 1.5868e-06, 1.5083e-06,\n",
       "             1.4604e-06, 1.3229e-06, 1.1445e-06, 1.1409e-06, 1.0957e-06, 1.0242e-06,\n",
       "             8.0613e-07, 7.1527e-07, 7.0778e-07, 6.1398e-07, 5.7409e-07, 5.6036e-07,\n",
       "             4.4428e-07, 3.1682e-07, 2.7270e-07, 1.3086e-07, 1.1890e-07, 9.0668e-08,\n",
       "             8.3845e-08, 8.3323e-08, 7.6464e-08, 7.3348e-08, 6.6068e-08, 6.4276e-08,\n",
       "             5.2150e-08, 5.0825e-08, 4.7216e-08, 4.6207e-08, 4.5815e-08, 4.4705e-08,\n",
       "             3.9971e-08, 3.8692e-08, 2.7682e-08, 2.2107e-08, 1.9312e-08, 1.8963e-08,\n",
       "             1.7567e-08, 1.4953e-08, 1.4002e-08, 8.7553e-09, 8.0542e-09, 7.6910e-09,\n",
       "             6.8148e-09, 3.4485e-09, 3.3473e-09, 2.8710e-09, 2.5653e-09, 2.2644e-09,\n",
       "             2.0469e-09, 1.8982e-09, 1.7965e-09, 1.2992e-09, 1.1736e-09, 9.0156e-10,\n",
       "             8.5375e-10, 7.7809e-10, 7.6770e-10, 5.5510e-10, 5.0530e-10, 5.0210e-10,\n",
       "             3.9234e-10, 3.6347e-10, 3.4144e-10, 2.7172e-10, 2.5942e-10, 2.5756e-10,\n",
       "             2.3960e-10, 1.3474e-10, 1.2373e-10, 1.0025e-10, 6.1434e-11, 5.4665e-11,\n",
       "             4.8749e-11, 4.3882e-11, 4.1858e-11, 3.9763e-11, 3.8400e-11, 3.1821e-11,\n",
       "             2.7596e-11, 2.4727e-11, 2.1759e-11, 2.1354e-11, 1.5169e-11, 5.3018e-12,\n",
       "             4.8904e-12, 3.6403e-12, 3.5952e-12, 1.9553e-12, 1.6336e-12, 1.4189e-12,\n",
       "             1.0418e-12, 7.4736e-13, 5.7341e-13, 4.0922e-13, 3.4555e-13, 3.0602e-13,\n",
       "             1.6929e-13, 1.4972e-13, 1.0869e-13, 1.0820e-13, 1.0252e-13, 3.1188e-14,\n",
       "             3.0694e-14, 2.7041e-14, 1.0342e-14, 9.1505e-15, 1.9945e-15, 5.2114e-16,\n",
       "             2.8243e-16, 9.2959e-17, 4.4538e-17, 2.3259e-17, 3.1942e-21])}},\n",
       "   {'fpr': np.float64(0.13157894736842105),\n",
       "    'tpr': np.float64(0.993127147766323),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0099, 0.0099, 0.0099, 0.0132, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "             0.0164, 0.0164, 0.0164, 0.0164, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0230, 0.0230, 0.0230, 0.0263, 0.0263, 0.0263, 0.0296, 0.0329,\n",
       "             0.0329, 0.0362, 0.0362, 0.0362, 0.0395, 0.0395, 0.0395, 0.0428, 0.0428,\n",
       "             0.0428, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461,\n",
       "             0.0461, 0.0461, 0.0461, 0.0461, 0.0493, 0.0526, 0.0526, 0.0526, 0.0559,\n",
       "             0.0559, 0.0559, 0.0592, 0.0625, 0.0625, 0.0625, 0.0658, 0.0658, 0.0658,\n",
       "             0.0691, 0.0691, 0.0724, 0.0757, 0.0757, 0.0757, 0.0757, 0.0789, 0.0789,\n",
       "             0.0822, 0.0822, 0.0822, 0.0855, 0.0888, 0.0888, 0.0888, 0.0888, 0.0921,\n",
       "             0.0954, 0.0987, 0.1020, 0.1020, 0.1053, 0.1086, 0.1118, 0.1151, 0.1184,\n",
       "             0.1217, 0.1250, 0.1283, 0.1316, 0.1349, 0.1382, 0.1414, 0.1447, 0.1480,\n",
       "             0.1513, 0.1546, 0.1579, 0.1612, 0.1645, 0.1678, 0.1711, 0.1743, 0.1776,\n",
       "             0.1809, 0.1809, 0.1842, 0.1875, 0.1908, 0.1941, 0.1941, 0.1974, 0.2007,\n",
       "             0.2039, 0.2072, 0.2105, 0.2138, 0.2171, 0.2204, 0.2237, 0.2237, 0.2270,\n",
       "             0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467, 0.2500, 0.2533, 0.2566,\n",
       "             0.2599, 0.2632, 0.2632, 0.2664, 0.2664, 0.2697, 0.2730, 0.2763, 0.2796,\n",
       "             0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2961, 0.2993, 0.3026, 0.3059,\n",
       "             0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322, 0.3355,\n",
       "             0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618, 0.3651,\n",
       "             0.3684, 0.3717, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882, 0.3914, 0.3947,\n",
       "             0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178, 0.4211, 0.4243,\n",
       "             0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474, 0.4507, 0.4539,\n",
       "             0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770, 0.4803, 0.4836,\n",
       "             0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066, 0.5099, 0.5132,\n",
       "             0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362, 0.5395, 0.5428,\n",
       "             0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658, 0.5691, 0.5724,\n",
       "             0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954, 0.5987, 0.6020,\n",
       "             0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250, 0.6283, 0.6316,\n",
       "             0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546, 0.6579, 0.6612,\n",
       "             0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842, 0.6875, 0.6908,\n",
       "             0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138, 0.7171, 0.7204,\n",
       "             0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434, 0.7467, 0.7500,\n",
       "             0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730, 0.7763, 0.7796,\n",
       "             0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026, 0.8059, 0.8092,\n",
       "             0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322, 0.8355, 0.8388,\n",
       "             0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618, 0.8651, 0.8684,\n",
       "             0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914, 0.8947, 0.8980,\n",
       "             0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211, 0.9243, 0.9276,\n",
       "             0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507, 0.9539, 0.9572,\n",
       "             0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803, 0.9836, 0.9868,\n",
       "             0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7950, 0.8282, 0.8373, 0.8499, 0.8580, 0.8648, 0.8683, 0.8729,\n",
       "             0.8763, 0.8797, 0.8832, 0.8843, 0.8855, 0.8866, 0.8900, 0.8923, 0.8935,\n",
       "             0.8958, 0.8981, 0.8992, 0.9015, 0.9061, 0.9084, 0.9095, 0.9118, 0.9129,\n",
       "             0.9141, 0.9152, 0.9164, 0.9175, 0.9187, 0.9198, 0.9210, 0.9221, 0.9233,\n",
       "             0.9244, 0.9255, 0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324, 0.9336,\n",
       "             0.9347, 0.9359, 0.9370, 0.9381, 0.9393, 0.9404, 0.9416, 0.9427, 0.9439,\n",
       "             0.9450, 0.9450, 0.9462, 0.9473, 0.9473, 0.9485, 0.9496, 0.9507, 0.9507,\n",
       "             0.9519, 0.9519, 0.9530, 0.9542, 0.9542, 0.9553, 0.9565, 0.9565, 0.9576,\n",
       "             0.9588, 0.9588, 0.9599, 0.9611, 0.9622, 0.9633, 0.9645, 0.9656, 0.9668,\n",
       "             0.9679, 0.9691, 0.9702, 0.9714, 0.9714, 0.9714, 0.9725, 0.9737, 0.9737,\n",
       "             0.9748, 0.9759, 0.9759, 0.9759, 0.9771, 0.9782, 0.9782, 0.9794, 0.9805,\n",
       "             0.9805, 0.9817, 0.9817, 0.9817, 0.9828, 0.9840, 0.9851, 0.9851, 0.9863,\n",
       "             0.9863, 0.9874, 0.9885, 0.9885, 0.9885, 0.9897, 0.9908, 0.9920, 0.9920,\n",
       "             0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931, 0.9931,\n",
       "             0.9931, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9954, 0.9954, 0.9954,\n",
       "             0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9954, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9977, 0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01, 9.9996e-01,\n",
       "             9.9996e-01, 9.9996e-01, 9.9996e-01, 9.9995e-01, 9.9994e-01, 9.9993e-01,\n",
       "             9.9993e-01, 9.9993e-01, 9.9993e-01, 9.9991e-01, 9.9989e-01, 9.9988e-01,\n",
       "             9.9988e-01, 9.9987e-01, 9.9986e-01, 9.9982e-01, 9.9977e-01, 9.9976e-01,\n",
       "             9.9975e-01, 9.9974e-01, 9.9967e-01, 9.9966e-01, 9.9964e-01, 9.9950e-01,\n",
       "             9.9949e-01, 9.9944e-01, 9.9919e-01, 9.9916e-01, 9.9902e-01, 9.9874e-01,\n",
       "             9.9852e-01, 9.9831e-01, 9.9830e-01, 9.9781e-01, 9.9735e-01, 9.9702e-01,\n",
       "             9.9694e-01, 9.9676e-01, 9.9634e-01, 9.9609e-01, 9.9406e-01, 9.9389e-01,\n",
       "             9.9335e-01, 9.9309e-01, 9.9234e-01, 9.9131e-01, 9.9098e-01, 9.8872e-01,\n",
       "             9.8518e-01, 9.8264e-01, 9.7291e-01, 9.5423e-01, 9.5342e-01, 9.5060e-01,\n",
       "             9.4909e-01, 9.4882e-01, 9.4556e-01, 9.4061e-01, 9.3727e-01, 9.1842e-01,\n",
       "             9.0650e-01, 8.8941e-01, 8.7347e-01, 8.6123e-01, 8.6108e-01, 8.5113e-01,\n",
       "             7.7457e-01, 7.3035e-01, 6.6470e-01, 6.6446e-01, 6.2278e-01, 5.7795e-01,\n",
       "             5.6275e-01, 5.5854e-01, 5.2411e-01, 5.0130e-01, 4.5169e-01, 4.3158e-01,\n",
       "             3.6414e-01, 2.1954e-01, 1.9185e-01, 1.4197e-01, 1.3954e-01, 1.3597e-01,\n",
       "             9.6032e-02, 8.4016e-02, 5.2165e-02, 4.8580e-02, 4.5914e-02, 2.8372e-02,\n",
       "             2.5319e-02, 1.8770e-02, 1.8338e-02, 1.7950e-02, 1.4605e-02, 1.4549e-02,\n",
       "             1.3584e-02, 1.1691e-02, 1.0098e-02, 9.7631e-03, 8.8883e-03, 7.8653e-03,\n",
       "             7.0442e-03, 5.2857e-03, 4.5150e-03, 3.6618e-03, 2.6304e-03, 2.2645e-03,\n",
       "             1.8807e-03, 1.4082e-03, 9.3250e-04, 9.0116e-04, 8.2317e-04, 7.7757e-04,\n",
       "             5.1538e-04, 4.1842e-04, 3.8197e-04, 3.5731e-04, 2.5923e-04, 2.5755e-04,\n",
       "             1.2145e-04, 1.1065e-04, 1.0940e-04, 8.7445e-05, 7.9692e-05, 7.0703e-05,\n",
       "             4.8156e-05, 3.9831e-05, 3.7507e-05, 3.7225e-05, 2.3282e-05, 2.2304e-05,\n",
       "             2.1540e-05, 2.1371e-05, 2.0706e-05, 1.9575e-05, 1.8770e-05, 1.6750e-05,\n",
       "             1.5349e-05, 1.3825e-05, 1.0113e-05, 9.2036e-06, 8.5331e-06, 7.7266e-06,\n",
       "             7.1806e-06, 5.8582e-06, 5.4885e-06, 5.1387e-06, 4.8250e-06, 4.0289e-06,\n",
       "             3.1569e-06, 3.1511e-06, 2.8240e-06, 2.6724e-06, 2.5671e-06, 2.1737e-06,\n",
       "             1.7429e-06, 9.9936e-07, 8.1196e-07, 8.0434e-07, 6.4418e-07, 5.8661e-07,\n",
       "             5.7933e-07, 5.7566e-07, 5.4391e-07, 5.3248e-07, 5.0547e-07, 4.6375e-07,\n",
       "             4.2435e-07, 3.6169e-07, 3.3805e-07, 3.3455e-07, 3.3277e-07, 3.0262e-07,\n",
       "             2.8188e-07, 2.3830e-07, 2.0680e-07, 1.9049e-07, 1.6427e-07, 1.2594e-07,\n",
       "             1.1840e-07, 8.0713e-08, 7.8793e-08, 7.8782e-08, 5.7112e-08, 5.3930e-08,\n",
       "             3.6286e-08, 3.4501e-08, 3.2654e-08, 2.5635e-08, 2.2616e-08, 2.0641e-08,\n",
       "             1.9872e-08, 1.8969e-08, 1.8116e-08, 1.6682e-08, 1.6558e-08, 1.5859e-08,\n",
       "             1.5312e-08, 1.1135e-08, 1.1021e-08, 1.0730e-08, 8.9364e-09, 8.4541e-09,\n",
       "             7.8123e-09, 7.3091e-09, 7.1136e-09, 6.2477e-09, 6.1086e-09, 5.9087e-09,\n",
       "             5.8897e-09, 5.5949e-09, 5.0585e-09, 4.7929e-09, 4.4455e-09, 3.9121e-09,\n",
       "             3.3342e-09, 3.2197e-09, 2.8984e-09, 2.8250e-09, 2.7219e-09, 1.7912e-09,\n",
       "             1.5696e-09, 1.3134e-09, 1.1925e-09, 1.1780e-09, 1.0732e-09, 9.8221e-10,\n",
       "             9.6175e-10, 8.2748e-10, 7.9363e-10, 6.0986e-10, 5.9861e-10, 5.7320e-10,\n",
       "             4.1987e-10, 3.6584e-10, 2.9200e-10, 2.5496e-10, 2.4972e-10, 2.2540e-10,\n",
       "             1.5285e-10, 1.3122e-10, 1.2483e-10, 1.1313e-10, 9.6272e-11, 7.7510e-11,\n",
       "             7.3713e-11, 6.4173e-11, 5.8209e-11, 5.7305e-11, 5.0626e-11, 4.4844e-11,\n",
       "             4.2383e-11, 4.0488e-11, 4.0139e-11, 3.4055e-11, 2.7292e-11, 2.5550e-11,\n",
       "             2.4468e-11, 2.2829e-11, 1.5903e-11, 1.3712e-11, 1.2224e-11, 1.1257e-11,\n",
       "             9.6460e-12, 7.7197e-12, 5.6371e-12, 4.5658e-12, 3.2727e-12, 3.2019e-12,\n",
       "             2.7752e-12, 2.5617e-12, 1.7159e-12, 1.5845e-12, 1.5565e-12, 1.4828e-12,\n",
       "             1.1418e-12, 1.0255e-12, 9.2246e-13, 8.3785e-13, 7.7438e-13, 7.1671e-13,\n",
       "             6.5876e-13, 6.5564e-13, 5.7458e-13, 5.6083e-13, 4.9392e-13, 4.4118e-13,\n",
       "             4.1608e-13, 3.8826e-13, 3.4182e-13, 3.2735e-13, 2.9248e-13, 2.4837e-13,\n",
       "             2.0433e-13, 1.8641e-13, 1.4622e-13, 7.7741e-14, 7.3269e-14, 7.2671e-14,\n",
       "             7.1698e-14, 6.9794e-14, 6.6237e-14, 6.5352e-14, 4.4759e-14, 3.9862e-14,\n",
       "             2.4474e-14, 2.0791e-14, 1.7938e-14, 1.5844e-14, 6.2983e-15, 5.6475e-15,\n",
       "             3.6196e-15, 3.3536e-15, 2.6120e-15, 2.1364e-15, 1.3300e-15, 1.2471e-15,\n",
       "             1.2438e-15, 1.2341e-15, 1.0267e-15, 1.0108e-15, 8.4275e-16, 5.8087e-16,\n",
       "             5.4593e-16, 1.5754e-16, 1.4745e-16, 9.9001e-17, 5.6980e-17, 5.5530e-17,\n",
       "             2.6460e-17, 1.6027e-17, 1.5665e-17, 9.0270e-18, 4.0013e-18, 1.6302e-18,\n",
       "             1.1097e-18, 9.5623e-19, 4.6432e-19, 2.4010e-19, 1.1908e-19, 3.6979e-21,\n",
       "             1.3850e-22, 1.4794e-23, 1.2177e-23, 3.2834e-24])}},\n",
       "   {'fpr': np.float64(0.05592105263157895),\n",
       "    'tpr': np.float64(0.9782359679266895),\n",
       "    'model': LitSimpleCNN(\n",
       "      (model): SimpleCNN(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU()\n",
       "          (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (10): ReLU()\n",
       "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "        (relu_fc): ReLU()\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "      (criterion): BCEWithLogitsLoss()\n",
       "      (train_accuracy): BinaryAccuracy()\n",
       "      (val_accuracy): BinaryAccuracy()\n",
       "      (val_auc): BinaryAUROC()\n",
       "      (test_accuracy): BinaryAccuracy()\n",
       "      (test_auc): BinaryAUROC()\n",
       "      (test_precision): BinaryPrecision()\n",
       "      (test_recall): BinaryRecall()\n",
       "      (test_f1): BinaryF1Score()\n",
       "    ),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
       "             0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
       "             0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
       "             0.0099, 0.0099, 0.0099, 0.0099, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
       "             0.0132, 0.0132, 0.0132, 0.0164, 0.0164, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "             0.0197, 0.0230, 0.0230, 0.0230, 0.0263, 0.0263, 0.0263, 0.0263, 0.0296,\n",
       "             0.0329, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0362, 0.0395, 0.0428,\n",
       "             0.0428, 0.0461, 0.0493, 0.0526, 0.0559, 0.0559, 0.0592, 0.0592, 0.0592,\n",
       "             0.0625, 0.0625, 0.0658, 0.0658, 0.0658, 0.0658, 0.0691, 0.0724, 0.0757,\n",
       "             0.0789, 0.0789, 0.0789, 0.0789, 0.0789, 0.0822, 0.0855, 0.0855, 0.0888,\n",
       "             0.0921, 0.0954, 0.0954, 0.0987, 0.1020, 0.1020, 0.1053, 0.1053, 0.1086,\n",
       "             0.1118, 0.1151, 0.1184, 0.1217, 0.1250, 0.1283, 0.1316, 0.1349, 0.1382,\n",
       "             0.1382, 0.1414, 0.1414, 0.1447, 0.1480, 0.1513, 0.1546, 0.1579, 0.1612,\n",
       "             0.1645, 0.1678, 0.1711, 0.1743, 0.1776, 0.1809, 0.1809, 0.1842, 0.1875,\n",
       "             0.1908, 0.1941, 0.1974, 0.2007, 0.2039, 0.2072, 0.2105, 0.2138, 0.2171,\n",
       "             0.2204, 0.2237, 0.2270, 0.2303, 0.2336, 0.2368, 0.2401, 0.2434, 0.2467,\n",
       "             0.2500, 0.2533, 0.2566, 0.2599, 0.2632, 0.2664, 0.2697, 0.2730, 0.2763,\n",
       "             0.2796, 0.2796, 0.2829, 0.2862, 0.2895, 0.2928, 0.2961, 0.2993, 0.3026,\n",
       "             0.3059, 0.3092, 0.3125, 0.3158, 0.3191, 0.3224, 0.3257, 0.3289, 0.3322,\n",
       "             0.3355, 0.3388, 0.3421, 0.3454, 0.3487, 0.3520, 0.3553, 0.3586, 0.3618,\n",
       "             0.3651, 0.3684, 0.3717, 0.3750, 0.3750, 0.3783, 0.3816, 0.3849, 0.3882,\n",
       "             0.3914, 0.3947, 0.3980, 0.4013, 0.4046, 0.4079, 0.4112, 0.4145, 0.4178,\n",
       "             0.4211, 0.4243, 0.4276, 0.4309, 0.4342, 0.4375, 0.4408, 0.4441, 0.4474,\n",
       "             0.4507, 0.4539, 0.4572, 0.4605, 0.4638, 0.4671, 0.4704, 0.4737, 0.4770,\n",
       "             0.4803, 0.4836, 0.4868, 0.4901, 0.4934, 0.4967, 0.5000, 0.5033, 0.5066,\n",
       "             0.5099, 0.5132, 0.5164, 0.5197, 0.5230, 0.5263, 0.5296, 0.5329, 0.5362,\n",
       "             0.5395, 0.5428, 0.5461, 0.5493, 0.5526, 0.5559, 0.5592, 0.5625, 0.5658,\n",
       "             0.5691, 0.5724, 0.5757, 0.5789, 0.5822, 0.5855, 0.5888, 0.5921, 0.5954,\n",
       "             0.5987, 0.6020, 0.6053, 0.6086, 0.6118, 0.6151, 0.6184, 0.6217, 0.6250,\n",
       "             0.6283, 0.6316, 0.6349, 0.6382, 0.6414, 0.6447, 0.6480, 0.6513, 0.6546,\n",
       "             0.6579, 0.6612, 0.6645, 0.6678, 0.6711, 0.6743, 0.6776, 0.6809, 0.6842,\n",
       "             0.6875, 0.6908, 0.6941, 0.6974, 0.7007, 0.7039, 0.7072, 0.7105, 0.7138,\n",
       "             0.7171, 0.7204, 0.7237, 0.7270, 0.7303, 0.7336, 0.7368, 0.7401, 0.7434,\n",
       "             0.7467, 0.7500, 0.7533, 0.7566, 0.7599, 0.7632, 0.7664, 0.7697, 0.7730,\n",
       "             0.7763, 0.7796, 0.7829, 0.7862, 0.7895, 0.7928, 0.7961, 0.7993, 0.8026,\n",
       "             0.8059, 0.8092, 0.8125, 0.8158, 0.8191, 0.8224, 0.8257, 0.8289, 0.8322,\n",
       "             0.8355, 0.8388, 0.8421, 0.8454, 0.8487, 0.8520, 0.8553, 0.8586, 0.8618,\n",
       "             0.8651, 0.8684, 0.8717, 0.8750, 0.8783, 0.8816, 0.8849, 0.8882, 0.8914,\n",
       "             0.8947, 0.8980, 0.9013, 0.9046, 0.9079, 0.9112, 0.9145, 0.9178, 0.9211,\n",
       "             0.9243, 0.9276, 0.9309, 0.9342, 0.9375, 0.9408, 0.9441, 0.9474, 0.9507,\n",
       "             0.9539, 0.9572, 0.9605, 0.9638, 0.9671, 0.9704, 0.9737, 0.9770, 0.9803,\n",
       "             0.9836, 0.9868, 0.9901, 0.9934, 0.9967, 1.0000]),\n",
       "     'tpr': tensor([0.0000, 0.7801, 0.8087, 0.8236, 0.8259, 0.8316, 0.8362, 0.8419, 0.8465,\n",
       "             0.8499, 0.8511, 0.8522, 0.8534, 0.8557, 0.8580, 0.8603, 0.8603, 0.8614,\n",
       "             0.8625, 0.8637, 0.8648, 0.8660, 0.8683, 0.8706, 0.8717, 0.8729, 0.8740,\n",
       "             0.8751, 0.8763, 0.8786, 0.8797, 0.8809, 0.8820, 0.8820, 0.8832, 0.8843,\n",
       "             0.8855, 0.8866, 0.8877, 0.8889, 0.8912, 0.8923, 0.8935, 0.8946, 0.8958,\n",
       "             0.8969, 0.8981, 0.8992, 0.9003, 0.9015, 0.9026, 0.9038, 0.9049, 0.9049,\n",
       "             0.9061, 0.9072, 0.9084, 0.9095, 0.9107, 0.9118, 0.9129, 0.9141, 0.9152,\n",
       "             0.9164, 0.9175, 0.9187, 0.9198, 0.9210, 0.9221, 0.9233, 0.9244, 0.9255,\n",
       "             0.9267, 0.9278, 0.9290, 0.9301, 0.9313, 0.9324, 0.9336, 0.9347, 0.9359,\n",
       "             0.9370, 0.9381, 0.9393, 0.9404, 0.9416, 0.9427, 0.9439, 0.9450, 0.9462,\n",
       "             0.9473, 0.9485, 0.9496, 0.9507, 0.9507, 0.9519, 0.9530, 0.9542, 0.9553,\n",
       "             0.9565, 0.9576, 0.9588, 0.9588, 0.9599, 0.9599, 0.9611, 0.9622, 0.9633,\n",
       "             0.9645, 0.9645, 0.9656, 0.9668, 0.9668, 0.9679, 0.9691, 0.9702, 0.9702,\n",
       "             0.9702, 0.9702, 0.9714, 0.9725, 0.9737, 0.9748, 0.9759, 0.9759, 0.9759,\n",
       "             0.9771, 0.9771, 0.9771, 0.9771, 0.9771, 0.9782, 0.9782, 0.9794, 0.9805,\n",
       "             0.9805, 0.9817, 0.9817, 0.9828, 0.9840, 0.9851, 0.9851, 0.9851, 0.9851,\n",
       "             0.9851, 0.9863, 0.9874, 0.9885, 0.9897, 0.9897, 0.9897, 0.9908, 0.9908,\n",
       "             0.9908, 0.9908, 0.9920, 0.9920, 0.9920, 0.9931, 0.9931, 0.9943, 0.9943,\n",
       "             0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943, 0.9943,\n",
       "             0.9954, 0.9954, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966,\n",
       "             0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9966, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977, 0.9977,\n",
       "             0.9977, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989, 0.9989,\n",
       "             0.9989, 0.9989, 0.9989, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "             1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "     'thresholds': tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "             1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01,\n",
       "             9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9998e-01, 9.9997e-01,\n",
       "             9.9997e-01, 9.9997e-01, 9.9996e-01, 9.9995e-01, 9.9995e-01, 9.9994e-01,\n",
       "             9.9994e-01, 9.9994e-01, 9.9993e-01, 9.9992e-01, 9.9991e-01, 9.9990e-01,\n",
       "             9.9989e-01, 9.9985e-01, 9.9985e-01, 9.9985e-01, 9.9984e-01, 9.9981e-01,\n",
       "             9.9980e-01, 9.9979e-01, 9.9978e-01, 9.9976e-01, 9.9976e-01, 9.9974e-01,\n",
       "             9.9965e-01, 9.9964e-01, 9.9963e-01, 9.9963e-01, 9.9963e-01, 9.9962e-01,\n",
       "             9.9955e-01, 9.9948e-01, 9.9938e-01, 9.9936e-01, 9.9931e-01, 9.9918e-01,\n",
       "             9.9909e-01, 9.9865e-01, 9.9833e-01, 9.9775e-01, 9.9763e-01, 9.9697e-01,\n",
       "             9.9688e-01, 9.9652e-01, 9.9639e-01, 9.9530e-01, 9.9518e-01, 9.9497e-01,\n",
       "             9.9479e-01, 9.9463e-01, 9.9459e-01, 9.9410e-01, 9.9409e-01, 9.9329e-01,\n",
       "             9.9284e-01, 9.9264e-01, 9.9261e-01, 9.9172e-01, 9.9024e-01, 9.8904e-01,\n",
       "             9.8416e-01, 9.8398e-01, 9.8391e-01, 9.7604e-01, 9.7567e-01, 9.7433e-01,\n",
       "             9.7411e-01, 9.7227e-01, 9.7196e-01, 9.6967e-01, 9.6310e-01, 9.5781e-01,\n",
       "             9.5628e-01, 9.5200e-01, 9.4686e-01, 9.4650e-01, 9.4070e-01, 9.2112e-01,\n",
       "             9.1821e-01, 8.9795e-01, 8.9766e-01, 8.7184e-01, 8.6806e-01, 8.6769e-01,\n",
       "             8.6021e-01, 7.6720e-01, 7.0475e-01, 6.8686e-01, 6.2231e-01, 5.7857e-01,\n",
       "             3.9709e-01, 3.2676e-01, 3.0241e-01, 2.9333e-01, 2.0963e-01, 1.5054e-01,\n",
       "             1.4476e-01, 1.4173e-01, 1.3058e-01, 8.5378e-02, 7.9462e-02, 3.8723e-02,\n",
       "             3.6934e-02, 2.8715e-02, 2.8198e-02, 1.7384e-02, 1.7369e-02, 8.9205e-03,\n",
       "             7.0605e-03, 3.9990e-03, 2.6726e-03, 1.7290e-03, 1.4921e-03, 9.4835e-04,\n",
       "             8.6367e-04, 7.5790e-04, 5.3087e-04, 5.1943e-04, 4.2687e-04, 3.4470e-04,\n",
       "             1.7904e-04, 1.6877e-04, 1.4132e-04, 1.1914e-04, 9.8637e-05, 8.9513e-05,\n",
       "             8.6545e-05, 6.9600e-05, 6.2527e-05, 6.0310e-05, 2.6988e-05, 2.2882e-05,\n",
       "             2.2272e-05, 1.9570e-05, 1.6078e-05, 1.4407e-05, 1.3017e-05, 7.1417e-06,\n",
       "             4.6072e-06, 2.1109e-06, 1.0926e-06, 9.6170e-07, 9.3919e-07, 7.8920e-07,\n",
       "             4.7735e-07, 4.6905e-07, 4.1466e-07, 3.4436e-07, 3.4264e-07, 3.1507e-07,\n",
       "             2.9750e-07, 2.9715e-07, 2.8353e-07, 2.7946e-07, 1.8027e-07, 1.7115e-07,\n",
       "             1.3285e-07, 1.0440e-07, 8.6431e-08, 7.7255e-08, 5.2340e-08, 4.0141e-08,\n",
       "             2.9723e-08, 2.1754e-08, 1.7415e-08, 1.6314e-08, 1.5192e-08, 1.0983e-08,\n",
       "             6.8954e-09, 5.8400e-09, 5.7724e-09, 3.8409e-09, 3.4806e-09, 3.4059e-09,\n",
       "             2.9491e-09, 2.7990e-09, 2.7227e-09, 2.3597e-09, 2.3477e-09, 2.2509e-09,\n",
       "             1.7261e-09, 1.3116e-09, 1.2724e-09, 1.2621e-09, 1.0407e-09, 9.8080e-10,\n",
       "             6.0515e-10, 5.3904e-10, 5.3397e-10, 5.1410e-10, 5.0718e-10, 4.3403e-10,\n",
       "             4.2179e-10, 4.0485e-10, 3.9125e-10, 3.5307e-10, 3.4842e-10, 3.1009e-10,\n",
       "             2.9597e-10, 2.6320e-10, 2.2231e-10, 1.2995e-10, 7.0321e-11, 6.2436e-11,\n",
       "             6.2319e-11, 6.0762e-11, 6.0435e-11, 5.5361e-11, 5.3550e-11, 4.6332e-11,\n",
       "             3.7187e-11, 2.7731e-11, 2.5191e-11, 2.4558e-11, 2.4493e-11, 2.4479e-11,\n",
       "             2.2311e-11, 1.5546e-11, 1.5147e-11, 1.2765e-11, 1.0986e-11, 1.0282e-11,\n",
       "             1.0236e-11, 1.0169e-11, 7.8837e-12, 6.9329e-12, 6.2287e-12, 6.0182e-12,\n",
       "             5.2228e-12, 4.6890e-12, 3.7130e-12, 2.6920e-12, 2.0165e-12, 1.2999e-12,\n",
       "             1.1915e-12, 8.6548e-13, 7.8341e-13, 7.4957e-13, 7.3163e-13, 6.5100e-13,\n",
       "             5.4638e-13, 3.2502e-13, 2.9503e-13, 2.5495e-13, 1.7923e-13, 1.5603e-13,\n",
       "             1.2555e-13, 1.0282e-13, 5.0474e-14, 4.6205e-14, 2.5839e-14, 1.9891e-14,\n",
       "             1.9184e-14, 1.7409e-14, 1.5447e-14, 1.5329e-14, 1.2466e-14, 1.2024e-14,\n",
       "             1.0160e-14, 1.0101e-14, 8.6405e-15, 8.5331e-15, 4.4710e-15, 4.4622e-15,\n",
       "             4.2292e-15, 4.2190e-15, 3.8142e-15, 3.7275e-15, 3.4074e-15, 3.0326e-15,\n",
       "             2.7170e-15, 2.3550e-15, 2.2612e-15, 1.8260e-15, 1.2404e-15, 1.0730e-15,\n",
       "             9.1413e-16, 9.0923e-16, 8.6207e-16, 8.3795e-16, 7.6821e-16, 7.3214e-16,\n",
       "             7.2846e-16, 6.7984e-16, 5.9535e-16, 5.9358e-16, 4.8837e-16, 4.4510e-16,\n",
       "             3.6368e-16, 3.5848e-16, 3.0656e-16, 2.7301e-16, 2.4867e-16, 2.4135e-16,\n",
       "             1.8515e-16, 1.8011e-16, 1.5361e-16, 1.2985e-16, 1.0588e-16, 9.4896e-17,\n",
       "             7.7639e-17, 7.1235e-17, 6.4968e-17, 5.8188e-17, 4.9143e-17, 4.7618e-17,\n",
       "             4.2758e-17, 3.9464e-17, 3.4049e-17, 3.0811e-17, 2.2151e-17, 1.9834e-17,\n",
       "             1.8522e-17, 1.6347e-17, 1.1987e-17, 1.1379e-17, 1.0964e-17, 1.0082e-17,\n",
       "             9.3393e-18, 7.3540e-18, 5.4252e-18, 4.6941e-18, 4.5822e-18, 4.0521e-18,\n",
       "             3.6058e-18, 3.5278e-18, 2.1278e-18, 1.9591e-18, 1.6842e-18, 1.4964e-18,\n",
       "             1.3369e-18, 1.1036e-18, 9.2863e-19, 9.0231e-19, 8.6086e-19, 8.3988e-19,\n",
       "             7.3995e-19, 6.8766e-19, 5.6539e-19, 4.4663e-19, 4.0895e-19, 4.0132e-19,\n",
       "             3.9078e-19, 3.8575e-19, 2.9082e-19, 2.6984e-19, 2.6691e-19, 1.9478e-19,\n",
       "             1.6975e-19, 1.5867e-19, 1.2293e-19, 7.3191e-20, 6.2268e-20, 5.5952e-20,\n",
       "             5.1220e-20, 4.0869e-20, 3.6580e-20, 9.6597e-21, 5.3104e-21, 2.1943e-21,\n",
       "             1.8005e-21, 1.7627e-21, 1.7251e-21, 1.4454e-21, 1.1475e-21, 8.5215e-22,\n",
       "             7.4256e-22, 7.2293e-22, 6.8830e-22, 6.5427e-22, 3.5722e-22, 2.8469e-22,\n",
       "             2.7477e-22, 2.5860e-22, 1.2840e-22, 1.1152e-22, 8.1854e-23, 3.7000e-23,\n",
       "             1.5634e-23, 6.0398e-24, 5.9221e-24, 5.4105e-24, 5.1001e-24, 4.3057e-24,\n",
       "             3.4070e-24, 2.8045e-24, 9.2160e-25, 7.9068e-25, 1.5910e-25, 1.5539e-25,\n",
       "             3.3493e-26, 1.5164e-26, 4.1168e-28, 8.8454e-30, 4.8627e-31, 2.1256e-32])}}]],\n",
       " 'roc_results': {'fpr': tensor([0.0000, 0.0043, 0.0043, 0.0043, 0.0043, 0.0085, 0.0085, 0.0085, 0.0085,\n",
       "          0.0085, 0.0085, 0.0085, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
       "          0.0171, 0.0171, 0.0171, 0.0171, 0.0171, 0.0214, 0.0256, 0.0256, 0.0256,\n",
       "          0.0256, 0.0256, 0.0256, 0.0299, 0.0299, 0.0299, 0.0299, 0.0299, 0.0299,\n",
       "          0.0299, 0.0299, 0.0342, 0.0342, 0.0385, 0.0385, 0.0385, 0.0427, 0.0427,\n",
       "          0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0427, 0.0470, 0.0470,\n",
       "          0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470, 0.0470,\n",
       "          0.0470, 0.0470, 0.0470, 0.0513, 0.0513, 0.0513, 0.0513, 0.0513, 0.0513,\n",
       "          0.0556, 0.0556, 0.0556, 0.0598, 0.0598, 0.0598, 0.0598, 0.0641, 0.0641,\n",
       "          0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641,\n",
       "          0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0641, 0.0684, 0.0684,\n",
       "          0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0726, 0.0726, 0.0726, 0.0726,\n",
       "          0.0726, 0.0726, 0.0726, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0812,\n",
       "          0.0855, 0.0897, 0.0897, 0.0897, 0.0897, 0.0897, 0.0897, 0.0897, 0.0940,\n",
       "          0.0940, 0.0940, 0.0940, 0.0940, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,\n",
       "          0.1026, 0.1026, 0.1068, 0.1111, 0.1111, 0.1111, 0.1111, 0.1154, 0.1154,\n",
       "          0.1154, 0.1154, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197,\n",
       "          0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197, 0.1197,\n",
       "          0.1239, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282, 0.1282,\n",
       "          0.1282, 0.1282, 0.1282, 0.1325, 0.1368, 0.1368, 0.1368, 0.1368, 0.1368,\n",
       "          0.1410, 0.1410, 0.1410, 0.1453, 0.1453, 0.1496, 0.1538, 0.1538, 0.1581,\n",
       "          0.1624, 0.1624, 0.1667, 0.1667, 0.1667, 0.1709, 0.1709, 0.1795, 0.1838,\n",
       "          0.1838, 0.1838, 0.1838, 0.1838, 0.1880, 0.1923, 0.1966, 0.2009, 0.2009,\n",
       "          0.2009, 0.2009, 0.2051, 0.2051, 0.2051, 0.2051, 0.2094, 0.2094, 0.2094,\n",
       "          0.2094, 0.2137, 0.2179, 0.2179, 0.2222, 0.2265, 0.2308, 0.2308, 0.2350,\n",
       "          0.2393, 0.2436, 0.2436, 0.2436, 0.2479, 0.2479, 0.2479, 0.2521, 0.2521,\n",
       "          0.2564, 0.2607, 0.2650, 0.2692, 0.2735, 0.2735, 0.2778, 0.2778, 0.2821,\n",
       "          0.2863, 0.2906, 0.2949, 0.2991, 0.2991, 0.3034, 0.3034, 0.3077, 0.3077,\n",
       "          0.3120, 0.3162, 0.3205, 0.3205, 0.3248, 0.3291, 0.3291, 0.3333, 0.3376,\n",
       "          0.3419, 0.3462, 0.3504, 0.3547, 0.3590, 0.3632, 0.3675, 0.3718, 0.3761,\n",
       "          0.3803, 0.3803, 0.3846, 0.3889, 0.3932, 0.3932, 0.3932, 0.3974, 0.4017,\n",
       "          0.4060, 0.4060, 0.4103, 0.4145, 0.4188, 0.4231, 0.4274, 0.4316, 0.4359,\n",
       "          0.4359, 0.4359, 0.4359, 0.4402, 0.4444, 0.4487, 0.4530, 0.4530, 0.4573,\n",
       "          0.4615, 0.4658, 0.4658, 0.4701, 0.4744, 0.4786, 0.4829, 0.4872, 0.4915,\n",
       "          0.4915, 0.4915, 0.4957, 0.5000, 0.5043, 0.5043, 0.5085, 0.5128, 0.5171,\n",
       "          0.5214, 0.5256, 0.5299, 0.5299, 0.5342, 0.5385, 0.5427, 0.5470, 0.5513,\n",
       "          0.5556, 0.5641, 0.5684, 0.5726, 0.5726, 0.5769, 0.5812, 0.5855, 0.5897,\n",
       "          0.5983, 0.6026, 0.6068, 0.6111, 0.6154, 0.6197, 0.6239, 0.6282, 0.6282,\n",
       "          0.6368, 0.6410, 0.6453, 0.6538, 0.6581, 0.6624, 0.6667, 0.6709, 0.6752,\n",
       "          0.6795, 0.6838, 0.6880, 0.6923, 0.6966, 0.7009, 0.7051, 0.7094, 0.7137,\n",
       "          0.7179, 0.7222, 0.7265, 0.7308, 0.7350, 0.7393, 0.7436, 0.7479, 0.7521,\n",
       "          0.7564, 0.7607, 0.7692, 0.7735, 0.7778, 0.7821, 0.7863, 0.7906, 0.7949,\n",
       "          0.8034, 0.8077, 0.8120, 0.8162, 0.8205, 0.8248, 0.8291, 0.8333, 0.8376,\n",
       "          0.8419, 0.8462, 0.8504, 0.8547, 0.8590, 0.8632, 0.8675, 0.8718, 0.8761,\n",
       "          0.8803, 0.8846, 0.8889, 0.8932, 0.8974, 0.9017, 0.9060, 0.9103, 0.9145,\n",
       "          0.9188, 0.9231, 0.9274, 0.9316, 0.9359, 0.9402, 0.9444, 0.9487, 0.9530,\n",
       "          0.9573, 0.9615, 0.9658, 0.9701, 0.9744, 0.9786, 0.9829, 0.9872, 0.9915,\n",
       "          0.9957, 1.0000]),\n",
       "  'tpr': tensor([0.0000, 0.0000, 0.0026, 0.0077, 0.0179, 0.0308, 0.0538, 0.0744, 0.1026,\n",
       "          0.1308, 0.1410, 0.1641, 0.1897, 0.2051, 0.2154, 0.2256, 0.2436, 0.2564,\n",
       "          0.2615, 0.2744, 0.2846, 0.2872, 0.3026, 0.3128, 0.3205, 0.3231, 0.3359,\n",
       "          0.3436, 0.3462, 0.3564, 0.3641, 0.3692, 0.3744, 0.3795, 0.3872, 0.4051,\n",
       "          0.4077, 0.4154, 0.4256, 0.4359, 0.4385, 0.4436, 0.4462, 0.4564, 0.4641,\n",
       "          0.4718, 0.4795, 0.4821, 0.4872, 0.4923, 0.4949, 0.5026, 0.5026, 0.5077,\n",
       "          0.5103, 0.5179, 0.5205, 0.5282, 0.5308, 0.5333, 0.5385, 0.5410, 0.5462,\n",
       "          0.5487, 0.5538, 0.5564, 0.5564, 0.5590, 0.5615, 0.5667, 0.5692, 0.5718,\n",
       "          0.5769, 0.5872, 0.5897, 0.5923, 0.5974, 0.6000, 0.6026, 0.6026, 0.6051,\n",
       "          0.6077, 0.6103, 0.6128, 0.6154, 0.6179, 0.6205, 0.6231, 0.6256, 0.6282,\n",
       "          0.6333, 0.6359, 0.6385, 0.6410, 0.6436, 0.6487, 0.6538, 0.6538, 0.6564,\n",
       "          0.6590, 0.6615, 0.6667, 0.6692, 0.6718, 0.6718, 0.6744, 0.6769, 0.6795,\n",
       "          0.6821, 0.6897, 0.6923, 0.6949, 0.7000, 0.7026, 0.7051, 0.7103, 0.7103,\n",
       "          0.7128, 0.7154, 0.7179, 0.7205, 0.7231, 0.7256, 0.7282, 0.7333, 0.7333,\n",
       "          0.7359, 0.7385, 0.7410, 0.7436, 0.7436, 0.7462, 0.7513, 0.7538, 0.7564,\n",
       "          0.7564, 0.7590, 0.7641, 0.7641, 0.7667, 0.7692, 0.7718, 0.7744, 0.7769,\n",
       "          0.7795, 0.7821, 0.7821, 0.7846, 0.7872, 0.7897, 0.7923, 0.7949, 0.7974,\n",
       "          0.8000, 0.8026, 0.8051, 0.8077, 0.8103, 0.8128, 0.8154, 0.8179, 0.8205,\n",
       "          0.8205, 0.8205, 0.8231, 0.8256, 0.8282, 0.8308, 0.8333, 0.8359, 0.8410,\n",
       "          0.8436, 0.8462, 0.8487, 0.8487, 0.8487, 0.8513, 0.8538, 0.8564, 0.8590,\n",
       "          0.8615, 0.8641, 0.8667, 0.8667, 0.8692, 0.8692, 0.8692, 0.8718, 0.8718,\n",
       "          0.8744, 0.8769, 0.8769, 0.8821, 0.8846, 0.8846, 0.8872, 0.8872, 0.8872,\n",
       "          0.8897, 0.8923, 0.8949, 0.8974, 0.9000, 0.9000, 0.9000, 0.9000, 0.9026,\n",
       "          0.9051, 0.9077, 0.9077, 0.9103, 0.9128, 0.9154, 0.9154, 0.9179, 0.9205,\n",
       "          0.9231, 0.9231, 0.9256, 0.9282, 0.9282, 0.9282, 0.9282, 0.9308, 0.9308,\n",
       "          0.9308, 0.9308, 0.9333, 0.9359, 0.9359, 0.9385, 0.9410, 0.9410, 0.9436,\n",
       "          0.9436, 0.9436, 0.9436, 0.9436, 0.9436, 0.9462, 0.9462, 0.9487, 0.9487,\n",
       "          0.9487, 0.9487, 0.9487, 0.9487, 0.9513, 0.9513, 0.9538, 0.9538, 0.9564,\n",
       "          0.9564, 0.9564, 0.9564, 0.9590, 0.9590, 0.9590, 0.9615, 0.9615, 0.9615,\n",
       "          0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615, 0.9615,\n",
       "          0.9615, 0.9641, 0.9641, 0.9641, 0.9641, 0.9667, 0.9692, 0.9692, 0.9692,\n",
       "          0.9692, 0.9718, 0.9718, 0.9718, 0.9718, 0.9718, 0.9718, 0.9718, 0.9718,\n",
       "          0.9744, 0.9769, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9821, 0.9821,\n",
       "          0.9821, 0.9821, 0.9846, 0.9846, 0.9846, 0.9846, 0.9846, 0.9846, 0.9846,\n",
       "          0.9872, 0.9897, 0.9897, 0.9897, 0.9897, 0.9923, 0.9923, 0.9923, 0.9923,\n",
       "          0.9923, 0.9923, 0.9923, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949, 0.9949,\n",
       "          0.9949, 0.9949, 0.9949, 0.9949, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974,\n",
       "          0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 0.9974, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000]),\n",
       "  'thresholds': tensor([1.0000, 1.0000, 0.9995, 0.9990, 0.9985, 0.9980, 0.9976, 0.9971, 0.9966,\n",
       "          0.9961, 0.9956, 0.9951, 0.9946, 0.9941, 0.9937, 0.9932, 0.9927, 0.9922,\n",
       "          0.9917, 0.9912, 0.9907, 0.9902, 0.9897, 0.9893, 0.9888, 0.9883, 0.9878,\n",
       "          0.9873, 0.9868, 0.9863, 0.9858, 0.9854, 0.9849, 0.9839, 0.9834, 0.9829,\n",
       "          0.9824, 0.9819, 0.9814, 0.9810, 0.9800, 0.9795, 0.9790, 0.9785, 0.9780,\n",
       "          0.9775, 0.9771, 0.9766, 0.9761, 0.9756, 0.9746, 0.9736, 0.9731, 0.9727,\n",
       "          0.9722, 0.9717, 0.9712, 0.9707, 0.9702, 0.9692, 0.9688, 0.9678, 0.9658,\n",
       "          0.9653, 0.9648, 0.9639, 0.9634, 0.9629, 0.9619, 0.9614, 0.9609, 0.9604,\n",
       "          0.9600, 0.9575, 0.9570, 0.9565, 0.9556, 0.9546, 0.9531, 0.9517, 0.9507,\n",
       "          0.9502, 0.9492, 0.9487, 0.9468, 0.9443, 0.9424, 0.9414, 0.9399, 0.9395,\n",
       "          0.9390, 0.9385, 0.9380, 0.9370, 0.9346, 0.9331, 0.9302, 0.9297, 0.9292,\n",
       "          0.9287, 0.9268, 0.9243, 0.9233, 0.9229, 0.9131, 0.9111, 0.9102, 0.9092,\n",
       "          0.9072, 0.9067, 0.9038, 0.9028, 0.9014, 0.8994, 0.8989, 0.8960, 0.8950,\n",
       "          0.8940, 0.8901, 0.8896, 0.8872, 0.8867, 0.8862, 0.8857, 0.8853, 0.8843,\n",
       "          0.8828, 0.8809, 0.8804, 0.8799, 0.8789, 0.8770, 0.8726, 0.8706, 0.8696,\n",
       "          0.8691, 0.8682, 0.8647, 0.8638, 0.8604, 0.8599, 0.8579, 0.8545, 0.8530,\n",
       "          0.8521, 0.8516, 0.8481, 0.8462, 0.8457, 0.8433, 0.8418, 0.8413, 0.8398,\n",
       "          0.8389, 0.8345, 0.8340, 0.8325, 0.8315, 0.8252, 0.8228, 0.8193, 0.8179,\n",
       "          0.8169, 0.8164, 0.8159, 0.8145, 0.8110, 0.8076, 0.8071, 0.8022, 0.7998,\n",
       "          0.7983, 0.7939, 0.7920, 0.7915, 0.7905, 0.7871, 0.7837, 0.7798, 0.7764,\n",
       "          0.7749, 0.7744, 0.7686, 0.7666, 0.7646, 0.7588, 0.7524, 0.7490, 0.7485,\n",
       "          0.7461, 0.7422, 0.7402, 0.7383, 0.7373, 0.7344, 0.7334, 0.7324, 0.7305,\n",
       "          0.7144, 0.7090, 0.7080, 0.7051, 0.7046, 0.6973, 0.6953, 0.6943, 0.6895,\n",
       "          0.6865, 0.6807, 0.6787, 0.6772, 0.6768, 0.6748, 0.6704, 0.6646, 0.6636,\n",
       "          0.6611, 0.6572, 0.6543, 0.6538, 0.6504, 0.6489, 0.6416, 0.6382, 0.6372,\n",
       "          0.6323, 0.6167, 0.6108, 0.5967, 0.5962, 0.5942, 0.5928, 0.5898, 0.5806,\n",
       "          0.5776, 0.5757, 0.5752, 0.5684, 0.5674, 0.5605, 0.5601, 0.5552, 0.5459,\n",
       "          0.5405, 0.5371, 0.5342, 0.5269, 0.5200, 0.5098, 0.5034, 0.4883, 0.4878,\n",
       "          0.4824, 0.4785, 0.4736, 0.4673, 0.4590, 0.4587, 0.4343, 0.4341, 0.4302,\n",
       "          0.4285, 0.4253, 0.4189, 0.4163, 0.4133, 0.4102, 0.4031, 0.4001, 0.3965,\n",
       "          0.3901, 0.3899, 0.3882, 0.3789, 0.3782, 0.3730, 0.3669, 0.3577, 0.3572,\n",
       "          0.3567, 0.3560, 0.3555, 0.3540, 0.3538, 0.3501, 0.3481, 0.3418, 0.3416,\n",
       "          0.3274, 0.3254, 0.3201, 0.3025, 0.2954, 0.2947, 0.2932, 0.2930, 0.2896,\n",
       "          0.2878, 0.2849, 0.2708, 0.2627, 0.2625, 0.2561, 0.2551, 0.2542, 0.2482,\n",
       "          0.2441, 0.2424, 0.2405, 0.2402, 0.2383, 0.2379, 0.2340, 0.2225, 0.2153,\n",
       "          0.2120, 0.2075, 0.2017, 0.1981, 0.1865, 0.1724, 0.1580, 0.1549, 0.1521,\n",
       "          0.1482, 0.1404, 0.1379, 0.1367, 0.1364, 0.1350, 0.1348, 0.1333, 0.1309,\n",
       "          0.1298, 0.1272, 0.1270, 0.1220, 0.1195, 0.1128, 0.1115, 0.1101, 0.1099,\n",
       "          0.1076, 0.1061, 0.1054, 0.1049, 0.1036, 0.1025, 0.1007, 0.0988, 0.0962,\n",
       "          0.0952, 0.0932, 0.0919, 0.0891, 0.0887, 0.0871, 0.0856, 0.0853, 0.0840,\n",
       "          0.0834, 0.0830, 0.0823, 0.0775, 0.0770, 0.0760, 0.0753, 0.0752, 0.0746,\n",
       "          0.0743, 0.0715, 0.0710, 0.0704, 0.0646, 0.0642, 0.0640, 0.0638, 0.0637,\n",
       "          0.0598, 0.0592, 0.0576, 0.0574, 0.0569, 0.0541, 0.0531, 0.0526, 0.0523,\n",
       "          0.0503, 0.0500, 0.0472, 0.0454, 0.0429, 0.0417, 0.0403, 0.0390, 0.0387,\n",
       "          0.0371, 0.0365, 0.0349, 0.0347, 0.0343, 0.0341, 0.0333, 0.0304, 0.0302,\n",
       "          0.0293, 0.0291, 0.0274, 0.0249, 0.0244, 0.0233, 0.0226, 0.0216, 0.0213,\n",
       "          0.0195, 0.0193, 0.0189, 0.0185, 0.0175, 0.0172, 0.0170, 0.0165, 0.0155,\n",
       "          0.0137, 0.0134], dtype=torch.float16),\n",
       "  'name': 'Original NN PneumoniaMNIST',\n",
       "  'auc': tensor(0.9214, device='cuda:0'),\n",
       "  'model': LitSimpleCNN(\n",
       "    (model): SimpleCNN(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "        (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): ReLU()\n",
       "        (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "      (relu_fc): ReLU()\n",
       "      (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (criterion): BCEWithLogitsLoss()\n",
       "    (train_accuracy): BinaryAccuracy()\n",
       "    (val_accuracy): BinaryAccuracy()\n",
       "    (val_auc): BinaryAUROC()\n",
       "    (test_accuracy): BinaryAccuracy()\n",
       "    (test_auc): BinaryAUROC()\n",
       "    (test_precision): BinaryPrecision()\n",
       "    (test_recall): BinaryRecall()\n",
       "    (test_f1): BinaryF1Score()\n",
       "  )},\n",
       " 'test_loader': <torch.utils.data.dataloader.DataLoader at 0x723fd0359400>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Save the data\n",
    "save_to_pickle(list_folds_best_models, list_folds_weighted_clfs, results_original_roc, test_loader, filename='pickle/medMNIST_weighted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d29cb",
   "metadata": {},
   "source": [
    "# Load pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bbf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from predict_ensemble_and_evaluate_paper import save_to_pickle, load_from_pickle\n",
    "from predict_ensemble_and_evaluate_paper import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate_paper import plot_roc_comparison\n",
    "from predict_ensemble_and_evaluate_paper import make_curve_monotonic\n",
    "\n",
    "# Load the data\n",
    "list_folds_best_models, list_folds_weighted_clfs, results_original_roc, test_loader = load_from_pickle(filename='pickle/medMNIST_weighted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a49e4",
   "metadata": {},
   "source": [
    "### Ensemble prediction using voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77ee047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a single ensemble from 93 models across all folds.\n",
      "Extracting full dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from all models...\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Loop through all the FPR values to generate the ensemble ROC curve ---\n",
    "\n",
    "\n",
    "\n",
    "results_tuple, prior_proba = predict_ensemble_and_evaluate(list_folds_best_models=list_folds_best_models,\n",
    "    test_loader=test_loader)\n",
    "\n",
    "ensemble_results_soft = results_tuple['soft_voting']\n",
    "ensemble_results_hard = results_tuple['hard_voting']\n",
    "misclassification_risk = results_tuple['misclassification_risk']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f000d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "misclassification_risk_half_CV = results_tuple['misclassification_risk_half']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a29a7a",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate Neyman Pearson ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dca388dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class SklearnPyTorchCNNWrapper:\n",
    "    def __init__(self, model, epochs=5, lr=0.001, image_dims=(1, 224, 224)):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.image_dims = image_dims\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.train()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).view(-1, 1))\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        print(\"Starting PyTorch model training inside wrapper...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, labels in loader:\n",
    "                inputs = inputs.view(-1, *self.image_dims).to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            all_outputs = []\n",
    "            for i in range(0, len(X_tensor), BATCH_SIZE):\n",
    "                batch = X_tensor[i:i+BATCH_SIZE]\n",
    "                batch = batch.view(-1, *self.image_dims).to(self.device)\n",
    "                output_batch = self.model(batch).cpu().numpy()\n",
    "                all_outputs.append(output_batch)\n",
    "            return np.vstack(all_outputs).flatten()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            all_probs = []\n",
    "            for i in range(0, len(X_tensor), BATCH_SIZE):\n",
    "                batch = X_tensor[i:i+BATCH_SIZE]\n",
    "                batch = batch.view(-1, *self.image_dims).to(self.device)\n",
    "                prob_class_1_batch = self.model(batch).cpu().numpy()\n",
    "                all_probs.append(prob_class_1_batch)\n",
    "            \n",
    "            prob_class_1 = np.vstack(all_probs)\n",
    "            prob_class_0 = 1 - prob_class_1\n",
    "            \n",
    "            return np.hstack((prob_class_0, prob_class_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bede4159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n",
      "Number of training samples: 4708\n",
      "Number of test samples: 624\n",
      "Number of validation samples: 524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136029/2793182030.py:47: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  X_list.append(np.array(img).flatten())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating constrained ROC curve (Neyman-Pearson Simulation)...\n",
      "Generating constrained ROC curve (Neyman-Pearson Simulation)...\n",
      "Starting PyTorch model training inside wrapper...\n",
      "Epoch 1/1, Loss: 0.0811\n",
      "Phase 1 Complete.\n",
      "\n",
      "Phase 2: Evaluating on the unseen test set...\n",
      "Sample size is too small for the given alpha. Try a larger alpha.\n",
      "Alpha 0.02 (2/51): FPR=0.145, TPR=0.795\n",
      "Alpha 0.04 (3/51): FPR=0.248, TPR=0.903\n",
      "Alpha 0.06 (4/51): FPR=0.303, TPR=0.936\n",
      "Alpha 0.08 (5/51): FPR=0.333, TPR=0.954\n",
      "Alpha 0.10 (6/51): FPR=0.376, TPR=0.956\n",
      "Alpha 0.12 (7/51): FPR=0.415, TPR=0.974\n",
      "Alpha 0.14 (8/51): FPR=0.453, TPR=0.987\n",
      "Alpha 0.16 (9/51): FPR=0.509, TPR=0.990\n",
      "Alpha 0.18 (10/51): FPR=0.517, TPR=0.997\n",
      "Alpha 0.20 (11/51): FPR=0.551, TPR=1.000\n",
      "Alpha 0.22 (12/51): FPR=0.568, TPR=1.000\n",
      "Alpha 0.24 (13/51): FPR=0.585, TPR=1.000\n",
      "Alpha 0.26 (14/51): FPR=0.598, TPR=1.000\n",
      "Alpha 0.28 (15/51): FPR=0.624, TPR=1.000\n",
      "Alpha 0.30 (16/51): FPR=0.641, TPR=1.000\n",
      "Alpha 0.32 (17/51): FPR=0.654, TPR=1.000\n",
      "Alpha 0.34 (18/51): FPR=0.662, TPR=1.000\n",
      "Alpha 0.36 (19/51): FPR=0.675, TPR=1.000\n",
      "Alpha 0.38 (20/51): FPR=0.692, TPR=1.000\n",
      "Alpha 0.40 (21/51): FPR=0.701, TPR=1.000\n",
      "Alpha 0.42 (22/51): FPR=0.714, TPR=1.000\n",
      "Alpha 0.44 (23/51): FPR=0.714, TPR=1.000\n",
      "Alpha 0.46 (24/51): FPR=0.735, TPR=1.000\n",
      "Alpha 0.48 (25/51): FPR=0.735, TPR=1.000\n",
      "Alpha 0.50 (26/51): FPR=0.752, TPR=1.000\n",
      "Alpha 0.52 (27/51): FPR=0.765, TPR=1.000\n",
      "Alpha 0.54 (28/51): FPR=0.786, TPR=1.000\n",
      "Alpha 0.56 (29/51): FPR=0.791, TPR=1.000\n",
      "Alpha 0.58 (30/51): FPR=0.791, TPR=1.000\n",
      "Alpha 0.60 (31/51): FPR=0.803, TPR=1.000\n",
      "Alpha 0.62 (32/51): FPR=0.812, TPR=1.000\n",
      "Alpha 0.64 (33/51): FPR=0.816, TPR=1.000\n",
      "Alpha 0.66 (34/51): FPR=0.821, TPR=1.000\n",
      "Alpha 0.68 (35/51): FPR=0.829, TPR=1.000\n",
      "Alpha 0.70 (36/51): FPR=0.838, TPR=1.000\n",
      "Alpha 0.72 (37/51): FPR=0.855, TPR=1.000\n",
      "Alpha 0.74 (38/51): FPR=0.863, TPR=1.000\n",
      "Alpha 0.76 (39/51): FPR=0.863, TPR=1.000\n",
      "Alpha 0.78 (40/51): FPR=0.872, TPR=1.000\n",
      "Alpha 0.80 (41/51): FPR=0.893, TPR=1.000\n",
      "Alpha 0.82 (42/51): FPR=0.893, TPR=1.000\n",
      "Alpha 0.84 (43/51): FPR=0.910, TPR=1.000\n",
      "Alpha 0.86 (44/51): FPR=0.919, TPR=1.000\n",
      "Alpha 0.88 (45/51): FPR=0.927, TPR=1.000\n",
      "Alpha 0.90 (46/51): FPR=0.936, TPR=1.000\n",
      "Alpha 0.92 (47/51): FPR=0.974, TPR=1.000\n",
      "Alpha 0.94 (48/51): FPR=0.983, TPR=1.000\n",
      "Alpha 0.96 (49/51): FPR=0.987, TPR=1.000\n",
      "Alpha 0.98 (50/51): FPR=1.000, TPR=1.000\n",
      "Alpha 1.00 (51/51): FPR=1.000, TPR=1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import scipy\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nproc import npc\n",
    "\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 1\n",
    "NUM_CHANNELS = 1\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = generate_data(\"pneumoniaMNIST\")\n",
    "\n",
    "\n",
    "# Ensure you have your train_loader and val_loader defined here\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset.transform = data_transforms\n",
    "test_dataset.transform = data_transforms\n",
    "val_dataset.transform = data_transforms\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#new_train_loader = data.DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) to be done inside the sample_ratio loop\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=int(BATCH_SIZE/4), shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "def flatten_dataset(dataset):\n",
    "    X_list, y_list = [], []\n",
    "    for img, label in dataset:\n",
    "        X_list.append(np.array(img).flatten())\n",
    "        y_list.append(label[0])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "X_train, y_train = flatten_dataset(train_dataset)\n",
    "X_test, y_test = flatten_dataset(test_dataset)\n",
    "\n",
    "print(\"Generating constrained ROC curve (Neyman-Pearson Simulation)...\")\n",
    "\n",
    "npc_instance = npc()\n",
    "\n",
    "# 1. Instantiate your PyTorch model\n",
    "pytorch_cnn = SimpleCNN(\n",
    "    in_channels=NUM_CHANNELS,\n",
    "    num_classes=1,\n",
    "    image_height=IMAGE_SIZE,\n",
    "    image_width=IMAGE_SIZE\n",
    ")\n",
    "wrapped_model = SklearnPyTorchCNNWrapper(\n",
    "    model=pytorch_cnn, \n",
    "    epochs=NUM_EPOCHS, \n",
    "    lr=LEARNING_RATE, \n",
    "    image_dims=(NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    ")\n",
    "\n",
    "print(\"Generating constrained ROC curve (Neyman-Pearson Simulation)...\")\n",
    "\n",
    "result = npc_instance.npc(\n",
    "    x=X_train, \n",
    "    y=y_train, \n",
    "    method=\"\", # Leave blank to use the provided model\n",
    "    model=wrapped_model\n",
    ")\n",
    "\n",
    "# Extract the results from the single run\n",
    "fit_results = result[0][0]\n",
    "final_model = fit_results[0]             # The single trained model object\n",
    "y_calib_labels = fit_results[1]          # The labels from the internal calibration set\n",
    "y_calib_scores = fit_results[2]          # The model's scores on the calibration set\n",
    "initial_sign = fit_results[4]\n",
    "print(\"Phase 1 Complete.\")\n",
    "\n",
    "# --- 5. Phase 2: Evaluate on the HELD-OUT Test Set ---\n",
    "print(\"\\nPhase 2: Evaluating on the unseen test set...\")\n",
    "alphas = np.linspace(0, 1, 51)\n",
    "roc_points = []\n",
    "\n",
    "# Get the model's scores on the completely separate test set\n",
    "#y_test_scores = final_model.predict_proba(X_test)[:, 1]\n",
    "y_test_scores = final_model.decision_function(X_test)\n",
    "\n",
    "# SAFETY CHECK: Align Units (Logits vs Probabilities)\n",
    "# If Calibration scores are Probs [0,1] but Test scores are Logits [-inf, inf], \n",
    "# we must convert Calibration scores to Logits.\n",
    "if y_calib_scores.min() >= 0 and y_calib_scores.max() <= 1.0:\n",
    "    print(\"⚠️ Mismatch Detected: Converting calibration probabilities to logits...\")\n",
    "    eps = 1e-15\n",
    "    y_calib_scores = np.clip(y_calib_scores, eps, 1 - eps)\n",
    "    y_calib_scores = scipy.special.logit(y_calib_scores)\n",
    "\n",
    "# The sign must be consistent between calibration and testing\n",
    "if initial_sign:\n",
    "    y_calib_scores = -y_calib_scores\n",
    "    y_test_scores = -y_test_scores\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    # Use the CALIBRATION data to find the optimal cutoff for this alpha\n",
    "    core_result = npc_instance.npc_core(y_calib_labels, y_calib_scores, alpha, 0.05, 1)\n",
    "    if not core_result or core_result[6]: continue\n",
    "    \n",
    "    cutoff = core_result[0]\n",
    "    \n",
    "    # Apply that cutoff to the TEST data scores to get final predictions\n",
    "    y_pred_test = (y_test_scores >= cutoff).astype(int)\n",
    "    \n",
    "    # Calculate performance on the TEST data\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "    current_fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    current_tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    roc_points.append({'fpr': current_fpr, 'tpr': current_tpr})\n",
    "    print(f\"Alpha {alpha:.2f} ({i+1}/{len(alphas)}): FPR={current_fpr:.3f}, TPR={current_tpr:.3f}\")\n",
    "\n",
    "# --- 4. Process and Plot the Results ---\n",
    "# Remove duplicate points\n",
    "unique_points_dict = {(p['fpr'], p['tpr']): p for p in roc_points}\n",
    "constrained_points = list(unique_points_dict.values())\n",
    "constrained_points = sorted(constrained_points, key=lambda x: x['fpr'])\n",
    "\n",
    "# ensure 0,0 and 1,1 are included\n",
    "if constrained_points[0]['fpr'] > 0:\n",
    "    constrained_points.insert(0, {'fpr': 0.0, 'tpr': 0.0})\n",
    "if constrained_points[-1]['fpr'] < 1:\n",
    "    constrained_points.append({'fpr': 1.0, 'tpr': 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccedd7b",
   "metadata": {},
   "source": [
    "\n",
    "## Store NP curve pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "342d40bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained ROC curve points saved to pickle/NN_pneumoniaMNIST_NP_roc_curve.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from predict_ensemble_and_evaluate_paper import save_to_pickle_constrained_roc\n",
    "\n",
    "# Save the constrained ROC curve results\n",
    "save_to_pickle_constrained_roc(constrained_points, filename='pickle/NN_pneumoniaMNIST_NP_roc_curve.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4a755",
   "metadata": {},
   "source": [
    "## Load NP curve pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794916d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_ensemble_and_evaluate_paper import load_from_pickle_constrained_roc\n",
    "\n",
    "# Load the constrained ROC curve results\n",
    "constrained_points = load_from_pickle_constrained_roc(filename='pickle/NN_pneumoniaMNIST_NP_roc_curve.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3173afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plotting all ROC curves for comparison ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "#results_lists.extend(list_folds_best_models)\n",
    "#results_lists.extend(list_folds_weighted_clfs)\n",
    "results_lists.append(constrained_points)\n",
    "results_lists.append(ensemble_results_hard)\n",
    "results_lists.append(list_full_weighted_clfs)\n",
    "#results_lists.append(misclassification_risk)\n",
    "\n",
    "\n",
    "# --- Plot the final comparison ---\n",
    "print(\"\\n--- Plotting all ROC curves for comparison ---\")\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    #names=[\"MaxROCFold 1\", \"MaxROCFold 2\", \"MaxROCFold 3\", \"MaxROCFold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\", \"Neyman_Pearson\", \"Ensemble_voting_hard\", \"Misclassification_Risk\"],\n",
    "    names=[\"Neyman_Pearson\", \"Ensemble_voting_hard\", \"Weighted Classifiers\"],\n",
    "    results_original_roc=results_original_roc, plot_name=\"NN_weighted_PneumoniaMNIST\", prior_prob=prior_proba, misclassification_risk=[misclassification_risk_orig, misclassification_risk_half_CV]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3712cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plotting all ROC curves for comparison ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "results_lists.extend(list_folds_best_models)\n",
    "results_lists.extend(list_folds_weighted_clfs)\n",
    "#results_lists.append(constrained_points)\n",
    "#results_lists.append(ensemble_results_hard)\n",
    "#results_lists.append(misclassification_risk)\n",
    "\n",
    "\n",
    "# --- Plot the final comparison ---\n",
    "print(\"\\n--- Plotting all ROC curves for comparison ---\")\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    #names=[\"MaxROCFold 1\", \"MaxROCFold 2\", \"MaxROCFold 3\", \"MaxROCFold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\", \"Neyman_Pearson\", \"Ensemble_voting_hard\", \"Misclassification_Risk\"],\n",
    "    names=[\"MaxROCFold 1\", \"MaxROCFold 2\", \"MaxROCFold 3\", \"MaxROCFold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\"],\n",
    "    results_original_roc=results_original_roc, plot_name=\"NN_weighted_intermediate_pneumoniaMNIST\", prior_prob=prior_proba, misclassification_risk=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caef9ac",
   "metadata": {},
   "source": [
    "\n",
    "### Tabulate the misclassification risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e78d0ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassification Risk Comparison NN sonar:\n",
      "+----------------------+------------------------+--------+--------+--------+\n",
      "| Method               | Misclassification Risk | F1     | FPR    | TPR    |\n",
      "+----------------------+------------------------+--------+--------+--------+\n",
      "| Original Model       | 0.1426                 | 0.8932 | 0.3034 | 0.9538 |\n",
      "+----------------------+------------------------+--------+--------+--------+\n",
      "| Expert Ensemble (CV) | 0.1571                 | 0.8874 | 0.4017 | 0.9897 |\n",
      "+----------------------+------------------------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import termtables as tt\n",
    "\n",
    "table_data = [\n",
    "    [\"Method\", \"Misclassification Risk\", \"F1\", \"FPR\", \"TPR\"],\n",
    "    [\"Original Model\", f\"{misclassification_risk_orig['risk']:.4f}\", f\"{misclassification_risk_orig['f1']:.4f}\", f\"{misclassification_risk_orig['fpr']:.4f}\", f\"{misclassification_risk_orig['tpr']:.4f}\"],\n",
    "    [\"Expert Ensemble (CV)\", f\"{misclassification_risk_half_CV['risk']:.4f}\", f\"{misclassification_risk_half_CV['f1']:.4f}\", f\"{misclassification_risk_half_CV['fpr']:.4f}\", f\"{misclassification_risk_half_CV['tpr']:.4f}\"],\n",
    "   # [\"Expert Ensemble (Bootstrap)\", f\"{misclassification_risk_half_bootstrap['risk']:.4f}\", f\"{misclassification_risk_half_bootstrap['f1']:.4f}\", f\"{misclassification_risk_half_bootstrap['fpr']:.4f}\", f\"{misclassification_risk_half_bootstrap['tpr']:.4f}\"],\n",
    "]\n",
    "\n",
    "headers = table_data[0]\n",
    "rows = table_data[1:]\n",
    "\n",
    "print(\"\\nMisclassification Risk Comparison NN sonar:\")\n",
    "print(tt.to_string(rows, header=headers, style=tt.styles.ascii_thin))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLlabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
