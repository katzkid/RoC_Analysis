{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01892f0f",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/dataset/14/breast+cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55d2fc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5642a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_ensemble_and_evaluate import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate import plot_roc_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d840881",
   "metadata": {},
   "source": [
    "## Display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c24af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for displaying ROC curves of multiple classifiers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def plot_roc_connected_points(results_list, results_original_roc, name=\"All Classifiers\"):\n",
    "    \"\"\"\n",
    "    Creates a plot showing the performance of all classifiers as points,\n",
    "    and connects them with a line, sorted by FPR.\n",
    "\n",
    "    Args:\n",
    "        results_list: A list of dictionaries, where each dict contains 'fpr', 'tpr'.\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # Convert results to a pandas DataFrame for easy sorting\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Sort the models by their False Positive Rate to create a left-to-right line\n",
    "    df_sorted = df.sort_values(by=['fpr','tpr']).reset_index(drop=True)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Plot all the individual model points as a scatter plot\n",
    "    # This helps see the actual performance of each model\n",
    "    plt.scatter(df_sorted['fpr'], df_sorted['tpr'], c='black', marker='o', \n",
    "                alpha=0.6, s=100, zorder=3) # zorder=3 puts dots on top\n",
    "\n",
    "    # Plot the line connecting the points\n",
    "    plt.plot(df_sorted['fpr'], df_sorted['tpr'], color='red', lw=1.5,\n",
    "             alpha=0.8, label=f'{name} (AUC = {auc(df_sorted[\"fpr\"], df_sorted[\"tpr\"]):.2f})')\n",
    "    \n",
    "    #plot the original ROC curve\n",
    "    plt.plot(results_original_roc[\"fpr\"], results_original_roc[\"tpr\"], color='blue', \n",
    "             label=f'{results_original_roc[\"name\"]} (AUC = {results_original_roc[\"auc\"]:.2f})')\n",
    "\n",
    "    # Plot the \"no-skill\" line for reference\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Performance Path of All Trained Classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9508c52",
   "metadata": {},
   "source": [
    "## Best ROC envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "535c203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def get_best_roc_envelope(results_list, results_original_roc=None):\n",
    "    \"\"\"\n",
    "    Constructs the upper envelope of multiple ROC curves to find the best\n",
    "    possible operating points from a collection of classifiers.\n",
    "\n",
    "    This function is guaranteed to produce a list of points that starts at\n",
    "    (FPR=0, TPR=0) and ends at (FPR=1, TPR=1), making it suitable for direct\n",
    "    plotting and AUC calculation.\n",
    "\n",
    "    Args:\n",
    "        results_list (list): A list of dictionaries. Each dictionary must\n",
    "                             contain a 'model' name and the full ROC results\n",
    "                             under the 'full_roc' key.\n",
    "        results_original_roc (dict, optional): A dictionary containing the\n",
    "                                               ROC results of a baseline model,\n",
    "                                               which will also be included in\n",
    "                                               the comparison. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of dictionaries representing the optimal ROC\n",
    "              envelope. Each dictionary includes 'model', 'fpr', 'tpr',\n",
    "              and 'threshold'.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "\n",
    "    # Step 1: Gather all ROC points from all provided models\n",
    "    all_results = results_list[:]\n",
    "    if results_original_roc:\n",
    "        all_results.append({\n",
    "            'model': 'original_baseline',\n",
    "            'full_roc': results_original_roc\n",
    "        })\n",
    "\n",
    "    for result in all_results:\n",
    "        model_name = result.get('model', 'Unknown Model')\n",
    "        roc = result.get('full_roc')\n",
    "        if not roc: continue\n",
    "\n",
    "        fprs = roc.get('fpr', [])\n",
    "        tprs = roc.get('tpr', [])\n",
    "        thresholds = roc.get('thresholds', [])\n",
    "\n",
    "        for i in range(len(fprs)):\n",
    "            threshold = thresholds[i] if i < len(thresholds) else thresholds[-1]\n",
    "            all_points.append({\n",
    "                'model': model_name, 'fpr': fprs[i],\n",
    "                'tpr': tprs[i], 'threshold': threshold\n",
    "            })\n",
    "\n",
    "    # Handle case with no data by returning a default diagonal line\n",
    "    if not all_points:\n",
    "        return [\n",
    "            {'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf},\n",
    "            {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}\n",
    "        ]\n",
    "\n",
    "    # Step 2: Sort all points by FPR (asc), then by TPR (desc)\n",
    "    all_points.sort(key=lambda p: (p['fpr'], -p['tpr']))\n",
    "\n",
    "    # Step 3: Build the core ROC envelope\n",
    "    best_roc_envelope = []\n",
    "    max_tpr_so_far = -1.0\n",
    "    for point in all_points:\n",
    "        # Add a point only if it has a strictly higher TPR\n",
    "        if point['tpr'] > max_tpr_so_far:\n",
    "            # To prevent adding multiple points at the same FPR, check if the last point\n",
    "            # has the same FPR and replace it if so (since this one has a higher TPR)\n",
    "            if best_roc_envelope and best_roc_envelope[-1]['fpr'] == point['fpr']:\n",
    "                best_roc_envelope[-1] = point\n",
    "            else:\n",
    "                best_roc_envelope.append(point)\n",
    "            max_tpr_so_far = point['tpr']\n",
    "    \n",
    "    # Step 4: Manually add extremities if they are missing\n",
    "    \n",
    "    # Ensure the curve starts at (0, 0)\n",
    "    if not best_roc_envelope or best_roc_envelope[0]['fpr'] > 0.0:\n",
    "        best_roc_envelope.insert(0, {\n",
    "            'model': 'start', 'fpr': 0.0, 'tpr': 0.0, 'threshold': np.inf\n",
    "        })\n",
    "\n",
    "    # Ensure the curve ends at (1, 1)\n",
    "    if best_roc_envelope[-1]['fpr'] < 1.0 or best_roc_envelope[-1]['tpr'] < 1.0:\n",
    "        best_roc_envelope.append({\n",
    "            'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0\n",
    "        })\n",
    "\n",
    "    return best_roc_envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b781e9",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8c3deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(dataset):\n",
    "    if dataset == \"data1\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [0, 2], [2, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "    elif dataset == \"data2\":\n",
    "        no_predictors = 2\n",
    "        no_samples = 1600\n",
    "        no_classes = 2\n",
    "        #kerneltype = 'linear'  #comment out for polynomial kernel\n",
    "        #kerneltype = 'poly'  #comment out for linear kernel\n",
    "\n",
    "\n",
    "\n",
    "        n_samples_per_cluster = 400\n",
    "        n_clusters = 4\n",
    "\n",
    "        # Create the clusters deterministically\n",
    "        np.random.seed(0) # for reproducibility\n",
    "\n",
    "        X = np.zeros((n_samples_per_cluster * n_clusters, 2))\n",
    "        y = np.zeros(n_samples_per_cluster * n_clusters)\n",
    "\n",
    "        # Cluster centers in corners of a square\n",
    "        centers = [[0, 0], [2, 0], [1, 2], [1, 2]]\n",
    "\n",
    "        # Assign points to clusters deterministically\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = slice(i * n_samples_per_cluster, (i + 1) * n_samples_per_cluster)\n",
    "            X[cluster_indices] = centers[i] + np.random.normal(scale=0.5, size=(n_samples_per_cluster, 2))\n",
    "            if i == 0 or i == 3:\n",
    "                y[cluster_indices] = 0\n",
    "            else:\n",
    "                y[cluster_indices] = 1\n",
    "\n",
    "\n",
    "\n",
    "        #Visualize\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=50)\n",
    "        plt.title('Four Clusters of Equal Size')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "        #add intercept to X\n",
    "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # # shuffle data\n",
    "        # permutation = np.random.permutation(no_samples)\n",
    "        # X = X[permutation]\n",
    "        # y = y[permutation]\n",
    "\n",
    "\n",
    "        #Split data into training and testing sets\n",
    "        # Step 1: Split off the 1000 training samples. The rest (600) go into a temporary pool.\n",
    "        # We can use absolute numbers for the size.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            train_size=1000,  # Specify exactly 1000 samples for training\n",
    "            random_state=42,\n",
    "            stratify=y        # Crucial for classification: keeps class proportions the same\n",
    "        )\n",
    "\n",
    "\n",
    "        train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        val_data = test_data\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    elif dataset == \"pneumoniaMNIST\":\n",
    "        import medmnist\n",
    "        print(medmnist.__version__)\n",
    "        # read in the PneumoniaMNIST dataset\n",
    "        from medmnist import PneumoniaMNIST\n",
    "        train_data = PneumoniaMNIST(split='train', download=True, size=224)\n",
    "        test_data = PneumoniaMNIST(split='test', download=True, size=224)\n",
    "        val_data = PneumoniaMNIST(split='val', download=True, size=224)\n",
    "        # print the dataset info\n",
    "        print(f\"Number of training samples: {len(train_data)}\")\n",
    "        print(f\"Number of test samples: {len(test_data)}\")\n",
    "        print(f\"Number of validation samples: {len(val_data)}\")\n",
    "\n",
    "    elif dataset == \"breast_cancer\":\n",
    "        from ucimlrepo import fetch_ucirepo\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        # Fetch and print columns for the dataset your code is using\n",
    "\n",
    "        # 1. Fetch data for Breast Cancer (ID 15)\n",
    "        bc = fetch_ucirepo(id=14)\n",
    "        X_df, y_df = bc.data.features, bc.data.targets\n",
    "\n",
    "        # 2. Replace '?' with a standard missing value format\n",
    "        X_df = X_df.replace('?', np.nan)\n",
    "\n",
    "        # 3. Encode the target variable\n",
    "        y = LabelEncoder().fit_transform(y_df.to_numpy().ravel())\n",
    "\n",
    "        # 4. Define column lists using the correct names for this dataset\n",
    "        categorical_features = ['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'breast', 'breast-quad', 'irradiat']\n",
    "        numeric_features = ['deg-malig']\n",
    "\n",
    "        # 5. Create preprocessing pipelines\n",
    "        numeric_pipeline = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        categorical_pipeline = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        # 6. Build the master preprocessor\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_pipeline, numeric_features),\n",
    "                ('cat', categorical_pipeline, categorical_features)\n",
    "            ])\n",
    "\n",
    "        # 7. Apply the transformations\n",
    "        X_sparse = preprocessor.fit_transform(X_df)\n",
    "\n",
    "        X = X_sparse.toarray()\n",
    "\n",
    "        # --- CRITICAL DEBUGGING STEP ---\n",
    "        print(f\"Shape of X after preprocessing: {X.shape}\")\n",
    "        # This MUST print a shape like (286, 46). If it prints (286, 0), the preprocessor failed.\n",
    "        # -----------------------------\n",
    "\n",
    "        # 8. Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, train_size=0.7, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "        y_train = y_train.reshape(-1, 1) if len(y_train.shape) == 1 else y_train\n",
    "        y_test = y_test.reshape(-1, 1) if len(y_test.shape) == 1 else y_test\n",
    "\n",
    "\n",
    "        # 9. Concatenate the data\n",
    "        train_data = np.concatenate((X_train, y_train), axis=1)\n",
    "        test_data = np.concatenate((X_test, y_test), axis=1)\n",
    "        val_data = test_data  # For simplicity, using test data as validation data\n",
    "\n",
    "        print(f\"Shape of training data: {train_data.shape}\")\n",
    "        print(f\"Shape of test data: {test_data.shape}\")\n",
    "\n",
    "        print(\"\\nData successfully processed and concatenated.\")\n",
    "        print(f\"Shape of final training data: {train_data.shape}\")\n",
    "\n",
    "\n",
    "    return train_data, test_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfe355",
   "metadata": {},
   "source": [
    "## Calculate Statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af73be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_fpr_tpr(clf_model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the False Positive Rate (FPR) and True Positive Rate (TPR) at a given threshold.\n",
    "\n",
    "    Args:\n",
    "        X_test: The test features.\n",
    "        y_test: The true test labels (0 or 1).\n",
    "        threshold: The probability threshold.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the FPR and TPR. Returns None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_prob = clf_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  #Avoid division by zero\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0 #Avoid division by zero\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        misclassification_rate = 1 - accuracy\n",
    "\n",
    "        return {\"fpr\": fpr, \"tpr\": tpr, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy, \"misclassification_rate\": misclassification_rate}\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error calculating FPR and TPR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2be8897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 1\n",
    "NUM_CHANNELS = 1\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 4\n",
    "K_FOLDS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59034e",
   "metadata": {},
   "source": [
    "## Data1: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a060ccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after preprocessing: (286, 39)\n",
      "Training data shape: (200, 39), (200,)\n",
      "Test data shape: (86, 39), (86,)\n",
      "Shape of training data: (200, 40)\n",
      "Shape of test data: (86, 40)\n",
      "\n",
      "Data successfully processed and concatenated.\n",
      "Shape of final training data: (200, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "train_data, test_data, val_data = generate_data(\"breast_cancer\")  # Change to \"data1\", \"data2\", or \"pneumoniaMNIST\" as needed\n",
    "\n",
    "\n",
    "train_data_tensor = data.TensorDataset(torch.tensor(train_data[:, :-1], dtype=torch.float32),\n",
    "                                        torch.tensor(train_data[:, -1], dtype=torch.float32))\n",
    "train_loader = data.DataLoader(train_data_tensor, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS) #over-write later for k-fold cross-validation\n",
    "test_data_tensor = data.TensorDataset(torch.tensor(test_data[:, :-1], dtype=torch.float32),\n",
    "                                       torch.tensor(test_data[:, -1], dtype=torch.float32))\n",
    "test_loader = data.DataLoader(test_data_tensor, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# initialize fold\n",
    "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786cfeb",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "295ae27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# --- This block generates the list of ratios for your experiment ---\n",
    "def generate_ratios(train_data):\n",
    "    # 1. Get original class counts from your train_dataset\n",
    "\n",
    "    try:\n",
    "        original_labels = np.array(train_data.targets).flatten()\n",
    "    except AttributeError:\n",
    "        original_labels = train_data[:, -1]\n",
    "\n",
    "    original_counts = Counter(original_labels)\n",
    "    num_pos_original = original_counts.get(1, 0)  \n",
    "    num_neg_original = original_counts.get(0, 0)  \n",
    "    print(f\"Original class counts: {num_pos_original} positives, {num_neg_original} negatives\")\n",
    "\n",
    "    # The pivot point for your function's logic\n",
    "    orig_sample_ratio = num_pos_original / num_neg_original \n",
    "\n",
    "    # 2. Define how many steps for each regime\n",
    "    N_POINTS_PER_REGIME = 50  # You can change this\n",
    "\n",
    "    # 3. Generate ratios for Regime 1 (from near 0 up to the pivot)\n",
    "    # This will test scenarios from extreme negative-class dominance up to the original balance.\n",
    "    print(f\"Generating ratios for Regime 1 (target ratio < {orig_sample_ratio})...\")\n",
    "    ratios_regime1 = np.geomspace(\n",
    "        start=1/num_neg_original,                      # A small starting ratio (e.g., 1 positive for every 10 negatives)\n",
    "        stop=orig_sample_ratio,         # Go up to the original ratio\n",
    "        num=N_POINTS_PER_REGIME,\n",
    "        endpoint=False                  # Exclude the pivot itself to avoid the 'else' block\n",
    "    )\n",
    "\n",
    "    # 4. Generate ratios for Regime 2 (from the pivot up to 3494)\n",
    "    # This will test scenarios from the original balance up to extreme positive-class dominance.\n",
    "    print(f\"Generating ratios for Regime 2 (target ratio > {orig_sample_ratio})...\")\n",
    "    ratios_regime2 = np.geomspace(\n",
    "        start=orig_sample_ratio, # Start just above the pivot\n",
    "        stop=num_pos_original,                      # Your specified upper limit\n",
    "        num=N_POINTS_PER_REGIME\n",
    "    )\n",
    "\n",
    "    # 5. Combine, sort, and create the final list for the loop\n",
    "    #    We also add the original ratio to ensure we have a baseline run.\n",
    "    all_ratios = sorted(list(np.concatenate([ratios_regime1, ratios_regime2, [orig_sample_ratio]])))\n",
    "\n",
    "    print(f\"\\nGenerated {len(all_ratios)} unique sample ratios to test.\")\n",
    "    print(\"First few ratios:\", np.round(all_ratios[:5], 3))\n",
    "    print(\"Last few ratios:\", np.round(all_ratios[-5:], 2))\n",
    "\n",
    "    return all_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d661c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def undersample_dataset(train_dataset, sample_ratio):\n",
    "\n",
    "    \n",
    "    \n",
    "    # Get the labels from the dataset (0 for normal, 1 for pneumonia)\n",
    "    try:\n",
    "        labels = np.array(train_dataset.targets).flatten()\n",
    "    except AttributeError:\n",
    "        labels = train_dataset[:, -1]\n",
    "\n",
    "    # Find the indices for the positive (pneumonia) and negative (normal) classes\n",
    "    positive_indices = np.where(labels == 1)[0]\n",
    "    negative_indices = np.where(labels == 0)[0]\n",
    "    num_orig_positive = len(positive_indices)\n",
    "    num_orig_negative = len(negative_indices)\n",
    "\n",
    "    orig_sample_ratio = num_orig_positive / num_orig_negative\n",
    "    print(f\"Original sample ratio (positive:negative): {orig_sample_ratio:.2f}\")\n",
    "\n",
    "    #based on sample ratio find the number of positive or negative samples\n",
    "    if sample_ratio>orig_sample_ratio:\n",
    "        neg_samples = int(num_orig_positive / sample_ratio)\n",
    "        pos_samples = num_orig_positive\n",
    "        sampled_negative_indices = np.random.choice(negative_indices, neg_samples, replace=False)\n",
    "        final_indices = np.concatenate([sampled_negative_indices, positive_indices])\n",
    "    elif sample_ratio<orig_sample_ratio:\n",
    "        pos_samples = int(sample_ratio * num_orig_negative)\n",
    "        neg_samples = num_orig_negative\n",
    "        sampled_positive_indices = np.random.choice(positive_indices, pos_samples, replace=False)\n",
    "        final_indices = np.concatenate([sampled_positive_indices, negative_indices])\n",
    "    else:\n",
    "        pos_samples = num_orig_positive\n",
    "        neg_samples = num_orig_negative\n",
    "        final_indices = np.concatenate([positive_indices, negative_indices])\n",
    "        \n",
    "    # Shuffle the final indices to mix positive and negative samples\n",
    "    np.random.shuffle(final_indices)\n",
    "\n",
    "    # Create a subset of the original dataset with the sampled indices\n",
    "    return train_dataset[final_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a602762",
   "metadata": {},
   "source": [
    "### Regular training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cb33b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Logistic Regression\n",
    "X_train = train_data[:, :-1]\n",
    "y_train = train_data[:, -1]\n",
    "X_test = test_data[:, :-1]\n",
    "y_test = test_data[:, -1]\n",
    "clf = LogisticRegression(fit_intercept=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#calculate ROC results\n",
    "fpr_roc, tpr_roc, threshold_roc = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "results_original_roc = {\"fpr\": fpr_roc, \"tpr\": tpr_roc, \"thresholds\": threshold_roc, \"name\": \"Logistic Regression\", \"auc\": auc(fpr_roc, tpr_roc), \"model\": clf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b01cb",
   "metadata": {},
   "source": [
    "## Undersampling ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83d23d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fold 1/4 ---\n",
      "Original class counts: 45 positives, 105 negatives\n",
      "Generating ratios for Regime 1 (target ratio < 0.42857142857142855)...\n",
      "Generating ratios for Regime 2 (target ratio > 0.42857142857142855)...\n",
      "\n",
      "Generated 101 unique sample ratios to test.\n",
      "First few ratios: [0.01  0.01  0.011 0.012 0.013]\n",
      "Last few ratios: [30.78 33.84 37.21 40.92 45.  ]\n",
      "============================================================\n",
      "--- STARTING STAGE 1 FOR SAMPLE RATIO: 0.009523809523809525 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 2 FOR SAMPLE RATIO: 0.010277203383606973 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 3 FOR SAMPLE RATIO: 0.011090195485742362 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 4 FOR SAMPLE RATIO: 0.011967500429948081 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 5 FOR SAMPLE RATIO: 0.012914205770757919 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 6 FOR SAMPLE RATIO: 0.01393580152060214 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 7 FOR SAMPLE RATIO: 0.015038211986784771 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 8 FOR SAMPLE RATIO: 0.016227830126968247 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 9 FOR SAMPLE RATIO: 0.01751155462239509 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 10 FOR SAMPLE RATIO: 0.01889682988383719 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 11 FOR SAMPLE RATIO: 0.020391689222269766 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 12 FOR SAMPLE RATIO: 0.022004801434620105 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 13 FOR SAMPLE RATIO: 0.023745521074745105 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 14 FOR SAMPLE RATIO: 0.025623942701162482 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 15 FOR SAMPLE RATIO: 0.027650959416122504 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 16 FOR SAMPLE RATIO: 0.029838326035492076 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 17 FOR SAMPLE RATIO: 0.03219872725577832 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 18 FOR SAMPLE RATIO: 0.03474585121359688 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 19 FOR SAMPLE RATIO: 0.03749446886416161 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 20 FOR SAMPLE RATIO: 0.04046051963911732 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 21 FOR SAMPLE RATIO: 0.04366120388045141 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 22 FOR SAMPLE RATIO: 0.047115082586515526 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 23 FOR SAMPLE RATIO: 0.05084218504859117 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 24 FOR SAMPLE RATIO: 0.05486412500219204 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 25 FOR SAMPLE RATIO: 0.05920422596667218 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 26 FOR SAMPLE RATIO: 0.06388765649999402 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 27 FOR SAMPLE RATIO: 0.06894157615300807 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 28 FOR SAMPLE RATIO: 0.07439529296964364 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 29 FOR SAMPLE RATIO: 0.0802804334463656 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 30 FOR SAMPLE RATIO: 0.08663112593650435 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 31 FOR SAMPLE RATIO: 0.09348419856303394 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 32 FOR SAMPLE RATIO: 0.10087939278750874 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 33 FOR SAMPLE RATIO: 0.10885959387366011 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 34 FOR SAMPLE RATIO: 0.11747107958212828 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 35 FOR SAMPLE RATIO: 0.1267637885385282 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 36 FOR SAMPLE RATIO: 0.136791609831135 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 37 FOR SAMPLE RATIO: 0.14761269551758632 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 38 FOR SAMPLE RATIO: 0.15928979785285172 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 39 FOR SAMPLE RATIO: 0.17189063319407666 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 40 FOR SAMPLE RATIO: 0.1854882746926134 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 41 FOR SAMPLE RATIO: 0.2001615760504861 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 42 FOR SAMPLE RATIO: 0.2159956287986867 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 43 FOR SAMPLE RATIO: 0.2330822557490886 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 44 FOR SAMPLE RATIO: 0.25152054348154407 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 45 FOR SAMPLE RATIO: 0.2714174169540947 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 46 FOR SAMPLE RATIO: 0.2928882595685008 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 47 FOR SAMPLE RATIO: 0.31605758228689596 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 48 FOR SAMPLE RATIO: 0.3410597456798199 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 49 FOR SAMPLE RATIO: 0.3680397390928415 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 50 FOR SAMPLE RATIO: 0.3971540214502115 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 51 FOR SAMPLE RATIO: 0.42857142857142855 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 52 FOR SAMPLE RATIO: 0.42857142857142855 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 53 FOR SAMPLE RATIO: 0.471272367228734 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 54 FOR SAMPLE RATIO: 0.5182278362645409 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 55 FOR SAMPLE RATIO: 0.5698617380405017 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 56 FOR SAMPLE RATIO: 0.6266402106519985 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 57 FOR SAMPLE RATIO: 0.6890758361075863 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 58 FOR SAMPLE RATIO: 0.7577322677925965 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 59 FOR SAMPLE RATIO: 0.8332293189924991 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 60 FOR SAMPLE RATIO: 0.9162485584139557 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 61 FOR SAMPLE RATIO: 1.0075394632185397 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 62 FOR SAMPLE RATIO: 1.1079261851171947 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 63 FOR SAMPLE RATIO: 1.2183149906080555 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 64 FOR SAMPLE RATIO: 1.3397024425262594 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 65 FOR SAMPLE RATIO: 1.4731843967667568 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 66 FOR SAMPLE RATIO: 1.619965895400309 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 67 FOR SAMPLE RATIO: 1.7813720454952788 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 68 FOR SAMPLE RATIO: 1.9588599818565215 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 69 FOR SAMPLE RATIO: 2.154032021678035 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 70 FOR SAMPLE RATIO: 2.3686501298663076 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 71 FOR SAMPLE RATIO: 2.6046518256237334 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 72 FOR SAMPLE RATIO: 2.8641676738927946 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 73 FOR SAMPLE RATIO: 3.1495405195694395 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 74 FOR SAMPLE RATIO: 3.4633466381273816 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 75 FOR SAMPLE RATIO: 3.808418993595927 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 76 FOR SAMPLE RATIO: 4.1878728138585934 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 77 FOR SAMPLE RATIO: 4.605133714159995 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 78 FOR SAMPLE RATIO: 5.063968622713078 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 79 FOR SAMPLE RATIO: 5.56851978759539 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 80 FOR SAMPLE RATIO: 6.123342171940293 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 81 FOR SAMPLE RATIO: 6.733444575017627 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 82 FOR SAMPLE RATIO: 7.404334850434746 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 83 FOR SAMPLE RATIO: 8.142069629676724 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 84 FOR SAMPLE RATIO: 8.953308999877509 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 85 FOR SAMPLE RATIO: 9.845376629439407 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 86 FOR SAMPLE RATIO: 10.82632588430018 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 87 FOR SAMPLE RATIO: 11.90501253172901 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 88 FOR SAMPLE RATIO: 13.091174688003248 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 89 FOR SAMPLE RATIO: 14.395520731713734 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 90 FOR SAMPLE RATIO: 15.829825976358443 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 91 FOR SAMPLE RATIO: 17.407038974960493 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 92 FOR SAMPLE RATIO: 19.141398416402414 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 93 FOR SAMPLE RATIO: 21.048561668788043 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 94 FOR SAMPLE RATIO: 23.145746130289393 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 95 FOR SAMPLE RATIO: 25.45188466355917 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 96 FOR SAMPLE RATIO: 27.98779651693255 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 97 FOR SAMPLE RATIO: 30.776375275452168 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 98 FOR SAMPLE RATIO: 33.84279553849184 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 99 FOR SAMPLE RATIO: 37.21474018981386 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 100 FOR SAMPLE RATIO: 40.92265031179709 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "============================================================\n",
      "--- STARTING STAGE 101 FOR SAMPLE RATIO: 45.0 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.43\n",
      "--- Starting Fold 2/4 ---\n",
      "Original class counts: 44 positives, 106 negatives\n",
      "Generating ratios for Regime 1 (target ratio < 0.41509433962264153)...\n",
      "Generating ratios for Regime 2 (target ratio > 0.41509433962264153)...\n",
      "\n",
      "Generated 101 unique sample ratios to test.\n",
      "First few ratios: [0.009 0.01  0.011 0.012 0.013]\n",
      "Last few ratios: [30.07 33.07 36.37 40.01 44.  ]\n",
      "============================================================\n",
      "--- STARTING STAGE 1 FOR SAMPLE RATIO: 0.009433962264150943 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 2 FOR SAMPLE RATIO: 0.010175674077613522 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 3 FOR SAMPLE RATIO: 0.01097570035098448 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 4 FOR SAMPLE RATIO: 0.01183862585178764 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 5 FOR SAMPLE RATIO: 0.01276939580862769 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 6 FOR SAMPLE RATIO: 0.013773344251163798 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 7 FOR SAMPLE RATIO: 0.014856224578213164 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 8 FOR SAMPLE RATIO: 0.01602424252916324 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 9 FOR SAMPLE RATIO: 0.01728409174764424 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 10 FOR SAMPLE RATIO: 0.018642992141269323 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 11 FOR SAMPLE RATIO: 0.020108731257273096 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 12 FOR SAMPLE RATIO: 0.021689708911163043 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 13 FOR SAMPLE RATIO: 0.02339498532414033 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 14 FOR SAMPLE RATIO: 0.025234333045154374 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 15 FOR SAMPLE RATIO: 0.02721829295514504 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 16 FOR SAMPLE RATIO: 0.02935823467441939 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 17 FOR SAMPLE RATIO: 0.031666421719344286 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 18 FOR SAMPLE RATIO: 0.03415608178175284 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 19 FOR SAMPLE RATIO: 0.03684148253381956 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 20 FOR SAMPLE RATIO: 0.03973801339282536 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 21 FOR SAMPLE RATIO: 0.042862273714386605 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 22 FOR SAMPLE RATIO: 0.04623216791956428 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 23 FOR SAMPLE RATIO: 0.04986700810100451 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 24 FOR SAMPLE RATIO: 0.05378762469612275 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 25 FOR SAMPLE RATIO: 0.05801648586157459 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 26 FOR SAMPLE RATIO: 0.06257782623312076 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 27 FOR SAMPLE RATIO: 0.06749778580877966 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 28 FOR SAMPLE RATIO: 0.07280455975117518 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 29 FOR SAMPLE RATIO: 0.07852855996756244 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 30 FOR SAMPLE RATIO: 0.08470258939350987 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 31 FOR SAMPLE RATIO: 0.09136202997901782 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 32 FOR SAMPLE RATIO: 0.09854504545437795 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 33 FOR SAMPLE RATIO: 0.1062928000377802 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 34 FOR SAMPLE RATIO: 0.1146496943380282 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 35 FOR SAMPLE RATIO: 0.12366361980426953 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 36 FOR SAMPLE RATIO: 0.1333862331809329 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 37 FOR SAMPLE RATIO: 0.14387325254071162 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 38 FOR SAMPLE RATIO: 0.15518477659208915 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 39 FOR SAMPLE RATIO: 0.167385629091287 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 40 FOR SAMPLE RATIO: 0.18054573033237956 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 41 FOR SAMPLE RATIO: 0.19474049784450168 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 42 FOR SAMPLE RATIO: 0.21005127859245196 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 43 FOR SAMPLE RATIO: 0.2265658151575358 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 44 FOR SAMPLE RATIO: 0.244378748570223 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 45 FOR SAMPLE RATIO: 0.2635921606762389 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 46 FOR SAMPLE RATIO: 0.2843161591442665 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 47 FOR SAMPLE RATIO: 0.3066695084678012 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 48 FOR SAMPLE RATIO: 0.3307803105772904 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 49 FOR SAMPLE RATIO: 0.35678673896298624 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 50 FOR SAMPLE RATIO: 0.38483783051560366 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 51 FOR SAMPLE RATIO: 0.41509433962264153 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 52 FOR SAMPLE RATIO: 0.41509433962264153 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 53 FOR SAMPLE RATIO: 0.4565407878893289 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 54 FOR SAMPLE RATIO: 0.5021255919704676 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 55 FOR SAMPLE RATIO: 0.5522619595005647 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 56 FOR SAMPLE RATIO: 0.6074043561781681 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 57 FOR SAMPLE RATIO: 0.668052625311843 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 58 FOR SAMPLE RATIO: 0.7347565186956536 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 59 FOR SAMPLE RATIO: 0.8081206798846866 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 60 FOR SAMPLE RATIO: 0.8888101250419723 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 61 FOR SAMPLE RATIO: 0.9775562710384441 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 62 FOR SAMPLE RATIO: 1.0751635654482006 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 63 FOR SAMPLE RATIO: 1.1825167785372699 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 64 FOR SAMPLE RATIO: 1.3005890233447766 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 65 FOR SAMPLE RATIO: 1.4304505765552709 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 66 FOR SAMPLE RATIO: 1.5732785801198303 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 67 FOR SAMPLE RATIO: 1.7303677115671603 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 68 FOR SAMPLE RATIO: 1.9031419197267134 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 69 FOR SAMPLE RATIO: 2.0931673322433593 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 70 FOR SAMPLE RATIO: 2.3021664518849616 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 71 FOR SAMPLE RATIO: 2.5320337703265854 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 72 FOR SAMPLE RATIO: 2.784852940943919 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 73 FOR SAMPLE RATIO: 3.062915666280269 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 74 FOR SAMPLE RATIO: 3.3687424713943006 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 75 FOR SAMPLE RATIO: 3.7051055513904427 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 76 FOR SAMPLE RATIO: 4.0750539002354875 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 77 FOR SAMPLE RATIO: 4.481940948643844 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 78 FOR SAMPLE RATIO: 4.929454961557602 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 79 FOR SAMPLE RATIO: 5.42165247076213 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 80 FOR SAMPLE RATIO: 5.96299504569023 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 81 FOR SAMPLE RATIO: 6.558389735727174 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 82 FOR SAMPLE RATIO: 7.213233550609594 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 83 FOR SAMPLE RATIO: 7.9334623821148815 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 84 FOR SAMPLE RATIO: 8.725604810496238 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 85 FOR SAMPLE RATIO: 9.596841283396731 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 86 FOR SAMPLE RATIO: 10.555069203675066 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 87 FOR SAMPLE RATIO: 11.608974516137584 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 88 FOR SAMPLE RATIO: 12.768110442081065 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 89 FOR SAMPLE RATIO: 14.042984075342718 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 90 FOR SAMPLE RATIO: 15.445151624815267 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 91 FOR SAMPLE RATIO: 16.98732316676161 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 92 FOR SAMPLE RATIO: 18.683477856465984 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 93 FOR SAMPLE RATIO: 20.548990643568267 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 94 FOR SAMPLE RATIO: 22.600771639704114 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 95 FOR SAMPLE RATIO: 24.85741940176171 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 96 FOR SAMPLE RATIO: 27.339389520205287 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 97 FOR SAMPLE RATIO: 30.06918004064966 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 98 FOR SAMPLE RATIO: 33.071535399456664 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 99 FOR SAMPLE RATIO: 36.37367072194646 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 100 FOR SAMPLE RATIO: 40.005518516395284 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 101 FOR SAMPLE RATIO: 44.0 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "--- Starting Fold 3/4 ---\n",
      "Original class counts: 44 positives, 106 negatives\n",
      "Generating ratios for Regime 1 (target ratio < 0.41509433962264153)...\n",
      "Generating ratios for Regime 2 (target ratio > 0.41509433962264153)...\n",
      "\n",
      "Generated 101 unique sample ratios to test.\n",
      "First few ratios: [0.009 0.01  0.011 0.012 0.013]\n",
      "Last few ratios: [30.07 33.07 36.37 40.01 44.  ]\n",
      "============================================================\n",
      "--- STARTING STAGE 1 FOR SAMPLE RATIO: 0.009433962264150943 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 2 FOR SAMPLE RATIO: 0.010175674077613522 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 3 FOR SAMPLE RATIO: 0.01097570035098448 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 4 FOR SAMPLE RATIO: 0.01183862585178764 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 5 FOR SAMPLE RATIO: 0.01276939580862769 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 6 FOR SAMPLE RATIO: 0.013773344251163798 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 7 FOR SAMPLE RATIO: 0.014856224578213164 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 8 FOR SAMPLE RATIO: 0.01602424252916324 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 9 FOR SAMPLE RATIO: 0.01728409174764424 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 10 FOR SAMPLE RATIO: 0.018642992141269323 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 11 FOR SAMPLE RATIO: 0.020108731257273096 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 12 FOR SAMPLE RATIO: 0.021689708911163043 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 13 FOR SAMPLE RATIO: 0.02339498532414033 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 14 FOR SAMPLE RATIO: 0.025234333045154374 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 15 FOR SAMPLE RATIO: 0.02721829295514504 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 16 FOR SAMPLE RATIO: 0.02935823467441939 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 17 FOR SAMPLE RATIO: 0.031666421719344286 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 18 FOR SAMPLE RATIO: 0.03415608178175284 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 19 FOR SAMPLE RATIO: 0.03684148253381956 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 20 FOR SAMPLE RATIO: 0.03973801339282536 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 21 FOR SAMPLE RATIO: 0.042862273714386605 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 22 FOR SAMPLE RATIO: 0.04623216791956428 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 23 FOR SAMPLE RATIO: 0.04986700810100451 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 24 FOR SAMPLE RATIO: 0.05378762469612275 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 25 FOR SAMPLE RATIO: 0.05801648586157459 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 26 FOR SAMPLE RATIO: 0.06257782623312076 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 27 FOR SAMPLE RATIO: 0.06749778580877966 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 28 FOR SAMPLE RATIO: 0.07280455975117518 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 29 FOR SAMPLE RATIO: 0.07852855996756244 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 30 FOR SAMPLE RATIO: 0.08470258939350987 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 31 FOR SAMPLE RATIO: 0.09136202997901782 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 32 FOR SAMPLE RATIO: 0.09854504545437795 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 33 FOR SAMPLE RATIO: 0.1062928000377802 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 34 FOR SAMPLE RATIO: 0.1146496943380282 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 35 FOR SAMPLE RATIO: 0.12366361980426953 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 36 FOR SAMPLE RATIO: 0.1333862331809329 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 37 FOR SAMPLE RATIO: 0.14387325254071162 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 38 FOR SAMPLE RATIO: 0.15518477659208915 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 39 FOR SAMPLE RATIO: 0.167385629091287 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 40 FOR SAMPLE RATIO: 0.18054573033237956 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 41 FOR SAMPLE RATIO: 0.19474049784450168 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 42 FOR SAMPLE RATIO: 0.21005127859245196 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 43 FOR SAMPLE RATIO: 0.2265658151575358 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 44 FOR SAMPLE RATIO: 0.244378748570223 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 45 FOR SAMPLE RATIO: 0.2635921606762389 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 46 FOR SAMPLE RATIO: 0.2843161591442665 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 47 FOR SAMPLE RATIO: 0.3066695084678012 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 48 FOR SAMPLE RATIO: 0.3307803105772904 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 49 FOR SAMPLE RATIO: 0.35678673896298624 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 50 FOR SAMPLE RATIO: 0.38483783051560366 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 51 FOR SAMPLE RATIO: 0.41509433962264153 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 52 FOR SAMPLE RATIO: 0.41509433962264153 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 53 FOR SAMPLE RATIO: 0.4565407878893289 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 54 FOR SAMPLE RATIO: 0.5021255919704676 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 55 FOR SAMPLE RATIO: 0.5522619595005647 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 56 FOR SAMPLE RATIO: 0.6074043561781681 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 57 FOR SAMPLE RATIO: 0.668052625311843 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 58 FOR SAMPLE RATIO: 0.7347565186956536 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 59 FOR SAMPLE RATIO: 0.8081206798846866 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 60 FOR SAMPLE RATIO: 0.8888101250419723 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 61 FOR SAMPLE RATIO: 0.9775562710384441 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 62 FOR SAMPLE RATIO: 1.0751635654482006 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 63 FOR SAMPLE RATIO: 1.1825167785372699 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 64 FOR SAMPLE RATIO: 1.3005890233447766 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 65 FOR SAMPLE RATIO: 1.4304505765552709 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 66 FOR SAMPLE RATIO: 1.5732785801198303 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 67 FOR SAMPLE RATIO: 1.7303677115671603 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 68 FOR SAMPLE RATIO: 1.9031419197267134 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 69 FOR SAMPLE RATIO: 2.0931673322433593 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 70 FOR SAMPLE RATIO: 2.3021664518849616 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 71 FOR SAMPLE RATIO: 2.5320337703265854 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 72 FOR SAMPLE RATIO: 2.784852940943919 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 73 FOR SAMPLE RATIO: 3.062915666280269 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 74 FOR SAMPLE RATIO: 3.3687424713943006 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 75 FOR SAMPLE RATIO: 3.7051055513904427 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 76 FOR SAMPLE RATIO: 4.0750539002354875 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 77 FOR SAMPLE RATIO: 4.481940948643844 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 78 FOR SAMPLE RATIO: 4.929454961557602 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 79 FOR SAMPLE RATIO: 5.42165247076213 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 80 FOR SAMPLE RATIO: 5.96299504569023 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 81 FOR SAMPLE RATIO: 6.558389735727174 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 82 FOR SAMPLE RATIO: 7.213233550609594 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 83 FOR SAMPLE RATIO: 7.9334623821148815 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 84 FOR SAMPLE RATIO: 8.725604810496238 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 85 FOR SAMPLE RATIO: 9.596841283396731 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 86 FOR SAMPLE RATIO: 10.555069203675066 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 87 FOR SAMPLE RATIO: 11.608974516137584 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 88 FOR SAMPLE RATIO: 12.768110442081065 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 89 FOR SAMPLE RATIO: 14.042984075342718 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 90 FOR SAMPLE RATIO: 15.445151624815267 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 91 FOR SAMPLE RATIO: 16.98732316676161 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 92 FOR SAMPLE RATIO: 18.683477856465984 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 93 FOR SAMPLE RATIO: 20.548990643568267 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 94 FOR SAMPLE RATIO: 22.600771639704114 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 95 FOR SAMPLE RATIO: 24.85741940176171 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 96 FOR SAMPLE RATIO: 27.339389520205287 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 97 FOR SAMPLE RATIO: 30.06918004064966 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 98 FOR SAMPLE RATIO: 33.071535399456664 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 99 FOR SAMPLE RATIO: 36.37367072194646 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 100 FOR SAMPLE RATIO: 40.005518516395284 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 101 FOR SAMPLE RATIO: 44.0 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "--- Starting Fold 4/4 ---\n",
      "Original class counts: 44 positives, 106 negatives\n",
      "Generating ratios for Regime 1 (target ratio < 0.41509433962264153)...\n",
      "Generating ratios for Regime 2 (target ratio > 0.41509433962264153)...\n",
      "\n",
      "Generated 101 unique sample ratios to test.\n",
      "First few ratios: [0.009 0.01  0.011 0.012 0.013]\n",
      "Last few ratios: [30.07 33.07 36.37 40.01 44.  ]\n",
      "============================================================\n",
      "--- STARTING STAGE 1 FOR SAMPLE RATIO: 0.009433962264150943 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 2 FOR SAMPLE RATIO: 0.010175674077613522 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 3 FOR SAMPLE RATIO: 0.01097570035098448 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 4 FOR SAMPLE RATIO: 0.01183862585178764 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 5 FOR SAMPLE RATIO: 0.01276939580862769 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 6 FOR SAMPLE RATIO: 0.013773344251163798 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 7 FOR SAMPLE RATIO: 0.014856224578213164 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 8 FOR SAMPLE RATIO: 0.01602424252916324 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 9 FOR SAMPLE RATIO: 0.01728409174764424 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 10 FOR SAMPLE RATIO: 0.018642992141269323 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 11 FOR SAMPLE RATIO: 0.020108731257273096 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 12 FOR SAMPLE RATIO: 0.021689708911163043 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 13 FOR SAMPLE RATIO: 0.02339498532414033 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 14 FOR SAMPLE RATIO: 0.025234333045154374 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 15 FOR SAMPLE RATIO: 0.02721829295514504 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 16 FOR SAMPLE RATIO: 0.02935823467441939 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 17 FOR SAMPLE RATIO: 0.031666421719344286 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 18 FOR SAMPLE RATIO: 0.03415608178175284 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 19 FOR SAMPLE RATIO: 0.03684148253381956 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 20 FOR SAMPLE RATIO: 0.03973801339282536 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 21 FOR SAMPLE RATIO: 0.042862273714386605 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 22 FOR SAMPLE RATIO: 0.04623216791956428 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 23 FOR SAMPLE RATIO: 0.04986700810100451 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 24 FOR SAMPLE RATIO: 0.05378762469612275 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 25 FOR SAMPLE RATIO: 0.05801648586157459 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 26 FOR SAMPLE RATIO: 0.06257782623312076 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 27 FOR SAMPLE RATIO: 0.06749778580877966 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 28 FOR SAMPLE RATIO: 0.07280455975117518 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 29 FOR SAMPLE RATIO: 0.07852855996756244 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 30 FOR SAMPLE RATIO: 0.08470258939350987 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 31 FOR SAMPLE RATIO: 0.09136202997901782 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 32 FOR SAMPLE RATIO: 0.09854504545437795 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 33 FOR SAMPLE RATIO: 0.1062928000377802 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 34 FOR SAMPLE RATIO: 0.1146496943380282 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 35 FOR SAMPLE RATIO: 0.12366361980426953 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 36 FOR SAMPLE RATIO: 0.1333862331809329 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 37 FOR SAMPLE RATIO: 0.14387325254071162 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 38 FOR SAMPLE RATIO: 0.15518477659208915 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 39 FOR SAMPLE RATIO: 0.167385629091287 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 40 FOR SAMPLE RATIO: 0.18054573033237956 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 41 FOR SAMPLE RATIO: 0.19474049784450168 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 42 FOR SAMPLE RATIO: 0.21005127859245196 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 43 FOR SAMPLE RATIO: 0.2265658151575358 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 44 FOR SAMPLE RATIO: 0.244378748570223 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 45 FOR SAMPLE RATIO: 0.2635921606762389 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 46 FOR SAMPLE RATIO: 0.2843161591442665 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 47 FOR SAMPLE RATIO: 0.3066695084678012 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 48 FOR SAMPLE RATIO: 0.3307803105772904 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 49 FOR SAMPLE RATIO: 0.35678673896298624 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 50 FOR SAMPLE RATIO: 0.38483783051560366 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 51 FOR SAMPLE RATIO: 0.41509433962264153 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 52 FOR SAMPLE RATIO: 0.41509433962264153 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 53 FOR SAMPLE RATIO: 0.4565407878893289 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 54 FOR SAMPLE RATIO: 0.5021255919704676 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 55 FOR SAMPLE RATIO: 0.5522619595005647 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 56 FOR SAMPLE RATIO: 0.6074043561781681 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 57 FOR SAMPLE RATIO: 0.668052625311843 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 58 FOR SAMPLE RATIO: 0.7347565186956536 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 59 FOR SAMPLE RATIO: 0.8081206798846866 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 60 FOR SAMPLE RATIO: 0.8888101250419723 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 61 FOR SAMPLE RATIO: 0.9775562710384441 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 62 FOR SAMPLE RATIO: 1.0751635654482006 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 63 FOR SAMPLE RATIO: 1.1825167785372699 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 64 FOR SAMPLE RATIO: 1.3005890233447766 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 65 FOR SAMPLE RATIO: 1.4304505765552709 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 66 FOR SAMPLE RATIO: 1.5732785801198303 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 67 FOR SAMPLE RATIO: 1.7303677115671603 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 68 FOR SAMPLE RATIO: 1.9031419197267134 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 69 FOR SAMPLE RATIO: 2.0931673322433593 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 70 FOR SAMPLE RATIO: 2.3021664518849616 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 71 FOR SAMPLE RATIO: 2.5320337703265854 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 72 FOR SAMPLE RATIO: 2.784852940943919 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 73 FOR SAMPLE RATIO: 3.062915666280269 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 74 FOR SAMPLE RATIO: 3.3687424713943006 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 75 FOR SAMPLE RATIO: 3.7051055513904427 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 76 FOR SAMPLE RATIO: 4.0750539002354875 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 77 FOR SAMPLE RATIO: 4.481940948643844 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 78 FOR SAMPLE RATIO: 4.929454961557602 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 79 FOR SAMPLE RATIO: 5.42165247076213 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 80 FOR SAMPLE RATIO: 5.96299504569023 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 81 FOR SAMPLE RATIO: 6.558389735727174 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 82 FOR SAMPLE RATIO: 7.213233550609594 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 83 FOR SAMPLE RATIO: 7.9334623821148815 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 84 FOR SAMPLE RATIO: 8.725604810496238 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 85 FOR SAMPLE RATIO: 9.596841283396731 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 86 FOR SAMPLE RATIO: 10.555069203675066 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 87 FOR SAMPLE RATIO: 11.608974516137584 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 88 FOR SAMPLE RATIO: 12.768110442081065 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 89 FOR SAMPLE RATIO: 14.042984075342718 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 90 FOR SAMPLE RATIO: 15.445151624815267 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 91 FOR SAMPLE RATIO: 16.98732316676161 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 92 FOR SAMPLE RATIO: 18.683477856465984 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 93 FOR SAMPLE RATIO: 20.548990643568267 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 94 FOR SAMPLE RATIO: 22.600771639704114 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 95 FOR SAMPLE RATIO: 24.85741940176171 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 96 FOR SAMPLE RATIO: 27.339389520205287 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 97 FOR SAMPLE RATIO: 30.06918004064966 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 98 FOR SAMPLE RATIO: 33.071535399456664 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 99 FOR SAMPLE RATIO: 36.37367072194646 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 100 FOR SAMPLE RATIO: 40.005518516395284 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n",
      "============================================================\n",
      "--- STARTING STAGE 101 FOR SAMPLE RATIO: 44.0 ---\n",
      "============================================================\n",
      "Original sample ratio (positive:negative): 0.42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#store the weighted classifiers of all the folds\n",
    "list_folds_weighted_clfs = []  # List to store classifiers from all folds\n",
    "#store the best classfiers of all the folds \n",
    "list_folds_best_models = []  # List to store best models from all folds\n",
    "\n",
    "list_weighted_clfs = []\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_data[:, :-1], train_data[:, -1])):\n",
    "    list_weighted_clfs = [] # List to store the models trained at each stage\n",
    "    best_clfs = [] # List to store the best models from each stage\n",
    "\n",
    "    print(f\"--- Starting Fold {fold + 1}/{K_FOLDS} ---\")\n",
    "    X_train_fold = train_data[train_ids][:, :-1]\n",
    "    y_train_fold = train_data[train_ids][:, -1]\n",
    "    X_test_fold = train_data[val_ids][:, :-1]\n",
    "    y_test_fold = train_data[val_ids][:, -1]\n",
    "    all_ratios = generate_ratios(np.c_[X_train_fold, y_train_fold])  # Combine features and labels for ratio generation\n",
    "    for i, sample_ratio in enumerate(all_ratios):\n",
    "        print(\"=\"*60)\n",
    "        print(f\"--- STARTING STAGE {i+1} FOR SAMPLE RATIO: {sample_ratio} ---\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        undersampled_train_dataset = undersample_dataset(np.c_[X_train_fold, y_train_fold], sample_ratio)\n",
    "        X_train_resampled = undersampled_train_dataset[:, :-1]  # All columns except the last one\n",
    "        y_train_resampled = undersampled_train_dataset[:, -1]   # Last column is the target\n",
    "\n",
    "        clf_weighted = LogisticRegression(fit_intercept=True)\n",
    "        clf_weighted.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        res = get_fpr_tpr(clf_weighted, X_test_fold, y_test_fold, threshold=0.5)\n",
    "\n",
    "        array_of_all_fprs, array_of_all_tprs, threshold_vals = roc_curve(y_test_fold, clf_weighted.predict_proba(X_test_fold)[:, 1])\n",
    "\n",
    "        current_result = {\n",
    "            \"model\": clf_weighted,\n",
    "            \"fpr\": res[\"fpr\"],\n",
    "            \"tpr\": res[\"tpr\"],\n",
    "            \"threshold\": 0.5,\n",
    "            \"full_roc\": {\"fpr\": array_of_all_fprs, \"tpr\": array_of_all_tprs, \"thresholds\": threshold_vals},\n",
    "        }\n",
    "        list_weighted_clfs.append(current_result)\n",
    "    best_clfs = get_best_roc_envelope(list_weighted_clfs, results_original_roc)\n",
    "    list_folds_weighted_clfs.append(list_weighted_clfs)\n",
    "    list_folds_best_models.append(best_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7be1feb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to pickle/Logistic_breast_cancer_undersampling.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_models': [[{'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.21428571428571427),\n",
       "    'threshold': np.float64(0.05816122418176974)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.027777777777777776),\n",
       "    'tpr': np.float64(0.35714285714285715),\n",
       "    'threshold': np.float64(0.0460789645546135)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05555555555555555),\n",
       "    'tpr': np.float64(0.42857142857142855),\n",
       "    'threshold': np.float64(0.03216388685916777)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.1111111111111111),\n",
       "    'tpr': np.float64(0.5),\n",
       "    'threshold': np.float64(0.9445188272914538)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.1388888888888889),\n",
       "    'tpr': np.float64(0.6428571428571429),\n",
       "    'threshold': np.float64(0.75958147852421)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2222222222222222),\n",
       "    'tpr': np.float64(0.7142857142857143),\n",
       "    'threshold': np.float64(0.689571151893318)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.25),\n",
       "    'tpr': np.float64(0.7857142857142857),\n",
       "    'threshold': np.float64(0.9740539020258188)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.3055555555555556),\n",
       "    'tpr': np.float64(0.8571428571428571),\n",
       "    'threshold': np.float64(0.9685520307985567)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5),\n",
       "    'tpr': np.float64(0.9285714285714286),\n",
       "    'threshold': np.float64(0.9539605086148901)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7222222222222222),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': np.float64(0.7802534242867217)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.4666666666666667),\n",
       "    'threshold': np.float64(0.935122146879399)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': np.float64(0.938386943830816)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': np.float64(0.9055596948659116)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08571428571428572),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': np.float64(0.8837232151970016)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.17142857142857143),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': np.float64(0.005273684161205398)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': np.float64(0.4291183217101296)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2571428571428571),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': np.float64(0.00957708960987918)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.5333333333333333),\n",
       "    'threshold': np.float64(0.9837449550113558)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08571428571428572),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': np.float64(0.9910147083787307)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': np.float64(0.83349453126322)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.22857142857142856),\n",
       "    'tpr': np.float64(0.7333333333333333),\n",
       "    'threshold': np.float64(0.7294256755269074)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2857142857142857),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': np.float64(0.9314295799790461)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.4),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': np.float64(0.08456127908319917)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.4857142857142857),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': np.float64(0.8878293172190501)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6571428571428571),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': np.float64(0.8330453889908211)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}],\n",
       "  [{'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': np.float64(0.6198269456129525)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.3333333333333333),\n",
       "    'threshold': np.float64(0.01572438629549039)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.5333333333333333),\n",
       "    'threshold': np.float64(0.9933707725561904)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2571428571428571),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': np.float64(0.006649587817830314)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2857142857142857),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': np.float64(0.006187157193719549)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.3142857142857143),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': np.float64(0.7541603957790403)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5428571428571428),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': np.float64(0.6051335905634316)},\n",
       "   {'model': 'original_baseline',\n",
       "    'fpr': np.float64(0.6666666666666666),\n",
       "    'tpr': np.float64(0.8846153846153846),\n",
       "    'threshold': np.float64(0.12646516222523113)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6857142857142857),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': np.float64(0.9744405810901278)},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7142857142857143),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': np.float64(0.9479073189356759)},\n",
       "   {'model': 'end', 'fpr': 1.0, 'tpr': 1.0, 'threshold': 0.0}]],\n",
       " 'weighted_clfs': [[{'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.11111111, 0.11111111, 0.16666667,\n",
       "            0.16666667, 0.30555556, 0.30555556, 0.61111111, 0.61111111,\n",
       "            0.72222222, 0.72222222, 0.75      , 0.75      , 0.77777778,\n",
       "            0.77777778, 0.86111111, 0.86111111, 0.94444444, 0.94444444,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.42857143, 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.19239671, 0.02239233, 0.02227182, 0.01354167,\n",
       "            0.01255173, 0.00805547, 0.00785393, 0.00519199, 0.00475874,\n",
       "            0.00379022, 0.0035337 , 0.003328  , 0.00332306, 0.0030472 ,\n",
       "            0.00247058, 0.00239924, 0.00234538, 0.00155689, 0.00142334,\n",
       "            0.00132443])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.08333333, 0.08333333,\n",
       "            0.11111111, 0.11111111, 0.16666667, 0.16666667, 0.22222222,\n",
       "            0.22222222, 0.38888889, 0.38888889, 0.41666667, 0.41666667,\n",
       "            0.44444444, 0.44444444, 0.94444444, 0.94444444, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.42857143, 0.42857143, 0.57142857, 0.57142857, 0.71428571,\n",
       "            0.71428571, 0.85714286, 0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.02820177, 0.02661818, 0.02256172, 0.02194755,\n",
       "            0.02092101, 0.01611837, 0.01347788, 0.01257122, 0.01136052,\n",
       "            0.01116547, 0.00660453, 0.00571123, 0.00560277, 0.00469171,\n",
       "            0.0046541 , 0.00439741, 0.00141497, 0.00117163, 0.00054526])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.08333333, 0.08333333,\n",
       "            0.16666667, 0.16666667, 0.33333333, 0.33333333, 0.38888889,\n",
       "            0.38888889, 0.52777778, 0.52777778, 0.58333333, 0.58333333,\n",
       "            0.61111111, 0.61111111, 0.63888889, 0.63888889, 0.69444444,\n",
       "            0.69444444, 0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.14285714, 0.14285714, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.42857143, 0.42857143, 0.5       , 0.5       , 0.64285714,\n",
       "            0.64285714, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.10631624, 0.03116471, 0.02028914, 0.01711846,\n",
       "            0.01147381, 0.01095659, 0.00840108, 0.00818478, 0.00791228,\n",
       "            0.00739643, 0.00513319, 0.00510634, 0.00408319, 0.00351386,\n",
       "            0.00335486, 0.00295791, 0.00282803, 0.00276439, 0.00267342,\n",
       "            0.00200468, 0.00106312, 0.00088369, 0.00061282])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.11111111, 0.11111111, 0.16666667,\n",
       "            0.16666667, 0.30555556, 0.30555556, 0.61111111, 0.61111111,\n",
       "            0.72222222, 0.72222222, 0.75      , 0.75      , 0.77777778,\n",
       "            0.77777778, 0.86111111, 0.86111111, 0.94444444, 0.94444444,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.42857143, 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.19239671, 0.02239233, 0.02227182, 0.01354167,\n",
       "            0.01255173, 0.00805547, 0.00785393, 0.00519199, 0.00475874,\n",
       "            0.00379022, 0.0035337 , 0.003328  , 0.00332306, 0.0030472 ,\n",
       "            0.00247058, 0.00239924, 0.00234538, 0.00155689, 0.00142334,\n",
       "            0.00132443])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.11111111, 0.11111111,\n",
       "            0.25      , 0.25      , 0.33333333, 0.33333333, 0.38888889,\n",
       "            0.38888889, 0.44444444, 0.44444444, 0.5       , 0.5       ,\n",
       "            0.55555556, 0.55555556, 0.69444444, 0.69444444, 0.77777778,\n",
       "            0.77777778, 0.94444444, 0.94444444, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.28571429, 0.28571429,\n",
       "            0.42857143, 0.42857143, 0.5       , 0.5       , 0.64285714,\n",
       "            0.64285714, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.06140412, 0.03922323, 0.02501366, 0.02061711,\n",
       "            0.01247717, 0.01215732, 0.01022985, 0.00899317, 0.00718135,\n",
       "            0.0055341 , 0.00541889, 0.0049282 , 0.00416759, 0.00376627,\n",
       "            0.00357143, 0.00325857, 0.00302942, 0.00290031, 0.00267997,\n",
       "            0.00222734, 0.00191749, 0.00136024, 0.00107899])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.13888889,\n",
       "            0.13888889, 0.19444444, 0.19444444, 0.25      , 0.25      ,\n",
       "            0.30555556, 0.30555556, 0.33333333, 0.33333333, 0.36111111,\n",
       "            0.36111111, 0.47222222, 0.47222222, 0.5       , 0.5       ,\n",
       "            0.55555556, 0.55555556, 0.75      , 0.75      , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.57142857, 0.57142857,\n",
       "            0.64285714, 0.64285714, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.85714286, 0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.05052921, 0.03922021, 0.02139308, 0.01302312,\n",
       "            0.01107832, 0.00943273, 0.00922733, 0.00899681, 0.00805532,\n",
       "            0.00732593, 0.00611563, 0.00609066, 0.00601901, 0.00566482,\n",
       "            0.00528098, 0.00466936, 0.00443581, 0.00438952, 0.00422922,\n",
       "            0.00343556, 0.00297258, 0.00274296, 0.00270117, 0.00154356])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05555556, 0.05555556,\n",
       "            0.11111111, 0.11111111, 0.19444444, 0.19444444, 0.22222222,\n",
       "            0.22222222, 0.33333333, 0.33333333, 0.36111111, 0.36111111,\n",
       "            0.5       , 0.5       , 0.86111111, 0.86111111, 0.97222222,\n",
       "            0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.14285714, 0.14285714, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.57142857, 0.57142857, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.05513259, 0.04524112, 0.02670297, 0.02215239,\n",
       "            0.02061634, 0.01817825, 0.01256831, 0.01239771, 0.01103949,\n",
       "            0.01025284, 0.00934047, 0.00771649, 0.00758572, 0.00731236,\n",
       "            0.0048537 , 0.00473411, 0.00262755, 0.00255521, 0.00178795,\n",
       "            0.00167111, 0.00162762])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05555556, 0.05555556,\n",
       "            0.11111111, 0.11111111, 0.19444444, 0.19444444, 0.22222222,\n",
       "            0.22222222, 0.33333333, 0.33333333, 0.36111111, 0.36111111,\n",
       "            0.5       , 0.5       , 0.86111111, 0.86111111, 0.97222222,\n",
       "            0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.14285714, 0.14285714, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.57142857, 0.57142857, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.05513259, 0.04524112, 0.02670297, 0.02215239,\n",
       "            0.02061634, 0.01817825, 0.01256831, 0.01239771, 0.01103949,\n",
       "            0.01025284, 0.00934047, 0.00771649, 0.00758572, 0.00731236,\n",
       "            0.0048537 , 0.00473411, 0.00262755, 0.00255521, 0.00178795,\n",
       "            0.00167111, 0.00162762])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.13888889, 0.13888889, 0.22222222,\n",
       "            0.22222222, 0.27777778, 0.27777778, 0.36111111, 0.36111111,\n",
       "            0.38888889, 0.38888889, 0.44444444, 0.44444444, 0.66666667,\n",
       "            0.66666667, 0.69444444, 0.69444444, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.71428571, 0.71428571,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.08859697, 0.02055527, 0.019401  , 0.01787121,\n",
       "            0.01678735, 0.01583175, 0.01543013, 0.00952984, 0.0069473 ,\n",
       "            0.00682052, 0.00666271, 0.00591755, 0.00500894, 0.00304284,\n",
       "            0.00277454, 0.00215866, 0.00199207, 0.00062139, 0.00054507])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.08333333, 0.08333333, 0.16666667,\n",
       "            0.16666667, 0.19444444, 0.19444444, 0.33333333, 0.33333333,\n",
       "            0.44444444, 0.44444444, 0.5       , 0.5       , 0.55555556,\n",
       "            0.55555556, 0.61111111, 0.61111111, 0.77777778, 0.77777778,\n",
       "            0.94444444, 0.94444444, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.57142857, 0.57142857,\n",
       "            0.64285714, 0.64285714, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.02641582, 0.01460018, 0.01336346, 0.01156493,\n",
       "            0.01114484, 0.01080032, 0.01056554, 0.00883195, 0.00855371,\n",
       "            0.00748434, 0.00738416, 0.00672516, 0.00666914, 0.00613707,\n",
       "            0.00598864, 0.00521377, 0.00490128, 0.00404926, 0.0040484 ,\n",
       "            0.00293871, 0.00282367, 0.00214241, 0.00210379])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08333333, 0.08333333, 0.33333333,\n",
       "            0.33333333, 0.38888889, 0.38888889, 0.44444444, 0.44444444,\n",
       "            0.47222222, 0.47222222, 0.63888889, 0.63888889, 0.69444444,\n",
       "            0.69444444, 0.77777778, 0.77777778, 0.80555556, 0.80555556,\n",
       "            0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.35714286, 0.35714286, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.57142857, 0.57142857,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.16024536, 0.06920144, 0.06707328, 0.02407182,\n",
       "            0.02390732, 0.02164581, 0.01863025, 0.01619249, 0.01605459,\n",
       "            0.01505024, 0.01382386, 0.01160267, 0.01100113, 0.00761289,\n",
       "            0.00662146, 0.00540572, 0.00480555, 0.00478338, 0.00465995,\n",
       "            0.00345742, 0.00312502, 0.00255195])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08333333, 0.08333333,\n",
       "            0.11111111, 0.11111111, 0.13888889, 0.13888889, 0.22222222,\n",
       "            0.22222222, 0.25      , 0.25      , 0.30555556, 0.30555556,\n",
       "            0.52777778, 0.52777778, 0.91666667, 0.91666667, 0.94444444,\n",
       "            0.94444444, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.14285714, 0.14285714, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.42857143, 0.42857143, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.29821351, 0.12104594, 0.07422553, 0.04528247,\n",
       "            0.04243411, 0.02814952, 0.02600089, 0.02534954, 0.01890049,\n",
       "            0.01809758, 0.01668282, 0.01013779, 0.00940918, 0.00907836,\n",
       "            0.00642096, 0.00631381, 0.00232337, 0.00159725, 0.00158263,\n",
       "            0.00148952, 0.00122412])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02777778, 0.02777778,\n",
       "            0.05555556, 0.05555556, 0.22222222, 0.22222222, 0.52777778,\n",
       "            0.52777778, 0.55555556, 0.55555556, 0.63888889, 0.63888889,\n",
       "            0.77777778, 0.77777778, 0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.21428571, 0.21428571, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.64285714, 0.64285714, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.11334393, 0.05816122, 0.05393063, 0.04607896,\n",
       "            0.03274631, 0.03216389, 0.02125877, 0.0210865 , 0.01294779,\n",
       "            0.01238856, 0.01191252, 0.01140458, 0.01027702, 0.01015934,\n",
       "            0.00690567, 0.00678112, 0.00421622, 0.00415962, 0.00343793])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05555556, 0.05555556, 0.11111111,\n",
       "            0.11111111, 0.13888889, 0.13888889, 0.44444444, 0.44444444,\n",
       "            0.5       , 0.5       , 0.52777778, 0.52777778, 0.63888889,\n",
       "            0.63888889, 0.66666667, 0.66666667, 0.69444444, 0.69444444,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.35714286, 0.35714286, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.24421846, 0.05362432, 0.05322191, 0.04909455,\n",
       "            0.02571688, 0.02117858, 0.02036877, 0.008646  , 0.00757537,\n",
       "            0.00691548, 0.00652072, 0.00543789, 0.00524142, 0.00411   ,\n",
       "            0.00387215, 0.00382962, 0.00354913, 0.00350055, 0.00313985,\n",
       "            0.00104344, 0.00052056])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.11111111, 0.11111111, 0.16666667,\n",
       "            0.16666667, 0.27777778, 0.27777778, 0.47222222, 0.47222222,\n",
       "            0.52777778, 0.52777778, 0.75      , 0.75      , 0.80555556,\n",
       "            0.80555556, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.42857143, 0.42857143, 0.57142857, 0.57142857, 0.71428571,\n",
       "            0.71428571, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.1272557 , 0.08929831, 0.08388797, 0.05528646,\n",
       "            0.02615213, 0.01747267, 0.01524621, 0.0089556 , 0.00813379,\n",
       "            0.00792529, 0.00681846, 0.00209304, 0.00196199, 0.00178096,\n",
       "            0.00171436, 0.0007426 , 0.00051289])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.16666667, 0.16666667,\n",
       "            0.22222222, 0.22222222, 0.25      , 0.25      , 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.38888889, 0.38888889,\n",
       "            0.41666667, 0.41666667, 0.47222222, 0.47222222, 0.97222222,\n",
       "            0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.28571429,\n",
       "            0.28571429, 0.35714286, 0.35714286, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.64285714, 0.64285714, 0.71428571,\n",
       "            0.71428571, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.1707914 , 0.13665905, 0.07455303, 0.05733999,\n",
       "            0.04688869, 0.04427451, 0.04191153, 0.0346496 , 0.03154074,\n",
       "            0.02911362, 0.02884469, 0.02779909, 0.02383674, 0.02234905,\n",
       "            0.021052  , 0.01614662, 0.01386614, 0.01309603, 0.00279637,\n",
       "            0.00213664, 0.00155605])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.08333333, 0.08333333,\n",
       "            0.16666667, 0.16666667, 0.19444444, 0.19444444, 0.25      ,\n",
       "            0.25      , 0.27777778, 0.27777778, 0.41666667, 0.41666667,\n",
       "            0.44444444, 0.44444444, 0.5       , 0.5       , 0.61111111,\n",
       "            0.61111111, 0.63888889, 0.63888889, 0.97222222, 0.97222222,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.5       , 0.5       , 0.57142857, 0.57142857, 0.64285714,\n",
       "            0.64285714, 0.71428571, 0.71428571, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.38891684, 0.27410038, 0.13426451, 0.06503292,\n",
       "            0.0489018 , 0.0373193 , 0.03534662, 0.03391113, 0.0303485 ,\n",
       "            0.02500646, 0.02147913, 0.01964313, 0.01327034, 0.01273702,\n",
       "            0.01222972, 0.0122221 , 0.0120207 , 0.01070496, 0.00778647,\n",
       "            0.00614169, 0.00602753, 0.00504619, 0.00163776, 0.00147006,\n",
       "            0.00063954])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.08333333,\n",
       "            0.08333333, 0.11111111, 0.11111111, 0.16666667, 0.16666667,\n",
       "            0.30555556, 0.30555556, 0.44444444, 0.44444444, 0.5       ,\n",
       "            0.5       , 0.61111111, 0.61111111, 0.66666667, 0.66666667,\n",
       "            0.77777778, 0.77777778, 0.80555556, 0.80555556, 0.97222222,\n",
       "            0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.21428571, 0.21428571, 0.28571429,\n",
       "            0.28571429, 0.35714286, 0.35714286, 0.42857143, 0.42857143,\n",
       "            0.5       , 0.5       , 0.64285714, 0.64285714, 0.71428571,\n",
       "            0.71428571, 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.11560478, 0.1031026 , 0.08908643, 0.08573167,\n",
       "            0.08190516, 0.06975709, 0.06350172, 0.0369461 , 0.03084764,\n",
       "            0.02314092, 0.02208557, 0.012362  , 0.01019129, 0.00726294,\n",
       "            0.00716252, 0.0062524 , 0.00593172, 0.00523884, 0.00495924,\n",
       "            0.00339193, 0.00237806, 0.00208961, 0.00198255, 0.00039914,\n",
       "            0.00036356, 0.00019431])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02777778, 0.02777778, 0.08333333,\n",
       "            0.08333333, 0.11111111, 0.11111111, 0.19444444, 0.19444444,\n",
       "            0.27777778, 0.27777778, 0.38888889, 0.38888889, 0.55555556,\n",
       "            0.55555556, 0.61111111, 0.61111111, 0.77777778, 0.77777778,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.92857143,\n",
       "            0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.22982256, 0.15805681, 0.15195888, 0.08788952,\n",
       "            0.05950389, 0.05901734, 0.0436368 , 0.0373458 , 0.03405937,\n",
       "            0.02959038, 0.02165904, 0.01583389, 0.01543948, 0.01025098,\n",
       "            0.01007747, 0.00830029, 0.00784115, 0.00390104, 0.0025378 ,\n",
       "            0.00074471, 0.00073956])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.11111111, 0.11111111,\n",
       "            0.19444444, 0.19444444, 0.22222222, 0.22222222, 0.25      ,\n",
       "            0.25      , 0.27777778, 0.27777778, 0.30555556, 0.30555556,\n",
       "            0.33333333, 0.33333333, 0.80555556, 0.80555556, 0.94444444,\n",
       "            0.94444444, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.28571429, 0.28571429,\n",
       "            0.42857143, 0.42857143, 0.57142857, 0.57142857, 0.71428571,\n",
       "            0.71428571, 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.22186674, 0.20868354, 0.17540246, 0.13591536,\n",
       "            0.08329646, 0.05307452, 0.05051456, 0.04781692, 0.04281369,\n",
       "            0.01933009, 0.01815181, 0.01485641, 0.01382884, 0.01134173,\n",
       "            0.01130144, 0.00992211, 0.00286012, 0.0026404 , 0.0017816 ,\n",
       "            0.00161125, 0.0004957 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.11111111, 0.11111111,\n",
       "            0.13888889, 0.13888889, 0.25      , 0.25      , 0.36111111,\n",
       "            0.36111111, 0.38888889, 0.38888889, 0.55555556, 0.55555556,\n",
       "            0.63888889, 0.63888889, 0.66666667, 0.66666667, 0.72222222,\n",
       "            0.72222222, 0.88888889, 0.88888889, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.42857143, 0.42857143, 0.57142857, 0.57142857, 0.64285714,\n",
       "            0.64285714, 0.71428571, 0.71428571, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.40474015, 0.21596978, 0.09480782, 0.0839112 ,\n",
       "            0.08320337, 0.0785132 , 0.06154229, 0.0590828 , 0.03580268,\n",
       "            0.02752222, 0.02737174, 0.02474931, 0.01413219, 0.01389636,\n",
       "            0.01225156, 0.01200007, 0.01192302, 0.0105211 , 0.00914605,\n",
       "            0.0084329 , 0.00428703, 0.00338567, 0.0024178 , 0.00231365])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.11111111, 0.11111111, 0.13888889,\n",
       "            0.13888889, 0.25      , 0.25      , 0.27777778, 0.27777778,\n",
       "            0.36111111, 0.36111111, 0.69444444, 0.69444444, 0.75      ,\n",
       "            0.75      , 0.80555556, 0.80555556, 0.86111111, 0.86111111,\n",
       "            0.94444444, 0.94444444, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.21428571, 0.21428571, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.20120833, 0.10142219, 0.07253491, 0.06354732,\n",
       "            0.06125695, 0.04410396, 0.03761174, 0.03543563, 0.03232319,\n",
       "            0.02578056, 0.02244039, 0.01074729, 0.0099951 , 0.00820974,\n",
       "            0.00810677, 0.00748134, 0.00717547, 0.00661197, 0.004527  ,\n",
       "            0.00401334, 0.00341541, 0.00237754, 0.00173784])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.16666667, 0.16666667,\n",
       "            0.27777778, 0.27777778, 0.33333333, 0.33333333, 0.38888889,\n",
       "            0.38888889, 0.44444444, 0.44444444, 0.69444444, 0.69444444,\n",
       "            0.75      , 0.75      , 0.80555556, 0.80555556, 0.83333333,\n",
       "            0.83333333, 0.88888889, 0.88888889, 0.97222222, 0.97222222,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.35714286, 0.35714286,\n",
       "            0.42857143, 0.42857143, 0.5       , 0.5       , 0.57142857,\n",
       "            0.57142857, 0.71428571, 0.71428571, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.18255392, 0.10712493, 0.07560187, 0.06567267,\n",
       "            0.05474327, 0.05179906, 0.04526493, 0.04492425, 0.04178957,\n",
       "            0.04119672, 0.03784335, 0.03666432, 0.02188903, 0.02081582,\n",
       "            0.01868317, 0.01709591, 0.01606704, 0.01463899, 0.0126811 ,\n",
       "            0.01241243, 0.01114624, 0.01067048, 0.00826608, 0.00766441,\n",
       "            0.00498528])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.13888889, 0.13888889, 0.16666667,\n",
       "            0.16666667, 0.33333333, 0.33333333, 0.36111111, 0.36111111,\n",
       "            0.58333333, 0.58333333, 0.61111111, 0.61111111, 0.69444444,\n",
       "            0.69444444, 0.80555556, 0.80555556, 0.86111111, 0.86111111,\n",
       "            0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.57142857, 0.57142857,\n",
       "            0.64285714, 0.64285714, 0.78571429, 0.78571429, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.23889909, 0.16951885, 0.09414292, 0.09068403,\n",
       "            0.07640452, 0.0477243 , 0.04284004, 0.04075795, 0.03564383,\n",
       "            0.0211947 , 0.02052424, 0.02005388, 0.01957945, 0.01700775,\n",
       "            0.01698988, 0.01308272, 0.01099556, 0.00936608, 0.00868833,\n",
       "            0.00785357, 0.00634057, 0.00454558])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.16666667, 0.16666667,\n",
       "            0.25      , 0.25      , 0.30555556, 0.30555556, 0.36111111,\n",
       "            0.36111111, 0.41666667, 0.41666667, 0.72222222, 0.72222222,\n",
       "            0.86111111, 0.86111111, 0.97222222, 0.97222222, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.28571429,\n",
       "            0.28571429, 0.35714286, 0.35714286, 0.42857143, 0.42857143,\n",
       "            0.5       , 0.5       , 0.64285714, 0.64285714, 0.71428571,\n",
       "            0.71428571, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.35431717, 0.18003147, 0.07138865, 0.06431955,\n",
       "            0.06108405, 0.05736637, 0.05365809, 0.05255312, 0.04910508,\n",
       "            0.04854296, 0.04622102, 0.03683584, 0.02268976, 0.02243098,\n",
       "            0.01707508, 0.01676013, 0.01052124, 0.00832262, 0.008283  ,\n",
       "            0.00439297])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.05555556, 0.05555556,\n",
       "            0.11111111, 0.11111111, 0.38888889, 0.38888889, 0.44444444,\n",
       "            0.44444444, 0.52777778, 0.52777778, 0.63888889, 0.63888889,\n",
       "            0.69444444, 0.69444444, 0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.14285714, 0.14285714, 0.28571429,\n",
       "            0.28571429, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.71428571, 0.71428571, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.416847  , 0.22951941, 0.15348589, 0.13996913,\n",
       "            0.08859721, 0.08705209, 0.04459011, 0.03941784, 0.03466145,\n",
       "            0.03095456, 0.02299949, 0.02053811, 0.01450075, 0.01413281,\n",
       "            0.01319031, 0.01302842, 0.0036747 , 0.00351442, 0.00183563])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.22222222, 0.22222222, 0.36111111,\n",
       "            0.36111111, 0.58333333, 0.58333333, 0.63888889, 0.63888889,\n",
       "            0.66666667, 0.66666667, 0.69444444, 0.69444444, 0.80555556,\n",
       "            0.80555556, 0.83333333, 0.83333333, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.21428571, 0.21428571,\n",
       "            0.35714286, 0.35714286, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.3546165 , 0.09797867, 0.07598818, 0.04575019,\n",
       "            0.03501302, 0.02151203, 0.02079022, 0.0196504 , 0.01958316,\n",
       "            0.01704912, 0.01688479, 0.01612796, 0.01609991, 0.01210744,\n",
       "            0.0081401 , 0.00751546, 0.00597681, 0.00432101, 0.0042705 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05555555555555555),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.08333333,\n",
       "            0.08333333, 0.19444444, 0.19444444, 0.33333333, 0.33333333,\n",
       "            0.44444444, 0.44444444, 0.47222222, 0.47222222, 0.5       ,\n",
       "            0.5       , 0.55555556, 0.55555556, 0.69444444, 0.69444444,\n",
       "            0.80555556, 0.80555556, 0.94444444, 0.94444444, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.21428571, 0.21428571, 0.28571429,\n",
       "            0.28571429, 0.35714286, 0.35714286, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.64285714, 0.64285714, 0.71428571,\n",
       "            0.71428571, 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.57419149, 0.54417543, 0.23490938, 0.22775937,\n",
       "            0.21766041, 0.09225673, 0.0916049 , 0.05309074, 0.0519247 ,\n",
       "            0.04120682, 0.04012938, 0.03409869, 0.02684879, 0.02517112,\n",
       "            0.02454499, 0.02294342, 0.02256583, 0.0147155 , 0.01328338,\n",
       "            0.00982061, 0.00889185, 0.00697923, 0.00680064, 0.00373678,\n",
       "            0.00191062])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.19444444, 0.19444444, 0.30555556,\n",
       "            0.30555556, 0.47222222, 0.47222222, 0.5       , 0.5       ,\n",
       "            0.52777778, 0.52777778, 0.58333333, 0.58333333, 0.69444444,\n",
       "            0.69444444, 0.72222222, 0.72222222, 0.83333333, 0.83333333,\n",
       "            0.88888889, 0.88888889, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.35714286, 0.35714286, 0.42857143,\n",
       "            0.42857143, 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.55973229, 0.16483227, 0.16479265, 0.09511235,\n",
       "            0.07281985, 0.05556427, 0.0492818 , 0.04842325, 0.04458801,\n",
       "            0.04455861, 0.04140569, 0.03861251, 0.0386028 , 0.03280862,\n",
       "            0.03259878, 0.02663339, 0.02489839, 0.02005983, 0.0183239 ,\n",
       "            0.01462017, 0.01436567, 0.01156284, 0.01043959])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.11111111, 0.11111111, 0.13888889,\n",
       "            0.13888889, 0.19444444, 0.19444444, 0.30555556, 0.30555556,\n",
       "            0.41666667, 0.41666667, 0.5       , 0.5       , 0.61111111,\n",
       "            0.61111111, 0.66666667, 0.66666667, 0.86111111, 0.86111111,\n",
       "            0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.21428571, 0.21428571, 0.35714286,\n",
       "            0.35714286, 0.5       , 0.5       , 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.34686091, 0.16391605, 0.16040608, 0.14680056,\n",
       "            0.13832529, 0.10056496, 0.0832935 , 0.07538458, 0.0710173 ,\n",
       "            0.0648931 , 0.06143113, 0.0535886 , 0.05172664, 0.04276739,\n",
       "            0.04225514, 0.04074669, 0.03586785, 0.02268969, 0.02239248,\n",
       "            0.01495651, 0.01320424, 0.01074556])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02777778, 0.02777778, 0.16666667,\n",
       "            0.16666667, 0.25      , 0.25      , 0.30555556, 0.30555556,\n",
       "            0.41666667, 0.41666667, 0.44444444, 0.44444444, 0.72222222,\n",
       "            0.72222222, 0.75      , 0.75      , 0.97222222, 0.97222222,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.21428571, 0.21428571,\n",
       "            0.28571429, 0.28571429, 0.35714286, 0.35714286, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.83355444, 0.39786518, 0.32200411, 0.14234312,\n",
       "            0.1333509 , 0.10193474, 0.10013447, 0.07134703, 0.06590839,\n",
       "            0.05050949, 0.04740124, 0.04318369, 0.02745126, 0.0111216 ,\n",
       "            0.00931067, 0.00825896, 0.007719  , 0.00166079, 0.00128255,\n",
       "            0.00127738])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.11111111, 0.11111111, 0.19444444,\n",
       "            0.19444444, 0.30555556, 0.30555556, 0.38888889, 0.38888889,\n",
       "            0.5       , 0.5       , 0.52777778, 0.52777778, 0.55555556,\n",
       "            0.55555556, 0.63888889, 0.63888889, 0.69444444, 0.69444444,\n",
       "            0.77777778, 0.77777778, 0.86111111, 0.86111111, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.85714286, 0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.42859225, 0.28478986, 0.19216751, 0.16953874,\n",
       "            0.15794319, 0.11660375, 0.10937992, 0.09648163, 0.09458089,\n",
       "            0.07227536, 0.06953592, 0.06505084, 0.05361167, 0.05110131,\n",
       "            0.03868698, 0.03513277, 0.03086829, 0.02792129, 0.02730484,\n",
       "            0.0195045 , 0.01944279, 0.01499977, 0.01312524, 0.00479104])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05555555555555555),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.19444444,\n",
       "            0.19444444, 0.25      , 0.25      , 0.30555556, 0.30555556,\n",
       "            0.33333333, 0.33333333, 0.47222222, 0.47222222, 0.52777778,\n",
       "            0.52777778, 0.61111111, 0.61111111, 0.75      , 0.75      ,\n",
       "            0.88888889, 0.88888889, 0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.35714286, 0.35714286, 0.42857143,\n",
       "            0.42857143, 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.54228021, 0.5181556 , 0.45883805, 0.25534038,\n",
       "            0.2074568 , 0.16062957, 0.12588707, 0.10998568, 0.10361707,\n",
       "            0.09802164, 0.07734689, 0.05148006, 0.04653031, 0.04466473,\n",
       "            0.04298781, 0.03932673, 0.02991556, 0.02050382, 0.02041143,\n",
       "            0.01430593, 0.01415192, 0.01266134, 0.01225354, 0.00612696])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05555555555555555),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.19444444,\n",
       "            0.19444444, 0.33333333, 0.33333333, 0.41666667, 0.41666667,\n",
       "            0.44444444, 0.44444444, 0.55555556, 0.55555556, 0.77777778,\n",
       "            0.77777778, 0.86111111, 0.86111111, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.78571429, 0.78571429, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.54480468, 0.52580462, 0.40703462, 0.2067552 ,\n",
       "            0.14367484, 0.116459  , 0.09446397, 0.07779843, 0.07755048,\n",
       "            0.0749359 , 0.07341093, 0.05439261, 0.05273836, 0.03253708,\n",
       "            0.02335384, 0.01670755, 0.01488216, 0.00771163])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.11111111, 0.11111111, 0.13888889,\n",
       "            0.13888889, 0.16666667, 0.16666667, 0.19444444, 0.19444444,\n",
       "            0.25      , 0.25      , 0.33333333, 0.33333333, 0.5       ,\n",
       "            0.5       , 0.55555556, 0.55555556, 0.58333333, 0.58333333,\n",
       "            0.86111111, 0.86111111, 0.97222222, 0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.55748424, 0.32625588, 0.25166022, 0.25136922,\n",
       "            0.22311095, 0.21135485, 0.12686707, 0.1224648 , 0.1184657 ,\n",
       "            0.11045355, 0.10518047, 0.07602687, 0.07168921, 0.0439776 ,\n",
       "            0.03706692, 0.03348815, 0.0324304 , 0.0320637 , 0.02591837,\n",
       "            0.00979636, 0.00907197, 0.00574565, 0.00549736, 0.0039467 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.33333333, 0.33333333, 0.36111111,\n",
       "            0.36111111, 0.38888889, 0.38888889, 0.44444444, 0.44444444,\n",
       "            0.52777778, 0.52777778, 0.55555556, 0.55555556, 0.69444444,\n",
       "            0.69444444, 0.72222222, 0.72222222, 0.75      , 0.75      ,\n",
       "            0.80555556, 0.80555556, 0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.37729784, 0.1693398 , 0.15801978, 0.14030519,\n",
       "            0.13151473, 0.13012512, 0.0982718 , 0.08988288, 0.08854408,\n",
       "            0.0605498 , 0.05901655, 0.05821533, 0.05778944, 0.04132786,\n",
       "            0.02959239, 0.02867369, 0.02616598, 0.02379281, 0.02249534,\n",
       "            0.02078928, 0.01927535, 0.01471165, 0.01218369, 0.00455715])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.08333333, 0.08333333,\n",
       "            0.11111111, 0.11111111, 0.13888889, 0.13888889, 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.38888889, 0.38888889,\n",
       "            0.44444444, 0.44444444, 0.5       , 0.5       , 0.52777778,\n",
       "            0.52777778, 0.77777778, 0.77777778, 0.80555556, 0.80555556,\n",
       "            0.91666667, 0.91666667, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.28571429, 0.28571429,\n",
       "            0.35714286, 0.35714286, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.41912856, 0.37896396, 0.27641029, 0.27613242,\n",
       "            0.23396348, 0.23081331, 0.21317893, 0.21162135, 0.14669194,\n",
       "            0.1440865 , 0.13324231, 0.12067133, 0.10775316, 0.10705742,\n",
       "            0.10012402, 0.09871883, 0.0740532 , 0.06815747, 0.06587506,\n",
       "            0.0585703 , 0.03767613, 0.03745312, 0.03630144, 0.03608464,\n",
       "            0.02707929, 0.0266324 , 0.01427166, 0.01181884])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.027777777777777776),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.08333333, 0.08333333, 0.11111111,\n",
       "            0.11111111, 0.13888889, 0.13888889, 0.16666667, 0.16666667,\n",
       "            0.19444444, 0.19444444, 0.22222222, 0.22222222, 0.47222222,\n",
       "            0.47222222, 0.52777778, 0.52777778, 0.55555556, 0.55555556,\n",
       "            0.58333333, 0.58333333, 0.69444444, 0.69444444, 0.83333333,\n",
       "            0.83333333, 0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.64285714, 0.64285714, 0.71428571,\n",
       "            0.71428571, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.50674595, 0.36592941, 0.30002273, 0.26845926,\n",
       "            0.22934441, 0.20776921, 0.18787233, 0.18265583, 0.13153122,\n",
       "            0.13105992, 0.129206  , 0.11080974, 0.098291  , 0.07591015,\n",
       "            0.06640492, 0.0647362 , 0.05705295, 0.05354771, 0.05077228,\n",
       "            0.04847867, 0.04679378, 0.03496334, 0.03376062, 0.02895949,\n",
       "            0.02540953, 0.01877386, 0.01535278, 0.00621057])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.19444444, 0.19444444, 0.27777778,\n",
       "            0.27777778, 0.30555556, 0.30555556, 0.41666667, 0.41666667,\n",
       "            0.5       , 0.5       , 0.55555556, 0.55555556, 0.58333333,\n",
       "            0.58333333, 0.80555556, 0.80555556, 0.94444444, 0.94444444,\n",
       "            0.97222222, 0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.28571429, 0.28571429,\n",
       "            0.35714286, 0.35714286, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.68683449, 0.2398885 , 0.22519087, 0.18253171,\n",
       "            0.16958793, 0.16797284, 0.16461471, 0.12612245, 0.12095248,\n",
       "            0.08108244, 0.08093811, 0.07865748, 0.07815972, 0.07639559,\n",
       "            0.07427215, 0.04541017, 0.04283751, 0.03510782, 0.03049759,\n",
       "            0.02558712, 0.01660336, 0.01328876])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05555555555555555),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.11111111, 0.11111111, 0.13888889,\n",
       "            0.13888889, 0.22222222, 0.22222222, 0.25      , 0.25      ,\n",
       "            0.30555556, 0.30555556, 0.38888889, 0.38888889, 0.52777778,\n",
       "            0.52777778, 0.55555556, 0.55555556, 0.63888889, 0.63888889,\n",
       "            0.69444444, 0.69444444, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.57142857, 0.57142857,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.6720458 , 0.42674499, 0.39006914, 0.35679205,\n",
       "            0.33178347, 0.27345188, 0.2316012 , 0.22020353, 0.19393477,\n",
       "            0.16687055, 0.15980617, 0.10750425, 0.10406041, 0.08473809,\n",
       "            0.07376277, 0.06800654, 0.05710344, 0.04981903, 0.04890673,\n",
       "            0.04216112, 0.04032781, 0.00954852, 0.00926976])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.027777777777777776),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08333333, 0.08333333, 0.22222222,\n",
       "            0.22222222, 0.30555556, 0.30555556, 0.38888889, 0.38888889,\n",
       "            0.41666667, 0.41666667, 0.52777778, 0.52777778, 0.88888889,\n",
       "            0.88888889, 0.97222222, 0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.28571429, 0.28571429,\n",
       "            0.35714286, 0.35714286, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.78571429, 0.78571429,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.82345831, 0.42906927, 0.31233143, 0.23702832,\n",
       "            0.21218372, 0.19261316, 0.1716411 , 0.12353759, 0.1126596 ,\n",
       "            0.10657939, 0.10491803, 0.07880086, 0.06764319, 0.01793812,\n",
       "            0.01568184, 0.00912941, 0.00823692, 0.00616975])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05555555555555555),\n",
       "    'tpr': np.float64(0.14285714285714285),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05555556, 0.05555556, 0.11111111,\n",
       "            0.11111111, 0.19444444, 0.19444444, 0.22222222, 0.22222222,\n",
       "            0.25      , 0.25      , 0.30555556, 0.30555556, 0.38888889,\n",
       "            0.38888889, 0.63888889, 0.63888889, 0.75      , 0.75      ,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.69028748, 0.59865654, 0.5203393 , 0.45907673,\n",
       "            0.33933278, 0.2789692 , 0.24492134, 0.23927806, 0.20089884,\n",
       "            0.19607077, 0.18562454, 0.13687665, 0.13005327, 0.11808049,\n",
       "            0.11263675, 0.06890712, 0.06601774, 0.04467472, 0.04256392,\n",
       "            0.01181596, 0.01029729])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08333333333333333),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.27777778, 0.27777778, 0.30555556,\n",
       "            0.30555556, 0.41666667, 0.41666667, 0.47222222, 0.47222222,\n",
       "            0.55555556, 0.55555556, 0.58333333, 0.58333333, 0.63888889,\n",
       "            0.63888889, 0.75      , 0.75      , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.72755297, 0.32325336, 0.31733651, 0.28904612,\n",
       "            0.26534579, 0.16313427, 0.14473955, 0.13069336, 0.12601377,\n",
       "            0.10902591, 0.08526041, 0.08250181, 0.08214472, 0.07947471,\n",
       "            0.06862559, 0.05661894, 0.05056377, 0.01959922])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05555555555555555),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.25      , 0.25      ,\n",
       "            0.27777778, 0.27777778, 0.30555556, 0.30555556, 0.44444444,\n",
       "            0.44444444, 0.47222222, 0.47222222, 0.5       , 0.5       ,\n",
       "            0.52777778, 0.52777778, 0.63888889, 0.63888889, 0.72222222,\n",
       "            0.72222222, 0.86111111, 0.86111111, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.28571429, 0.28571429,\n",
       "            0.35714286, 0.35714286, 0.57142857, 0.57142857, 0.64285714,\n",
       "            0.64285714, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.66206765, 0.65350408, 0.30809884, 0.30256193,\n",
       "            0.28411662, 0.28338853, 0.28156223, 0.2487392 , 0.1935817 ,\n",
       "            0.17020421, 0.13578613, 0.12846019, 0.12100213, 0.11892333,\n",
       "            0.11115275, 0.10007211, 0.08773008, 0.08439401, 0.07398527,\n",
       "            0.07235385, 0.05356584, 0.05126561, 0.02437998])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05555555555555555),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.16666667, 0.16666667, 0.19444444,\n",
       "            0.19444444, 0.25      , 0.25      , 0.36111111, 0.36111111,\n",
       "            0.38888889, 0.38888889, 0.47222222, 0.47222222, 0.5       ,\n",
       "            0.5       , 0.55555556, 0.55555556, 0.80555556, 0.80555556,\n",
       "            0.94444444, 0.94444444, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.78571429, 0.78571429, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.6781143 , 0.39620256, 0.38568375, 0.33970625,\n",
       "            0.33841324, 0.30972373, 0.30718926, 0.25140272, 0.23068031,\n",
       "            0.21787996, 0.21647839, 0.18231766, 0.17393903, 0.16595819,\n",
       "            0.16267724, 0.11960796, 0.09412024, 0.06305705, 0.06084952,\n",
       "            0.02845324, 0.02816039, 0.01921726])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08333333333333333),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.25      , 0.25      , 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.36111111, 0.36111111,\n",
       "            0.55555556, 0.55555556, 0.66666667, 0.66666667, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.78571429, 0.78571429, 0.92857143, 0.92857143,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.7308336 , 0.34451978, 0.33745393, 0.30494621,\n",
       "            0.29422617, 0.28875673, 0.23774393, 0.2203782 , 0.22001358,\n",
       "            0.15391204, 0.1164658 , 0.09489475, 0.08617565, 0.03820325,\n",
       "            0.03605975])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08333333333333333),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.13888889, 0.13888889, 0.27777778,\n",
       "            0.27777778, 0.33333333, 0.33333333, 0.36111111, 0.36111111,\n",
       "            0.55555556, 0.55555556, 0.58333333, 0.58333333, 0.61111111,\n",
       "            0.61111111, 0.66666667, 0.66666667, 0.86111111, 0.86111111,\n",
       "            0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.75187644, 0.47689098, 0.45078447, 0.34334148,\n",
       "            0.29197201, 0.28703686, 0.21639728, 0.21633335, 0.20976172,\n",
       "            0.10905543, 0.10641002, 0.10586629, 0.09491967, 0.09470222,\n",
       "            0.09128874, 0.08828895, 0.08181303, 0.03876231, 0.03821334,\n",
       "            0.03481947, 0.03163423, 0.01680036])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.1388888888888889),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.16666667, 0.16666667,\n",
       "            0.36111111, 0.36111111, 0.41666667, 0.41666667, 0.47222222,\n",
       "            0.47222222, 0.55555556, 0.55555556, 0.61111111, 0.61111111,\n",
       "            0.66666667, 0.66666667, 0.86111111, 0.86111111, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.42857143, 0.42857143,\n",
       "            0.5       , 0.5       , 0.64285714, 0.64285714, 0.78571429,\n",
       "            0.78571429, 0.85714286, 0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.7675196 , 0.74246918, 0.46389215, 0.40545555,\n",
       "            0.28403279, 0.27637272, 0.2535688 , 0.2299679 , 0.20742761,\n",
       "            0.20460807, 0.1732087 , 0.14500645, 0.14334624, 0.13771001,\n",
       "            0.1159486 , 0.1099269 , 0.07665272, 0.06931941, 0.0317289 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.1388888888888889),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.19444444, 0.19444444, 0.25      ,\n",
       "            0.25      , 0.27777778, 0.27777778, 0.33333333, 0.33333333,\n",
       "            0.63888889, 0.63888889, 0.66666667, 0.66666667, 0.72222222,\n",
       "            0.72222222, 0.75      , 0.75      , 0.88888889, 0.88888889,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.42857143, 0.42857143, 0.5       , 0.5       , 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.7643021 , 0.46614879, 0.44558961, 0.41757444,\n",
       "            0.35060743, 0.34936556, 0.3220293 , 0.29770494, 0.25602527,\n",
       "            0.14352386, 0.14135882, 0.13114809, 0.12239393, 0.10401059,\n",
       "            0.10250056, 0.08454211, 0.08085082, 0.05235669, 0.04060861,\n",
       "            0.02191733])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.1111111111111111),\n",
       "    'tpr': np.float64(0.14285714285714285),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08333333, 0.08333333, 0.16666667,\n",
       "            0.16666667, 0.25      , 0.25      , 0.33333333, 0.33333333,\n",
       "            0.5       , 0.5       , 0.52777778, 0.52777778, 0.55555556,\n",
       "            0.55555556, 0.58333333, 0.58333333, 0.61111111, 0.61111111,\n",
       "            0.77777778, 0.77777778, 0.94444444, 0.94444444, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.90044647, 0.57750242, 0.57538428, 0.45923835,\n",
       "            0.45069763, 0.40369739, 0.36938823, 0.35103532, 0.30584518,\n",
       "            0.16818774, 0.14144235, 0.1412227 , 0.13864364, 0.13690604,\n",
       "            0.12238844, 0.1113796 , 0.10900193, 0.10157154, 0.08923328,\n",
       "            0.06889849, 0.06817133, 0.02883095, 0.02298504, 0.02091898])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.1388888888888889),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.25      , 0.25      , 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.44444444, 0.44444444,\n",
       "            0.55555556, 0.55555556, 0.58333333, 0.58333333, 0.61111111,\n",
       "            0.61111111, 0.69444444, 0.69444444, 0.91666667, 0.91666667,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.71428571, 0.71428571, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.83770052, 0.4427564 , 0.42436602, 0.41504089,\n",
       "            0.36841387, 0.34130329, 0.32840231, 0.25439295, 0.24204089,\n",
       "            0.16478375, 0.14346381, 0.14204788, 0.13730651, 0.1280046 ,\n",
       "            0.11672266, 0.11325618, 0.11069614, 0.05695497, 0.0486668 ,\n",
       "            0.03052294])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.1388888888888889),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.25      , 0.25      , 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.44444444, 0.44444444,\n",
       "            0.55555556, 0.55555556, 0.58333333, 0.58333333, 0.61111111,\n",
       "            0.61111111, 0.69444444, 0.69444444, 0.91666667, 0.91666667,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.71428571, 0.71428571, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.83770052, 0.4427564 , 0.42436602, 0.41504089,\n",
       "            0.36841387, 0.34130329, 0.32840231, 0.25439295, 0.24204089,\n",
       "            0.16478375, 0.14346381, 0.14204788, 0.13730651, 0.1280046 ,\n",
       "            0.11672266, 0.11325618, 0.11069614, 0.05695497, 0.0486668 ,\n",
       "            0.03052294])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.16666666666666666),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.27777778, 0.27777778, 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.38888889, 0.38888889,\n",
       "            0.52777778, 0.52777778, 0.55555556, 0.55555556, 0.61111111,\n",
       "            0.61111111, 0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.35714286, 0.35714286, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.64285714, 0.64285714, 0.78571429, 0.78571429,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.82484884, 0.45655581, 0.44852274, 0.43694229,\n",
       "            0.35640761, 0.32895377, 0.32379481, 0.311266  , 0.27980444,\n",
       "            0.21047413, 0.18369343, 0.16777646, 0.1652799 , 0.14437331,\n",
       "            0.12561381, 0.06237433, 0.05590557, 0.03565952])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.25),\n",
       "    'tpr': np.float64(0.07142857142857142),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.25      , 0.25      ,\n",
       "            0.27777778, 0.27777778, 0.30555556, 0.30555556, 0.41666667,\n",
       "            0.41666667, 0.47222222, 0.47222222, 0.5       , 0.5       ,\n",
       "            0.55555556, 0.55555556, 0.61111111, 0.61111111, 0.75      ,\n",
       "            0.75      , 0.77777778, 0.77777778, 0.91666667, 0.91666667,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.28571429, 0.28571429,\n",
       "            0.35714286, 0.35714286, 0.5       , 0.5       , 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.90145903, 0.8558988 , 0.5246576 , 0.4583683 ,\n",
       "            0.45557296, 0.43112909, 0.40264969, 0.34863431, 0.29752189,\n",
       "            0.29539443, 0.28389291, 0.24041928, 0.20901738, 0.20672868,\n",
       "            0.18870551, 0.18512977, 0.16263161, 0.16152018, 0.14169216,\n",
       "            0.13366051, 0.13299626, 0.12287509, 0.07173433, 0.06875748,\n",
       "            0.04185568])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.16666666666666666),\n",
       "    'tpr': np.float64(0.14285714285714285),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.13888889, 0.13888889, 0.19444444,\n",
       "            0.19444444, 0.27777778, 0.27777778, 0.36111111, 0.36111111,\n",
       "            0.44444444, 0.44444444, 0.55555556, 0.55555556, 0.63888889,\n",
       "            0.63888889, 0.69444444, 0.69444444, 0.91666667, 0.91666667,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.35714286, 0.35714286, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.57142857, 0.57142857,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.89770949, 0.57184543, 0.51077447, 0.48014312,\n",
       "            0.4783432 , 0.44761745, 0.43413191, 0.40518557, 0.38170987,\n",
       "            0.31623135, 0.30560944, 0.23493882, 0.19805155, 0.18374195,\n",
       "            0.15827159, 0.13444317, 0.1287248 , 0.07414229, 0.07309844,\n",
       "            0.03434522])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2777777777777778),\n",
       "    'tpr': np.float64(0.21428571428571427),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.22222222, 0.22222222, 0.38888889,\n",
       "            0.38888889, 0.47222222, 0.47222222, 0.52777778, 0.52777778,\n",
       "            0.58333333, 0.58333333, 0.75      , 0.75      , 0.88888889,\n",
       "            0.88888889, 0.97222222, 0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.21428571, 0.21428571,\n",
       "            0.42857143, 0.42857143, 0.5       , 0.5       , 0.57142857,\n",
       "            0.57142857, 0.71428571, 0.71428571, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.86282447, 0.60627372, 0.56206505, 0.44444965,\n",
       "            0.39862068, 0.35134156, 0.31607132, 0.23979366, 0.23834565,\n",
       "            0.20783074, 0.18890795, 0.1446094 , 0.13111605, 0.08782819,\n",
       "            0.08102434, 0.06561366, 0.06521708, 0.04224526])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.25),\n",
       "    'tpr': np.float64(0.21428571428571427),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.22222222, 0.22222222, 0.25      ,\n",
       "            0.25      , 0.36111111, 0.36111111, 0.41666667, 0.41666667,\n",
       "            0.47222222, 0.47222222, 0.55555556, 0.55555556, 0.63888889,\n",
       "            0.63888889, 0.80555556, 0.80555556, 0.83333333, 0.83333333,\n",
       "            0.88888889, 0.88888889, 0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.93224733, 0.61650052, 0.56115273, 0.55237756,\n",
       "            0.54391612, 0.37322913, 0.36469006, 0.34565049, 0.34170789,\n",
       "            0.32095198, 0.25664777, 0.2431758 , 0.23515075, 0.21009824,\n",
       "            0.2040272 , 0.14024935, 0.13933442, 0.11827761, 0.11760969,\n",
       "            0.11256731, 0.1114645 , 0.0851805 , 0.07299198, 0.0310254 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.3055555555555556),\n",
       "    'tpr': np.float64(0.2857142857142857),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.30555556, 0.30555556,\n",
       "            0.33333333, 0.33333333, 0.41666667, 0.41666667, 0.52777778,\n",
       "            0.52777778, 0.55555556, 0.55555556, 0.58333333, 0.58333333,\n",
       "            0.61111111, 0.61111111, 0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.64285714, 0.64285714, 0.71428571, 0.71428571, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.92494138, 0.87497275, 0.55654698, 0.46335403,\n",
       "            0.41878127, 0.40066331, 0.36025032, 0.35349916, 0.28217155,\n",
       "            0.26890268, 0.2639981 , 0.25920623, 0.2503186 , 0.22324194,\n",
       "            0.22104798, 0.21949538, 0.11914921, 0.09247602, 0.06943886])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.3055555555555556),\n",
       "    'tpr': np.float64(0.2857142857142857),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.27777778, 0.27777778, 0.30555556,\n",
       "            0.30555556, 0.41666667, 0.41666667, 0.47222222, 0.47222222,\n",
       "            0.52777778, 0.52777778, 0.72222222, 0.72222222, 0.83333333,\n",
       "            0.83333333, 0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.21428571, 0.21428571,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.64285714, 0.64285714, 0.78571429, 0.78571429,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96663934, 0.64732353, 0.54604791, 0.50670367,\n",
       "            0.50348381, 0.42869144, 0.39338775, 0.37173904, 0.31381526,\n",
       "            0.27315719, 0.26803573, 0.19140458, 0.1835421 , 0.16012913,\n",
       "            0.15247527, 0.09958836, 0.09081271, 0.04480553])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.3333333333333333),\n",
       "    'tpr': np.float64(0.2857142857142857),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.36111111, 0.36111111,\n",
       "            0.47222222, 0.47222222, 0.5       , 0.5       , 0.55555556,\n",
       "            0.55555556, 0.58333333, 0.58333333, 0.88888889, 0.88888889,\n",
       "            0.97222222, 0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.95979486, 0.94167337, 0.71829077, 0.62594128,\n",
       "            0.54804438, 0.50763919, 0.46270743, 0.45332014, 0.43721913,\n",
       "            0.38855604, 0.35690073, 0.34993302, 0.34130956, 0.32608983,\n",
       "            0.31014314, 0.2768    , 0.26345067, 0.13893731, 0.13497584,\n",
       "            0.11324669, 0.10021526, 0.08642342])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.3888888888888889),\n",
       "    'tpr': np.float64(0.42857142857142855),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.16666667, 0.16666667, 0.30555556,\n",
       "            0.30555556, 0.36111111, 0.36111111, 0.38888889, 0.38888889,\n",
       "            0.58333333, 0.58333333, 0.66666667, 0.66666667, 0.80555556,\n",
       "            0.80555556, 0.91666667, 0.91666667, 0.97222222, 0.97222222,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.35714286, 0.35714286, 0.64285714,\n",
       "            0.64285714, 0.71428571, 0.71428571, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.94810167, 0.76263732, 0.75864447, 0.69501908,\n",
       "            0.68426689, 0.6308123 , 0.62576757, 0.56613959, 0.44731172,\n",
       "            0.38772779, 0.37457102, 0.37013446, 0.29360532, 0.2352697 ,\n",
       "            0.23261101, 0.15962957, 0.1351972 , 0.07515109, 0.07271751,\n",
       "            0.06505434])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.4166666666666667),\n",
       "    'tpr': np.float64(0.42857142857142855),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.22222222, 0.22222222, 0.25      ,\n",
       "            0.25      , 0.38888889, 0.38888889, 0.41666667, 0.41666667,\n",
       "            0.58333333, 0.58333333, 0.61111111, 0.61111111, 0.66666667,\n",
       "            0.66666667, 0.69444444, 0.69444444, 0.83333333, 0.83333333,\n",
       "            0.97222222, 0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.35714286, 0.35714286, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9388096 , 0.74990944, 0.72444747, 0.69674894,\n",
       "            0.64847899, 0.5226937 , 0.52034227, 0.50513073, 0.48170949,\n",
       "            0.37345103, 0.31876779, 0.31221262, 0.30320253, 0.29508465,\n",
       "            0.28484598, 0.26412707, 0.2536789 , 0.18708214, 0.18032017,\n",
       "            0.09734455, 0.06293941, 0.05314368])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5),\n",
       "    'tpr': np.float64(0.2857142857142857),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.11111111, 0.11111111, 0.30555556,\n",
       "            0.30555556, 0.5       , 0.5       , 0.52777778, 0.52777778,\n",
       "            0.61111111, 0.61111111, 0.72222222, 0.72222222, 0.75      ,\n",
       "            0.75      , 0.80555556, 0.80555556, 0.94444444, 0.94444444,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.21428571, 0.21428571, 0.35714286, 0.35714286, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.57142857, 0.57142857,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.98332238, 0.96648951, 0.96083455, 0.80213792,\n",
       "            0.76854253, 0.5245299 , 0.45730854, 0.45718919, 0.40747077,\n",
       "            0.36150818, 0.36131425, 0.28190608, 0.25009582, 0.24235527,\n",
       "            0.21356243, 0.19370461, 0.19292355, 0.1267181 , 0.12314882,\n",
       "            0.0722898 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.4722222222222222),\n",
       "    'tpr': np.float64(0.42857142857142855),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.33333333, 0.33333333,\n",
       "            0.36111111, 0.36111111, 0.41666667, 0.41666667, 0.58333333,\n",
       "            0.58333333, 0.69444444, 0.69444444, 0.72222222, 0.72222222,\n",
       "            0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.21428571,\n",
       "            0.21428571, 0.35714286, 0.35714286, 0.42857143, 0.42857143,\n",
       "            0.5       , 0.5       , 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.990306  , 0.98922694, 0.79303891, 0.73172743,\n",
       "            0.72870194, 0.68411315, 0.66596042, 0.58836947, 0.39358949,\n",
       "            0.36924215, 0.31391763, 0.23541714, 0.21864009, 0.20120064,\n",
       "            0.13813819, 0.13532716, 0.02219385])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5277777777777778),\n",
       "    'tpr': np.float64(0.5714285714285714),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.13888889, 0.13888889, 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.38888889, 0.38888889,\n",
       "            0.41666667, 0.41666667, 0.63888889, 0.63888889, 0.75      ,\n",
       "            0.75      , 0.86111111, 0.86111111, 0.88888889, 0.88888889,\n",
       "            0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.35714286, 0.35714286, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.93239823, 0.7946894 , 0.79376249, 0.70998328,\n",
       "            0.65523638, 0.64202429, 0.63391749, 0.59534143, 0.59258748,\n",
       "            0.5820736 , 0.54726272, 0.45347938, 0.44343867, 0.37481073,\n",
       "            0.36373011, 0.27939358, 0.26886478, 0.26863194, 0.26847251,\n",
       "            0.26730897, 0.19593263, 0.12730002])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5555555555555556),\n",
       "    'tpr': np.float64(0.7857142857142857),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.11111111, 0.11111111,\n",
       "            0.13888889, 0.13888889, 0.22222222, 0.22222222, 0.33333333,\n",
       "            0.33333333, 0.63888889, 0.63888889, 0.97222222, 0.97222222,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.35714286, 0.35714286, 0.42857143,\n",
       "            0.42857143, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.98043424, 0.85871478, 0.84517576, 0.81202802,\n",
       "            0.80258115, 0.75958148, 0.72001995, 0.68957115, 0.60760679,\n",
       "            0.58568364, 0.48329161, 0.45568358, 0.23629099, 0.20659598,\n",
       "            0.11166865])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5277777777777778),\n",
       "    'tpr': np.float64(0.7142857142857143),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.22222222, 0.22222222, 0.27777778,\n",
       "            0.27777778, 0.33333333, 0.33333333, 0.36111111, 0.36111111,\n",
       "            0.41666667, 0.41666667, 0.44444444, 0.44444444, 0.5       ,\n",
       "            0.5       , 0.83333333, 0.83333333, 0.88888889, 0.88888889,\n",
       "            0.97222222, 0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.5       , 0.5       , 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.94735911, 0.83092731, 0.80960166, 0.75046504,\n",
       "            0.70669346, 0.70187595, 0.68709428, 0.68405303, 0.64229429,\n",
       "            0.61532492, 0.60280016, 0.59280401, 0.57428964, 0.52748713,\n",
       "            0.52394992, 0.29606508, 0.25319654, 0.23625705, 0.21660188,\n",
       "            0.18311725, 0.13434646, 0.11289546])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5277777777777778),\n",
       "    'tpr': np.float64(0.6428571428571429),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.13888889, 0.13888889, 0.27777778,\n",
       "            0.27777778, 0.30555556, 0.30555556, 0.47222222, 0.47222222,\n",
       "            0.5       , 0.5       , 0.58333333, 0.58333333, 0.61111111,\n",
       "            0.61111111, 0.94444444, 0.94444444, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.42857143, 0.42857143, 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96389957, 0.9327092 , 0.93195196, 0.86338287,\n",
       "            0.86246642, 0.82839771, 0.71717342, 0.62459738, 0.5631855 ,\n",
       "            0.55391769, 0.5386453 , 0.48918694, 0.48449109, 0.4730778 ,\n",
       "            0.46444263, 0.16514605, 0.15819862, 0.11256517])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5277777777777778),\n",
       "    'tpr': np.float64(0.6428571428571429),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.11111111,\n",
       "            0.11111111, 0.22222222, 0.22222222, 0.25      , 0.25      ,\n",
       "            0.27777778, 0.27777778, 0.41666667, 0.41666667, 0.47222222,\n",
       "            0.47222222, 0.63888889, 0.63888889, 0.86111111, 0.86111111,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.64285714, 0.64285714, 0.78571429, 0.78571429, 0.92857143,\n",
       "            0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96129725, 0.95821258, 0.94932725, 0.93774451,\n",
       "            0.89533566, 0.75695505, 0.74123464, 0.7340651 , 0.73351473,\n",
       "            0.70588065, 0.703882  , 0.64993074, 0.59803522, 0.55519182,\n",
       "            0.51922523, 0.4606675 , 0.43988773, 0.37691024, 0.36421747,\n",
       "            0.21415425, 0.19629711])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7222222222222222),\n",
       "    'tpr': np.float64(0.5714285714285714),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.22222222, 0.22222222, 0.30555556,\n",
       "            0.30555556, 0.41666667, 0.41666667, 0.44444444, 0.44444444,\n",
       "            0.58333333, 0.58333333, 0.72222222, 0.72222222, 0.75      ,\n",
       "            0.75      , 0.86111111, 0.86111111, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96419765, 0.85072297, 0.83181874, 0.77390474,\n",
       "            0.75533334, 0.73237178, 0.69997524, 0.67549298, 0.652665  ,\n",
       "            0.6047234 , 0.55787435, 0.50636521, 0.4783495 , 0.47338758,\n",
       "            0.46385337, 0.33180358, 0.29946738, 0.10681854])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8055555555555556),\n",
       "    'tpr': np.float64(0.8571428571428571),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.27777778, 0.27777778, 0.30555556,\n",
       "            0.30555556, 0.33333333, 0.33333333, 0.38888889, 0.38888889,\n",
       "            0.44444444, 0.44444444, 0.47222222, 0.47222222, 0.5       ,\n",
       "            0.5       , 0.61111111, 0.61111111, 0.66666667, 0.66666667,\n",
       "            0.86111111, 0.86111111, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.21428571, 0.21428571,\n",
       "            0.28571429, 0.28571429, 0.35714286, 0.35714286, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98574338, 0.89280913, 0.83878804, 0.83101765,\n",
       "            0.82320907, 0.79860885, 0.7417327 , 0.72418766, 0.68962695,\n",
       "            0.68087337, 0.67310078, 0.65952057, 0.65743276, 0.63690142,\n",
       "            0.63486544, 0.57264973, 0.56969476, 0.5593389 , 0.54393989,\n",
       "            0.42706364, 0.33459182, 0.26068588])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6944444444444444),\n",
       "    'tpr': np.float64(0.7857142857142857),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.08333333, 0.08333333,\n",
       "            0.30555556, 0.30555556, 0.33333333, 0.33333333, 0.44444444,\n",
       "            0.44444444, 0.47222222, 0.47222222, 0.63888889, 0.63888889,\n",
       "            0.69444444, 0.69444444, 0.77777778, 0.77777778, 0.80555556,\n",
       "            0.80555556, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.14285714, 0.14285714, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.42857143, 0.42857143, 0.57142857, 0.57142857, 0.71428571,\n",
       "            0.71428571, 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99319795, 0.98298574, 0.97590415, 0.97382596,\n",
       "            0.90882236, 0.88660268, 0.87528128, 0.87005054, 0.79871396,\n",
       "            0.79580731, 0.78192892, 0.75944417, 0.66488823, 0.62554238,\n",
       "            0.56097943, 0.4899625 , 0.43362912, 0.40488002, 0.39670741,\n",
       "            0.35309837, 0.12653167])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7777777777777778),\n",
       "    'tpr': np.float64(0.8571428571428571),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08333333, 0.08333333, 0.16666667,\n",
       "            0.16666667, 0.22222222, 0.22222222, 0.38888889, 0.38888889,\n",
       "            0.41666667, 0.41666667, 0.47222222, 0.47222222, 0.5       ,\n",
       "            0.5       , 0.61111111, 0.61111111, 0.77777778, 0.77777778,\n",
       "            0.83333333, 0.83333333, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99113611, 0.98553071, 0.9850871 , 0.97721582,\n",
       "            0.96886237, 0.94812043, 0.94202234, 0.92534094, 0.91999063,\n",
       "            0.91188455, 0.88595267, 0.86062367, 0.84988709, 0.82201184,\n",
       "            0.78483784, 0.68240306, 0.63513019, 0.5120879 , 0.4514787 ,\n",
       "            0.35781872, 0.35540238, 0.12524383])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8333333333333334),\n",
       "    'tpr': np.float64(0.9285714285714286),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.30555556, 0.30555556, 0.44444444,\n",
       "            0.44444444, 0.5       , 0.5       , 0.52777778, 0.52777778,\n",
       "            0.55555556, 0.55555556, 0.58333333, 0.58333333, 0.69444444,\n",
       "            0.69444444, 0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.28571429, 0.28571429,\n",
       "            0.42857143, 0.42857143, 0.5       , 0.5       , 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99619764, 0.94261328, 0.91154258, 0.85536386,\n",
       "            0.81320147, 0.78166995, 0.7681202 , 0.75835205, 0.74784555,\n",
       "            0.73941254, 0.73397434, 0.73370281, 0.66966357, 0.5846023 ,\n",
       "            0.58011262, 0.45460558, 0.38828008, 0.23770295])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7777777777777778),\n",
       "    'tpr': np.float64(0.8571428571428571),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.16666667, 0.16666667,\n",
       "            0.22222222, 0.22222222, 0.36111111, 0.36111111, 0.47222222,\n",
       "            0.47222222, 0.5       , 0.5       , 0.61111111, 0.61111111,\n",
       "            0.63888889, 0.63888889, 0.77777778, 0.77777778, 0.86111111,\n",
       "            0.86111111, 0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.42857143, 0.42857143,\n",
       "            0.5       , 0.5       , 0.57142857, 0.57142857, 0.64285714,\n",
       "            0.64285714, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99254659, 0.99211661, 0.97543092, 0.97091097,\n",
       "            0.96036471, 0.95701516, 0.92683404, 0.9133383 , 0.89292467,\n",
       "            0.88231151, 0.85497259, 0.83246208, 0.80589951, 0.78031259,\n",
       "            0.77888304, 0.69887113, 0.59720033, 0.59113238, 0.37181744,\n",
       "            0.35773983, 0.32328438, 0.30455956, 0.26295233])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(0.9285714285714286),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.08333333,\n",
       "            0.08333333, 0.13888889, 0.13888889, 0.16666667, 0.16666667,\n",
       "            0.33333333, 0.33333333, 0.47222222, 0.47222222, 0.52777778,\n",
       "            0.52777778, 0.55555556, 0.55555556, 0.63888889, 0.63888889,\n",
       "            0.80555556, 0.80555556, 0.91666667, 0.91666667, 0.94444444,\n",
       "            0.94444444, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.21428571, 0.21428571, 0.28571429,\n",
       "            0.28571429, 0.35714286, 0.35714286, 0.42857143, 0.42857143,\n",
       "            0.5       , 0.5       , 0.64285714, 0.64285714, 0.71428571,\n",
       "            0.71428571, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9864473 , 0.97034864, 0.96757472, 0.9659087 ,\n",
       "            0.9342252 , 0.93168991, 0.9182577 , 0.90558649, 0.90451856,\n",
       "            0.85790021, 0.85455726, 0.83952459, 0.83383828, 0.82666515,\n",
       "            0.82586644, 0.82219655, 0.80121644, 0.77564655, 0.77448244,\n",
       "            0.69698643, 0.69380666, 0.6036365 , 0.59228398, 0.58053741,\n",
       "            0.55525395, 0.5171747 , 0.49170804])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9166666666666666),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.13888889, 0.13888889, 0.19444444,\n",
       "            0.19444444, 0.22222222, 0.22222222, 0.30555556, 0.30555556,\n",
       "            0.36111111, 0.36111111, 0.55555556, 0.55555556, 0.66666667,\n",
       "            0.66666667, 0.69444444, 0.69444444, 0.75      , 0.75      ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99715518, 0.986576  , 0.98073209, 0.96799922,\n",
       "            0.94131934, 0.93657082, 0.92818456, 0.90510253, 0.90101864,\n",
       "            0.86840211, 0.85132429, 0.81870299, 0.81330939, 0.73251532,\n",
       "            0.73175586, 0.71174432, 0.67030077, 0.59418503, 0.59317741,\n",
       "            0.32658886])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8611111111111112),\n",
       "    'tpr': np.float64(0.9285714285714286),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.19444444,\n",
       "            0.19444444, 0.25      , 0.25      , 0.44444444, 0.44444444,\n",
       "            0.5       , 0.5       , 0.52777778, 0.52777778, 0.55555556,\n",
       "            0.55555556, 0.58333333, 0.58333333, 0.77777778, 0.77777778,\n",
       "            0.83333333, 0.83333333, 0.94444444, 0.94444444, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.21428571, 0.21428571, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.64285714, 0.64285714, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99253559, 0.98916441, 0.97754412, 0.96218875,\n",
       "            0.94876928, 0.94778595, 0.93471215, 0.88680333, 0.84906715,\n",
       "            0.83924508, 0.83737201, 0.83253619, 0.81890622, 0.81491906,\n",
       "            0.81091591, 0.78293748, 0.7768454 , 0.68583913, 0.63320192,\n",
       "            0.56994162, 0.56591965, 0.41183332, 0.36215798, 0.30451904])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9722222222222222),\n",
       "    'tpr': np.float64(0.8571428571428571),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.13888889, 0.13888889, 0.30555556,\n",
       "            0.30555556, 0.36111111, 0.36111111, 0.5       , 0.5       ,\n",
       "            0.52777778, 0.52777778, 0.58333333, 0.58333333, 0.86111111,\n",
       "            0.86111111, 0.94444444, 0.94444444, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.5       , 0.5       , 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98819253, 0.95618524, 0.95580807, 0.92115597,\n",
       "            0.9129575 , 0.89417252, 0.88147155, 0.84994469, 0.84512441,\n",
       "            0.84039698, 0.83884629, 0.82919258, 0.82735811, 0.67164062,\n",
       "            0.66806762, 0.58790853, 0.56177656, 0.43911797, 0.41050941])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8888888888888888),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.38888889, 0.38888889,\n",
       "            0.55555556, 0.55555556, 0.58333333, 0.58333333, 0.63888889,\n",
       "            0.63888889, 0.66666667, 0.66666667, 0.72222222, 0.72222222,\n",
       "            0.77777778, 0.77777778, 0.80555556, 0.80555556, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.5       , 0.5       , 0.57142857, 0.57142857, 0.71428571,\n",
       "            0.71428571, 0.85714286, 0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99206599, 0.99164826, 0.91978768, 0.91779261,\n",
       "            0.8695593 , 0.84132705, 0.83830755, 0.82346974, 0.8182408 ,\n",
       "            0.80049123, 0.7912101 , 0.75359733, 0.75019161, 0.72727874,\n",
       "            0.68792767, 0.67772986, 0.64165888, 0.63179534, 0.38341774])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9444444444444444),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.05555556, 0.05555556,\n",
       "            0.08333333, 0.08333333, 0.11111111, 0.11111111, 0.16666667,\n",
       "            0.16666667, 0.61111111, 0.61111111, 0.66666667, 0.66666667,\n",
       "            0.75      , 0.75      , 0.77777778, 0.77777778, 0.80555556,\n",
       "            0.80555556, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.14285714, 0.14285714, 0.21428571,\n",
       "            0.21428571, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.64285714, 0.64285714, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9938221 , 0.97834428, 0.97621244, 0.97231835,\n",
       "            0.96361071, 0.94842555, 0.94473041, 0.94451883, 0.92508487,\n",
       "            0.91497609, 0.81303479, 0.80541976, 0.78769505, 0.78686729,\n",
       "            0.76582906, 0.73435046, 0.72609974, 0.71822399, 0.71508352,\n",
       "            0.68102137, 0.47617728])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8888888888888888),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.08333333, 0.08333333, 0.22222222,\n",
       "            0.22222222, 0.41666667, 0.41666667, 0.44444444, 0.44444444,\n",
       "            0.5       , 0.5       , 0.52777778, 0.52777778, 0.63888889,\n",
       "            0.63888889, 0.66666667, 0.66666667, 0.72222222, 0.72222222,\n",
       "            0.83333333, 0.83333333, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.28571429, 0.28571429, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99379707, 0.99054939, 0.98982212, 0.97872392,\n",
       "            0.97682715, 0.95062196, 0.9460045 , 0.93444562, 0.9249267 ,\n",
       "            0.90938745, 0.89495783, 0.86227223, 0.85730917, 0.83955945,\n",
       "            0.80335696, 0.79969416, 0.75668261, 0.71183157, 0.71125203,\n",
       "            0.59154439, 0.58444178, 0.29189809])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9444444444444444),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.11111111, 0.11111111,\n",
       "            0.13888889, 0.13888889, 0.19444444, 0.19444444, 0.25      ,\n",
       "            0.25      , 0.27777778, 0.27777778, 0.5       , 0.5       ,\n",
       "            0.52777778, 0.52777778, 0.55555556, 0.55555556, 0.66666667,\n",
       "            0.66666667, 0.72222222, 0.72222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.21428571, 0.21428571, 0.35714286, 0.35714286,\n",
       "            0.42857143, 0.42857143, 0.57142857, 0.57142857, 0.64285714,\n",
       "            0.64285714, 0.71428571, 0.71428571, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99478031, 0.99271786, 0.98547584, 0.98354605,\n",
       "            0.97919781, 0.97735826, 0.96606749, 0.95990243, 0.95716886,\n",
       "            0.95529501, 0.95484156, 0.94753533, 0.89973792, 0.89646896,\n",
       "            0.8888386 , 0.88855535, 0.88801147, 0.87868905, 0.85058291,\n",
       "            0.82793001, 0.78639345, 0.78025342, 0.44353445])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.08333333, 0.08333333, 0.13888889,\n",
       "            0.13888889, 0.25      , 0.25      , 0.33333333, 0.33333333,\n",
       "            0.36111111, 0.36111111, 0.66666667, 0.66666667, 0.72222222,\n",
       "            0.72222222, 0.97222222, 0.97222222, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.5       ,\n",
       "            0.5       , 0.64285714, 0.64285714, 0.78571429, 0.78571429,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99191934, 0.98244691, 0.98157016, 0.97264459,\n",
       "            0.9691538 , 0.94852943, 0.94689419, 0.94275827, 0.9383023 ,\n",
       "            0.93658221, 0.9310225 , 0.90037685, 0.87823206, 0.85801061,\n",
       "            0.84513439, 0.6365616 , 0.58283052, 0.53563063, 0.51430918])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.33333333, 0.33333333,\n",
       "            0.44444444, 0.44444444, 0.55555556, 0.55555556, 0.61111111,\n",
       "            0.61111111, 0.80555556, 0.80555556, 0.83333333, 0.83333333,\n",
       "            0.91666667, 0.91666667, 0.94444444, 0.94444444, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.14285714, 0.14285714, 0.21428571,\n",
       "            0.21428571, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.42857143, 0.42857143, 0.57142857, 0.57142857, 0.71428571,\n",
       "            0.71428571, 0.78571429, 0.78571429, 0.85714286, 0.85714286,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.98949069, 0.97618285, 0.94644843, 0.94235263,\n",
       "            0.94076689, 0.93745749, 0.91727923, 0.91486631, 0.90219231,\n",
       "            0.89704878, 0.85458293, 0.84071353, 0.82841773, 0.82205333,\n",
       "            0.80322411, 0.79960012, 0.79635756, 0.7847399 , 0.75936699,\n",
       "            0.71261459])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.19444444, 0.19444444, 0.22222222,\n",
       "            0.22222222, 0.27777778, 0.27777778, 0.33333333, 0.33333333,\n",
       "            0.36111111, 0.36111111, 0.38888889, 0.38888889, 0.41666667,\n",
       "            0.41666667, 0.44444444, 0.44444444, 0.61111111, 0.61111111,\n",
       "            0.75      , 0.75      , 0.80555556, 0.80555556, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.35714286, 0.35714286, 0.42857143,\n",
       "            0.42857143, 0.5       , 0.5       , 0.57142857, 0.57142857,\n",
       "            0.64285714, 0.64285714, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99194408, 0.96701743, 0.96306109, 0.95896108,\n",
       "            0.95880617, 0.94594683, 0.93783821, 0.93146185, 0.92583771,\n",
       "            0.91954665, 0.91264852, 0.90639612, 0.9058778 , 0.90508322,\n",
       "            0.89836048, 0.89419342, 0.89187659, 0.86863533, 0.86830461,\n",
       "            0.83085239, 0.79508019, 0.7926326 , 0.7814664 , 0.57393584])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.02777778, 0.05555556, 0.05555556,\n",
       "            0.08333333, 0.08333333, 0.11111111, 0.11111111, 0.13888889,\n",
       "            0.13888889, 0.44444444, 0.44444444, 0.52777778, 0.52777778,\n",
       "            0.63888889, 0.63888889, 0.69444444, 0.69444444, 0.88888889,\n",
       "            0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.07142857, 0.07142857, 0.14285714,\n",
       "            0.14285714, 0.28571429, 0.28571429, 0.35714286, 0.35714286,\n",
       "            0.5       , 0.5       , 0.64285714, 0.64285714, 0.71428571,\n",
       "            0.71428571, 0.78571429, 0.78571429, 0.92857143, 0.92857143,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99630185, 0.99604707, 0.99603562, 0.995345  ,\n",
       "            0.99473383, 0.98984292, 0.98789685, 0.9861523 , 0.98383349,\n",
       "            0.98018999, 0.96358829, 0.96223425, 0.94610785, 0.94394793,\n",
       "            0.93159534, 0.9301378 , 0.92476092, 0.91727603, 0.84084443,\n",
       "            0.82881704, 0.61900292])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.08333333, 0.08333333, 0.11111111,\n",
       "            0.11111111, 0.30555556, 0.30555556, 0.36111111, 0.36111111,\n",
       "            0.38888889, 0.38888889, 0.47222222, 0.47222222, 0.52777778,\n",
       "            0.52777778, 0.55555556, 0.55555556, 0.58333333, 0.58333333,\n",
       "            0.69444444, 0.69444444, 0.80555556, 0.80555556, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.64285714, 0.64285714, 0.71428571, 0.71428571, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9975701 , 0.99467132, 0.99461749, 0.99078164,\n",
       "            0.99074409, 0.98445437, 0.98303551, 0.98021835, 0.97902589,\n",
       "            0.97858328, 0.97322666, 0.95937563, 0.95602056, 0.95246486,\n",
       "            0.94232204, 0.94076722, 0.93683096, 0.93442352, 0.92462848,\n",
       "            0.88104051, 0.87978778, 0.79786129, 0.72651255, 0.56556757])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.11111111,\n",
       "            0.11111111, 0.30555556, 0.30555556, 0.33333333, 0.33333333,\n",
       "            0.38888889, 0.38888889, 0.41666667, 0.41666667, 0.52777778,\n",
       "            0.52777778, 0.66666667, 0.66666667, 0.75      , 0.75      ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.92857143, 0.92857143, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99261871, 0.98925273, 0.98673582, 0.98298381,\n",
       "            0.97790262, 0.96754189, 0.96480778, 0.96376666, 0.95781822,\n",
       "            0.95583975, 0.95575661, 0.95399392, 0.9529464 , 0.94965386,\n",
       "            0.94566786, 0.91952083, 0.91534206, 0.89838541, 0.88665904,\n",
       "            0.65514352])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.08333333,\n",
       "            0.08333333, 0.11111111, 0.11111111, 0.16666667, 0.16666667,\n",
       "            0.19444444, 0.19444444, 0.22222222, 0.22222222, 0.25      ,\n",
       "            0.25      , 0.30555556, 0.30555556, 0.5       , 0.5       ,\n",
       "            0.83333333, 0.83333333, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.21428571, 0.21428571, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99805192, 0.99599251, 0.99462261, 0.9936626 ,\n",
       "            0.99115199, 0.99035953, 0.98940393, 0.98553845, 0.98468525,\n",
       "            0.9841626 , 0.98362191, 0.98286743, 0.97901254, 0.97463993,\n",
       "            0.9740539 , 0.97045174, 0.96855203, 0.95850633, 0.95396051,\n",
       "            0.88776918, 0.88511935, 0.709652  ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.13888889, 0.13888889, 0.33333333,\n",
       "            0.33333333, 0.44444444, 0.44444444, 0.52777778, 0.52777778,\n",
       "            0.66666667, 0.66666667, 0.72222222, 0.72222222, 0.88888889,\n",
       "            0.88888889, 0.97222222, 0.97222222, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.21428571, 0.21428571,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.85714286, 0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99653508, 0.98143577, 0.97991722, 0.97109894,\n",
       "            0.97100839, 0.9618548 , 0.95969639, 0.95284566, 0.95005869,\n",
       "            0.93686748, 0.93405627, 0.92796109, 0.92392622, 0.89201377,\n",
       "            0.88424088, 0.87084034, 0.84431544, 0.82697091, 0.77566873])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.16666667, 0.16666667, 0.25      ,\n",
       "            0.25      , 0.27777778, 0.27777778, 0.5       , 0.5       ,\n",
       "            0.55555556, 0.55555556, 0.80555556, 0.80555556, 0.86111111,\n",
       "            0.86111111, 0.91666667, 0.91666667, 0.94444444, 0.94444444,\n",
       "            0.97222222, 0.97222222, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.57142857, 0.57142857, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99362431, 0.98751742, 0.98720157, 0.98314757,\n",
       "            0.98292976, 0.98044257, 0.97824644, 0.96858207, 0.96817093,\n",
       "            0.96346565, 0.96281506, 0.94058525, 0.94037403, 0.92964799,\n",
       "            0.92894774, 0.92542627, 0.9206512 , 0.91930345, 0.90771171,\n",
       "            0.90733757, 0.90302044, 0.88626033, 0.85620059])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.13888889,\n",
       "            0.13888889, 0.16666667, 0.16666667, 0.22222222, 0.22222222,\n",
       "            0.44444444, 0.44444444, 0.5       , 0.5       , 0.52777778,\n",
       "            0.52777778, 0.77777778, 0.77777778, 0.83333333, 0.83333333,\n",
       "            0.86111111, 0.86111111, 0.88888889, 0.88888889, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.57142857, 0.57142857,\n",
       "            0.64285714, 0.64285714, 0.71428571, 0.71428571, 0.78571429,\n",
       "            0.78571429, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99183235, 0.9905481 , 0.98984329, 0.98810693,\n",
       "            0.98729476, 0.98723008, 0.98718438, 0.98626842, 0.98491186,\n",
       "            0.98205026, 0.9816784 , 0.98029063, 0.97397829, 0.97293001,\n",
       "            0.9723001 , 0.94906467, 0.94855672, 0.94710925, 0.94591072,\n",
       "            0.94465758, 0.93615961, 0.93498451, 0.92298678, 0.88403499])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05555556, 0.05555556, 0.16666667,\n",
       "            0.16666667, 0.30555556, 0.30555556, 0.33333333, 0.33333333,\n",
       "            0.38888889, 0.38888889, 0.47222222, 0.47222222, 0.75      ,\n",
       "            0.75      , 0.80555556, 0.80555556, 0.94444444, 0.94444444,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.21428571, 0.21428571,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99663527, 0.99528218, 0.99454789, 0.99331693,\n",
       "            0.99320942, 0.99059049, 0.99002793, 0.98852976, 0.98633445,\n",
       "            0.98610671, 0.98546559, 0.98416937, 0.98405391, 0.97217216,\n",
       "            0.96956792, 0.96435201, 0.95730797, 0.93821779, 0.93656049,\n",
       "            0.91549107, 0.88719229])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05555556, 0.05555556,\n",
       "            0.08333333, 0.08333333, 0.16666667, 0.16666667, 0.22222222,\n",
       "            0.22222222, 0.36111111, 0.36111111, 0.38888889, 0.38888889,\n",
       "            0.72222222, 0.72222222, 0.97222222, 0.97222222, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.14285714, 0.14285714, 0.21428571,\n",
       "            0.21428571, 0.35714286, 0.35714286, 0.42857143, 0.42857143,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.85714286,\n",
       "            0.85714286, 0.92857143, 0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99791432, 0.99700646, 0.99666726, 0.99587266,\n",
       "            0.9955625 , 0.99434998, 0.99342542, 0.9932844 , 0.99313374,\n",
       "            0.98822765, 0.98555827, 0.98529227, 0.98468658, 0.98449013,\n",
       "            0.97729293, 0.97625482, 0.94339657, 0.94150716, 0.91586699])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.16666667, 0.16666667, 0.25      ,\n",
       "            0.25      , 0.5       , 0.5       , 0.55555556, 0.55555556,\n",
       "            0.61111111, 0.61111111, 0.72222222, 0.72222222, 0.75      ,\n",
       "            0.75      , 0.80555556, 0.80555556, 0.91666667, 0.91666667,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 0.85714286, 0.85714286, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99755112, 0.99273741, 0.99208395, 0.98816265,\n",
       "            0.98814048, 0.98384041, 0.98253014, 0.9800263 , 0.97941182,\n",
       "            0.97678069, 0.97028031, 0.96212552, 0.96072378, 0.95994388,\n",
       "            0.95979484, 0.95667002, 0.9505045 , 0.94617712, 0.92907942,\n",
       "            0.87703461])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.05555556, 0.05555556, 0.11111111,\n",
       "            0.11111111, 0.25      , 0.25      , 0.30555556, 0.30555556,\n",
       "            0.33333333, 0.33333333, 0.36111111, 0.36111111, 0.41666667,\n",
       "            0.41666667, 0.5       , 0.5       , 0.63888889, 0.63888889,\n",
       "            0.75      , 0.75      , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.07142857, 0.07142857,\n",
       "            0.14285714, 0.14285714, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.64285714, 0.64285714, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99723932, 0.99586979, 0.99440181, 0.99264232,\n",
       "            0.99175008, 0.98799635, 0.98619999, 0.98555758, 0.98526897,\n",
       "            0.98456891, 0.9845591 , 0.98385595, 0.98153253, 0.98107102,\n",
       "            0.97989543, 0.9779899 , 0.97138246, 0.95977964, 0.95743278,\n",
       "            0.95504716, 0.94939691, 0.74901804])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.16666667, 0.16666667,\n",
       "            0.27777778, 0.27777778, 0.47222222, 0.47222222, 0.69444444,\n",
       "            0.69444444, 0.72222222, 0.72222222, 0.83333333, 0.83333333,\n",
       "            0.91666667, 0.91666667, 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.21428571, 0.21428571, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99793212, 0.99671667, 0.9954887 , 0.99518867,\n",
       "            0.99363936, 0.9933408 , 0.9918865 , 0.99148521, 0.98561781,\n",
       "            0.98523239, 0.9839697 , 0.98380063, 0.98039048, 0.97762724,\n",
       "            0.97134977, 0.96746625, 0.93989716])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.11111111, 0.11111111, 0.13888889,\n",
       "            0.13888889, 0.16666667, 0.16666667, 0.25      , 0.25      ,\n",
       "            0.27777778, 0.27777778, 0.30555556, 0.30555556, 0.38888889,\n",
       "            0.38888889, 0.55555556, 0.55555556, 0.80555556, 0.80555556,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.42857143, 0.42857143, 0.5       ,\n",
       "            0.5       , 0.57142857, 0.57142857, 0.64285714, 0.64285714,\n",
       "            0.71428571, 0.71428571, 0.78571429, 0.78571429, 0.92857143,\n",
       "            0.92857143, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.997877  , 0.99600394, 0.99590533, 0.99527229,\n",
       "            0.99524252, 0.99509592, 0.99366925, 0.99126541, 0.99044081,\n",
       "            0.98798022, 0.98741641, 0.987336  , 0.98563176, 0.9820085 ,\n",
       "            0.98173293, 0.97704915, 0.97451367, 0.96756127, 0.96405231,\n",
       "            0.89426133, 0.8701382 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.11111111, 0.11111111, 0.25      ,\n",
       "            0.25      , 0.27777778, 0.27777778, 0.33333333, 0.33333333,\n",
       "            0.47222222, 0.47222222, 0.63888889, 0.63888889, 0.66666667,\n",
       "            0.66666667, 0.69444444, 0.69444444, 0.80555556, 0.80555556,\n",
       "            0.83333333, 0.83333333, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.21428571, 0.21428571, 0.28571429, 0.28571429, 0.35714286,\n",
       "            0.35714286, 0.42857143, 0.42857143, 0.5       , 0.5       ,\n",
       "            0.64285714, 0.64285714, 0.85714286, 0.85714286, 0.92857143,\n",
       "            0.92857143, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99770687, 0.99490968, 0.99466966, 0.99359672,\n",
       "            0.9934541 , 0.9924864 , 0.99245752, 0.99190484, 0.99111393,\n",
       "            0.9898872 , 0.98875716, 0.98619669, 0.98432947, 0.98301262,\n",
       "            0.98125213, 0.98096843, 0.98062777, 0.97662891, 0.97556061,\n",
       "            0.97238179, 0.97146547, 0.87104583])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02777778, 0.27777778, 0.27777778, 0.44444444,\n",
       "            0.44444444, 0.47222222, 0.47222222, 0.52777778, 0.52777778,\n",
       "            0.61111111, 0.61111111, 0.77777778, 0.77777778, 0.94444444,\n",
       "            0.94444444, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.21428571, 0.21428571,\n",
       "            0.35714286, 0.35714286, 0.5       , 0.5       , 0.57142857,\n",
       "            0.57142857, 0.64285714, 0.64285714, 0.71428571, 0.71428571,\n",
       "            0.78571429, 0.78571429, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99766685, 0.99244606, 0.99093398, 0.98987008,\n",
       "            0.98709252, 0.98667629, 0.98585817, 0.98475113, 0.98269899,\n",
       "            0.98088306, 0.98071079, 0.97429446, 0.97129433, 0.95904504,\n",
       "            0.95338432, 0.93405964, 0.91476421])}}],\n",
       "  [{'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.22857143, 0.25714286, 0.42857143, 0.42857143,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.66666667, 0.66666667,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.08313029, 0.02329045, 0.01438104, 0.01249134,\n",
       "            0.01048664, 0.00871067, 0.00789187, 0.0071497 , 0.00684969,\n",
       "            0.00527368, 0.0051189 , 0.00484896, 0.00320289, 0.00315705,\n",
       "            0.00054054])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.17142857, 0.17142857, 0.34285714, 0.34285714, 0.45714286,\n",
       "            0.45714286, 0.48571429, 0.48571429, 0.62857143, 0.62857143,\n",
       "            0.97142857, 0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.06174594, 0.03127893, 0.01667699, 0.01477183,\n",
       "            0.01453775, 0.01347005, 0.01207522, 0.01078398, 0.01063716,\n",
       "            0.01042162, 0.00908407, 0.00715193, 0.00714809, 0.00499893,\n",
       "            0.00460617, 0.00456249, 0.00438959, 0.0031437 , 0.00314301,\n",
       "            0.00156437, 0.00131924, 0.00111483, 0.00101784])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.17142857,\n",
       "            0.2       , 0.4       , 0.4       , 0.51428571, 0.51428571,\n",
       "            0.65714286, 0.65714286, 0.71428571, 0.71428571, 0.82857143,\n",
       "            0.82857143, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.03015291, 0.02534994, 0.01939236, 0.01555624,\n",
       "            0.01453158, 0.01093317, 0.01067336, 0.00913291, 0.0089704 ,\n",
       "            0.0076527 , 0.00674901, 0.00584909, 0.00572748, 0.00529892,\n",
       "            0.00483063, 0.00316762, 0.0016598 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.14285714,\n",
       "            0.17142857, 0.22857143, 0.22857143, 0.31428571, 0.31428571,\n",
       "            0.51428571, 0.51428571, 0.57142857, 0.57142857, 0.6       ,\n",
       "            0.6       , 0.62857143, 0.62857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.53333333, 0.53333333, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.05023478, 0.02763532, 0.02690677, 0.01227139,\n",
       "            0.01146934, 0.01105779, 0.00985195, 0.00702262, 0.00693796,\n",
       "            0.0047357 , 0.00454179, 0.00425731, 0.00365884, 0.00316339,\n",
       "            0.00311979, 0.00265194, 0.00257232, 0.00070973])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.17142857, 0.17142857, 0.28571429, 0.28571429,\n",
       "            0.31428571, 0.31428571, 0.45714286, 0.48571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.05097413, 0.03723055, 0.02790931, 0.02597091,\n",
       "            0.02524058, 0.01247184, 0.01207244, 0.01180855, 0.01178663,\n",
       "            0.01025921, 0.00962468, 0.00859562, 0.00495049, 0.00458104,\n",
       "            0.00426298, 0.00386518, 0.0032831 , 0.00321628, 0.00069202])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.14285714,\n",
       "            0.17142857, 0.22857143, 0.22857143, 0.31428571, 0.31428571,\n",
       "            0.51428571, 0.51428571, 0.57142857, 0.57142857, 0.6       ,\n",
       "            0.6       , 0.62857143, 0.62857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.53333333, 0.53333333, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.05023478, 0.02763532, 0.02690677, 0.01227139,\n",
       "            0.01146934, 0.01105779, 0.00985195, 0.00702262, 0.00693796,\n",
       "            0.0047357 , 0.00454179, 0.00425731, 0.00365884, 0.00316339,\n",
       "            0.00311979, 0.00265194, 0.00257232, 0.00070973])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.17142857, 0.2       , 0.34285714,\n",
       "            0.34285714, 0.45714286, 0.45714286, 0.54285714, 0.54285714,\n",
       "            0.57142857, 0.57142857, 0.82857143, 0.82857143, 0.88571429,\n",
       "            0.88571429, 0.94285714, 0.94285714, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.06258208, 0.01515259, 0.01484259, 0.0102455 ,\n",
       "            0.00963977, 0.00756901, 0.00740422, 0.00670295, 0.00522689,\n",
       "            0.00520159, 0.00501499, 0.00381616, 0.00360604, 0.0030837 ,\n",
       "            0.00304074, 0.00269675, 0.00267344, 0.00204944, 0.00113201])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.17142857,\n",
       "            0.2       , 0.4       , 0.4       , 0.51428571, 0.51428571,\n",
       "            0.65714286, 0.65714286, 0.71428571, 0.71428571, 0.82857143,\n",
       "            0.82857143, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.03015291, 0.02534994, 0.01939236, 0.01555624,\n",
       "            0.01453158, 0.01093317, 0.01067336, 0.00913291, 0.0089704 ,\n",
       "            0.0076527 , 0.00674901, 0.00584909, 0.00572748, 0.00529892,\n",
       "            0.00483063, 0.00316762, 0.0016598 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.25714286, 0.25714286,\n",
       "            0.31428571, 0.31428571, 0.34285714, 0.37142857, 0.4       ,\n",
       "            0.4       , 0.51428571, 0.51428571, 0.71428571, 0.71428571,\n",
       "            0.82857143, 0.82857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.02897222, 0.02780698, 0.01183437, 0.01039862,\n",
       "            0.00922986, 0.00900461, 0.00827596, 0.00784525, 0.00778669,\n",
       "            0.00642519, 0.00439936, 0.00398066, 0.0030062 , 0.00284296,\n",
       "            0.00232114, 0.0015181 , 0.00088142])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.14285714,\n",
       "            0.14285714, 0.28571429, 0.28571429, 0.31428571, 0.31428571,\n",
       "            0.34285714, 0.34285714, 0.48571429, 0.51428571, 0.57142857,\n",
       "            0.57142857, 0.74285714, 0.74285714, 0.91428571, 0.91428571,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.73333333, 0.73333333, 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.035779  , 0.02981387, 0.02961067, 0.01526392,\n",
       "            0.01388458, 0.01005126, 0.00982818, 0.00932196, 0.00864448,\n",
       "            0.0082459 , 0.00704254, 0.00499737, 0.00482481, 0.00406288,\n",
       "            0.00389844, 0.00346271, 0.00323014, 0.00208556, 0.00193527,\n",
       "            0.00170444, 0.00154706, 0.00154164])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.42857143, 0.42857143, 0.45714286,\n",
       "            0.45714286, 0.51428571, 0.51428571, 0.6       , 0.6       ,\n",
       "            0.65714286, 0.68571429, 0.68571429, 0.71428571, 0.71428571,\n",
       "            0.74285714, 0.74285714, 0.94285714, 0.94285714, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.86666667, 0.86666667,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.0987305 , 0.01351922, 0.01121779, 0.01108299,\n",
       "            0.00990885, 0.00830009, 0.00763752, 0.00632097, 0.00561161,\n",
       "            0.00497239, 0.0047354 , 0.00430073, 0.00377963, 0.00299449,\n",
       "            0.00291131, 0.00260233, 0.00175918, 0.00116899, 0.0004766 ,\n",
       "            0.00022651])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.28571429, 0.28571429, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.4       , 0.45714286, 0.45714286, 0.51428571, 0.51428571,\n",
       "            0.6       , 0.6       , 0.71428571, 0.71428571, 0.74285714,\n",
       "            0.74285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.08161927, 0.06742531, 0.06166784, 0.05755628,\n",
       "            0.04257363, 0.02087323, 0.01988684, 0.01912679, 0.01814946,\n",
       "            0.01615372, 0.01498794, 0.01446978, 0.01323749, 0.01319035,\n",
       "            0.01223758, 0.00935424, 0.00859038, 0.0073302 , 0.00608994,\n",
       "            0.00402515, 0.00401845, 0.00337981, 0.00299848, 0.00266149,\n",
       "            0.00216624, 0.00069932])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.28571429, 0.28571429,\n",
       "            0.48571429, 0.51428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.14556052, 0.09708636, 0.09342614, 0.08021662,\n",
       "            0.07386696, 0.06839026, 0.06095793, 0.04278678, 0.04121814,\n",
       "            0.03993809, 0.03898118, 0.01611247, 0.01097071, 0.00939016,\n",
       "            0.00422486, 0.00389648, 0.00081993])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.25714286, 0.28571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.09662427, 0.06824733, 0.06606413, 0.05401614,\n",
       "            0.04571928, 0.03875223, 0.03106903, 0.02420371, 0.02244912,\n",
       "            0.01731991, 0.0169891 , 0.01490666, 0.01430401, 0.01330693,\n",
       "            0.01278966, 0.01205036, 0.00207925])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.14285714, 0.14285714,\n",
       "            0.17142857, 0.17142857, 0.4       , 0.4       , 0.54285714,\n",
       "            0.54285714, 0.74285714, 0.77142857, 0.77142857, 0.85714286,\n",
       "            0.85714286, 0.94285714, 0.94285714, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.0542512 , 0.03082434, 0.02413112, 0.02376481,\n",
       "            0.0227769 , 0.02002902, 0.01288813, 0.01019965, 0.0075468 ,\n",
       "            0.00679085, 0.00507449, 0.00477056, 0.00476263, 0.00337636,\n",
       "            0.00247562, 0.00210132, 0.00193489, 0.00174866, 0.0015438 ,\n",
       "            0.00145568])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.25714286, 0.28571429, 0.37142857, 0.37142857, 0.4       ,\n",
       "            0.4       , 0.48571429, 0.48571429, 0.51428571, 0.51428571,\n",
       "            0.68571429, 0.68571429, 0.71428571, 0.71428571, 0.77142857,\n",
       "            0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.17074101, 0.09783013, 0.06711053, 0.06377228,\n",
       "            0.03436875, 0.03161548, 0.02616152, 0.02568338, 0.02555147,\n",
       "            0.0187604 , 0.01618466, 0.01465048, 0.01444421, 0.0143378 ,\n",
       "            0.01221433, 0.01209955, 0.01189006, 0.01065882, 0.01016759,\n",
       "            0.00892607, 0.00275083])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.14285714, 0.14285714, 0.31428571, 0.31428571, 0.42857143,\n",
       "            0.42857143, 0.51428571, 0.51428571, 0.62857143, 0.65714286,\n",
       "            0.77142857, 0.77142857, 0.82857143, 0.82857143, 0.85714286,\n",
       "            0.85714286, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.06556521, 0.05347811, 0.04559517, 0.0370031 ,\n",
       "            0.03065396, 0.02577814, 0.02002212, 0.0192318 , 0.01641363,\n",
       "            0.0160881 , 0.01508391, 0.01506864, 0.01332186, 0.013222  ,\n",
       "            0.00972306, 0.00963571, 0.00883742, 0.00848445, 0.00814959,\n",
       "            0.00780175, 0.00677495, 0.00658708, 0.00390462])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.28571429, 0.31428571, 0.37142857, 0.37142857,\n",
       "            0.4       , 0.4       , 0.45714286, 0.45714286, 0.6       ,\n",
       "            0.6       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.17884393, 0.05206035, 0.04240052, 0.02673934,\n",
       "            0.02650723, 0.0242888 , 0.02102419, 0.02046595, 0.01989637,\n",
       "            0.01620045, 0.01388418, 0.01294689, 0.01146595, 0.01099048,\n",
       "            0.01045532, 0.01014164, 0.00849398, 0.00757926, 0.00461168,\n",
       "            0.00449127, 0.00045416])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.22857143, 0.22857143, 0.31428571,\n",
       "            0.34285714, 0.34285714, 0.42857143, 0.42857143, 0.51428571,\n",
       "            0.51428571, 0.54285714, 0.54285714, 0.77142857, 0.77142857,\n",
       "            0.91428571, 0.91428571, 0.97142857, 0.97142857, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.2       , 0.2       ,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.10519447, 0.03712881, 0.02745866, 0.02569221,\n",
       "            0.02334165, 0.02303744, 0.02083627, 0.01947481, 0.01572447,\n",
       "            0.01341359, 0.0133116 , 0.0130825 , 0.00922808, 0.00917644,\n",
       "            0.00800733, 0.00700417, 0.0063631 , 0.00538195, 0.00498815,\n",
       "            0.0042402 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.2       ,\n",
       "            0.2       , 0.22857143, 0.25714286, 0.25714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.4       , 0.4       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.26255315, 0.10633203, 0.07651254, 0.05856601,\n",
       "            0.04770082, 0.04569249, 0.04491713, 0.03539064, 0.02278504,\n",
       "            0.01321288, 0.01214504, 0.00973665, 0.00957709, 0.00065614])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.57142857, 0.57142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.53333333, 0.6       ,\n",
       "            0.6       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.2516363 , 0.18625939, 0.1585479 , 0.07582608,\n",
       "            0.06785895, 0.06681473, 0.05508415, 0.04577757, 0.03433272,\n",
       "            0.01163005, 0.00542984, 0.00095486])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.17142857,\n",
       "            0.17142857, 0.22857143, 0.22857143, 0.34285714, 0.37142857,\n",
       "            0.37142857, 0.45714286, 0.45714286, 0.57142857, 0.57142857,\n",
       "            0.62857143, 0.62857143, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.27447979, 0.10957609, 0.09477492, 0.04477035,\n",
       "            0.04234548, 0.04123914, 0.04011388, 0.02912343, 0.02749792,\n",
       "            0.02169858, 0.01765126, 0.01606115, 0.01325226, 0.01193408,\n",
       "            0.00996166, 0.00910885, 0.00573799, 0.00413897, 0.00147782])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.17142857, 0.17142857, 0.22857143, 0.22857143,\n",
       "            0.25714286, 0.25714286, 0.31428571, 0.31428571, 0.34285714,\n",
       "            0.34285714, 0.37142857, 0.37142857, 0.42857143, 0.42857143,\n",
       "            0.48571429, 0.48571429, 0.54285714, 0.57142857, 0.6       ,\n",
       "            0.6       , 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.1863388 , 0.08663104, 0.08201513, 0.07407733,\n",
       "            0.07085444, 0.05301312, 0.05298008, 0.04819912, 0.04681345,\n",
       "            0.04462567, 0.04392396, 0.03947943, 0.03825665, 0.03738553,\n",
       "            0.0369702 , 0.03498667, 0.03260875, 0.03077906, 0.03044988,\n",
       "            0.02908482, 0.02790359, 0.02541945, 0.02403849, 0.02331304,\n",
       "            0.02293311, 0.00911468, 0.00764952])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.25714286, 0.25714286, 0.4       , 0.4       ,\n",
       "            0.45714286, 0.45714286, 0.54285714, 0.54285714, 0.62857143,\n",
       "            0.65714286, 0.71428571, 0.71428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.29072549, 0.15483081, 0.14826192, 0.11702398,\n",
       "            0.09556534, 0.0898652 , 0.06638966, 0.05756285, 0.05328205,\n",
       "            0.05278102, 0.0494504 , 0.04675283, 0.03443418, 0.03357691,\n",
       "            0.02963829, 0.02865499, 0.02585156, 0.02474867, 0.02073372,\n",
       "            0.01929755, 0.01382097, 0.00999283, 0.00253224])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.28571429, 0.28571429, 0.4       ,\n",
       "            0.4       , 0.54285714, 0.54285714, 0.57142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.53584307, 0.37758874, 0.30185982, 0.27800564,\n",
       "            0.26007024, 0.20861404, 0.16983044, 0.15356027, 0.09920975,\n",
       "            0.07420447, 0.07058104, 0.06164514, 0.05875699, 0.03972758,\n",
       "            0.03309449, 0.01724395, 0.0158911 , 0.01556821, 0.00150044])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.22857143, 0.22857143,\n",
       "            0.31428571, 0.31428571, 0.45714286, 0.45714286, 0.48571429,\n",
       "            0.48571429, 0.51428571, 0.51428571, 0.57142857, 0.57142857,\n",
       "            0.85714286, 0.85714286, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.12842137, 0.11222434, 0.09177228, 0.08947008,\n",
       "            0.08902737, 0.0878335 , 0.08499996, 0.06959797, 0.0664934 ,\n",
       "            0.05055715, 0.04608095, 0.04002597, 0.0360419 , 0.03459893,\n",
       "            0.03331544, 0.03297912, 0.0319576 , 0.03017492, 0.02838546,\n",
       "            0.01116846, 0.01013935, 0.00921556, 0.00918293, 0.00362346])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.11428571, 0.11428571, 0.14285714,\n",
       "            0.14285714, 0.2       , 0.2       , 0.28571429, 0.28571429,\n",
       "            0.4       , 0.42857143, 0.48571429, 0.48571429, 0.6       ,\n",
       "            0.6       , 0.65714286, 0.65714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.25103908, 0.20920521, 0.1620546 , 0.15737894,\n",
       "            0.15428323, 0.13964748, 0.12793349, 0.12031683, 0.10867364,\n",
       "            0.08509998, 0.04778284, 0.043503  , 0.03932669, 0.03585589,\n",
       "            0.02607122, 0.02554197, 0.0217911 , 0.02173705, 0.01537653,\n",
       "            0.01485628, 0.01156164, 0.0113752 , 0.00193963])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.22857143, 0.22857143, 0.31428571, 0.31428571,\n",
       "            0.45714286, 0.45714286, 0.6       , 0.6       , 0.8       ,\n",
       "            0.8       , 0.82857143, 0.82857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.3038558 , 0.20620883, 0.17142715, 0.17009716,\n",
       "            0.12259873, 0.08690807, 0.07981564, 0.07774042, 0.07166939,\n",
       "            0.05474835, 0.04251065, 0.04186235, 0.03683947, 0.03610377,\n",
       "            0.03142617, 0.02999436, 0.0265099 , 0.02554377, 0.02144407,\n",
       "            0.02018291, 0.01837708, 0.01788186, 0.0046336 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.17142857,\n",
       "            0.17142857, 0.2       , 0.2       , 0.31428571, 0.31428571,\n",
       "            0.34285714, 0.34285714, 0.54285714, 0.54285714, 0.57142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.33333333, 0.33333333,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.52998543, 0.42029166, 0.18969137, 0.14053498,\n",
       "            0.08337506, 0.08289476, 0.05586863, 0.05085218, 0.0376699 ,\n",
       "            0.03672967, 0.03530198, 0.01947313, 0.01926421, 0.01405373,\n",
       "            0.00134591])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.14285714, 0.14285714, 0.22857143, 0.25714286, 0.37142857,\n",
       "            0.37142857, 0.48571429, 0.48571429, 0.51428571, 0.51428571,\n",
       "            0.54285714, 0.54285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.33209166, 0.16117107, 0.14706646, 0.09084469,\n",
       "            0.0772697 , 0.07154347, 0.0614475 , 0.05926893, 0.038651  ,\n",
       "            0.0356819 , 0.02497006, 0.02341991, 0.022445  , 0.01873026,\n",
       "            0.01755345, 0.01732364, 0.0031088 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.17142857, 0.17142857, 0.4       , 0.4       , 0.42857143,\n",
       "            0.42857143, 0.45714286, 0.45714286, 0.51428571, 0.51428571,\n",
       "            0.57142857, 0.57142857, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.53333333, 0.6       , 0.6       , 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.25569973, 0.15676437, 0.13063928, 0.11911923,\n",
       "            0.07987136, 0.07847979, 0.06076855, 0.05751701, 0.05730504,\n",
       "            0.05601814, 0.05449022, 0.05153982, 0.04867613, 0.04277201,\n",
       "            0.0387091 , 0.0383174 , 0.01298975, 0.00897827, 0.00533502])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.17142857, 0.17142857, 0.2       , 0.2       , 0.22857143,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.37142857, 0.37142857, 0.62857143, 0.62857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.53333333, 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.42469394, 0.29563808, 0.2302486 , 0.1248583 ,\n",
       "            0.08531797, 0.08328344, 0.06974468, 0.06491684, 0.06364355,\n",
       "            0.06347153, 0.06237247, 0.05888917, 0.05771803, 0.05761389,\n",
       "            0.04770187, 0.04769908, 0.02255889, 0.02245084, 0.00420143])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.22857143, 0.22857143, 0.25714286, 0.25714286, 0.45714286,\n",
       "            0.45714286, 0.51428571, 0.51428571, 0.57142857, 0.6       ,\n",
       "            0.8       , 0.8       , 0.82857143, 0.82857143, 0.85714286,\n",
       "            0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.63098612, 0.43032664, 0.24181345, 0.20926031,\n",
       "            0.13593018, 0.13037579, 0.12449619, 0.10425521, 0.05173905,\n",
       "            0.04946774, 0.0480823 , 0.04695745, 0.04444129, 0.04440315,\n",
       "            0.02419873, 0.01867206, 0.01838463, 0.01544161, 0.01536539,\n",
       "            0.01493646, 0.00263535])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.11428571, 0.11428571,\n",
       "            0.14285714, 0.14285714, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.28571429, 0.28571429, 0.45714286, 0.45714286,\n",
       "            0.54285714, 0.54285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.46666667, 0.46666667,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.33123092, 0.29841263, 0.24813473, 0.21754977,\n",
       "            0.21548226, 0.20373359, 0.18736653, 0.12486592, 0.087917  ,\n",
       "            0.08353292, 0.06852622, 0.06306313, 0.04784128, 0.03697489,\n",
       "            0.02776319, 0.02695983, 0.00378638])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.11428571, 0.11428571, 0.25714286,\n",
       "            0.28571429, 0.42857143, 0.42857143, 0.51428571, 0.51428571,\n",
       "            0.6       , 0.6       , 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.24351917, 0.16194759, 0.1539448 , 0.1431532 ,\n",
       "            0.1388218 , 0.13330349, 0.12025262, 0.1148365 , 0.10156212,\n",
       "            0.09756419, 0.08071474, 0.07706906, 0.06412001, 0.0626743 ,\n",
       "            0.05505251, 0.04746186, 0.02470919, 0.02465702, 0.02395869])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.11428571, 0.11428571,\n",
       "            0.14285714, 0.14285714, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.34285714, 0.37142857, 0.42857143, 0.42857143,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.33238761, 0.29962043, 0.27237077, 0.27018566,\n",
       "            0.25575359, 0.25498626, 0.24384996, 0.22009708, 0.21978885,\n",
       "            0.21227926, 0.1952393 , 0.18621586, 0.14304584, 0.1411558 ,\n",
       "            0.11245946, 0.08030959, 0.07845567, 0.07226181, 0.06895905,\n",
       "            0.00964786])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.14285714,\n",
       "            0.14285714, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.34285714, 0.34285714, 0.37142857, 0.4       , 0.4       ,\n",
       "            0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.58595835, 0.2981432 , 0.20762813, 0.16375911,\n",
       "            0.14890067, 0.09201137, 0.08995172, 0.06472535, 0.05981453,\n",
       "            0.04618534, 0.04154719, 0.04047407, 0.03279427, 0.02820788,\n",
       "            0.00856806, 0.00578886, 0.00290022])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.11428571, 0.11428571, 0.22857143,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.31428571, 0.34285714,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.54973171, 0.36712171, 0.29720315, 0.27403058,\n",
       "            0.27320352, 0.23989518, 0.23218543, 0.21689803, 0.18396352,\n",
       "            0.16408671, 0.15695766, 0.10882265, 0.09831569, 0.09203476,\n",
       "            0.00591368])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.2       , 0.2       ,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.34285714, 0.34285714,\n",
       "            0.4       , 0.4       , 0.51428571, 0.51428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.44267144, 0.40656937, 0.35874208, 0.30314185,\n",
       "            0.29952663, 0.2894753 , 0.23160638, 0.21475358, 0.18159588,\n",
       "            0.17921232, 0.16764822, 0.16712921, 0.15851859, 0.1539494 ,\n",
       "            0.129221  , 0.10189447, 0.07858733, 0.07846756, 0.01120759])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.2       , 0.2       , 0.22857143,\n",
       "            0.22857143, 0.31428571, 0.31428571, 0.4       , 0.4       ,\n",
       "            0.48571429, 0.48571429, 0.71428571, 0.71428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.54208815, 0.4054116 , 0.36154866, 0.34698609,\n",
       "            0.32008464, 0.17402849, 0.15758282, 0.15758282, 0.15193924,\n",
       "            0.15130117, 0.14470828, 0.13451359, 0.11722735, 0.11099687,\n",
       "            0.09783126, 0.09549757, 0.0742133 , 0.06831607, 0.0419579 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.2       , 0.2       ,\n",
       "            0.28571429, 0.28571429, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.37142857, 0.48571429, 0.51428571, 0.51428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.65929744, 0.5863478 , 0.38741181, 0.31866833,\n",
       "            0.30315059, 0.23218074, 0.21518956, 0.17438425, 0.15045705,\n",
       "            0.13380092, 0.13325804, 0.12517613, 0.11703837, 0.11691547,\n",
       "            0.11432189, 0.07993016, 0.07192558, 0.06867441, 0.00764923])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.45714286, 0.45714286, 0.51428571, 0.51428571, 0.54285714,\n",
       "            0.54285714, 0.6       , 0.6       , 0.68571429, 0.68571429,\n",
       "            0.71428571, 0.71428571, 0.85714286, 0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.33333333, 0.33333333, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.67780107, 0.50087195, 0.42920472, 0.42586835,\n",
       "            0.40675676, 0.26044897, 0.24190758, 0.21641055, 0.20249624,\n",
       "            0.13686339, 0.13451303, 0.12214745, 0.12106521, 0.11137781,\n",
       "            0.11021982, 0.10115773, 0.1007999 , 0.08580847, 0.08529531,\n",
       "            0.08517795, 0.08313114, 0.0663695 , 0.06127736, 0.02539633])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.14285714, 0.14285714, 0.17142857, 0.17142857,\n",
       "            0.28571429, 0.31428571, 0.48571429, 0.48571429, 0.51428571,\n",
       "            0.51428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.43173914, 0.38720158, 0.36126851, 0.33755131,\n",
       "            0.29754244, 0.28611866, 0.27465759, 0.27307408, 0.1791574 ,\n",
       "            0.16014246, 0.15933591, 0.10333819, 0.09682167, 0.09329931,\n",
       "            0.08908614, 0.0177934 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.17142857, 0.17142857,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.4       , 0.4       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.66811501, 0.425669  , 0.40324806, 0.40311183,\n",
       "            0.35837878, 0.33847231, 0.27995914, 0.23722746, 0.23059271,\n",
       "            0.20369844, 0.20369844, 0.20312468, 0.20090818, 0.1792417 ,\n",
       "            0.1706351 , 0.02130196])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.2       , 0.2       ,\n",
       "            0.22857143, 0.22857143, 0.31428571, 0.31428571, 0.4       ,\n",
       "            0.4       , 0.42857143, 0.42857143, 0.51428571, 0.51428571,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.73333333, 0.8       , 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.40284774, 0.39238067, 0.38349869, 0.36052427,\n",
       "            0.33253662, 0.32492795, 0.30821728, 0.27925947, 0.26430141,\n",
       "            0.25398378, 0.25117567, 0.20990974, 0.20935687, 0.1960469 ,\n",
       "            0.1832205 , 0.17420961, 0.15938448, 0.15143671, 0.14348383,\n",
       "            0.03859862])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.25714286,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.45714286,\n",
       "            0.45714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.51230902, 0.45221638, 0.44300028, 0.36422084,\n",
       "            0.34868813, 0.29425477, 0.28970825, 0.28793118, 0.23494504,\n",
       "            0.21830575, 0.18640511, 0.15758316, 0.14486109, 0.12717339,\n",
       "            0.12643707, 0.03017999])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.28571429,\n",
       "            0.28571429, 0.31428571, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.37142857, 0.42857143, 0.42857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.6225341 , 0.59018063, 0.45573537, 0.4456495 ,\n",
       "            0.43712593, 0.36173434, 0.34906124, 0.32815574, 0.26588916,\n",
       "            0.23590567, 0.23349495, 0.22917619, 0.20228654, 0.18657643,\n",
       "            0.18639206, 0.14680774, 0.14647594, 0.01650156])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.14285714, 0.22857143,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.31428571, 0.34285714, 0.34285714, 0.45714286, 0.45714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.86666667, 0.86666667, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.64371303, 0.62546807, 0.43145676, 0.40566994,\n",
       "            0.40144719, 0.37573888, 0.35434095, 0.34340015, 0.27172188,\n",
       "            0.26918385, 0.25144   , 0.24995945, 0.24328616, 0.24184525,\n",
       "            0.23159255, 0.22993853, 0.22898816, 0.19580946, 0.17461878,\n",
       "            0.04975319])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.2),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.34285714, 0.34285714, 0.37142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.33333333, 0.33333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.86666667, 0.86666667, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.7274839 , 0.64971127, 0.51102073, 0.46522481,\n",
       "            0.42859438, 0.42155869, 0.349815  , 0.34651337, 0.34044943,\n",
       "            0.31720642, 0.29417568, 0.24339395, 0.22460522, 0.21711251,\n",
       "            0.03867371])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.2),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.17142857,\n",
       "            0.17142857, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.31428571, 0.31428571, 0.34285714, 0.34285714, 0.45714286,\n",
       "            0.45714286, 0.51428571, 0.51428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.65340292, 0.50258164, 0.50123065, 0.47399416,\n",
       "            0.41822201, 0.41144352, 0.39148963, 0.3761875 , 0.37072048,\n",
       "            0.34883988, 0.32382057, 0.31358156, 0.30470254, 0.30232087,\n",
       "            0.26384263, 0.26328241, 0.21788469, 0.21367756, 0.20178645,\n",
       "            0.1830722 , 0.16761271, 0.16358953, 0.05155978])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.2),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.31428571, 0.31428571, 0.45714286, 0.45714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.69838705, 0.66937283, 0.54982751, 0.495232  ,\n",
       "            0.46874026, 0.4440546 , 0.36614729, 0.35830623, 0.35681301,\n",
       "            0.34871118, 0.34697847, 0.32995474, 0.30982078, 0.3016419 ,\n",
       "            0.29456298, 0.26675629, 0.2380297 , 0.23064318, 0.04396349])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.2),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.31428571, 0.31428571, 0.45714286, 0.45714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.69838705, 0.66937283, 0.54982751, 0.495232  ,\n",
       "            0.46874026, 0.4440546 , 0.36614729, 0.35830623, 0.35681301,\n",
       "            0.34871118, 0.34697847, 0.32995474, 0.30982078, 0.3016419 ,\n",
       "            0.29456298, 0.26675629, 0.2380297 , 0.23064318, 0.04396349])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.2),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.28571429, 0.45714286, 0.45714286, 0.48571429,\n",
       "            0.48571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.69738352, 0.67621733, 0.53824492, 0.51359904,\n",
       "            0.48165718, 0.47897481, 0.45748585, 0.40145999, 0.39906706,\n",
       "            0.37986749, 0.37673626, 0.37440217, 0.34639887, 0.34223113,\n",
       "            0.34086207, 0.31513396, 0.2699843 , 0.25951628, 0.25373492,\n",
       "            0.23634572, 0.04491823])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.4666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.22857143, 0.25714286, 0.25714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.86666667, 0.86666667, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.85952526, 0.71235419, 0.56724966, 0.55300754,\n",
       "            0.54409403, 0.50778719, 0.4730168 , 0.47077893, 0.47016695,\n",
       "            0.40227123, 0.37254133, 0.34452131, 0.34411247, 0.33578191,\n",
       "            0.05223759])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.4666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.22857143, 0.25714286, 0.28571429, 0.28571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.71368046, 0.70610889, 0.53396197, 0.53393253,\n",
       "            0.52408859, 0.51363725, 0.46397852, 0.45941191, 0.41696633,\n",
       "            0.36076551, 0.36000725, 0.35925168, 0.34446974, 0.05735327])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.14285714285714285),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.37142857, 0.4       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.66666667,\n",
       "            0.66666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.85946068, 0.70662199, 0.70585663, 0.6841459 ,\n",
       "            0.64482055, 0.62662749, 0.60024721, 0.55354106, 0.50613308,\n",
       "            0.48980778, 0.42911832, 0.33741278, 0.3223122 , 0.06638623])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2),\n",
       "    'tpr': np.float64(0.4666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.17142857, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.48571429, 0.48571429, 0.62857143, 0.62857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.53333333, 0.53333333, 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.86864094, 0.73348743, 0.69498956, 0.62002759,\n",
       "            0.51577506, 0.51511226, 0.46673134, 0.46457498, 0.41768037,\n",
       "            0.36088282, 0.3515918 , 0.28586676, 0.28028426, 0.07724362])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.22857142857142856),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.17142857, 0.17142857, 0.42857143, 0.42857143, 0.51428571,\n",
       "            0.51428571, 0.6       , 0.6       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.66666667,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.90150398, 0.7751116 , 0.77352048, 0.68300267,\n",
       "            0.63652381, 0.59493928, 0.59153731, 0.57384635, 0.55575419,\n",
       "            0.52721901, 0.52504217, 0.4402927 , 0.4101202 , 0.38581878,\n",
       "            0.35576678, 0.29472918, 0.26649707, 0.05198549])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.25714286, 0.25714286, 0.28571429,\n",
       "            0.28571429, 0.31428571, 0.34285714, 0.37142857, 0.37142857,\n",
       "            0.45714286, 0.45714286, 0.48571429, 0.48571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.85501811, 0.85179994, 0.80636203, 0.73354218,\n",
       "            0.67229887, 0.63120081, 0.61714177, 0.60620639, 0.60147535,\n",
       "            0.53617851, 0.50544386, 0.48982019, 0.4880328 , 0.48626597,\n",
       "            0.48134414, 0.47566943, 0.43950235, 0.43419263, 0.41797097,\n",
       "            0.40951218, 0.40679012, 0.39094446, 0.37961439, 0.0940449 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2571428571428571),\n",
       "    'tpr': np.float64(0.7333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.2       , 0.22857143, 0.22857143,\n",
       "            0.37142857, 0.37142857, 0.77142857, 0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.53333333, 0.73333333,\n",
       "            0.73333333, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.85271421, 0.82155931, 0.80203554, 0.68594068,\n",
       "            0.6637842 , 0.56138364, 0.52853757, 0.52042347, 0.514014  ,\n",
       "            0.46484132, 0.42262459, 0.26489051, 0.26409392, 0.13053562])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.17142857142857143),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.17142857, 0.17142857, 0.28571429, 0.28571429,\n",
       "            0.4       , 0.42857143, 0.45714286, 0.45714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.6       , 0.6       , 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.84467317, 0.78034136, 0.68625748, 0.68501717,\n",
       "            0.62172508, 0.54194788, 0.49628491, 0.45763525, 0.4500422 ,\n",
       "            0.39938585, 0.39555186, 0.38844768, 0.35947586, 0.07926032])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.4),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.42857143, 0.45714286, 0.45714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.4       , 0.4       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.95082329, 0.91982566, 0.87124993, 0.85836052,\n",
       "            0.80535924, 0.80146163, 0.72681761, 0.71809032, 0.69509659,\n",
       "            0.67653119, 0.67173139, 0.6656138 , 0.58934204, 0.57689329,\n",
       "            0.57080092, 0.44494941, 0.41426973, 0.407696  , 0.12096081])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.37142857142857144),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.11428571, 0.11428571, 0.14285714,\n",
       "            0.14285714, 0.17142857, 0.17142857, 0.22857143, 0.22857143,\n",
       "            0.37142857, 0.4       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.73333333, 0.73333333, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9110877 , 0.85413989, 0.84938055, 0.84585114,\n",
       "            0.81205906, 0.80141645, 0.77009124, 0.7660009 , 0.72852293,\n",
       "            0.72115572, 0.71757957, 0.63187223, 0.6102324 , 0.56348495,\n",
       "            0.50098048, 0.49830947, 0.09261914])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.4),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.22857143, 0.22857143, 0.25714286, 0.28571429,\n",
       "            0.28571429, 0.51428571, 0.51428571, 0.6       , 0.6       ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.91722804, 0.88506321, 0.86582729, 0.79379788,\n",
       "            0.74579362, 0.73344532, 0.72410669, 0.71467613, 0.71128051,\n",
       "            0.70829268, 0.66773876, 0.64029784, 0.62414489, 0.62214475,\n",
       "            0.5824257 , 0.45742586, 0.45455569, 0.43367926, 0.42728268,\n",
       "            0.08832914])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.45714285714285713),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.17142857, 0.17142857,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.28571429,\n",
       "            0.28571429, 0.34285714, 0.37142857, 0.45714286, 0.45714286,\n",
       "            0.68571429, 0.68571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.89186907, 0.84478286, 0.80168188, 0.76933825,\n",
       "            0.76667047, 0.76304507, 0.74879047, 0.71049844, 0.68498612,\n",
       "            0.67541873, 0.67215942, 0.66261656, 0.61782174, 0.59495712,\n",
       "            0.58811535, 0.57275201, 0.56221518, 0.51972264, 0.50747963,\n",
       "            0.39464649, 0.38822667, 0.20521719])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6571428571428571),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.25714286, 0.25714286, 0.37142857,\n",
       "            0.37142857, 0.4       , 0.4       , 0.45714286, 0.45714286,\n",
       "            0.54285714, 0.57142857, 0.6       , 0.6       , 0.62857143,\n",
       "            0.62857143, 0.74285714, 0.74285714, 0.91428571, 0.91428571,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.92618582, 0.92064014, 0.91058971, 0.89659668,\n",
       "            0.86568049, 0.82606196, 0.70549579, 0.66212227, 0.63193326,\n",
       "            0.62313049, 0.62117386, 0.61863615, 0.6073316 , 0.60605676,\n",
       "            0.55639698, 0.54833716, 0.53932306, 0.53018969, 0.5272197 ,\n",
       "            0.51924608, 0.45081727, 0.42504783, 0.26025104, 0.24892676,\n",
       "            0.17805005])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.4857142857142857),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.17142857, 0.17142857, 0.2       , 0.2       , 0.22857143,\n",
       "            0.22857143, 0.28571429, 0.28571429, 0.31428571, 0.34285714,\n",
       "            0.48571429, 0.48571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.53333333, 0.53333333, 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.94857609, 0.9045141 , 0.85820711, 0.82978067,\n",
       "            0.78835802, 0.76447598, 0.76418902, 0.75014774, 0.73635863,\n",
       "            0.69376776, 0.65141358, 0.61600736, 0.59808984, 0.59036429,\n",
       "            0.5113808 , 0.47432399, 0.14796758])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6285714285714286),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.14285714,\n",
       "            0.17142857, 0.2       , 0.2       , 0.34285714, 0.34285714,\n",
       "            0.42857143, 0.42857143, 0.62857143, 0.62857143, 0.8       ,\n",
       "            0.8       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.93747125, 0.90778063, 0.90064828, 0.89557997,\n",
       "            0.88850753, 0.81865967, 0.79195015, 0.77182231, 0.76912119,\n",
       "            0.76829521, 0.76391321, 0.73020952, 0.66152889, 0.64637639,\n",
       "            0.62256003, 0.60382059, 0.54557581, 0.53128417, 0.40739403,\n",
       "            0.40228456, 0.1973502 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6571428571428571),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.14285714,\n",
       "            0.14285714, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.45714286, 0.48571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98897816, 0.93512215, 0.91813124, 0.91305345,\n",
       "            0.9117395 , 0.90555969, 0.90199792, 0.8864388 , 0.88144888,\n",
       "            0.8746105 , 0.82297239, 0.80969496, 0.79357757, 0.76402083,\n",
       "            0.61365139, 0.61359883, 0.10429338])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5142857142857142),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.17142857, 0.17142857,\n",
       "            0.2       , 0.2       , 0.25714286, 0.25714286, 0.31428571,\n",
       "            0.31428571, 0.42857143, 0.42857143, 0.54285714, 0.54285714,\n",
       "            0.77142857, 0.8       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99158455, 0.98960464, 0.97901271, 0.97189689,\n",
       "            0.92434763, 0.91298304, 0.88227746, 0.84487474, 0.84104901,\n",
       "            0.837452  , 0.8140319 , 0.75762532, 0.75252994, 0.72662033,\n",
       "            0.72183914, 0.61237334, 0.59252676, 0.49905253, 0.49281838,\n",
       "            0.28164127, 0.27823898, 0.08089284])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6857142857142857),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.17142857, 0.22857143,\n",
       "            0.22857143, 0.37142857, 0.37142857, 0.4       , 0.4       ,\n",
       "            0.42857143, 0.42857143, 0.65714286, 0.65714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96069737, 0.93204744, 0.92863302, 0.89442902,\n",
       "            0.88012037, 0.87467038, 0.87003024, 0.86548733, 0.85927771,\n",
       "            0.85754126, 0.81863867, 0.81692908, 0.81448523, 0.74009703,\n",
       "            0.72102175, 0.70623489, 0.56826243, 0.5498543 , 0.14612507])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7714285714285715),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.22857143, 0.22857143, 0.37142857, 0.37142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.94133839, 0.90518495, 0.89788994, 0.88499713,\n",
       "            0.87816011, 0.8773909 , 0.85220628, 0.84459363, 0.84195218,\n",
       "            0.80443205, 0.79805312, 0.79725471, 0.75499976, 0.75379388,\n",
       "            0.14425663])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8285714285714286),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.17142857, 0.17142857, 0.25714286,\n",
       "            0.25714286, 0.28571429, 0.28571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.2       , 0.2       , 0.53333333,\n",
       "            0.53333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9683687 , 0.95982472, 0.9574055 , 0.91383292,\n",
       "            0.91255401, 0.88372322, 0.86820647, 0.85421521, 0.77817699,\n",
       "            0.77724066, 0.77349058, 0.7495929 , 0.22725007])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8571428571428571),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.17142857,\n",
       "            0.17142857, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.25714286, 0.25714286, 0.45714286, 0.45714286, 0.57142857,\n",
       "            0.57142857, 0.6       , 0.6       , 0.65714286, 0.65714286,\n",
       "            0.68571429, 0.68571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.97225802, 0.96792968, 0.95906058, 0.9585036 ,\n",
       "            0.95842057, 0.95659909, 0.94812096, 0.93510024, 0.92021704,\n",
       "            0.91849828, 0.89655118, 0.88627347, 0.88372763, 0.88308995,\n",
       "            0.8809655 , 0.85404497, 0.794952  , 0.78497177, 0.73799525,\n",
       "            0.7363941 , 0.73468627, 0.7309927 , 0.72940917, 0.72746891,\n",
       "            0.72449618, 0.72449618, 0.26839566])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.25714286, 0.25714286, 0.34285714, 0.34285714,\n",
       "            0.42857143, 0.42857143, 0.54285714, 0.54285714, 0.57142857,\n",
       "            0.57142857, 0.71428571, 0.74285714, 0.74285714, 0.94285714,\n",
       "            0.94285714, 0.97142857, 0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98828821, 0.98670716, 0.98372236, 0.98096201,\n",
       "            0.98001628, 0.96017685, 0.95747942, 0.94461717, 0.94454739,\n",
       "            0.9332832 , 0.92915628, 0.90353148, 0.90133611, 0.90000619,\n",
       "            0.89908452, 0.85335231, 0.84986814, 0.82254399, 0.75846786,\n",
       "            0.73782133, 0.62827823, 0.62010166, 0.61718132, 0.59910785])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9714285714285714),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.17142857,\n",
       "            0.17142857, 0.34285714, 0.34285714, 0.48571429, 0.48571429,\n",
       "            0.54285714, 0.54285714, 0.57142857, 0.57142857, 0.6       ,\n",
       "            0.65714286, 0.65714286, 0.68571429, 0.68571429, 0.8       ,\n",
       "            0.8       , 0.82857143, 0.82857143, 0.88571429, 0.88571429,\n",
       "            0.91428571, 0.91428571, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98660546, 0.98160639, 0.97663054, 0.96575774,\n",
       "            0.96024861, 0.94631022, 0.94619253, 0.91402685, 0.90675786,\n",
       "            0.87954443, 0.87558857, 0.85595576, 0.85203949, 0.84987941,\n",
       "            0.82870037, 0.81985641, 0.79578044, 0.7748995 , 0.74330226,\n",
       "            0.73177153, 0.72142259, 0.70525999, 0.69530542, 0.68921883,\n",
       "            0.65315296, 0.64796942, 0.47519852, 0.44063091])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9428571428571428),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.14285714, 0.14285714, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.28571429, 0.28571429, 0.34285714, 0.34285714,\n",
       "            0.37142857, 0.37142857, 0.4       , 0.4       , 0.42857143,\n",
       "            0.42857143, 0.6       , 0.6       , 0.8       , 0.82857143,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.97712489, 0.96628178, 0.95413403, 0.9523259 ,\n",
       "            0.94147816, 0.90130183, 0.90050718, 0.89385474, 0.88569187,\n",
       "            0.8666249 , 0.84493979, 0.84040896, 0.8331877 , 0.82438085,\n",
       "            0.81922873, 0.81480751, 0.79414487, 0.79078075, 0.78182424,\n",
       "            0.78177425, 0.69234307, 0.6797033 , 0.61194354, 0.59972082,\n",
       "            0.39221317])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9714285714285714),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.31428571,\n",
       "            0.31428571, 0.85714286, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.8       , 0.8       ,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9908211 , 0.97268914, 0.96526977, 0.93838694,\n",
       "            0.92887686, 0.9214762 , 0.91391867, 0.90858808, 0.84079363,\n",
       "            0.81913821, 0.62904187, 0.6228235 , 0.49336289])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.05714286,\n",
       "            0.2       , 0.2       , 0.28571429, 0.28571429, 0.34285714,\n",
       "            0.34285714, 0.37142857, 0.37142857, 0.42857143, 0.42857143,\n",
       "            0.45714286, 0.45714286, 0.48571429, 0.48571429, 0.51428571,\n",
       "            0.51428571, 0.54285714, 0.54285714, 0.57142857, 0.57142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.97788969, 0.97593716, 0.9719226 , 0.97046275,\n",
       "            0.95423614, 0.94651148, 0.93676628, 0.93480992, 0.92415021,\n",
       "            0.92399132, 0.91762467, 0.9054346 , 0.88860172, 0.8880238 ,\n",
       "            0.88583909, 0.88019852, 0.86790769, 0.86255367, 0.86121151,\n",
       "            0.86024068, 0.85735229, 0.84917908, 0.84732775, 0.84565075,\n",
       "            0.5006413 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.11428571, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.37142857, 0.37142857, 0.45714286,\n",
       "            0.45714286, 0.51428571, 0.51428571, 0.54285714, 0.54285714,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.98836393, 0.96811171, 0.96767542, 0.96718412,\n",
       "            0.96615511, 0.96489143, 0.95535187, 0.948106  , 0.94427005,\n",
       "            0.91407571, 0.9083718 , 0.89834551, 0.88312981, 0.87866277,\n",
       "            0.87539041, 0.86346557, 0.8597571 , 0.85867699, 0.84947213,\n",
       "            0.51932552])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.17142857, 0.17142857, 0.22857143,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.4       , 0.4       ,\n",
       "            0.48571429, 0.51428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.53333333, 0.53333333, 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98791932, 0.98570309, 0.98264187, 0.98136399,\n",
       "            0.97528071, 0.95695023, 0.94885815, 0.9388061 , 0.91907262,\n",
       "            0.91452961, 0.90894147, 0.89951309, 0.8603194 , 0.82935317,\n",
       "            0.7984569 , 0.79776247, 0.58835207])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.2       , 0.22857143, 0.25714286, 0.25714286,\n",
       "            0.6       , 0.6       , 0.8       , 0.8       , 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98787081, 0.97085635, 0.96879191, 0.96823316,\n",
       "            0.96679802, 0.95879324, 0.95813411, 0.95455656, 0.95114759,\n",
       "            0.94172836, 0.94056987, 0.93913505, 0.92783989, 0.9249852 ,\n",
       "            0.88183741, 0.87676862, 0.84519747, 0.83321388, 0.74924877,\n",
       "            0.70360137, 0.66574831])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.14285714, 0.14285714,\n",
       "            0.17142857, 0.17142857, 0.22857143, 0.22857143, 0.31428571,\n",
       "            0.31428571, 0.42857143, 0.42857143, 0.57142857, 0.57142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99663473, 0.99616165, 0.99566522, 0.99523956,\n",
       "            0.98786493, 0.98753553, 0.98500325, 0.97250369, 0.96899149,\n",
       "            0.96872879, 0.96294872, 0.95846556, 0.95288259, 0.94553362,\n",
       "            0.94364217, 0.92980286, 0.92854674, 0.90363389, 0.8956643 ,\n",
       "            0.53146861])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.22857143, 0.22857143, 0.25714286, 0.25714286,\n",
       "            0.34285714, 0.34285714, 0.42857143, 0.42857143, 0.57142857,\n",
       "            0.6       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99454734, 0.99330851, 0.99265491, 0.99134155,\n",
       "            0.98877102, 0.97996494, 0.97769115, 0.97222894, 0.96930453,\n",
       "            0.96877085, 0.96076212, 0.95998021, 0.95986296, 0.95704809,\n",
       "            0.94700613, 0.94647952, 0.91550597, 0.89714892, 0.85356287,\n",
       "            0.84092319, 0.61852244])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.14285714,\n",
       "            0.14285714, 0.17142857, 0.17142857, 0.2       , 0.2       ,\n",
       "            0.25714286, 0.25714286, 0.42857143, 0.45714286, 0.6       ,\n",
       "            0.6       , 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98988981, 0.98641742, 0.9650847 , 0.96163754,\n",
       "            0.9615133 , 0.95584406, 0.95462172, 0.95129928, 0.94515659,\n",
       "            0.94355921, 0.94001089, 0.92746808, 0.92065883, 0.902922  ,\n",
       "            0.90014112, 0.82413333, 0.80681314, 0.7289935 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.14285714, 0.2       ,\n",
       "            0.2       , 0.22857143, 0.22857143, 0.25714286, 0.25714286,\n",
       "            0.28571429, 0.37142857, 0.37142857, 0.42857143, 0.42857143,\n",
       "            0.54285714, 0.54285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98524014, 0.98212944, 0.97122089, 0.96851532,\n",
       "            0.96588214, 0.96492607, 0.95755192, 0.95638789, 0.95307847,\n",
       "            0.95063057, 0.94928503, 0.9418343 , 0.93372933, 0.92994412,\n",
       "            0.9297092 , 0.92799458, 0.92763565, 0.92679836, 0.92304005,\n",
       "            0.90817838, 0.90736635, 0.71224054])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.22857143, 0.22857143, 0.25714286, 0.25714286, 0.31428571,\n",
       "            0.31428571, 0.34285714, 0.34285714, 0.4       , 0.4       ,\n",
       "            0.57142857, 0.57142857, 0.68571429, 0.71428571, 0.97142857,\n",
       "            0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99180727, 0.99067389, 0.98794412, 0.98593694,\n",
       "            0.98102278, 0.97565985, 0.97383843, 0.97172631, 0.96621972,\n",
       "            0.96425176, 0.96402716, 0.95975431, 0.95772208, 0.9577197 ,\n",
       "            0.94743659, 0.94592671, 0.92818231, 0.92781851, 0.8532668 ,\n",
       "            0.83341876, 0.80328322, 0.72454549])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.11428571, 0.22857143,\n",
       "            0.22857143, 0.37142857, 0.37142857, 0.51428571, 0.51428571,\n",
       "            0.57142857, 0.57142857, 0.68571429, 0.68571429, 0.71428571,\n",
       "            0.71428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99684395, 0.98807914, 0.98741222, 0.98701521,\n",
       "            0.98465104, 0.98323622, 0.98256036, 0.98205867, 0.97850962,\n",
       "            0.9763494 , 0.96848619, 0.96614109, 0.95338968, 0.95217595,\n",
       "            0.9460469 , 0.94487637, 0.91907832, 0.91148666, 0.87783544,\n",
       "            0.87061781, 0.71700513])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.2       , 0.2       , 0.25714286, 0.25714286, 0.28571429,\n",
       "            0.31428571, 0.34285714, 0.34285714, 0.42857143, 0.42857143,\n",
       "            0.71428571, 0.71428571, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.4       , 0.4       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99642887, 0.98764813, 0.98625517, 0.98246531,\n",
       "            0.97811663, 0.97665874, 0.97490367, 0.9748019 , 0.97446685,\n",
       "            0.97382851, 0.9735528 , 0.97281128, 0.96924088, 0.96387229,\n",
       "            0.94302193, 0.94177735, 0.89034832, 0.88558821, 0.69133615])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.28571429,\n",
       "            0.28571429, 0.34285714, 0.34285714, 0.4       , 0.4       ,\n",
       "            0.6       , 0.62857143, 0.62857143, 0.65714286, 0.65714286,\n",
       "            0.68571429, 0.68571429, 0.74285714, 0.74285714, 0.8       ,\n",
       "            0.8       , 0.82857143, 0.82857143, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99576203, 0.99509345, 0.99445404, 0.99110784,\n",
       "            0.99067786, 0.9899918 , 0.98958967, 0.98752085, 0.98749464,\n",
       "            0.97738309, 0.9766194 , 0.97298514, 0.97241362, 0.97129356,\n",
       "            0.97002771, 0.96697563, 0.96283498, 0.96009033, 0.95520963,\n",
       "            0.94864838, 0.94313079, 0.94090189, 0.89647994, 0.88544178,\n",
       "            0.85430195])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.14285714,\n",
       "            0.14285714, 0.2       , 0.2       , 0.25714286, 0.25714286,\n",
       "            0.6       , 0.6       , 0.62857143, 0.62857143, 0.65714286,\n",
       "            0.65714286, 0.77142857, 0.77142857, 0.82857143, 0.82857143,\n",
       "            0.85714286, 0.85714286, 0.91428571, 0.91428571, 0.94285714,\n",
       "            0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98911371, 0.98379351, 0.98273924, 0.98176937,\n",
       "            0.97985721, 0.97972149, 0.97955948, 0.97690116, 0.97676249,\n",
       "            0.9675663 , 0.96445928, 0.96281329, 0.96265039, 0.96259628,\n",
       "            0.96235817, 0.95788028, 0.95489477, 0.95210291, 0.9517737 ,\n",
       "            0.94875385, 0.94798521, 0.94223444, 0.93408462, 0.92955229,\n",
       "            0.91651195, 0.91102168])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.08571429, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.34285714, 0.34285714, 0.37142857, 0.37142857,\n",
       "            0.42857143, 0.42857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99619308, 0.99493518, 0.99373873, 0.99029245,\n",
       "            0.98991347, 0.98758229, 0.98538277, 0.98270793, 0.98208303,\n",
       "            0.97958644, 0.97402976, 0.96714697, 0.96518426, 0.96408605,\n",
       "            0.962331  , 0.96014194, 0.72472018])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.11428571, 0.11428571,\n",
       "            0.4       , 0.4       , 0.45714286, 0.45714286, 0.54285714,\n",
       "            0.54285714, 0.62857143, 0.62857143, 0.74285714, 0.74285714,\n",
       "            0.8       , 0.8       , 0.85714286, 0.85714286, 0.88571429,\n",
       "            0.88571429, 0.97142857, 0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9943478 , 0.99426136, 0.99080835, 0.99029068,\n",
       "            0.98379771, 0.98028003, 0.9785213 , 0.97792243, 0.97395179,\n",
       "            0.97082045, 0.96597348, 0.96562425, 0.95473855, 0.95350807,\n",
       "            0.95014561, 0.95014561, 0.94336003, 0.94014268, 0.93844167,\n",
       "            0.93337678, 0.92041078, 0.90985491, 0.89824061, 0.87451442])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.22857143, 0.25714286, 0.25714286, 0.28571429,\n",
       "            0.28571429, 0.37142857, 0.37142857, 0.62857143, 0.62857143,\n",
       "            0.71428571, 0.71428571, 0.77142857, 0.77142857, 0.85714286,\n",
       "            0.85714286, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9971318 , 0.99481354, 0.9946992 , 0.99457217,\n",
       "            0.99436348, 0.9913377 , 0.99131104, 0.99101928, 0.99057663,\n",
       "            0.99054711, 0.98958872, 0.98947123, 0.9861192 , 0.98553632,\n",
       "            0.98306592, 0.97996449, 0.97217772, 0.97119497, 0.96866582,\n",
       "            0.96325051, 0.95294203, 0.94182544, 0.88376809])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.25714286, 0.25714286, 0.48571429,\n",
       "            0.48571429, 0.51428571, 0.51428571, 0.57142857, 0.57142857,\n",
       "            0.65714286, 0.65714286, 0.74285714, 0.74285714, 0.77142857,\n",
       "            0.85714286, 0.85714286, 0.88571429, 0.88571429, 0.97142857,\n",
       "            0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99817257, 0.9961298 , 0.99571206, 0.99284806,\n",
       "            0.99249214, 0.99199515, 0.99159062, 0.99124375, 0.99105422,\n",
       "            0.99018283, 0.98980121, 0.98862782, 0.98852189, 0.98794856,\n",
       "            0.98388506, 0.9836833 , 0.98239003, 0.98120849, 0.97744734,\n",
       "            0.97655883, 0.97611082, 0.95996322])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.11428571, 0.11428571, 0.17142857,\n",
       "            0.17142857, 0.25714286, 0.25714286, 0.34285714, 0.37142857,\n",
       "            0.4       , 0.4       , 0.48571429, 0.48571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99777056, 0.99680873, 0.99674355, 0.99480603,\n",
       "            0.99465697, 0.99435561, 0.9942357 , 0.99397456, 0.99290029,\n",
       "            0.99179513, 0.99086494, 0.9894232 , 0.98809329, 0.9880688 ,\n",
       "            0.98728268, 0.98539983, 0.97699261, 0.97651996, 0.71085481])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.17142857, 0.17142857, 0.2       , 0.2       , 0.22857143,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.51428571, 0.51428571, 0.71428571, 0.71428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99762703, 0.99594206, 0.99372522, 0.9934612 ,\n",
       "            0.99248447, 0.99218879, 0.9919615 , 0.99189922, 0.99119054,\n",
       "            0.99068171, 0.99052733, 0.99027431, 0.98861718, 0.98806305,\n",
       "            0.98225645, 0.98178548, 0.97362003, 0.97279907, 0.92788592])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.14285714, 0.14285714, 0.2       ,\n",
       "            0.2       , 0.28571429, 0.28571429, 0.31428571, 0.31428571,\n",
       "            0.37142857, 0.37142857, 0.54285714, 0.54285714, 0.68571429,\n",
       "            0.68571429, 0.8       , 0.8       , 0.85714286, 0.88571429,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99662781, 0.99560324, 0.99492014, 0.99389359,\n",
       "            0.9938017 , 0.99275717, 0.9919861 , 0.99195713, 0.99064802,\n",
       "            0.9900635 , 0.98990432, 0.98637044, 0.9847416 , 0.982146  ,\n",
       "            0.98100095, 0.97467988, 0.97445388, 0.97018229, 0.96951165,\n",
       "            0.96166918, 0.92891279, 0.91748551])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.08571429, 0.4       ,\n",
       "            0.4       , 0.42857143, 0.42857143, 0.45714286, 0.45714286,\n",
       "            0.54285714, 0.54285714, 0.65714286, 0.65714286, 0.68571429,\n",
       "            0.68571429, 0.71428571, 0.71428571, 0.77142857, 0.77142857,\n",
       "            0.85714286, 0.85714286, 0.88571429, 0.88571429, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99681234, 0.99613023, 0.99549907, 0.99191881,\n",
       "            0.99119833, 0.99110516, 0.99101793, 0.99100871, 0.99053684,\n",
       "            0.98899492, 0.98892618, 0.98538855, 0.98351536, 0.98318444,\n",
       "            0.98295976, 0.98262399, 0.98243238, 0.98076529, 0.97995818,\n",
       "            0.97782328, 0.97281003, 0.96952011, 0.96710679, 0.94854827,\n",
       "            0.9446605 , 0.89520752])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.08571429, 0.4       ,\n",
       "            0.4       , 0.42857143, 0.42857143, 0.45714286, 0.45714286,\n",
       "            0.54285714, 0.54285714, 0.65714286, 0.65714286, 0.68571429,\n",
       "            0.68571429, 0.71428571, 0.71428571, 0.77142857, 0.77142857,\n",
       "            0.85714286, 0.85714286, 0.88571429, 0.88571429, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99681234, 0.99613023, 0.99549907, 0.99191881,\n",
       "            0.99119833, 0.99110516, 0.99101793, 0.99100871, 0.99053684,\n",
       "            0.98899492, 0.98892618, 0.98538855, 0.98351536, 0.98318444,\n",
       "            0.98295976, 0.98262399, 0.98243238, 0.98076529, 0.97995818,\n",
       "            0.97782328, 0.97281003, 0.96952011, 0.96710679, 0.94854827,\n",
       "            0.9446605 , 0.89520752])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.2       ,\n",
       "            0.2       , 0.22857143, 0.22857143, 0.25714286, 0.25714286,\n",
       "            0.28571429, 0.28571429, 0.48571429, 0.51428571, 0.57142857,\n",
       "            0.57142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99789967, 0.99783503, 0.99693278, 0.99457912,\n",
       "            0.99452999, 0.99339277, 0.9927858 , 0.99208913, 0.99124965,\n",
       "            0.99104719, 0.98984229, 0.98659305, 0.98650386, 0.98430143,\n",
       "            0.98425134, 0.98395951, 0.97096433, 0.96732501, 0.9624061 ,\n",
       "            0.96228764, 0.8711516 ])}}],\n",
       "  [{'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.14285714, 0.14285714, 0.22857143, 0.22857143, 0.28571429,\n",
       "            0.28571429, 0.31428571, 0.31428571, 0.45714286, 0.45714286,\n",
       "            0.6       , 0.6       , 0.65714286, 0.65714286, 0.85714286,\n",
       "            0.85714286, 0.91428571, 0.91428571, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.03144693, 0.01748642, 0.01670125, 0.01371251,\n",
       "            0.01142802, 0.01033315, 0.0083522 , 0.00799568, 0.00731684,\n",
       "            0.00719614, 0.00700989, 0.00694574, 0.00600261, 0.00577764,\n",
       "            0.0051993 , 0.00510061, 0.00479832, 0.00459547, 0.00341148,\n",
       "            0.00322312, 0.00312381, 0.00294189, 0.00252593, 0.00222775])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.54285714,\n",
       "            0.54285714, 0.6       , 0.6       , 0.82857143, 0.82857143,\n",
       "            0.94285714, 0.94285714, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.4       , 0.4       ,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.86666667, 0.86666667, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.02221934, 0.02104715, 0.018073  , 0.01553496,\n",
       "            0.01295485, 0.01245558, 0.0107971 , 0.00991749, 0.00770171,\n",
       "            0.00692931, 0.00671136, 0.00593408, 0.00467402, 0.00452254,\n",
       "            0.00381812, 0.00338186, 0.00236165, 0.00195929])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.17142857, 0.17142857,\n",
       "            0.37142857, 0.37142857, 0.45714286, 0.45714286, 0.62857143,\n",
       "            0.62857143, 0.68571429, 0.68571429, 0.74285714, 0.74285714,\n",
       "            0.88571429, 0.88571429, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.0345086 , 0.03112752, 0.01866256, 0.01494781,\n",
       "            0.01406757, 0.01226716, 0.01170439, 0.01100997, 0.0099552 ,\n",
       "            0.00765492, 0.00764804, 0.00692777, 0.00659077, 0.00585142,\n",
       "            0.00542666, 0.0047993 , 0.0045493 , 0.00433564, 0.00410913,\n",
       "            0.00339428, 0.0032178 , 0.00308416, 0.00253816, 0.00201392])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.17142857, 0.17142857, 0.28571429,\n",
       "            0.28571429, 0.31428571, 0.31428571, 0.4       , 0.4       ,\n",
       "            0.48571429, 0.48571429, 0.54285714, 0.54285714, 0.65714286,\n",
       "            0.65714286, 0.74285714, 0.74285714, 0.91428571, 0.91428571,\n",
       "            0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.07812269, 0.01912959, 0.01697946, 0.01350414,\n",
       "            0.01343129, 0.01337475, 0.01106411, 0.01015348, 0.01009819,\n",
       "            0.00747694, 0.00686074, 0.00597352, 0.0054617 , 0.00486042,\n",
       "            0.00469262, 0.00344296, 0.00323876, 0.0021285 , 0.00187497,\n",
       "            0.00182982, 0.00125499, 0.00079106])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.17142857, 0.17142857, 0.31428571, 0.31428571, 0.45714286,\n",
       "            0.45714286, 0.6       , 0.6       , 0.94285714, 0.94285714,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.09561238, 0.03525311, 0.0324655 , 0.01904799,\n",
       "            0.01016601, 0.00866716, 0.00601075, 0.00595778, 0.00416005,\n",
       "            0.00367018, 0.00264836, 0.00263776, 0.00086883, 0.00072538,\n",
       "            0.00046913, 0.0004591 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.28571429, 0.28571429, 0.37142857, 0.37142857,\n",
       "            0.4       , 0.4       , 0.42857143, 0.42857143, 0.45714286,\n",
       "            0.45714286, 0.51428571, 0.51428571, 0.54285714, 0.54285714,\n",
       "            0.85714286, 0.85714286, 0.94285714, 0.94285714, 0.97142857,\n",
       "            0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.11670462, 0.02460394, 0.02151233, 0.01744392,\n",
       "            0.01723158, 0.01320808, 0.01313049, 0.01228894, 0.00911756,\n",
       "            0.00884628, 0.0085632 , 0.00798771, 0.00790464, 0.00763747,\n",
       "            0.00623998, 0.00575116, 0.00552314, 0.00518117, 0.00511119,\n",
       "            0.0027764 , 0.00266474, 0.00194042, 0.00168533, 0.00153918,\n",
       "            0.00113669, 0.0009998 , 0.00073681])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.22857143, 0.22857143, 0.42857143, 0.42857143, 0.51428571,\n",
       "            0.51428571, 0.68571429, 0.68571429, 0.8       , 0.8       ,\n",
       "            0.97142857, 0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.33333333, 0.33333333, 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.07138777, 0.03146363, 0.02568959, 0.01901784,\n",
       "            0.01575999, 0.01543719, 0.01376953, 0.01369082, 0.01351791,\n",
       "            0.01012356, 0.00995988, 0.00665252, 0.00654815, 0.00510025,\n",
       "            0.0048749 , 0.00345589, 0.00345527, 0.00202397, 0.00198515,\n",
       "            0.00095246, 0.00092772, 0.00091805, 0.00078566])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.11428571, 0.11428571, 0.4       , 0.4       , 0.42857143,\n",
       "            0.42857143, 0.48571429, 0.48571429, 0.68571429, 0.68571429,\n",
       "            0.71428571, 0.71428571, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.06010755, 0.03028322, 0.02978159, 0.01559777,\n",
       "            0.01040974, 0.01010692, 0.00599218, 0.00591795, 0.00535302,\n",
       "            0.00473095, 0.00442643, 0.00415858, 0.00211307, 0.00205538,\n",
       "            0.00201752, 0.00194586, 0.00113576, 0.00071885, 0.00070092])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.2       , 0.2       ,\n",
       "            0.22857143, 0.22857143, 0.4       , 0.4       , 0.51428571,\n",
       "            0.51428571, 0.68571429, 0.68571429, 0.85714286, 0.85714286,\n",
       "            0.88571429, 0.88571429, 0.94285714, 0.94285714, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.73333333,\n",
       "            0.73333333, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.03578801, 0.0345513 , 0.0254676 , 0.02056117,\n",
       "            0.01832413, 0.01692944, 0.01633187, 0.01456936, 0.01215127,\n",
       "            0.01213058, 0.0104353 , 0.0094588 , 0.0094524 , 0.00771906,\n",
       "            0.00651117, 0.00518442, 0.0050277 , 0.00406426, 0.00353368,\n",
       "            0.00339881, 0.00298936, 0.00262176, 0.00253003, 0.00189247,\n",
       "            0.00179926, 0.00115294])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.22857143, 0.22857143, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.37142857, 0.4       , 0.4       , 0.45714286, 0.45714286,\n",
       "            0.51428571, 0.51428571, 0.62857143, 0.62857143, 0.85714286,\n",
       "            0.85714286, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.03730744, 0.03456899, 0.03376262, 0.02114181,\n",
       "            0.0120147 , 0.01112035, 0.00969051, 0.00957245, 0.0093859 ,\n",
       "            0.00930177, 0.00853097, 0.00826104, 0.00791357, 0.00729932,\n",
       "            0.00703744, 0.00543013, 0.00400708, 0.00366756, 0.00217437,\n",
       "            0.00211989, 0.00160981, 0.00158164, 0.00150202])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.31428571,\n",
       "            0.31428571, 0.37142857, 0.37142857, 0.45714286, 0.45714286,\n",
       "            0.91428571, 0.91428571, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.19692943, 0.09627491, 0.09455951, 0.08378496,\n",
       "            0.03602571, 0.03413443, 0.03097426, 0.02578525, 0.02243605,\n",
       "            0.02051875, 0.01904771, 0.01651091, 0.0109942 , 0.00850985,\n",
       "            0.00133707, 0.00116968, 0.00091633, 0.00077815, 0.00062681])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.14285714, 0.14285714,\n",
       "            0.31428571, 0.31428571, 0.37142857, 0.37142857, 0.62857143,\n",
       "            0.62857143, 0.68571429, 0.68571429, 0.94285714, 0.94285714,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.26906994, 0.24218675, 0.15642005, 0.14006408,\n",
       "            0.06809574, 0.05452299, 0.04546765, 0.03765733, 0.03184636,\n",
       "            0.02057827, 0.02043575, 0.01866483, 0.01715699, 0.00940977,\n",
       "            0.00877407, 0.008084  , 0.00781045, 0.00325734, 0.00274978,\n",
       "            0.00262075, 0.002109  , 0.00125373])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.14285714, 0.14285714, 0.22857143,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.42857143, 0.42857143,\n",
       "            0.45714286, 0.45714286, 0.57142857, 0.57142857, 0.6       ,\n",
       "            0.6       , 0.8       , 0.8       , 0.88571429, 0.88571429,\n",
       "            0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.33333333, 0.33333333, 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.07507946, 0.05048403, 0.04268012, 0.03399036,\n",
       "            0.02556974, 0.02489635, 0.02397298, 0.01598471, 0.01576951,\n",
       "            0.01467274, 0.01426583, 0.01224644, 0.01130958, 0.01094648,\n",
       "            0.00986987, 0.0047282 , 0.00468485, 0.00326511, 0.00237712,\n",
       "            0.00233695, 0.00220675, 0.00153036])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.34285714,\n",
       "            0.34285714, 0.54285714, 0.54285714, 0.6       , 0.6       ,\n",
       "            0.65714286, 0.65714286, 0.68571429, 0.68571429, 0.77142857,\n",
       "            0.77142857, 0.8       , 0.8       , 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.13902994, 0.12677646, 0.10512349, 0.0874624 ,\n",
       "            0.07465569, 0.07377392, 0.05300479, 0.04868162, 0.02048881,\n",
       "            0.02036031, 0.01151617, 0.01080935, 0.0066349 , 0.00663222,\n",
       "            0.00520271, 0.00503242, 0.00479003, 0.00380313, 0.00238907,\n",
       "            0.00219113, 0.00214647, 0.00209808, 0.00065649, 0.00065528])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.2       ,\n",
       "            0.2       , 0.28571429, 0.28571429, 0.34285714, 0.34285714,\n",
       "            0.45714286, 0.45714286, 0.57142857, 0.57142857, 0.77142857,\n",
       "            0.77142857, 0.88571429, 0.88571429, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.20052738, 0.11670549, 0.08397873, 0.04108883,\n",
       "            0.02494083, 0.01832492, 0.0157919 , 0.01439493, 0.01416671,\n",
       "            0.01033216, 0.00988558, 0.00842131, 0.00751029, 0.00365629,\n",
       "            0.00365536, 0.00246102, 0.00211967, 0.00077732, 0.00070898,\n",
       "            0.00051003])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.17142857, 0.17142857, 0.25714286, 0.25714286,\n",
       "            0.28571429, 0.28571429, 0.31428571, 0.31428571, 0.51428571,\n",
       "            0.51428571, 0.77142857, 0.77142857, 0.82857143, 0.82857143,\n",
       "            0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.1710883 , 0.1674573 , 0.14870735, 0.12973509,\n",
       "            0.07563042, 0.06694031, 0.06305263, 0.04957893, 0.04757105,\n",
       "            0.04683877, 0.03454238, 0.0315401 , 0.02681217, 0.01813024,\n",
       "            0.01770039, 0.00779428, 0.00515853, 0.00458557, 0.00408578,\n",
       "            0.0038744 , 0.00349722, 0.00109057])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.11428571, 0.11428571,\n",
       "            0.14285714, 0.14285714, 0.25714286, 0.25714286, 0.42857143,\n",
       "            0.42857143, 0.54285714, 0.54285714, 0.71428571, 0.71428571,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.42486252, 0.19702449, 0.07384998, 0.03961579,\n",
       "            0.03874951, 0.03284327, 0.02944401, 0.02675481, 0.01465814,\n",
       "            0.01339138, 0.01185626, 0.01172111, 0.00625364, 0.00617086,\n",
       "            0.0021655 , 0.00168761, 0.0016796 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.34285714, 0.34285714, 0.51428571, 0.51428571,\n",
       "            0.54285714, 0.54285714, 0.74285714, 0.74285714, 0.91428571,\n",
       "            0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.19290216, 0.1160235 , 0.09902971, 0.09695465,\n",
       "            0.07972179, 0.07309245, 0.06474917, 0.05679837, 0.05412551,\n",
       "            0.05359938, 0.04328498, 0.04019302, 0.01905744, 0.01773288,\n",
       "            0.01695361, 0.0135692 , 0.00897055, 0.00766814, 0.00574632,\n",
       "            0.00268444, 0.00201242])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.17142857, 0.17142857,\n",
       "            0.22857143, 0.22857143, 0.42857143, 0.42857143, 0.51428571,\n",
       "            0.51428571, 0.71428571, 0.71428571, 0.74285714, 0.74285714,\n",
       "            0.77142857, 0.77142857, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.1934156 , 0.09753993, 0.06865262, 0.05986177,\n",
       "            0.05465209, 0.04826859, 0.03097635, 0.02639692, 0.02242772,\n",
       "            0.02093687, 0.01568095, 0.01388156, 0.0137913 , 0.01030545,\n",
       "            0.01027136, 0.00901322, 0.00470276, 0.00437662, 0.00279911])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.2       , 0.2       , 0.34285714,\n",
       "            0.34285714, 0.54285714, 0.54285714, 0.57142857, 0.57142857,\n",
       "            0.77142857, 0.77142857, 0.85714286, 0.85714286, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.16955481, 0.12871866, 0.12386193, 0.12289064,\n",
       "            0.11947384, 0.06826267, 0.046038  , 0.03956249, 0.02902223,\n",
       "            0.02821302, 0.01633484, 0.01579295, 0.01544396, 0.01421178,\n",
       "            0.00970238, 0.00933554, 0.00716503, 0.00694399, 0.00452994,\n",
       "            0.00373992, 0.00304311])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.25714286, 0.25714286, 0.28571429,\n",
       "            0.28571429, 0.31428571, 0.31428571, 0.34285714, 0.34285714,\n",
       "            0.45714286, 0.45714286, 0.57142857, 0.57142857, 0.82857143,\n",
       "            0.82857143, 0.88571429, 0.88571429, 0.91428571, 0.91428571,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.25481517, 0.22060329, 0.16035714, 0.13692935,\n",
       "            0.08705408, 0.08384372, 0.06995335, 0.06799805, 0.05977696,\n",
       "            0.05648495, 0.05482728, 0.05136784, 0.04626444, 0.04381997,\n",
       "            0.03154577, 0.0310953 , 0.02894591, 0.02650115, 0.01542519,\n",
       "            0.01043011, 0.00878038, 0.00835806, 0.00706045, 0.00524044,\n",
       "            0.00396132])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.45714286, 0.45714286, 0.57142857, 0.57142857, 0.62857143,\n",
       "            0.62857143, 0.65714286, 0.65714286, 0.68571429, 0.68571429,\n",
       "            0.74285714, 0.74285714, 0.77142857, 0.77142857, 0.88571429,\n",
       "            0.88571429, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.35821614, 0.18617984, 0.11117655, 0.10484589,\n",
       "            0.03852037, 0.03270169, 0.02466316, 0.02348332, 0.02196541,\n",
       "            0.02027937, 0.01789578, 0.01555985, 0.01346857, 0.01275446,\n",
       "            0.00612838, 0.00596226, 0.00506344, 0.00497294, 0.002498  ,\n",
       "            0.00194684, 0.00160678, 0.00151837, 0.00090706])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.31428571, 0.31428571, 0.37142857,\n",
       "            0.37142857, 0.6       , 0.6       , 0.68571429, 0.68571429,\n",
       "            0.82857143, 0.82857143, 0.85714286, 0.85714286, 0.91428571,\n",
       "            0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.31008403, 0.22811267, 0.21009417, 0.17670711,\n",
       "            0.10410236, 0.08661249, 0.06156563, 0.05853663, 0.05607832,\n",
       "            0.04980793, 0.03084442, 0.02979533, 0.02634837, 0.01345393,\n",
       "            0.00905542, 0.00635959, 0.00577768, 0.00548977, 0.00513807,\n",
       "            0.00489309, 0.00290142])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.28571429,\n",
       "            0.28571429, 0.4       , 0.4       , 0.54285714, 0.54285714,\n",
       "            0.65714286, 0.65714286, 0.85714286, 0.85714286, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.26666667,\n",
       "            0.26666667, 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.17806335, 0.16647314, 0.10573176, 0.08817511,\n",
       "            0.08636608, 0.07768599, 0.07379381, 0.06159148, 0.04398127,\n",
       "            0.04296938, 0.03212497, 0.03144065, 0.02797015, 0.02732945,\n",
       "            0.01798362, 0.01672099, 0.01118196, 0.01117294, 0.00422851,\n",
       "            0.00408718])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.14285714, 0.14285714, 0.28571429, 0.28571429, 0.4       ,\n",
       "            0.4       , 0.48571429, 0.48571429, 0.57142857, 0.57142857,\n",
       "            0.88571429, 0.88571429, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.23813968, 0.13870134, 0.10121877, 0.09271653,\n",
       "            0.07162534, 0.06935452, 0.05261523, 0.05202855, 0.04947237,\n",
       "            0.04788424, 0.03888282, 0.03825695, 0.03453932, 0.03391515,\n",
       "            0.01567657, 0.0154143 , 0.00663216, 0.00653874])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.17142857, 0.17142857,\n",
       "            0.4       , 0.4       , 0.54285714, 0.54285714, 0.57142857,\n",
       "            0.57142857, 0.85714286, 0.85714286, 0.91428571, 0.91428571,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.35586824, 0.18373658, 0.16097626, 0.1534054 ,\n",
       "            0.13635255, 0.1288155 , 0.09172052, 0.09136513, 0.07735532,\n",
       "            0.05618965, 0.05375335, 0.03418749, 0.03347408, 0.03314035,\n",
       "            0.03233305, 0.01491143, 0.00892129, 0.00759952, 0.00694874,\n",
       "            0.00514255])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.11428571, 0.11428571,\n",
       "            0.34285714, 0.34285714, 0.4       , 0.4       , 0.42857143,\n",
       "            0.42857143, 0.48571429, 0.48571429, 0.71428571, 0.71428571,\n",
       "            0.74285714, 0.74285714, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.57705356, 0.35123297, 0.18568599, 0.18245149,\n",
       "            0.11287946, 0.09512814, 0.08664982, 0.0818674 , 0.07269188,\n",
       "            0.05887251, 0.05168012, 0.04808778, 0.01397782, 0.0133084 ,\n",
       "            0.01016921, 0.0099643 , 0.00300257, 0.00287358, 0.00109434])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.14285714,\n",
       "            0.14285714, 0.17142857, 0.17142857, 0.25714286, 0.25714286,\n",
       "            0.34285714, 0.34285714, 0.42857143, 0.42857143, 0.57142857,\n",
       "            0.57142857, 0.71428571, 0.71428571, 0.85714286, 0.85714286,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.53680125, 0.40371961, 0.34646485, 0.19165458,\n",
       "            0.16907461, 0.16751909, 0.15641347, 0.09933634, 0.06441592,\n",
       "            0.05387724, 0.05243663, 0.04511975, 0.02996702, 0.01752946,\n",
       "            0.01707901, 0.00946739, 0.00867688, 0.00520619, 0.00427715,\n",
       "            0.00191221, 0.00181922])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.22857143,\n",
       "            0.22857143, 0.54285714, 0.54285714, 0.82857143, 0.82857143,\n",
       "            0.88571429, 0.88571429, 0.94285714, 0.94285714, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.5201569 , 0.49805822, 0.27177154, 0.24204254,\n",
       "            0.19914478, 0.18401631, 0.16331254, 0.14594783, 0.11265086,\n",
       "            0.08926014, 0.03559645, 0.0354593 , 0.02339345, 0.0231252 ,\n",
       "            0.02036988, 0.01754992, 0.014005  , 0.01316193, 0.00610132,\n",
       "            0.00311181])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.34285714, 0.34285714, 0.54285714, 0.54285714,\n",
       "            0.65714286, 0.65714286, 0.68571429, 0.68571429, 0.8       ,\n",
       "            0.8       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.36396689, 0.23458472, 0.16593783, 0.15092015,\n",
       "            0.14393648, 0.13947528, 0.11571142, 0.09875133, 0.09688992,\n",
       "            0.09645482, 0.0683509 , 0.06789852, 0.05166421, 0.05154553,\n",
       "            0.04428265, 0.04423845, 0.04348089, 0.04305339, 0.03238561,\n",
       "            0.0310343 , 0.00983883])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.28571429, 0.28571429, 0.34285714, 0.34285714,\n",
       "            0.37142857, 0.37142857, 0.74285714, 0.74285714, 0.85714286,\n",
       "            0.85714286, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.65834276, 0.51647918, 0.16410145, 0.15616547,\n",
       "            0.13287835, 0.07686821, 0.07320439, 0.06499936, 0.06494949,\n",
       "            0.06235562, 0.05996179, 0.02012926, 0.01809676, 0.01264348,\n",
       "            0.01206698, 0.0061308 , 0.0029062 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.14285714,\n",
       "            0.14285714, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.34285714, 0.34285714, 0.37142857, 0.37142857, 0.4       ,\n",
       "            0.4       , 0.48571429, 0.48571429, 0.65714286, 0.65714286,\n",
       "            0.71428571, 0.71428571, 0.74285714, 0.74285714, 0.77142857,\n",
       "            0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.38717977, 0.29025802, 0.22290562, 0.17826585,\n",
       "            0.16183461, 0.12046304, 0.11555878, 0.109487  , 0.10235965,\n",
       "            0.09820077, 0.09578575, 0.08673529, 0.0826255 , 0.07971801,\n",
       "            0.07885996, 0.06884814, 0.06687288, 0.05072767, 0.0491007 ,\n",
       "            0.04048983, 0.04038178, 0.0389505 , 0.03763652, 0.030632  ,\n",
       "            0.0284043 , 0.00886716])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.11428571, 0.11428571,\n",
       "            0.14285714, 0.14285714, 0.2       , 0.2       , 0.28571429,\n",
       "            0.28571429, 0.37142857, 0.37142857, 0.62857143, 0.62857143,\n",
       "            0.74285714, 0.74285714, 0.8       , 0.8       , 0.82857143,\n",
       "            0.82857143, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.48487931, 0.33671965, 0.29216648, 0.25856657,\n",
       "            0.21904469, 0.19727442, 0.17510016, 0.17092053, 0.15335938,\n",
       "            0.14391808, 0.13337679, 0.12840406, 0.12571644, 0.11212376,\n",
       "            0.11062973, 0.09241326, 0.090678  , 0.06539235, 0.06453983,\n",
       "            0.05472225, 0.04656634, 0.04125993, 0.03759063, 0.03718807,\n",
       "            0.03498187, 0.03052654, 0.02410536, 0.01142339])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.14285714, 0.14285714, 0.22857143, 0.22857143, 0.28571429,\n",
       "            0.28571429, 0.31428571, 0.31428571, 0.51428571, 0.51428571,\n",
       "            0.6       , 0.6       , 0.74285714, 0.74285714, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.65063639, 0.46847857, 0.36418345, 0.23889878,\n",
       "            0.22170958, 0.22037822, 0.20831617, 0.1995066 , 0.16799722,\n",
       "            0.13069216, 0.12696015, 0.12183954, 0.08909701, 0.08285055,\n",
       "            0.07753464, 0.07275911, 0.03200799, 0.03043791, 0.00921775,\n",
       "            0.0079881 , 0.00512808])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.14285714, 0.14285714, 0.17142857, 0.17142857, 0.28571429,\n",
       "            0.28571429, 0.31428571, 0.31428571, 0.4       , 0.4       ,\n",
       "            0.88571429, 0.88571429, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.69542159, 0.53072694, 0.51876298, 0.42467999,\n",
       "            0.23824427, 0.20195724, 0.18936702, 0.16528235, 0.10993398,\n",
       "            0.10867887, 0.10789184, 0.10535565, 0.08734964, 0.08456128,\n",
       "            0.01868585, 0.01572079, 0.00969648, 0.0076016 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.4),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.22857143, 0.22857143,\n",
       "            0.37142857, 0.37142857, 0.45714286, 0.45714286, 0.57142857,\n",
       "            0.57142857, 0.62857143, 0.62857143, 0.65714286, 0.65714286,\n",
       "            0.82857143, 0.82857143, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.75363895, 0.52449835, 0.26883618, 0.2398475 ,\n",
       "            0.2127026 , 0.19688096, 0.14829513, 0.13974327, 0.1029688 ,\n",
       "            0.08588577, 0.06126536, 0.05922923, 0.04977059, 0.04895698,\n",
       "            0.02072592, 0.01148149, 0.00501512, 0.0048961 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.14285714, 0.62857143,\n",
       "            0.62857143, 0.65714286, 0.65714286, 0.71428571, 0.71428571,\n",
       "            0.8       , 0.8       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.4334603 , 0.40256168, 0.3920585 , 0.34486617,\n",
       "            0.33602881, 0.27152106, 0.23990771, 0.17423392, 0.08179622,\n",
       "            0.0808422 , 0.0774988 , 0.07636239, 0.06854335, 0.05512206,\n",
       "            0.04316206, 0.04176979, 0.01779733])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.25714286, 0.25714286, 0.28571429,\n",
       "            0.28571429, 0.4       , 0.4       , 0.91428571, 0.91428571,\n",
       "            0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.64199704, 0.48673382, 0.44781654, 0.41809915,\n",
       "            0.38739121, 0.36798931, 0.20515879, 0.17738839, 0.15969661,\n",
       "            0.13882452, 0.09027385, 0.07670649, 0.02783074, 0.01726255,\n",
       "            0.0169253 , 0.015648  , 0.00850123])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.4),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.25714286,\n",
       "            0.25714286, 0.34285714, 0.34285714, 0.4       , 0.4       ,\n",
       "            0.85714286, 0.85714286, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.73001369, 0.60066594, 0.53410687, 0.5105597 ,\n",
       "            0.48562052, 0.41464662, 0.39289367, 0.29840171, 0.13767726,\n",
       "            0.13164823, 0.11443828, 0.11231877, 0.1021083 , 0.0934661 ,\n",
       "            0.02287806, 0.02276958, 0.00994658, 0.00716932])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.3333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.2       , 0.2       , 0.48571429,\n",
       "            0.48571429, 0.51428571, 0.51428571, 0.71428571, 0.71428571,\n",
       "            0.77142857, 0.77142857, 0.85714286, 0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.6580991 , 0.52375037, 0.47402722, 0.45093874,\n",
       "            0.38123852, 0.34758549, 0.29870788, 0.26684131, 0.15660338,\n",
       "            0.15456251, 0.13417492, 0.13105443, 0.10316998, 0.09959798,\n",
       "            0.08519027, 0.0850309 , 0.05604989, 0.04097814, 0.0225462 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.3333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.11428571, 0.11428571,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.28571429, 0.28571429, 0.71428571, 0.71428571,\n",
       "            0.8       , 0.8       , 0.85714286, 0.85714286, 0.94285714,\n",
       "            0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.79195227, 0.76189693, 0.54836255, 0.50423773,\n",
       "            0.41008508, 0.34881109, 0.34454159, 0.22768548, 0.19360171,\n",
       "            0.19274237, 0.18394964, 0.175529  , 0.05072932, 0.0501715 ,\n",
       "            0.03206136, 0.0229402 , 0.01953782, 0.01774862, 0.00872745,\n",
       "            0.00631926, 0.00408266])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08571428571428572),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.17142857, 0.17142857, 0.28571429, 0.28571429,\n",
       "            0.31428571, 0.31428571, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.37142857, 0.54285714, 0.54285714, 0.82857143, 0.82857143,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.70583776, 0.60513747, 0.59698276, 0.53270109,\n",
       "            0.51086285, 0.42417825, 0.39610831, 0.28469741, 0.2369622 ,\n",
       "            0.229951  , 0.21105009, 0.19257059, 0.1883381 , 0.17048353,\n",
       "            0.1577117 , 0.14258006, 0.13643433, 0.05750236, 0.05501727,\n",
       "            0.04651527, 0.02571313, 0.01042681])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.17142857,\n",
       "            0.17142857, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.34285714, 0.34285714, 0.37142857, 0.37142857, 0.62857143,\n",
       "            0.62857143, 0.65714286, 0.65714286, 0.88571429, 0.88571429,\n",
       "            0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.69276465, 0.49702999, 0.48109852, 0.46941829,\n",
       "            0.44292124, 0.4170719 , 0.39058666, 0.38602027, 0.3499444 ,\n",
       "            0.34231445, 0.2976673 , 0.28853753, 0.27649499, 0.25230171,\n",
       "            0.2332094 , 0.22560566, 0.20196073, 0.18725985, 0.10178459,\n",
       "            0.0950914 , 0.09108867, 0.08996203, 0.06103876, 0.05874266,\n",
       "            0.05131419, 0.04452025, 0.00901459])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.22857143,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.34285714, 0.34285714, 0.71428571, 0.71428571, 0.91428571,\n",
       "            0.91428571, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.71146616, 0.63244631, 0.47908439, 0.36345295,\n",
       "            0.35380735, 0.33731119, 0.32111121, 0.31905098, 0.29292392,\n",
       "            0.24646294, 0.24570782, 0.1241957 , 0.10298118, 0.05409471,\n",
       "            0.0528397 , 0.04378718, 0.03932402, 0.03624513])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.2       , 0.2       , 0.34285714, 0.34285714, 0.54285714,\n",
       "            0.54285714, 0.74285714, 0.74285714, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.69965269, 0.63977436, 0.47140999, 0.41016912,\n",
       "            0.38735177, 0.34373732, 0.2213678 , 0.22005192, 0.15073448,\n",
       "            0.13797032, 0.09254493, 0.06760587, 0.04226461, 0.03327511,\n",
       "            0.01007093])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08571428571428572),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.11428571, 0.11428571,\n",
       "            0.14285714, 0.14285714, 0.25714286, 0.25714286, 0.42857143,\n",
       "            0.42857143, 0.51428571, 0.51428571, 0.71428571, 0.71428571,\n",
       "            0.74285714, 0.74285714, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.7365394 , 0.57524969, 0.49476161, 0.48505346,\n",
       "            0.47457085, 0.43359168, 0.36647724, 0.34288416, 0.26897015,\n",
       "            0.26588835, 0.23762682, 0.22601339, 0.13706489, 0.13643398,\n",
       "            0.13109153, 0.1310694 , 0.06143195, 0.04184729, 0.02360062])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08571428571428572),\n",
       "    'tpr': np.float64(0.4666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.14285714,\n",
       "            0.14285714, 0.17142857, 0.17142857, 0.25714286, 0.25714286,\n",
       "            0.4       , 0.4       , 0.62857143, 0.62857143, 0.71428571,\n",
       "            0.71428571, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.83390485, 0.67157444, 0.66112972, 0.65900187,\n",
       "            0.63570767, 0.59286518, 0.51815966, 0.50496321, 0.49064542,\n",
       "            0.4514948 , 0.45098981, 0.38509537, 0.34144607, 0.34000992,\n",
       "            0.24867911, 0.24140354, 0.18477789, 0.1817586 , 0.1398306 ,\n",
       "            0.11439413, 0.0472861 , 0.03876813, 0.02218252])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08571428571428572),\n",
       "    'tpr': np.float64(0.4),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.22857143, 0.22857143,\n",
       "            0.37142857, 0.37142857, 0.57142857, 0.57142857, 0.82857143,\n",
       "            0.82857143, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.66924638, 0.58374674, 0.56175543, 0.53085742,\n",
       "            0.52548762, 0.51514409, 0.50751604, 0.44360371, 0.42773097,\n",
       "            0.42439626, 0.41276136, 0.40165453, 0.34935462, 0.33798945,\n",
       "            0.31374817, 0.29648266, 0.21682803, 0.20642626, 0.13762134,\n",
       "            0.11957685, 0.08444488, 0.08206475, 0.03201365])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08571428571428572),\n",
       "    'tpr': np.float64(0.4),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.31428571, 0.31428571,\n",
       "            0.62857143, 0.62857143, 0.74285714, 0.74285714, 0.77142857,\n",
       "            0.77142857, 0.8       , 0.8       , 0.94285714, 0.94285714,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.46666667, 0.46666667, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.68140084, 0.59199163, 0.58180828, 0.52354317,\n",
       "            0.43600583, 0.40993333, 0.37448591, 0.34638854, 0.34567107,\n",
       "            0.21032799, 0.19781588, 0.15298717, 0.15252486, 0.15207473,\n",
       "            0.14080254, 0.14062929, 0.12412203, 0.07399986, 0.06485596,\n",
       "            0.02896459])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.4666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.54285714, 0.54285714, 0.6       , 0.6       ,\n",
       "            0.82857143, 0.82857143, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.74790676, 0.6339569 , 0.62069475, 0.59977206,\n",
       "            0.56888463, 0.46743388, 0.42976523, 0.42904659, 0.42455661,\n",
       "            0.41159678, 0.28450841, 0.27990855, 0.24882344, 0.20743199,\n",
       "            0.13513752, 0.10438466, 0.08903829, 0.08067044, 0.02377635])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.5333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.54285714,\n",
       "            0.54285714, 0.71428571, 0.71428571, 0.74285714, 0.74285714,\n",
       "            0.82857143, 0.82857143, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.76361483, 0.66345452, 0.64702712, 0.60000725,\n",
       "            0.56805591, 0.50624124, 0.48177181, 0.46906814, 0.29332297,\n",
       "            0.27845742, 0.21196829, 0.20513394, 0.19098706, 0.18373688,\n",
       "            0.1521557 , 0.11234411, 0.08755827, 0.0838597 , 0.03381365])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.5333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.54285714,\n",
       "            0.54285714, 0.71428571, 0.71428571, 0.74285714, 0.74285714,\n",
       "            0.82857143, 0.82857143, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.76361483, 0.66345452, 0.64702712, 0.60000725,\n",
       "            0.56805591, 0.50624124, 0.48177181, 0.46906814, 0.29332297,\n",
       "            0.27845742, 0.21196829, 0.20513394, 0.19098706, 0.18373688,\n",
       "            0.1521557 , 0.11234411, 0.08755827, 0.0838597 , 0.03381365])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2),\n",
       "    'tpr': np.float64(0.4666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.14285714, 0.2       ,\n",
       "            0.2       , 0.22857143, 0.22857143, 0.65714286, 0.65714286,\n",
       "            0.71428571, 0.71428571, 0.77142857, 0.77142857, 0.82857143,\n",
       "            0.82857143, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.7890926 , 0.77266625, 0.69795322, 0.66445217,\n",
       "            0.64814469, 0.60935434, 0.57813183, 0.5499147 , 0.52344091,\n",
       "            0.49441621, 0.48149929, 0.46911766, 0.26144369, 0.26087231,\n",
       "            0.21966494, 0.20794114, 0.18700512, 0.17983022, 0.15682709,\n",
       "            0.11661525, 0.08266025, 0.08151685, 0.04125626])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.22857142857142856),\n",
       "    'tpr': np.float64(0.5333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.08571429,\n",
       "            0.08571429, 0.22857143, 0.22857143, 0.25714286, 0.25714286,\n",
       "            0.42857143, 0.42857143, 0.68571429, 0.68571429, 0.71428571,\n",
       "            0.71428571, 0.82857143, 0.82857143, 0.85714286, 0.85714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.76856725, 0.76492813, 0.70419379, 0.65782985,\n",
       "            0.64774887, 0.54334253, 0.5324441 , 0.47611209, 0.47487591,\n",
       "            0.42553764, 0.38589321, 0.24636853, 0.24605577, 0.2420927 ,\n",
       "            0.23585642, 0.19125175, 0.14045867, 0.11580254, 0.10965801,\n",
       "            0.05936187])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.48571429,\n",
       "            0.48571429, 0.8       , 0.8       , 0.85714286, 0.85714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.46666667, 0.46666667, 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.8221492 , 0.76711163, 0.70923624, 0.63976572,\n",
       "            0.63466223, 0.59741153, 0.57674281, 0.52690207, 0.38310568,\n",
       "            0.38125643, 0.22600798, 0.16958974, 0.15374491, 0.14024551,\n",
       "            0.04596691])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.4),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.11428571, 0.11428571,\n",
       "            0.25714286, 0.25714286, 0.28571429, 0.28571429, 0.71428571,\n",
       "            0.71428571, 0.8       , 0.8       , 0.85714286, 0.85714286,\n",
       "            0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.88364499, 0.85004549, 0.82086776, 0.81545751,\n",
       "            0.79526303, 0.77356548, 0.71663798, 0.66428823, 0.65638255,\n",
       "            0.57653052, 0.56775894, 0.56575638, 0.56164055, 0.32182765,\n",
       "            0.31021687, 0.23491057, 0.19621944, 0.18387529, 0.1692858 ,\n",
       "            0.11232246, 0.08933208, 0.04793111])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.37142857142857144),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.25714286, 0.25714286,\n",
       "            0.28571429, 0.28571429, 0.4       , 0.4       , 0.74285714,\n",
       "            0.74285714, 0.77142857, 0.77142857, 0.91428571, 0.91428571,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.844369  , 0.79583842, 0.75043399, 0.69615274,\n",
       "            0.6714787 , 0.665467  , 0.61865539, 0.57732461, 0.56457641,\n",
       "            0.56034093, 0.54115099, 0.49632739, 0.49011862, 0.30362886,\n",
       "            0.26617484, 0.25472401, 0.24618563, 0.15113392, 0.14414467,\n",
       "            0.10989968, 0.08818085, 0.04910194])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.22857142857142856),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.17142857, 0.17142857, 0.22857143, 0.22857143, 0.42857143,\n",
       "            0.42857143, 0.71428571, 0.71428571, 0.77142857, 0.77142857,\n",
       "            0.88571429, 0.88571429, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9089099 , 0.7541067 , 0.73795826, 0.72802428,\n",
       "            0.59071736, 0.57809968, 0.52638484, 0.4946635 , 0.45787377,\n",
       "            0.43783147, 0.27070934, 0.25896427, 0.24225239, 0.23913519,\n",
       "            0.18009509, 0.17986737, 0.17983954, 0.14216977, 0.08075427])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2857142857142857),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.11428571, 0.11428571,\n",
       "            0.14285714, 0.14285714, 0.28571429, 0.28571429, 0.45714286,\n",
       "            0.45714286, 0.68571429, 0.68571429, 0.82857143, 0.82857143,\n",
       "            0.88571429, 0.88571429, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.85100541, 0.7944988 , 0.69094091, 0.63156307,\n",
       "            0.63133391, 0.62743555, 0.55321649, 0.50064576, 0.46807455,\n",
       "            0.4498465 , 0.32366805, 0.30817961, 0.25809209, 0.25670953,\n",
       "            0.2246149 , 0.21816145, 0.15662613, 0.10382133, 0.05514183])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5428571428571428),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.31428571, 0.31428571, 0.45714286,\n",
       "            0.45714286, 0.65714286, 0.65714286, 0.77142857, 0.77142857,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.84281946, 0.82476576, 0.7601939 , 0.71444372,\n",
       "            0.71040275, 0.69192281, 0.63876587, 0.61500558, 0.54197182,\n",
       "            0.51695632, 0.44857962, 0.41289438, 0.35753789, 0.26370464,\n",
       "            0.14614551, 0.11763876, 0.0746656 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.17142857, 0.17142857, 0.34285714, 0.34285714,\n",
       "            0.45714286, 0.45714286, 0.74285714, 0.74285714, 0.8       ,\n",
       "            0.8       , 0.85714286, 0.85714286, 0.88571429, 0.88571429,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.95690953, 0.87985738, 0.86608944, 0.84586321,\n",
       "            0.81346699, 0.78915724, 0.77338318, 0.72539003, 0.72442853,\n",
       "            0.64180043, 0.62220556, 0.35750957, 0.34962481, 0.34308404,\n",
       "            0.32115657, 0.29620991, 0.26563125, 0.24160901, 0.2190403 ,\n",
       "            0.147142  , 0.14237945, 0.04770229])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.31428571,\n",
       "            0.31428571, 0.34285714, 0.34285714, 0.4       , 0.4       ,\n",
       "            0.51428571, 0.51428571, 0.77142857, 0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.93270667, 0.91393772, 0.86306   , 0.83341772,\n",
       "            0.8294889 , 0.79322016, 0.71701166, 0.71590895, 0.69201216,\n",
       "            0.68520866, 0.67706054, 0.66385604, 0.63386542, 0.59000118,\n",
       "            0.54076994, 0.52606556, 0.39558948, 0.33737319, 0.18188706])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6571428571428571),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.22857143,\n",
       "            0.22857143, 0.68571429, 0.68571429, 0.74285714, 0.74285714,\n",
       "            0.82857143, 0.82857143, 0.85714286, 0.85714286, 0.91428571,\n",
       "            0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.92069708, 0.87042213, 0.80775309, 0.77564138,\n",
       "            0.77249832, 0.75935442, 0.75594962, 0.75552663, 0.72629923,\n",
       "            0.68602109, 0.49175445, 0.4734557 , 0.46660196, 0.46590525,\n",
       "            0.42195197, 0.39693743, 0.34004339, 0.21318392, 0.18050469,\n",
       "            0.17247828, 0.09725062])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6857142857142857),\n",
       "    'tpr': np.float64(0.7333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.11428571, 0.11428571, 0.17142857,\n",
       "            0.17142857, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.74285714, 0.74285714, 0.77142857, 0.77142857, 0.88571429,\n",
       "            0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.97024309, 0.91426574, 0.90999539, 0.89734092,\n",
       "            0.88699616, 0.87279218, 0.8571194 , 0.8519068 , 0.77724914,\n",
       "            0.76336524, 0.76310407, 0.74905591, 0.74006481, 0.72942568,\n",
       "            0.45869873, 0.41656093, 0.40648582, 0.36339639, 0.16300122,\n",
       "            0.15818668, 0.10663828])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.14285714, 0.14285714, 0.17142857, 0.17142857, 0.22857143,\n",
       "            0.22857143, 0.25714286, 0.25714286, 0.37142857, 0.37142857,\n",
       "            0.4       , 0.4       , 0.57142857, 0.57142857, 0.8       ,\n",
       "            0.8       , 0.85714286, 0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.90766504, 0.87643566, 0.87599088, 0.8628447 ,\n",
       "            0.80091078, 0.78567436, 0.75580816, 0.74749432, 0.72749782,\n",
       "            0.69149469, 0.69142604, 0.68195964, 0.65250281, 0.64196002,\n",
       "            0.62268262, 0.57694825, 0.51970371, 0.51958401, 0.45343567,\n",
       "            0.42618124, 0.40212402, 0.32920125, 0.22417011])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7428571428571429),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.45714286, 0.45714286, 0.6       ,\n",
       "            0.6       , 0.82857143, 0.82857143, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.97777095, 0.94719161, 0.91414113, 0.91352058,\n",
       "            0.91109435, 0.83349453, 0.62983797, 0.61555362, 0.58453361,\n",
       "            0.58110492, 0.39130807, 0.36673551, 0.24148917, 0.18395484,\n",
       "            0.06881794])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6857142857142857),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.17142857, 0.17142857, 0.54285714, 0.54285714,\n",
       "            0.62857143, 0.62857143, 0.71428571, 0.71428571, 0.77142857,\n",
       "            0.77142857, 0.8       , 0.8       , 0.91428571, 0.91428571,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.97917105, 0.94513564, 0.92275895, 0.9217375 ,\n",
       "            0.88731573, 0.85630882, 0.8515492 , 0.66314006, 0.66193826,\n",
       "            0.56032021, 0.56011054, 0.48320623, 0.48208965, 0.46300152,\n",
       "            0.42963438, 0.38535512, 0.34544257, 0.24704459, 0.22371208,\n",
       "            0.12662818])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7142857142857143),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.14285714, 0.14285714, 0.22857143, 0.22857143, 0.37142857,\n",
       "            0.37142857, 0.54285714, 0.54285714, 0.57142857, 0.57142857,\n",
       "            0.65714286, 0.65714286, 0.68571429, 0.68571429, 0.85714286,\n",
       "            0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9672605 , 0.94703802, 0.93047141, 0.91345168,\n",
       "            0.88691542, 0.87603213, 0.84562633, 0.84207832, 0.77876762,\n",
       "            0.77711558, 0.69494036, 0.67719152, 0.65874721, 0.65225256,\n",
       "            0.58188264, 0.56551145, 0.56447016, 0.51942061, 0.38740984,\n",
       "            0.38441185, 0.24378351])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7428571428571429),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.25714286,\n",
       "            0.25714286, 0.51428571, 0.51428571, 0.57142857, 0.57142857,\n",
       "            0.62857143, 0.62857143, 0.71428571, 0.71428571, 0.74285714,\n",
       "            0.74285714, 0.77142857, 0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96587058, 0.94039874, 0.93150511, 0.92774782,\n",
       "            0.92108554, 0.91264128, 0.90326122, 0.89852708, 0.84145814,\n",
       "            0.83988864, 0.767164  , 0.75993462, 0.71303274, 0.66782474,\n",
       "            0.6430911 , 0.61238416, 0.54509007, 0.5224966 , 0.5116963 ,\n",
       "            0.47128639, 0.45424424, 0.45071473, 0.12849992])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9428571428571428),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.14285714,\n",
       "            0.14285714, 0.2       , 0.2       , 0.45714286, 0.45714286,\n",
       "            0.65714286, 0.65714286, 0.94285714, 0.94285714, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.46666667, 0.46666667, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96144468, 0.94022703, 0.93923541, 0.92375749,\n",
       "            0.89467292, 0.8572101 , 0.82985089, 0.76771669, 0.76736798,\n",
       "            0.68714522, 0.67724945, 0.5379805 , 0.49488485, 0.42839294,\n",
       "            0.37786487, 0.20336692])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8571428571428571),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.11428571, 0.11428571, 0.22857143,\n",
       "            0.22857143, 0.37142857, 0.37142857, 0.51428571, 0.51428571,\n",
       "            0.82857143, 0.82857143, 0.94285714, 0.94285714, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.95852728, 0.9453112 , 0.94193137, 0.9338033 ,\n",
       "            0.91717029, 0.90381237, 0.89308424, 0.88762507, 0.86028865,\n",
       "            0.84885388, 0.81271378, 0.77157346, 0.74328048, 0.72787623,\n",
       "            0.55590755, 0.53427466, 0.41619158, 0.38806007, 0.31802377,\n",
       "            0.28973724, 0.22066532])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8571428571428571),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.22857143, 0.22857143,\n",
       "            0.25714286, 0.25714286, 0.4       , 0.4       , 0.62857143,\n",
       "            0.62857143, 0.65714286, 0.65714286, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.98893148, 0.97828618, 0.95235398, 0.94451722,\n",
       "            0.93850512, 0.92296593, 0.90757428, 0.86588017, 0.8612454 ,\n",
       "            0.85611705, 0.83802838, 0.77095115, 0.74207578, 0.64111581,\n",
       "            0.63983931, 0.62880744, 0.60318377, 0.44334231, 0.379421  ,\n",
       "            0.31223593])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8857142857142857),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.14285714, 0.14285714, 0.25714286,\n",
       "            0.25714286, 0.48571429, 0.48571429, 0.51428571, 0.51428571,\n",
       "            0.57142857, 0.57142857, 0.65714286, 0.65714286, 0.71428571,\n",
       "            0.71428571, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98597068, 0.9678589 , 0.96436521, 0.9549012 ,\n",
       "            0.95365759, 0.94823676, 0.94122233, 0.93895097, 0.92798677,\n",
       "            0.9154861 , 0.87051812, 0.85389796, 0.8482965 , 0.84820541,\n",
       "            0.84436786, 0.80177201, 0.7081859 , 0.67393658, 0.65700605,\n",
       "            0.64157066, 0.49545602, 0.43745625, 0.15526294])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9142857142857143),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.31428571,\n",
       "            0.31428571, 0.45714286, 0.45714286, 0.62857143, 0.62857143,\n",
       "            0.65714286, 0.65714286, 0.77142857, 0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.73333333, 0.73333333, 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.97137054, 0.95433679, 0.94851148, 0.9451763 ,\n",
       "            0.92836092, 0.92606038, 0.92277045, 0.91870715, 0.87832378,\n",
       "            0.87131162, 0.83787524, 0.82371808, 0.73641688, 0.72996505,\n",
       "            0.7272546 , 0.71350884, 0.63927331, 0.63743015, 0.35536588])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.25714286, 0.25714286, 0.34285714, 0.34285714,\n",
       "            0.37142857, 0.37142857, 0.4       , 0.4       , 0.8       ,\n",
       "            0.8       , 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99660708, 0.99251845, 0.9902895 , 0.98703815,\n",
       "            0.98363918, 0.98218249, 0.97549347, 0.97165701, 0.96514313,\n",
       "            0.95581228, 0.93939413, 0.9384491 , 0.92249556, 0.92128105,\n",
       "            0.91237835, 0.91007816, 0.84107646, 0.84060432, 0.60261997,\n",
       "            0.48488129, 0.38881148, 0.3664565 , 0.17835255])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9714285714285714),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.22857143, 0.22857143,\n",
       "            0.25714286, 0.25714286, 0.48571429, 0.48571429, 0.57142857,\n",
       "            0.57142857, 0.68571429, 0.68571429, 0.85714286, 0.85714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.98296152, 0.97691729, 0.97260314, 0.97221901,\n",
       "            0.95748092, 0.93178621, 0.92147236, 0.89795994, 0.89204072,\n",
       "            0.88631101, 0.88013389, 0.8574532 , 0.84993594, 0.83275665,\n",
       "            0.82892191, 0.80699119, 0.74839131, 0.67179981, 0.66333848,\n",
       "            0.34454179])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8285714285714286),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.08571429, 0.08571429, 0.17142857,\n",
       "            0.17142857, 0.22857143, 0.22857143, 0.25714286, 0.25714286,\n",
       "            0.31428571, 0.31428571, 0.71428571, 0.71428571, 0.82857143,\n",
       "            0.82857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99740142, 0.99388931, 0.99203328, 0.99111722,\n",
       "            0.99074673, 0.98707679, 0.98662154, 0.98592211, 0.9732461 ,\n",
       "            0.97322596, 0.94977868, 0.94954534, 0.94408071, 0.94305795,\n",
       "            0.92480488, 0.91146756, 0.76821291, 0.76440903, 0.6048173 ,\n",
       "            0.5845893 , 0.31598851])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.31428571,\n",
       "            0.31428571, 0.4       , 0.4       , 0.51428571, 0.51428571,\n",
       "            0.65714286, 0.65714286, 0.71428571, 0.71428571, 0.82857143,\n",
       "            0.82857143, 0.85714286, 0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.53333333, 0.53333333, 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98972805, 0.98324723, 0.97887092, 0.9433758 ,\n",
       "            0.94288476, 0.93910162, 0.92645697, 0.90813973, 0.90011163,\n",
       "            0.86771524, 0.85104077, 0.80936633, 0.79195761, 0.72294506,\n",
       "            0.6917989 , 0.68792732, 0.64803364, 0.5163344 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9714285714285714),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.37142857, 0.37142857, 0.6       , 0.6       , 0.65714286,\n",
       "            0.65714286, 0.94285714, 0.94285714, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99591791, 0.98287934, 0.96260762, 0.95959681,\n",
       "            0.92423133, 0.90169349, 0.84627286, 0.84538779, 0.8106433 ,\n",
       "            0.78606806, 0.68477823, 0.6606508 , 0.65597829, 0.58375309,\n",
       "            0.24969939])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.22857143, 0.22857143,\n",
       "            0.4       , 0.4       , 0.54285714, 0.54285714, 0.71428571,\n",
       "            0.71428571, 0.74285714, 0.74285714, 0.82857143, 0.82857143,\n",
       "            0.88571429, 0.88571429, 0.91428571, 0.91428571, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99031329, 0.98701166, 0.97067829, 0.97051863,\n",
       "            0.9455782 , 0.94034052, 0.93215795, 0.92877552, 0.91479065,\n",
       "            0.90802125, 0.89022626, 0.87716256, 0.86096397, 0.83659457,\n",
       "            0.80673652, 0.79115329, 0.7901508 , 0.71839449, 0.62588553,\n",
       "            0.53138802])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.11428571, 0.11428571,\n",
       "            0.14285714, 0.14285714, 0.2       , 0.2       , 0.25714286,\n",
       "            0.25714286, 0.28571429, 0.28571429, 0.48571429, 0.48571429,\n",
       "            0.65714286, 0.65714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99023709, 0.98063696, 0.97331562, 0.97114171,\n",
       "            0.9637185 , 0.96223679, 0.95797545, 0.94960932, 0.94760211,\n",
       "            0.94293254, 0.93844411, 0.93142958, 0.9033738 , 0.88782932,\n",
       "            0.84189689, 0.83304539, 0.54138553])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.17142857,\n",
       "            0.17142857, 0.22857143, 0.22857143, 0.37142857, 0.37142857,\n",
       "            0.54285714, 0.54285714, 0.57142857, 0.57142857, 0.94285714,\n",
       "            0.94285714, 0.97142857, 0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99231621, 0.98865282, 0.97817059, 0.96530687,\n",
       "            0.96198163, 0.95520703, 0.94368591, 0.90878605, 0.9074469 ,\n",
       "            0.8935846 , 0.89298159, 0.89122716, 0.87200293, 0.73637266,\n",
       "            0.71512964, 0.69587914, 0.63979842, 0.56514453, 0.54091177])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9714285714285714),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.28571429, 0.28571429, 0.31428571,\n",
       "            0.31428571, 0.51428571, 0.51428571, 0.71428571, 0.71428571,\n",
       "            0.77142857, 0.77142857, 0.85714286, 0.85714286, 0.88571429,\n",
       "            0.88571429, 0.91428571, 0.91428571, 0.94285714, 0.94285714,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.6       ,\n",
       "            0.6       , 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99648685, 0.9948656 , 0.99471935, 0.99292983,\n",
       "            0.99114556, 0.99056139, 0.98250292, 0.98050365, 0.98040289,\n",
       "            0.97650831, 0.95307153, 0.94821648, 0.91032324, 0.90260567,\n",
       "            0.84960741, 0.83561411, 0.77847279, 0.75313017, 0.72280377,\n",
       "            0.71926629, 0.71117731, 0.66506207, 0.65592841, 0.60488958,\n",
       "            0.33738506])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.28571429, 0.28571429, 0.34285714, 0.34285714,\n",
       "            0.45714286, 0.45714286, 0.57142857, 0.57142857, 0.65714286,\n",
       "            0.65714286, 0.77142857, 0.77142857, 0.8       , 0.8       ,\n",
       "            0.88571429, 0.88571429, 0.91428571, 0.91428571, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99552355, 0.9859633 , 0.98272438, 0.98109757,\n",
       "            0.97121465, 0.97022786, 0.96778813, 0.96184756, 0.96069156,\n",
       "            0.94458277, 0.94365424, 0.92305874, 0.92198862, 0.91408878,\n",
       "            0.91160257, 0.89187754, 0.88879118, 0.88604326, 0.88489304,\n",
       "            0.84136907, 0.83576675, 0.81836376, 0.78466675, 0.67871184,\n",
       "            0.61792107, 0.55720501])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.2       , 0.2       ,\n",
       "            0.22857143, 0.22857143, 0.42857143, 0.42857143, 0.57142857,\n",
       "            0.57142857, 0.6       , 0.6       , 0.71428571, 0.71428571,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.4       , 0.4       , 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99746786, 0.99265354, 0.98062165, 0.97932265,\n",
       "            0.9741456 , 0.97257013, 0.94483084, 0.94292986, 0.90625874,\n",
       "            0.90591442, 0.90015458, 0.89720205, 0.85457962, 0.84505703,\n",
       "            0.64389665, 0.63067755, 0.62354764])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.37142857, 0.37142857, 0.74285714,\n",
       "            0.74285714, 0.82857143, 0.82857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.53333333, 0.53333333, 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99413157, 0.98374496, 0.98040162, 0.97703993,\n",
       "            0.97129217, 0.96969291, 0.96254475, 0.95107373, 0.90394855,\n",
       "            0.89988335, 0.88888156, 0.8143707 , 0.53432199])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.14285714, 0.14285714, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.37142857, 0.4       , 0.4       , 0.57142857, 0.57142857,\n",
       "            0.65714286, 0.65714286, 0.88571429, 0.88571429, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99455944, 0.98891138, 0.98442664, 0.9838998 ,\n",
       "            0.97374288, 0.97331093, 0.95721738, 0.95592356, 0.95214882,\n",
       "            0.94812009, 0.9434437 , 0.94248192, 0.93358426, 0.93127766,\n",
       "            0.92782585, 0.92673497, 0.8840564 , 0.87442257, 0.80191028,\n",
       "            0.77892286])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.22857143,\n",
       "            0.22857143, 0.28571429, 0.28571429, 0.48571429, 0.48571429,\n",
       "            0.51428571, 0.51428571, 0.6       , 0.6       , 0.65714286,\n",
       "            0.65714286, 0.71428571, 0.71428571, 0.85714286, 0.85714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99666166, 0.99124178, 0.99116421, 0.9812789 ,\n",
       "            0.98056777, 0.97904482, 0.97805473, 0.97059181, 0.97052096,\n",
       "            0.97038588, 0.96970036, 0.96373572, 0.95440491, 0.95381913,\n",
       "            0.94613143, 0.9420528 , 0.94115278, 0.91184616, 0.91119364,\n",
       "            0.71035838])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.11428571, 0.11428571, 0.2       ,\n",
       "            0.2       , 0.22857143, 0.22857143, 0.37142857, 0.37142857,\n",
       "            0.4       , 0.4       , 0.54285714, 0.54285714, 0.71428571,\n",
       "            0.71428571, 0.74285714, 0.74285714, 0.88571429, 0.88571429,\n",
       "            0.91428571, 0.91428571, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99577188, 0.99071121, 0.99042897, 0.98924627,\n",
       "            0.98875902, 0.98789343, 0.98093611, 0.97494014, 0.97126819,\n",
       "            0.96921791, 0.96580404, 0.95241882, 0.94979324, 0.91945557,\n",
       "            0.89291997, 0.89120475, 0.88236986, 0.85103541, 0.84982852,\n",
       "            0.82873292, 0.82460541, 0.73829045, 0.6905466 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.22857143, 0.22857143, 0.48571429,\n",
       "            0.48571429, 0.68571429, 0.68571429, 0.82857143, 0.82857143,\n",
       "            0.85714286, 0.85714286, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.4       ,\n",
       "            0.4       , 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99654361, 0.99571336, 0.99537063, 0.99260765,\n",
       "            0.99202671, 0.99101471, 0.98212839, 0.98124065, 0.96571118,\n",
       "            0.96317404, 0.94419625, 0.94391109, 0.91720438, 0.91149863,\n",
       "            0.89461379, 0.88607361, 0.8507753 , 0.76909255, 0.7500909 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.25714286, 0.25714286, 0.31428571, 0.31428571,\n",
       "            0.4       , 0.4       , 0.42857143, 0.42857143, 0.51428571,\n",
       "            0.51428571, 0.54285714, 0.54285714, 0.8       , 0.8       ,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9975574 , 0.99091723, 0.99060107, 0.99037031,\n",
       "            0.98984529, 0.98577566, 0.98564663, 0.98350994, 0.97800175,\n",
       "            0.97205899, 0.97156706, 0.9709982 , 0.96910592, 0.9659029 ,\n",
       "            0.96470641, 0.96454697, 0.96353667, 0.94827967, 0.94528604,\n",
       "            0.89779195, 0.89422388])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.14285714,\n",
       "            0.14285714, 0.17142857, 0.17142857, 0.2       , 0.2       ,\n",
       "            0.28571429, 0.28571429, 0.31428571, 0.31428571, 0.71428571,\n",
       "            0.71428571, 0.91428571, 0.91428571, 0.94285714, 0.94285714,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99702766, 0.99274334, 0.99242771, 0.99110249,\n",
       "            0.99081458, 0.98967918, 0.98940982, 0.98770219, 0.98619808,\n",
       "            0.98331287, 0.98033773, 0.97697051, 0.97624705, 0.94124888,\n",
       "            0.93985512, 0.89068921, 0.876871  , 0.87268162, 0.86883297,\n",
       "            0.76935   , 0.76160778])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.17142857, 0.17142857, 0.42857143, 0.42857143,\n",
       "            0.51428571, 0.51428571, 0.6       , 0.6       , 0.62857143,\n",
       "            0.62857143, 0.68571429, 0.68571429, 0.77142857, 0.77142857,\n",
       "            0.8       , 0.8       , 0.88571429, 0.88571429, 0.97142857,\n",
       "            0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99618528, 0.99225249, 0.9921148 , 0.99083899,\n",
       "            0.9904227 , 0.99000158, 0.98892104, 0.98179418, 0.9817854 ,\n",
       "            0.97792719, 0.9777485 , 0.9716965 , 0.97091441, 0.96980526,\n",
       "            0.96905179, 0.96472329, 0.96240492, 0.95628495, 0.95614484,\n",
       "            0.95594173, 0.95407257, 0.93625071, 0.93365085, 0.92395648,\n",
       "            0.91213123, 0.90664105, 0.79032258])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.2       , 0.2       ,\n",
       "            0.54285714, 0.54285714, 0.57142857, 0.57142857, 0.74285714,\n",
       "            0.74285714, 0.82857143, 0.82857143, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.33333333, 0.33333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99752739, 0.99747168, 0.99707118, 0.99695827,\n",
       "            0.9961815 , 0.99532036, 0.99252932, 0.99104939, 0.99081885,\n",
       "            0.97706073, 0.97692681, 0.97617244, 0.97615704, 0.96866706,\n",
       "            0.96445434, 0.95399961, 0.94494001, 0.91223595, 0.87855081])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.28571429, 0.28571429, 0.6       ,\n",
       "            0.6       , 0.94285714, 0.94285714, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.2       , 0.2       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.9984617 , 0.99740691, 0.9967838 , 0.99545889,\n",
       "            0.99488889, 0.994856  , 0.991801  , 0.99178216, 0.97530076,\n",
       "            0.97452777, 0.92852798, 0.88198971, 0.88187107, 0.86772079,\n",
       "            0.85914897])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.17142857, 0.17142857, 0.2       , 0.2       , 0.34285714,\n",
       "            0.34285714, 0.37142857, 0.37142857, 0.45714286, 0.45714286,\n",
       "            0.62857143, 0.62857143, 0.8       , 0.8       , 0.91428571,\n",
       "            0.91428571, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99791102, 0.99768153, 0.99611819, 0.99606524,\n",
       "            0.99390257, 0.99320298, 0.99180711, 0.99116836, 0.9878274 ,\n",
       "            0.98697402, 0.98598482, 0.98537424, 0.98386906, 0.98292482,\n",
       "            0.97650502, 0.9763341 , 0.9728505 , 0.97189375, 0.96457226,\n",
       "            0.95608879, 0.94960627, 0.9404865 , 0.93870952])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.31428571, 0.31428571, 0.54285714, 0.54285714, 0.62857143,\n",
       "            0.62857143, 0.71428571, 0.71428571, 0.77142857, 0.77142857,\n",
       "            0.85714286, 0.85714286, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99803266, 0.99704683, 0.99596688, 0.99389157,\n",
       "            0.99244784, 0.99109206, 0.99106839, 0.99012489, 0.98980732,\n",
       "            0.98876011, 0.9879289 , 0.98214607, 0.98145889, 0.97652162,\n",
       "            0.97633259, 0.9731843 , 0.96931432, 0.96643141, 0.96576267,\n",
       "            0.96103892, 0.95880752, 0.93925821, 0.92927407, 0.89447586])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.2       , 0.2       ,\n",
       "            0.22857143, 0.22857143, 0.37142857, 0.37142857, 0.45714286,\n",
       "            0.45714286, 0.51428571, 0.51428571, 0.54285714, 0.54285714,\n",
       "            0.71428571, 0.71428571, 0.82857143, 0.82857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99873809, 0.99761455, 0.99484586, 0.99474084,\n",
       "            0.99450704, 0.99448955, 0.99086417, 0.99061196, 0.9895498 ,\n",
       "            0.98910434, 0.98738275, 0.98722056, 0.98490967, 0.9823534 ,\n",
       "            0.97039354, 0.96894306, 0.95090526, 0.933859  , 0.78226647])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.34285714, 0.34285714, 0.42857143,\n",
       "            0.42857143, 0.48571429, 0.48571429, 0.51428571, 0.51428571,\n",
       "            0.57142857, 0.57142857, 0.6       , 0.6       , 0.71428571,\n",
       "            0.71428571, 0.74285714, 0.74285714, 0.77142857, 0.77142857,\n",
       "            0.85714286, 0.85714286, 0.88571429, 0.88571429, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99753101, 0.99279875, 0.99261471, 0.99199163,\n",
       "            0.9919376 , 0.99179537, 0.99090701, 0.98994158, 0.98930553,\n",
       "            0.98867464, 0.98855421, 0.98665316, 0.98609834, 0.98452557,\n",
       "            0.98367203, 0.98238729, 0.98187228, 0.98080748, 0.98046486,\n",
       "            0.97650087, 0.97537285, 0.97431027, 0.97317532, 0.9644709 ,\n",
       "            0.96236494, 0.9536154 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.11428571, 0.11428571, 0.14285714,\n",
       "            0.14285714, 0.17142857, 0.17142857, 0.25714286, 0.25714286,\n",
       "            0.45714286, 0.45714286, 0.48571429, 0.48571429, 0.54285714,\n",
       "            0.54285714, 0.65714286, 0.65714286, 0.85714286, 0.85714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.46666667, 0.46666667, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99789245, 0.99502524, 0.99461066, 0.99447362,\n",
       "            0.99441665, 0.99436089, 0.99330228, 0.99253496, 0.99074421,\n",
       "            0.98913591, 0.9883013 , 0.98809486, 0.98775875, 0.98697578,\n",
       "            0.98666454, 0.98029775, 0.97811931, 0.96443538, 0.96380398,\n",
       "            0.92002057])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.25714286, 0.25714286,\n",
       "            0.54285714, 0.54285714, 0.6       , 0.6       , 0.65714286,\n",
       "            0.65714286, 0.74285714, 0.74285714, 0.77142857, 0.77142857,\n",
       "            0.82857143, 0.82857143, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99782954, 0.99752382, 0.9965736 , 0.99614202,\n",
       "            0.99597651, 0.99576284, 0.99534486, 0.99182701, 0.99116216,\n",
       "            0.98736746, 0.98628   , 0.98167329, 0.98122614, 0.97896808,\n",
       "            0.97895299, 0.97809942, 0.97755999, 0.97673941, 0.97030363,\n",
       "            0.95940508, 0.95823332, 0.95222644, 0.94229071, 0.89643157])}}],\n",
       "  [{'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.42857143, 0.42857143, 0.48571429,\n",
       "            0.48571429, 0.57142857, 0.57142857, 0.6       , 0.62857143,\n",
       "            0.62857143, 0.65714286, 0.65714286, 0.77142857, 0.77142857,\n",
       "            0.82857143, 0.82857143, 0.85714286, 0.85714286, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.05223184, 0.00822761, 0.00725029, 0.00647037,\n",
       "            0.0063709 , 0.00595906, 0.00482857, 0.00476367, 0.00471152,\n",
       "            0.00462221, 0.00457595, 0.00399485, 0.00353087, 0.00343331,\n",
       "            0.00282794, 0.00263389, 0.00261388, 0.00241526, 0.00167802,\n",
       "            0.00073558])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.17142857,\n",
       "            0.17142857, 0.42857143, 0.45714286, 0.45714286, 0.48571429,\n",
       "            0.48571429, 0.62857143, 0.62857143, 0.68571429, 0.68571429,\n",
       "            0.71428571, 0.71428571, 0.91428571, 0.91428571, 0.94285714,\n",
       "            0.94285714, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.0564812 , 0.03856408, 0.03512174, 0.02224775,\n",
       "            0.02043324, 0.00865126, 0.00808754, 0.00800274, 0.00770089,\n",
       "            0.00745987, 0.00584858, 0.00508339, 0.00480313, 0.0045147 ,\n",
       "            0.00436257, 0.00430794, 0.00365707, 0.00362791, 0.0035634 ,\n",
       "            0.0034002 , 0.00316077, 0.00201198, 0.00173147])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.2       , 0.2       , 0.31428571, 0.31428571,\n",
       "            0.37142857, 0.37142857, 0.48571429, 0.48571429, 0.51428571,\n",
       "            0.62857143, 0.62857143, 0.8       , 0.8       , 0.82857143,\n",
       "            0.82857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.02361511, 0.01749895, 0.01203298, 0.01085815,\n",
       "            0.00890925, 0.00796382, 0.00777603, 0.00729654, 0.00713322,\n",
       "            0.0071185 , 0.00693314, 0.00600906, 0.00584659, 0.0055353 ,\n",
       "            0.0046556 , 0.00428758, 0.00327649, 0.00321613, 0.00298745,\n",
       "            0.00248117, 0.00156024])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.22857143, 0.22857143, 0.28571429,\n",
       "            0.28571429, 0.37142857, 0.37142857, 0.42857143, 0.42857143,\n",
       "            0.45714286, 0.45714286, 0.51428571, 0.54285714, 0.54285714,\n",
       "            0.57142857, 0.57142857, 0.6       , 0.6       , 0.71428571,\n",
       "            0.71428571, 0.91428571, 0.91428571, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.03216554, 0.01734605, 0.01632111, 0.01615553,\n",
       "            0.01509075, 0.01031346, 0.00997307, 0.00855523, 0.00848729,\n",
       "            0.00816882, 0.00797694, 0.00742975, 0.0074285 , 0.00632467,\n",
       "            0.0058043 , 0.00576919, 0.00559175, 0.00494498, 0.00434579,\n",
       "            0.00386335, 0.00254476, 0.00250632, 0.00242516, 0.00238883,\n",
       "            0.00230354])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.42857143, 0.42857143, 0.48571429,\n",
       "            0.48571429, 0.57142857, 0.57142857, 0.6       , 0.62857143,\n",
       "            0.62857143, 0.65714286, 0.65714286, 0.77142857, 0.77142857,\n",
       "            0.82857143, 0.82857143, 0.85714286, 0.85714286, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.05223184, 0.00822761, 0.00725029, 0.00647037,\n",
       "            0.0063709 , 0.00595906, 0.00482857, 0.00476367, 0.00471152,\n",
       "            0.00462221, 0.00457595, 0.00399485, 0.00353087, 0.00343331,\n",
       "            0.00282794, 0.00263389, 0.00261388, 0.00241526, 0.00167802,\n",
       "            0.00073558])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.11428571, 0.11428571, 0.22857143,\n",
       "            0.22857143, 0.34285714, 0.34285714, 0.54285714, 0.54285714,\n",
       "            0.57142857, 0.6       , 0.68571429, 0.68571429, 0.8       ,\n",
       "            0.8       , 0.91428571, 0.91428571, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.05265653, 0.02205959, 0.02064904, 0.01432439,\n",
       "            0.01385304, 0.01104084, 0.00995608, 0.00589696, 0.00542209,\n",
       "            0.00529595, 0.005111  , 0.00463962, 0.00429635, 0.0035158 ,\n",
       "            0.00348428, 0.00251633, 0.00187943, 0.00168854, 0.00064214])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.14285714, 0.14285714, 0.42857143, 0.42857143, 0.54285714,\n",
       "            0.54285714, 0.74285714, 0.74285714, 0.77142857, 0.8       ,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.11125562, 0.01572439, 0.01444106, 0.01251996,\n",
       "            0.00822813, 0.00717586, 0.003731  , 0.00336346, 0.0029308 ,\n",
       "            0.00287378, 0.00193769, 0.00180039, 0.00167044, 0.00163273,\n",
       "            0.00128405, 0.00084761, 0.00063613])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.14285714, 0.14285714, 0.42857143, 0.42857143, 0.54285714,\n",
       "            0.54285714, 0.74285714, 0.74285714, 0.77142857, 0.8       ,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.11125562, 0.01572439, 0.01444106, 0.01251996,\n",
       "            0.00822813, 0.00717586, 0.003731  , 0.00336346, 0.0029308 ,\n",
       "            0.00287378, 0.00193769, 0.00180039, 0.00167044, 0.00163273,\n",
       "            0.00128405, 0.00084761, 0.00063613])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.25714286, 0.25714286,\n",
       "            0.4       , 0.4       , 0.48571429, 0.48571429, 0.54285714,\n",
       "            0.57142857, 0.68571429, 0.68571429, 0.71428571, 0.71428571,\n",
       "            0.77142857, 0.77142857, 0.97142857, 0.97142857, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.09373791, 0.041874  , 0.01430048, 0.01230506,\n",
       "            0.00775542, 0.00768948, 0.00603611, 0.00512432, 0.00466094,\n",
       "            0.00452623, 0.0038778 , 0.00383362, 0.00367096, 0.00300136,\n",
       "            0.00249124, 0.00228869, 0.00136878, 0.00127926, 0.00122822,\n",
       "            0.00106923])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.2       , 0.2       , 0.31428571, 0.31428571,\n",
       "            0.37142857, 0.37142857, 0.48571429, 0.48571429, 0.51428571,\n",
       "            0.62857143, 0.62857143, 0.8       , 0.8       , 0.82857143,\n",
       "            0.82857143, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.02361511, 0.01749895, 0.01203298, 0.01085815,\n",
       "            0.00890925, 0.00796382, 0.00777603, 0.00729654, 0.00713322,\n",
       "            0.0071185 , 0.00693314, 0.00600906, 0.00584659, 0.0055353 ,\n",
       "            0.0046556 , 0.00428758, 0.00327649, 0.00321613, 0.00298745,\n",
       "            0.00248117, 0.00156024])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.08571429, 0.08571429, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.28571429, 0.28571429, 0.37142857, 0.37142857,\n",
       "            0.45714286, 0.45714286, 0.68571429, 0.68571429, 0.71428571,\n",
       "            0.71428571, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.18007083, 0.11079982, 0.08321282, 0.07412385,\n",
       "            0.06606542, 0.03592294, 0.01657975, 0.01077821, 0.00856849,\n",
       "            0.00664959, 0.00642557, 0.00618716, 0.00476315, 0.00457016,\n",
       "            0.00448776, 0.0042639 , 0.00237022, 0.00213529, 0.00212348,\n",
       "            0.00204867, 0.00135549, 0.00108231, 0.00063412])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.22857143, 0.22857143, 0.4       ,\n",
       "            0.4       , 0.42857143, 0.42857143, 0.51428571, 0.51428571,\n",
       "            0.62857143, 0.62857143, 0.74285714, 0.77142857, 0.77142857,\n",
       "            0.82857143, 0.82857143, 0.85714286, 0.85714286, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.19326541, 0.03993481, 0.03486956, 0.02286199,\n",
       "            0.02088967, 0.01836003, 0.01500384, 0.0131481 , 0.01260777,\n",
       "            0.01143903, 0.01126237, 0.00849222, 0.0080006 , 0.00753201,\n",
       "            0.00606814, 0.00498224, 0.00405459, 0.004036  , 0.00320621,\n",
       "            0.00243378, 0.00175308])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.14285714, 0.14285714, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.22857143, 0.22857143, 0.25714286, 0.25714286,\n",
       "            0.45714286, 0.48571429, 0.54285714, 0.54285714, 0.71428571,\n",
       "            0.71428571, 0.74285714, 0.74285714, 0.94285714, 0.94285714,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.12736043, 0.05383752, 0.04599548, 0.04237771,\n",
       "            0.01536347, 0.01353373, 0.01306415, 0.01227597, 0.01014411,\n",
       "            0.00973542, 0.00967461, 0.00865515, 0.00739875, 0.00616119,\n",
       "            0.00481894, 0.00450794, 0.00397427, 0.0039219 , 0.00350142,\n",
       "            0.00349129, 0.00338797, 0.00314725, 0.0022654 , 0.00213802,\n",
       "            0.0019488 , 0.001877  , 0.0017854 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.22857143, 0.22857143, 0.51428571, 0.51428571,\n",
       "            0.54285714, 0.6       , 0.6       , 0.68571429, 0.68571429,\n",
       "            0.74285714, 0.74285714, 0.88571429, 0.88571429, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.86666667, 0.86666667,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.23317541, 0.10639007, 0.08683416, 0.08556563,\n",
       "            0.05185877, 0.04036103, 0.03065451, 0.01274511, 0.01269566,\n",
       "            0.01034956, 0.00935278, 0.00924068, 0.00709982, 0.00695446,\n",
       "            0.00575654, 0.00506484, 0.00294777, 0.00253798, 0.00154931,\n",
       "            0.0010677 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.34285714, 0.34285714, 0.37142857, 0.37142857,\n",
       "            0.42857143, 0.42857143, 0.45714286, 0.45714286, 0.54285714,\n",
       "            0.57142857, 0.68571429, 0.68571429, 0.74285714, 0.74285714,\n",
       "            0.85714286, 0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.18822943, 0.10053702, 0.03464983, 0.02884865,\n",
       "            0.0280912 , 0.00855963, 0.00852895, 0.00837037, 0.0082118 ,\n",
       "            0.00758412, 0.00723453, 0.00684305, 0.00620846, 0.00532998,\n",
       "            0.00530633, 0.00456244, 0.00424411, 0.00270052, 0.00268247,\n",
       "            0.00161907, 0.0015436 , 0.00054411])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.2       , 0.2       , 0.37142857, 0.37142857,\n",
       "            0.82857143, 0.82857143, 0.88571429, 0.88571429, 0.91428571,\n",
       "            0.91428571, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.2       , 0.2       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.13000931, 0.07473404, 0.04955697, 0.04427001,\n",
       "            0.02938934, 0.02606682, 0.02588158, 0.01465259, 0.01274574,\n",
       "            0.00592438, 0.00514557, 0.00471684, 0.00435548, 0.00374913,\n",
       "            0.00312418, 0.00268373, 0.00245626])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.4       ,\n",
       "            0.4       , 0.51428571, 0.51428571, 0.54285714, 0.57142857,\n",
       "            0.65714286, 0.65714286, 0.8       , 0.8       , 0.91428571,\n",
       "            0.91428571, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.42003946, 0.14168112, 0.09027117, 0.05913588,\n",
       "            0.04530514, 0.04391809, 0.04173   , 0.04018528, 0.01912575,\n",
       "            0.01850414, 0.01655958, 0.01429245, 0.014246  , 0.01365931,\n",
       "            0.01054616, 0.009367  , 0.00488272, 0.00413047, 0.00272566,\n",
       "            0.00208087, 0.00129919, 0.00119057])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.48571429, 0.48571429, 0.54285714, 0.54285714, 0.65714286,\n",
       "            0.65714286, 0.68571429, 0.68571429, 0.94285714, 0.94285714,\n",
       "            0.97142857, 0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.73333333, 0.8       , 0.8       , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.16499519, 0.07073991, 0.06821567, 0.0680101 ,\n",
       "            0.04719521, 0.04538605, 0.04435277, 0.04408674, 0.04268173,\n",
       "            0.02610015, 0.02605807, 0.02562705, 0.024561  , 0.02058893,\n",
       "            0.02053753, 0.01936137, 0.01834257, 0.012108  , 0.01072239,\n",
       "            0.01011462, 0.00990627, 0.00915932, 0.00486109])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.14285714, 0.14285714, 0.17142857,\n",
       "            0.17142857, 0.25714286, 0.25714286, 0.31428571, 0.31428571,\n",
       "            0.51428571, 0.51428571, 0.57142857, 0.57142857, 0.6       ,\n",
       "            0.77142857, 0.77142857, 0.88571429, 0.88571429, 0.91428571,\n",
       "            0.91428571, 0.97142857, 0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.14963378, 0.08310205, 0.07291769, 0.05467996,\n",
       "            0.0448681 , 0.03941577, 0.03536873, 0.02804714, 0.01794257,\n",
       "            0.01200448, 0.01154475, 0.00986029, 0.009058  , 0.00875157,\n",
       "            0.00538998, 0.00517018, 0.00395588, 0.00376958, 0.00331984,\n",
       "            0.00308903, 0.00252354, 0.00230465, 0.00191574, 0.00163011])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.17142857, 0.17142857, 0.28571429,\n",
       "            0.28571429, 0.31428571, 0.31428571, 0.4       , 0.4       ,\n",
       "            0.42857143, 0.45714286, 0.6       , 0.6       , 0.68571429,\n",
       "            0.68571429, 0.77142857, 0.77142857, 0.88571429, 0.88571429,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.73333333, 0.73333333, 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.39116639, 0.0811126 , 0.07686445, 0.04562714,\n",
       "            0.04362074, 0.04231857, 0.03399234, 0.0236647 , 0.02244666,\n",
       "            0.02137723, 0.02092937, 0.0133172 , 0.01288403, 0.01268842,\n",
       "            0.00958621, 0.00819116, 0.00720426, 0.00455391, 0.00322358,\n",
       "            0.00155433, 0.00085936])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.34285714, 0.34285714, 0.42857143, 0.42857143, 0.62857143,\n",
       "            0.62857143, 0.65714286, 0.65714286, 0.71428571, 0.71428571,\n",
       "            0.74285714, 0.74285714, 0.82857143, 0.82857143, 0.91428571,\n",
       "            0.91428571, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.16346623, 0.0851406 , 0.08024085, 0.07836263,\n",
       "            0.0609351 , 0.05107507, 0.03806844, 0.03474984, 0.0223885 ,\n",
       "            0.01795161, 0.01742456, 0.01724999, 0.01412633, 0.01387927,\n",
       "            0.01227687, 0.01081137, 0.01043854, 0.00997059, 0.00762995,\n",
       "            0.00511412, 0.00465081, 0.00456325, 0.00414897])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.17142857, 0.17142857,\n",
       "            0.37142857, 0.37142857, 0.42857143, 0.42857143, 0.45714286,\n",
       "            0.45714286, 0.48571429, 0.48571429, 0.71428571, 0.71428571,\n",
       "            0.8       , 0.8       , 0.82857143, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.27533652, 0.17747869, 0.14301288, 0.09173908,\n",
       "            0.08201294, 0.07424629, 0.06494183, 0.05828734, 0.05799607,\n",
       "            0.04539671, 0.0444316 , 0.03894766, 0.03893466, 0.03767347,\n",
       "            0.03299812, 0.03056   , 0.0276527 , 0.01659232, 0.01244621,\n",
       "            0.01025864, 0.00999816, 0.00996187, 0.0062072 , 0.00286315])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.2       , 0.2       ,\n",
       "            0.22857143, 0.22857143, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.37142857, 0.6       , 0.6       , 0.68571429, 0.71428571,\n",
       "            0.71428571, 0.77142857, 0.77142857, 0.8       , 0.8       ,\n",
       "            0.85714286, 0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.29635816, 0.21449629, 0.10381025, 0.07567105,\n",
       "            0.06708829, 0.06231769, 0.04323518, 0.04253958, 0.04202782,\n",
       "            0.04058682, 0.02420403, 0.02391636, 0.01648264, 0.01624396,\n",
       "            0.01346191, 0.01149753, 0.01105057, 0.01065358, 0.0092522 ,\n",
       "            0.00851677, 0.00846056, 0.00477553])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.42857143, 0.42857143, 0.65714286, 0.65714286, 0.74285714,\n",
       "            0.74285714, 0.85714286, 0.85714286, 0.97142857, 0.97142857,\n",
       "            1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.86666667,\n",
       "            0.86666667, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.31084567, 0.05373948, 0.05062196, 0.0472542 ,\n",
       "            0.04557328, 0.04019814, 0.02384752, 0.02256632, 0.01913745,\n",
       "            0.01651556, 0.0096634 , 0.00857797, 0.00588741, 0.00504474,\n",
       "            0.00445477, 0.00153675])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.34285714, 0.34285714,\n",
       "            0.37142857, 0.37142857, 0.42857143, 0.42857143, 0.48571429,\n",
       "            0.48571429, 0.51428571, 0.54285714, 0.65714286, 0.65714286,\n",
       "            0.85714286, 0.85714286, 0.91428571, 0.91428571, 0.94285714,\n",
       "            0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.66666667,\n",
       "            0.66666667, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.73482849, 0.40986749, 0.11258797, 0.10670281,\n",
       "            0.06849988, 0.06669309, 0.05173353, 0.04554071, 0.03828879,\n",
       "            0.03787838, 0.03367221, 0.03358442, 0.01847629, 0.01530209,\n",
       "            0.01105019, 0.00737673, 0.00628091, 0.00605052, 0.00492252,\n",
       "            0.00455732, 0.00206692])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.17142857, 0.17142857, 0.34285714, 0.34285714,\n",
       "            0.4       , 0.4       , 0.45714286, 0.45714286, 0.48571429,\n",
       "            0.57142857, 0.57142857, 0.68571429, 0.68571429, 0.77142857,\n",
       "            0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.23771292, 0.16261683, 0.15039357, 0.11362007,\n",
       "            0.1039899 , 0.0858919 , 0.08386412, 0.0640502 , 0.05147019,\n",
       "            0.03925842, 0.03398424, 0.02372657, 0.02298348, 0.01974281,\n",
       "            0.01717047, 0.01617439, 0.01228056, 0.01011966, 0.00985426,\n",
       "            0.0071245 , 0.00157787])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.11428571, 0.11428571,\n",
       "            0.17142857, 0.17142857, 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.31428571, 0.31428571, 0.42857143, 0.42857143,\n",
       "            0.45714286, 0.45714286, 0.77142857, 0.77142857, 0.8       ,\n",
       "            0.8       , 0.91428571, 0.94285714, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.38878689, 0.35904651, 0.17177517, 0.16615689,\n",
       "            0.13811163, 0.13369376, 0.11932241, 0.09098167, 0.09035079,\n",
       "            0.08780173, 0.06970356, 0.06846386, 0.05252844, 0.05201432,\n",
       "            0.05200074, 0.04204154, 0.01830166, 0.01651406, 0.01635461,\n",
       "            0.01607474, 0.01324325, 0.01235207, 0.01092264, 0.0068115 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.28571429, 0.28571429, 0.37142857, 0.37142857,\n",
       "            0.4       , 0.54285714, 0.54285714, 0.57142857, 0.57142857,\n",
       "            0.62857143, 0.62857143, 0.65714286, 0.65714286, 0.77142857,\n",
       "            0.77142857, 0.82857143, 0.82857143, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.37407287, 0.30550475, 0.167322  , 0.14932569,\n",
       "            0.14261266, 0.07454076, 0.06687372, 0.05421799, 0.05123049,\n",
       "            0.04875644, 0.03998092, 0.03928353, 0.03780698, 0.03306555,\n",
       "            0.02815486, 0.02262208, 0.0224933 , 0.02126115, 0.01687385,\n",
       "            0.01505286, 0.01217073, 0.01107886, 0.00624494, 0.00602013,\n",
       "            0.00587105])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.31428571, 0.31428571, 0.34285714, 0.34285714, 0.65714286,\n",
       "            0.65714286, 0.77142857, 0.77142857, 0.82857143, 0.82857143,\n",
       "            0.85714286, 0.85714286, 0.91428571, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.33556878, 0.2900924 , 0.27754346, 0.23805165,\n",
       "            0.17837537, 0.13763747, 0.12161266, 0.11686608, 0.09106857,\n",
       "            0.0683388 , 0.06060843, 0.0442364 , 0.04416015, 0.01688034,\n",
       "            0.01679179, 0.01073025, 0.01027179, 0.00936832, 0.00771836,\n",
       "            0.00746313, 0.00742039, 0.00567893, 0.00552889, 0.00270622])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.25714286, 0.25714286, 0.34285714,\n",
       "            0.34285714, 0.42857143, 0.42857143, 0.65714286, 0.65714286,\n",
       "            0.68571429, 0.68571429, 0.88571429, 0.88571429, 0.91428571,\n",
       "            0.91428571, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.59617355, 0.33913214, 0.23262506, 0.2146505 ,\n",
       "            0.16655468, 0.15984724, 0.10806213, 0.1077517 , 0.09572757,\n",
       "            0.07719679, 0.06604355, 0.05279   , 0.02304836, 0.02150586,\n",
       "            0.02078239, 0.01948084, 0.0109341 , 0.00983579, 0.00865451,\n",
       "            0.0072806 , 0.00503222, 0.00434529, 0.00300352])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.17142857, 0.2       , 0.25714286, 0.25714286, 0.28571429,\n",
       "            0.28571429, 0.42857143, 0.42857143, 0.57142857, 0.57142857,\n",
       "            0.65714286, 0.65714286, 0.88571429, 0.88571429, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.47725103, 0.26620591, 0.14517925, 0.10603868,\n",
       "            0.0740298 , 0.05572879, 0.04639923, 0.03799819, 0.035892  ,\n",
       "            0.03565838, 0.02504413, 0.02382203, 0.0195219 , 0.01703637,\n",
       "            0.01455725, 0.01331278, 0.00954618, 0.00819212, 0.00589494,\n",
       "            0.00566106])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.2       , 0.2       , 0.25714286, 0.25714286,\n",
       "            0.31428571, 0.31428571, 0.48571429, 0.51428571, 0.57142857,\n",
       "            0.57142857, 0.6       , 0.6       , 0.65714286, 0.65714286,\n",
       "            0.77142857, 0.77142857, 0.88571429, 0.88571429, 0.94285714,\n",
       "            0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.5380514 , 0.43908335, 0.41269882, 0.40504544,\n",
       "            0.40130422, 0.23389233, 0.21649591, 0.13771045, 0.13642407,\n",
       "            0.08534505, 0.07772662, 0.04298514, 0.04273021, 0.0321285 ,\n",
       "            0.03105808, 0.02941001, 0.02540601, 0.02323038, 0.02263957,\n",
       "            0.01856116, 0.01152049, 0.00860956, 0.00807846, 0.00658802,\n",
       "            0.00437056, 0.00132836])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.14285714, 0.14285714, 0.17142857, 0.17142857, 0.28571429,\n",
       "            0.28571429, 0.42857143, 0.42857143, 0.45714286, 0.54285714,\n",
       "            0.54285714, 0.62857143, 0.62857143, 0.74285714, 0.74285714,\n",
       "            0.77142857, 0.77142857, 0.8       , 0.8       , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.55523245, 0.45356748, 0.33387859, 0.18118687,\n",
       "            0.14386793, 0.12687086, 0.11860041, 0.09288048, 0.05341946,\n",
       "            0.05065363, 0.03384087, 0.03355319, 0.03275449, 0.0291445 ,\n",
       "            0.02746401, 0.02149355, 0.02094198, 0.01855301, 0.01778526,\n",
       "            0.0174977 , 0.01607257, 0.01601713, 0.01538065, 0.00521811])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.34285714, 0.34285714, 0.45714286,\n",
       "            0.45714286, 0.57142857, 0.6       , 0.71428571, 0.71428571,\n",
       "            0.74285714, 0.74285714, 0.88571429, 0.88571429, 0.91428571,\n",
       "            0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.69970088, 0.39891866, 0.38880856, 0.32248396,\n",
       "            0.3091696 , 0.11714109, 0.06534701, 0.0588298 , 0.0470118 ,\n",
       "            0.04615022, 0.04217547, 0.0407857 , 0.03131033, 0.02979593,\n",
       "            0.02825797, 0.02514192, 0.01784536, 0.01662439, 0.01578566,\n",
       "            0.01480587, 0.01170538])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.14285714, 0.14285714,\n",
       "            0.25714286, 0.25714286, 0.31428571, 0.31428571, 0.34285714,\n",
       "            0.34285714, 0.42857143, 0.42857143, 0.57142857, 0.57142857,\n",
       "            0.8       , 0.8       , 0.88571429, 0.91428571, 0.91428571,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.31594599, 0.17155997, 0.15039746, 0.1343776 ,\n",
       "            0.11505515, 0.11282384, 0.10129258, 0.10128258, 0.0997296 ,\n",
       "            0.0925446 , 0.08225607, 0.07612384, 0.05893079, 0.05493637,\n",
       "            0.03758647, 0.03583189, 0.02726627, 0.02717774, 0.01940997,\n",
       "            0.0112423 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.28571429,\n",
       "            0.28571429, 0.54285714, 0.54285714, 0.6       , 0.6       ,\n",
       "            0.71428571, 0.71428571, 0.8       , 0.8       , 0.82857143,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.86666667,\n",
       "            0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.79182806, 0.60159982, 0.55760018, 0.39407958,\n",
       "            0.32647465, 0.2129678 , 0.19814051, 0.18379223, 0.12382759,\n",
       "            0.11552508, 0.04974208, 0.04766931, 0.04379948, 0.04335249,\n",
       "            0.02075739, 0.01590501, 0.01315731, 0.01074331, 0.00772322,\n",
       "            0.0041228 , 0.00304147, 0.00266609])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.0),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.17142857, 0.17142857,\n",
       "            0.22857143, 0.22857143, 0.34285714, 0.34285714, 0.37142857,\n",
       "            0.37142857, 0.4       , 0.4       , 0.57142857, 0.57142857,\n",
       "            0.71428571, 0.74285714, 0.77142857, 0.77142857, 0.88571429,\n",
       "            0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.60178447, 0.42160305, 0.41902358, 0.41116616,\n",
       "            0.34044414, 0.2879601 , 0.27572629, 0.2248335 , 0.20544574,\n",
       "            0.15906152, 0.1001942 , 0.05346257, 0.05337151, 0.05154289,\n",
       "            0.0462888 , 0.04272102, 0.04032247, 0.0164103 , 0.01459757,\n",
       "            0.00957261, 0.00916361, 0.00846816, 0.00705201, 0.00437896,\n",
       "            0.00377441, 0.00152894])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.11428571, 0.11428571, 0.22857143, 0.22857143, 0.34285714,\n",
       "            0.34285714, 0.37142857, 0.37142857, 0.62857143, 0.62857143,\n",
       "            0.68571429, 0.68571429, 0.82857143, 0.85714286, 0.91428571,\n",
       "            0.91428571, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.65920687, 0.5819008 , 0.56707957, 0.55935881,\n",
       "            0.37127892, 0.33882484, 0.15552486, 0.14447128, 0.09948356,\n",
       "            0.09594194, 0.0928619 , 0.09206624, 0.04970964, 0.04208635,\n",
       "            0.03090518, 0.02191236, 0.01426033, 0.01390541, 0.00729565,\n",
       "            0.00682751, 0.00667447, 0.00424666, 0.00234222])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.08571428571428572),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.51428571,\n",
       "            0.51428571, 0.62857143, 0.65714286, 0.65714286, 0.82857143,\n",
       "            0.82857143, 0.88571429, 0.88571429, 0.97142857, 0.97142857,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.73333333, 0.73333333,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.71969542, 0.68960326, 0.60638394, 0.46636051,\n",
       "            0.40657383, 0.39855179, 0.34407606, 0.3063033 , 0.10641511,\n",
       "            0.10009453, 0.07897613, 0.07378395, 0.057176  , 0.03537961,\n",
       "            0.02990416, 0.02549631, 0.02537691, 0.01843631, 0.01785037,\n",
       "            0.00792894])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.2),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.34285714, 0.34285714, 0.42857143, 0.42857143, 0.45714286,\n",
       "            0.45714286, 0.51428571, 0.51428571, 0.57142857, 0.57142857,\n",
       "            0.65714286, 0.65714286, 0.68571429, 0.68571429, 0.85714286,\n",
       "            0.85714286, 0.88571429, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.63852752, 0.52555296, 0.49857776, 0.45922436,\n",
       "            0.14717279, 0.13920543, 0.12357449, 0.12113526, 0.12079447,\n",
       "            0.10491196, 0.10406135, 0.09536767, 0.08141111, 0.0764405 ,\n",
       "            0.06332831, 0.058869  , 0.0529924 , 0.040657  , 0.02626454,\n",
       "            0.02556172, 0.02166129, 0.01730812, 0.01671808, 0.00804249])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.02857142857142857),\n",
       "    'tpr': np.float64(0.06666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.17142857,\n",
       "            0.17142857, 0.25714286, 0.25714286, 0.45714286, 0.45714286,\n",
       "            0.54285714, 0.54285714, 0.65714286, 0.65714286, 0.77142857,\n",
       "            0.77142857, 0.85714286, 0.85714286, 0.88571429, 0.91428571,\n",
       "            0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.4       , 0.4       , 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.57402   , 0.53393973, 0.49037736, 0.27145305,\n",
       "            0.25539145, 0.22866377, 0.21158533, 0.13674141, 0.12989564,\n",
       "            0.10427556, 0.0885208 , 0.06694064, 0.06202503, 0.05332419,\n",
       "            0.05045348, 0.03786505, 0.0371734 , 0.02514626, 0.02096109,\n",
       "            0.02067266, 0.01889049, 0.01634171])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.11428571, 0.11428571, 0.17142857, 0.17142857,\n",
       "            0.2       , 0.2       , 0.48571429, 0.48571429, 0.51428571,\n",
       "            0.51428571, 0.57142857, 0.57142857, 0.65714286, 0.65714286,\n",
       "            0.68571429, 0.71428571, 0.88571429, 0.88571429, 0.97142857,\n",
       "            0.97142857, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.87690341, 0.69098162, 0.5912426 , 0.52127365,\n",
       "            0.48129151, 0.45611355, 0.39754523, 0.3866929 , 0.36031854,\n",
       "            0.2610073 , 0.21059125, 0.08620947, 0.08509481, 0.08233963,\n",
       "            0.07756529, 0.06782891, 0.06318124, 0.05432378, 0.03789838,\n",
       "            0.0347131 , 0.03109263, 0.00813213, 0.00765266, 0.00504219,\n",
       "            0.00372719, 0.00330739, 0.00241378])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.2       , 0.2       , 0.25714286, 0.25714286,\n",
       "            0.51428571, 0.51428571, 0.62857143, 0.62857143, 0.8       ,\n",
       "            0.8       , 0.82857143, 0.85714286, 0.85714286, 0.91428571,\n",
       "            0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.59901937, 0.55986489, 0.5096377 , 0.46635844,\n",
       "            0.34890421, 0.33025751, 0.32924352, 0.21789051, 0.17211153,\n",
       "            0.11411308, 0.1090301 , 0.0709208 , 0.06854472, 0.04697034,\n",
       "            0.04423591, 0.03234176, 0.02727556, 0.02723203, 0.02475409,\n",
       "            0.02349958, 0.00965012])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.05714285714285714),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.17142857, 0.17142857, 0.25714286, 0.25714286, 0.45714286,\n",
       "            0.45714286, 0.54285714, 0.54285714, 0.74285714, 0.77142857,\n",
       "            0.88571429, 0.88571429, 0.91428571, 0.91428571, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.71520963, 0.5658246 , 0.55782282, 0.46079661,\n",
       "            0.44293219, 0.43913951, 0.37631715, 0.32904279, 0.23241567,\n",
       "            0.22781872, 0.22348091, 0.17151358, 0.16726987, 0.11001122,\n",
       "            0.09924897, 0.08614484, 0.07626012, 0.03638057, 0.03411448,\n",
       "            0.02189944, 0.01947501, 0.01560807, 0.01388754, 0.00636595,\n",
       "            0.00566123])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.11428571, 0.11428571,\n",
       "            0.17142857, 0.17142857, 0.31428571, 0.31428571, 0.34285714,\n",
       "            0.34285714, 0.45714286, 0.45714286, 0.51428571, 0.51428571,\n",
       "            0.62857143, 0.62857143, 0.71428571, 0.74285714, 0.88571429,\n",
       "            0.88571429, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.6       ,\n",
       "            0.6       , 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.76121367, 0.68775571, 0.55706956, 0.47473201,\n",
       "            0.35742205, 0.33074839, 0.2210748 , 0.22101379, 0.21798132,\n",
       "            0.21683717, 0.17873054, 0.1708477 , 0.15060948, 0.13987999,\n",
       "            0.10450028, 0.08597954, 0.07690034, 0.0667259 , 0.03518363,\n",
       "            0.02843567, 0.01924805, 0.01875669, 0.01666085])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.14285714285714285),\n",
       "    'tpr': np.float64(0.3333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.14285714, 0.14285714,\n",
       "            0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.54285714,\n",
       "            0.54285714, 0.57142857, 0.57142857, 0.82857143, 0.82857143,\n",
       "            0.85714286, 0.85714286, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.67159818, 0.61982695, 0.53599748, 0.5250597 ,\n",
       "            0.22885995, 0.21542048, 0.16073147, 0.15823127, 0.12912295,\n",
       "            0.1258705 , 0.0979505 , 0.0970199 , 0.03904482, 0.03718041,\n",
       "            0.0306631 , 0.02695111, 0.0232781 , 0.02113638, 0.00941672])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.14285714285714285),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.17142857, 0.17142857,\n",
       "            0.42857143, 0.42857143, 0.45714286, 0.45714286, 0.48571429,\n",
       "            0.48571429, 0.62857143, 0.62857143, 0.71428571, 0.71428571,\n",
       "            0.74285714, 0.77142857, 0.85714286, 0.85714286, 0.91428571,\n",
       "            0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.74400011, 0.71503284, 0.69778328, 0.55799495,\n",
       "            0.55444196, 0.54429354, 0.52540926, 0.40980923, 0.40347766,\n",
       "            0.22297235, 0.18452272, 0.18169139, 0.15755044, 0.15153242,\n",
       "            0.14020762, 0.12709577, 0.11958896, 0.09707731, 0.08625021,\n",
       "            0.07573273, 0.06921195, 0.03624392, 0.03393638, 0.03041678,\n",
       "            0.02546083, 0.01851837])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.11428571428571428),\n",
       "    'tpr': np.float64(0.13333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.17142857, 0.17142857, 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.31428571, 0.31428571, 0.45714286, 0.45714286,\n",
       "            0.54285714, 0.54285714, 0.62857143, 0.62857143, 0.8       ,\n",
       "            0.8       , 0.85714286, 0.88571429, 1.        , 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ]),\n",
       "     'thresholds': array([       inf, 0.78322837, 0.75269776, 0.68935338, 0.65951887,\n",
       "            0.44260362, 0.4337952 , 0.4147289 , 0.39351979, 0.3200738 ,\n",
       "            0.28111897, 0.24929699, 0.21860806, 0.1894752 , 0.17189736,\n",
       "            0.13011497, 0.12515154, 0.09515365, 0.08448088, 0.05399764,\n",
       "            0.03469991, 0.02788835, 0.02696991, 0.0115323 , 0.01147829])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.14285714285714285),\n",
       "    'tpr': np.float64(0.2),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.4       ,\n",
       "            0.4       , 0.45714286, 0.45714286, 0.48571429, 0.48571429,\n",
       "            0.54285714, 0.54285714, 0.62857143, 0.62857143, 0.85714286,\n",
       "            0.85714286, 0.88571429, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.46666667, 0.46666667, 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.76140907, 0.75932134, 0.74194065, 0.70119863,\n",
       "            0.62533146, 0.52310555, 0.49311956, 0.48635955, 0.27950327,\n",
       "            0.26254916, 0.22988672, 0.19908761, 0.17517555, 0.1544052 ,\n",
       "            0.14259508, 0.13966426, 0.11892753, 0.11262043, 0.04565231,\n",
       "            0.04400001, 0.03525329, 0.03417552, 0.03413392, 0.02200402])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.14285714285714285),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.14285714, 0.14285714, 0.31428571, 0.31428571, 0.42857143,\n",
       "            0.42857143, 0.48571429, 0.48571429, 0.57142857, 0.57142857,\n",
       "            0.68571429, 0.68571429, 0.71428571, 0.74285714, 0.91428571,\n",
       "            0.91428571, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.82563019, 0.78448195, 0.766401  , 0.68731989,\n",
       "            0.59069376, 0.54815442, 0.3143266 , 0.30205945, 0.25719827,\n",
       "            0.25602016, 0.20700582, 0.16949849, 0.1560955 , 0.13193958,\n",
       "            0.10019116, 0.09818367, 0.08134049, 0.06935281, 0.03423101,\n",
       "            0.03396984, 0.03178145, 0.02450411, 0.01935555])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.14285714285714285),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.17142857, 0.17142857, 0.42857143, 0.42857143, 0.48571429,\n",
       "            0.48571429, 0.51428571, 0.51428571, 0.62857143, 0.62857143,\n",
       "            0.65714286, 0.65714286, 0.74285714, 0.77142857, 0.82857143,\n",
       "            0.82857143, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.77157223, 0.76142564, 0.73187317, 0.73136578,\n",
       "            0.72972586, 0.68562701, 0.66728487, 0.62198044, 0.58609023,\n",
       "            0.44513461, 0.43478941, 0.25159127, 0.22827255, 0.22295505,\n",
       "            0.20666568, 0.20425113, 0.1987445 , 0.15556657, 0.15354642,\n",
       "            0.14209081, 0.13825486, 0.08167934, 0.07049079, 0.05243151,\n",
       "            0.04901634, 0.04115893, 0.03784311, 0.01841259])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.14285714285714285),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.17142857, 0.17142857, 0.42857143, 0.42857143, 0.48571429,\n",
       "            0.48571429, 0.51428571, 0.51428571, 0.62857143, 0.62857143,\n",
       "            0.65714286, 0.65714286, 0.74285714, 0.77142857, 0.82857143,\n",
       "            0.82857143, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.77157223, 0.76142564, 0.73187317, 0.73136578,\n",
       "            0.72972586, 0.68562701, 0.66728487, 0.62198044, 0.58609023,\n",
       "            0.44513461, 0.43478941, 0.25159127, 0.22827255, 0.22295505,\n",
       "            0.20666568, 0.20425113, 0.1987445 , 0.15556657, 0.15354642,\n",
       "            0.14209081, 0.13825486, 0.08167934, 0.07049079, 0.05243151,\n",
       "            0.04901634, 0.04115893, 0.03784311, 0.01841259])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.17142857142857143),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.14285714,\n",
       "            0.14285714, 0.17142857, 0.17142857, 0.28571429, 0.28571429,\n",
       "            0.42857143, 0.42857143, 0.45714286, 0.45714286, 0.48571429,\n",
       "            0.48571429, 0.54285714, 0.54285714, 0.57142857, 0.57142857,\n",
       "            0.74285714, 0.77142857, 0.77142857, 0.91428571, 0.91428571,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.8       ,\n",
       "            0.8       , 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.79173036, 0.76849219, 0.75333669, 0.66345449,\n",
       "            0.64738921, 0.56869108, 0.50817189, 0.37096359, 0.37092149,\n",
       "            0.29870745, 0.24884611, 0.24294821, 0.24163702, 0.23459422,\n",
       "            0.21797999, 0.212841  , 0.2069106 , 0.20651722, 0.17702628,\n",
       "            0.08922299, 0.07685306, 0.0740634 , 0.04565502, 0.04428241,\n",
       "            0.02483841])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.17142857142857143),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.11428571,\n",
       "            0.11428571, 0.17142857, 0.17142857, 0.31428571, 0.31428571,\n",
       "            0.37142857, 0.37142857, 0.45714286, 0.45714286, 0.57142857,\n",
       "            0.57142857, 0.62857143, 0.62857143, 0.65714286, 0.65714286,\n",
       "            0.74285714, 0.77142857, 0.82857143, 0.82857143, 0.91428571,\n",
       "            0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.77557689, 0.76695168, 0.74919874, 0.72364487,\n",
       "            0.64167438, 0.50372832, 0.46764114, 0.34316957, 0.3189027 ,\n",
       "            0.3096349 , 0.29383836, 0.28722701, 0.25582388, 0.20976516,\n",
       "            0.19304182, 0.18319707, 0.17968119, 0.17365557, 0.15999329,\n",
       "            0.09652598, 0.09131958, 0.06273146, 0.05959817, 0.04818646,\n",
       "            0.04357058, 0.02135643])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.3142857142857143),\n",
       "    'tpr': np.float64(0.26666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.08571429, 0.08571429,\n",
       "            0.14285714, 0.14285714, 0.28571429, 0.28571429, 0.31428571,\n",
       "            0.31428571, 0.4       , 0.4       , 0.51428571, 0.51428571,\n",
       "            0.57142857, 0.57142857, 0.71428571, 0.71428571, 0.82857143,\n",
       "            0.85714286, 0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.53333333, 0.53333333, 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.85887526, 0.85620566, 0.82491396, 0.71660071,\n",
       "            0.67036455, 0.59917323, 0.54463771, 0.52922855, 0.51890694,\n",
       "            0.44492543, 0.38745363, 0.37099373, 0.29891626, 0.27053623,\n",
       "            0.2434448 , 0.2230942 , 0.17265723, 0.10833381, 0.09033934,\n",
       "            0.06823805, 0.06261405, 0.02869671])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2),\n",
       "    'tpr': np.float64(0.3333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.6       , 0.6       ,\n",
       "            0.62857143, 0.62857143, 0.65714286, 0.65714286, 0.74285714,\n",
       "            0.77142857, 0.85714286, 0.85714286, 0.94285714, 0.94285714,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.33333333, 0.33333333, 0.53333333,\n",
       "            0.53333333, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.89661816, 0.85523266, 0.85430249, 0.85077594,\n",
       "            0.79285994, 0.74695017, 0.65405699, 0.25562774, 0.2152825 ,\n",
       "            0.20054472, 0.19395776, 0.16307644, 0.16227368, 0.10008108,\n",
       "            0.0956088 , 0.06917684, 0.06788145, 0.03914017, 0.03910197,\n",
       "            0.03219465])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.22857142857142856),\n",
       "    'tpr': np.float64(0.4),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.54285714,\n",
       "            0.54285714, 0.68571429, 0.68571429, 0.71428571, 0.71428571,\n",
       "            0.88571429, 0.88571429, 0.91428571, 0.94285714, 0.94285714,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.93461768, 0.87418784, 0.80367653, 0.7443143 ,\n",
       "            0.54747599, 0.51631887, 0.50089501, 0.46986111, 0.28403592,\n",
       "            0.26505207, 0.17575867, 0.15384956, 0.1462413 , 0.13996646,\n",
       "            0.07817206, 0.07691733, 0.0729499 , 0.05444621, 0.0539428 ,\n",
       "            0.02440586])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.42857142857142855),\n",
       "    'tpr': np.float64(0.3333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.25714286, 0.25714286, 0.48571429, 0.48571429,\n",
       "            0.6       , 0.6       , 0.71428571, 0.74285714, 0.74285714,\n",
       "            0.85714286, 0.85714286, 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.90914544, 0.90429293, 0.88613239, 0.87254125,\n",
       "            0.79103532, 0.6052699 , 0.58098711, 0.47233461, 0.33461502,\n",
       "            0.26578105, 0.23630482, 0.19872393, 0.19722013, 0.17585336,\n",
       "            0.1082242 , 0.09889106, 0.08613648, 0.06885907, 0.02992976])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.2857142857142857),\n",
       "    'tpr': np.float64(0.3333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.14285714, 0.14285714, 0.4       , 0.4       ,\n",
       "            0.42857143, 0.45714286, 0.45714286, 0.62857143, 0.62857143,\n",
       "            0.68571429, 0.68571429, 0.71428571, 0.71428571, 0.82857143,\n",
       "            0.82857143, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.94474205, 0.90403761, 0.89901185, 0.82074156,\n",
       "            0.78567567, 0.71406112, 0.69598962, 0.40549793, 0.38696513,\n",
       "            0.34054491, 0.32429881, 0.31317785, 0.27779912, 0.26537403,\n",
       "            0.24674455, 0.24588842, 0.24385835, 0.20439629, 0.1045987 ,\n",
       "            0.09363343, 0.0862755 , 0.07544705, 0.03751682])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.3142857142857143),\n",
       "    'tpr': np.float64(0.3333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.28571429, 0.28571429, 0.37142857,\n",
       "            0.37142857, 0.42857143, 0.42857143, 0.51428571, 0.51428571,\n",
       "            0.62857143, 0.62857143, 0.65714286, 0.65714286, 0.74285714,\n",
       "            0.77142857, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.93302615, 0.91291937, 0.89864436, 0.89270538,\n",
       "            0.88716338, 0.86394918, 0.58783351, 0.52373709, 0.43735921,\n",
       "            0.4308224 , 0.41440304, 0.39968351, 0.36740397, 0.35297793,\n",
       "            0.28418725, 0.28282367, 0.26929711, 0.25446856, 0.21677634,\n",
       "            0.15908194, 0.11010988, 0.10305851, 0.06636815])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.37142857142857144),\n",
       "    'tpr': np.float64(0.4),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.37142857, 0.37142857, 0.4       , 0.4       , 0.45714286,\n",
       "            0.45714286, 0.57142857, 0.57142857, 0.62857143, 0.62857143,\n",
       "            0.65714286, 0.68571429, 0.68571429, 0.71428571, 0.71428571,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.95138649, 0.87414765, 0.8458938 , 0.8456072 ,\n",
       "            0.832375  , 0.60046061, 0.57908045, 0.5737994 , 0.56291137,\n",
       "            0.50561966, 0.48371344, 0.4703467 , 0.4561191 , 0.44381139,\n",
       "            0.43188832, 0.38948158, 0.3344972 , 0.32014001, 0.31697728,\n",
       "            0.31311021, 0.30877339, 0.290023  , 0.27397945, 0.26006712,\n",
       "            0.07331011, 0.05417465, 0.04253552])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.45714285714285713),\n",
       "    'tpr': np.float64(0.4666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.17142857, 0.17142857, 0.34285714,\n",
       "            0.34285714, 0.57142857, 0.57142857, 0.62857143, 0.65714286,\n",
       "            0.65714286, 0.71428571, 0.71428571, 0.77142857, 0.77142857,\n",
       "            0.85714286, 0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.46666667, 0.46666667, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9060116 , 0.79367806, 0.73867604, 0.62666922,\n",
       "            0.60121768, 0.41062257, 0.39286257, 0.3180403 , 0.31465081,\n",
       "            0.30951765, 0.26978084, 0.25792463, 0.20850026, 0.19803678,\n",
       "            0.13859643, 0.10588028, 0.03775904])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.42857142857142855),\n",
       "    'tpr': np.float64(0.5333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.22857143,\n",
       "            0.22857143, 0.34285714, 0.34285714, 0.37142857, 0.37142857,\n",
       "            0.45714286, 0.45714286, 0.6       , 0.62857143, 0.68571429,\n",
       "            0.68571429, 0.71428571, 0.71428571, 0.85714286, 0.85714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.96782633, 0.95484976, 0.94034415, 0.81122991,\n",
       "            0.77648325, 0.62362326, 0.58426142, 0.57011215, 0.53196775,\n",
       "            0.48241555, 0.40694899, 0.35975421, 0.35597445, 0.34361159,\n",
       "            0.30060933, 0.24341248, 0.24164004, 0.15753232, 0.1221257 ,\n",
       "            0.04144704])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.17142857, 0.17142857, 0.28571429, 0.28571429,\n",
       "            0.42857143, 0.42857143, 0.45714286, 0.45714286, 0.57142857,\n",
       "            0.57142857, 0.71428571, 0.71428571, 0.74285714, 0.74285714,\n",
       "            0.77142857, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.6       , 0.6       , 0.8       , 0.8       , 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.97483743, 0.97075344, 0.95758168, 0.95379371,\n",
       "            0.95340471, 0.9223264 , 0.89134609, 0.84763843, 0.8246087 ,\n",
       "            0.69254525, 0.69158553, 0.68098685, 0.62018096, 0.55579474,\n",
       "            0.52808261, 0.3606738 , 0.31822422, 0.31319315, 0.30634984,\n",
       "            0.30043591, 0.17691963, 0.1423071 , 0.0609639 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5428571428571428),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.14285714,\n",
       "            0.14285714, 0.25714286, 0.25714286, 0.28571429, 0.31428571,\n",
       "            0.4       , 0.4       , 0.48571429, 0.48571429, 0.51428571,\n",
       "            0.51428571, 0.54285714, 0.54285714, 0.6       , 0.6       ,\n",
       "            0.68571429, 0.68571429, 0.8       , 0.8       , 0.82857143,\n",
       "            0.82857143, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96649588, 0.94565585, 0.93345555, 0.87921762,\n",
       "            0.83180797, 0.7724185 , 0.76606534, 0.75027461, 0.7317727 ,\n",
       "            0.71335387, 0.69358201, 0.5786692 , 0.56762722, 0.56526291,\n",
       "            0.55921011, 0.52709413, 0.51858904, 0.49098504, 0.47075936,\n",
       "            0.3833295 , 0.36199132, 0.33790184, 0.32299939, 0.30899333,\n",
       "            0.29938434, 0.13619674, 0.12425124, 0.05065341])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.5714285714285714),\n",
       "    'tpr': np.float64(0.5333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.22857143, 0.22857143, 0.25714286,\n",
       "            0.25714286, 0.31428571, 0.31428571, 0.4       , 0.4       ,\n",
       "            0.45714286, 0.45714286, 0.54285714, 0.54285714, 0.6       ,\n",
       "            0.6       , 0.65714286, 0.65714286, 0.71428571, 0.71428571,\n",
       "            0.77142857, 0.77142857, 0.82857143, 0.85714286, 0.94285714,\n",
       "            0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98933877, 0.92372288, 0.90388604, 0.90246872,\n",
       "            0.86660811, 0.77935068, 0.74164929, 0.71671059, 0.70763773,\n",
       "            0.69710085, 0.6759707 , 0.58890789, 0.58694644, 0.49180545,\n",
       "            0.44636946, 0.3806337 , 0.3716466 , 0.32079924, 0.30145747,\n",
       "            0.25419066, 0.23744847, 0.19632859, 0.18304959, 0.14540006,\n",
       "            0.1251248 , 0.11210197])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6571428571428571),\n",
       "    'tpr': np.float64(0.7333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.51428571, 0.51428571, 0.62857143, 0.62857143,\n",
       "            0.65714286, 0.65714286, 0.68571429, 0.68571429, 0.74285714,\n",
       "            0.74285714, 0.88571429, 0.88571429, 0.94285714, 0.94285714,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.53333333,\n",
       "            0.6       , 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.9468855 , 0.9405848 , 0.89319959, 0.8705367 ,\n",
       "            0.86493394, 0.64521153, 0.61253862, 0.58645341, 0.55983116,\n",
       "            0.53411252, 0.50431068, 0.48210594, 0.4785173 , 0.42757363,\n",
       "            0.41728861, 0.29625218, 0.27962394, 0.24518496, 0.17550199,\n",
       "            0.08143826])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6285714285714286),\n",
       "    'tpr': np.float64(0.6),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.37142857,\n",
       "            0.37142857, 0.42857143, 0.42857143, 0.48571429, 0.48571429,\n",
       "            0.62857143, 0.62857143, 0.71428571, 0.74285714, 0.74285714,\n",
       "            0.8       , 0.8       , 0.88571429, 0.88571429, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.6       ,\n",
       "            0.6       , 0.66666667, 0.66666667, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98424336, 0.97338633, 0.92394608, 0.72977931,\n",
       "            0.70757164, 0.66236389, 0.66081189, 0.6150279 , 0.60888158,\n",
       "            0.50631666, 0.47203377, 0.39103982, 0.37566068, 0.37271963,\n",
       "            0.33938098, 0.30288282, 0.25348302, 0.23349374, 0.18628418,\n",
       "            0.0867849 , 0.08288425])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6571428571428571),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.22857143, 0.22857143,\n",
       "            0.25714286, 0.25714286, 0.28571429, 0.28571429, 0.37142857,\n",
       "            0.37142857, 0.54285714, 0.54285714, 0.57142857, 0.62857143,\n",
       "            0.62857143, 0.68571429, 0.68571429, 0.8       , 0.8       ,\n",
       "            0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.2       ,\n",
       "            0.2       , 0.26666667, 0.26666667, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.8       , 0.8       , 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98692209, 0.98312912, 0.91772357, 0.90867051,\n",
       "            0.89037952, 0.88540714, 0.8530352 , 0.83771875, 0.75593885,\n",
       "            0.7462583 , 0.66737487, 0.61474266, 0.6002869 , 0.54729731,\n",
       "            0.53706938, 0.47507513, 0.41568947, 0.32492188, 0.29354248,\n",
       "            0.28827949, 0.19552694, 0.08788794])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.6285714285714286),\n",
       "    'tpr': np.float64(0.6666666666666666),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.2       , 0.2       , 0.22857143, 0.22857143,\n",
       "            0.34285714, 0.34285714, 0.57142857, 0.57142857, 0.6       ,\n",
       "            0.6       , 0.68571429, 0.68571429, 0.74285714, 0.74285714,\n",
       "            0.8       , 0.8       , 0.82857143, 0.82857143, 0.85714286,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.95650304, 0.95058472, 0.94755914, 0.92552397,\n",
       "            0.91078319, 0.85064397, 0.81957679, 0.81878978, 0.81524115,\n",
       "            0.68419758, 0.68159715, 0.60938837, 0.51875066, 0.51434849,\n",
       "            0.51150574, 0.44448218, 0.42095236, 0.39980779, 0.39026161,\n",
       "            0.36179919, 0.35536854, 0.35406017, 0.3497139 , 0.29141889,\n",
       "            0.15829522])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8),\n",
       "    'tpr': np.float64(0.7333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.2       , 0.2       ,\n",
       "            0.31428571, 0.31428571, 0.34285714, 0.37142857, 0.45714286,\n",
       "            0.45714286, 0.48571429, 0.48571429, 0.54285714, 0.54285714,\n",
       "            0.74285714, 0.74285714, 0.82857143, 0.82857143, 0.88571429,\n",
       "            0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.26666667, 0.26666667,\n",
       "            0.4       , 0.4       , 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98849662, 0.98497986, 0.94729498, 0.94480778,\n",
       "            0.9106422 , 0.90049356, 0.89836737, 0.86277298, 0.83454226,\n",
       "            0.81303217, 0.80247324, 0.74930227, 0.71345758, 0.68443728,\n",
       "            0.5643556 , 0.55877461, 0.49503167, 0.48713603, 0.34193509,\n",
       "            0.28329769, 0.06957948])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7428571428571429),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.14285714,\n",
       "            0.14285714, 0.22857143, 0.22857143, 0.4       , 0.4       ,\n",
       "            0.42857143, 0.45714286, 0.51428571, 0.51428571, 0.6       ,\n",
       "            0.6       , 0.65714286, 0.65714286, 0.68571429, 0.68571429,\n",
       "            0.71428571, 0.71428571, 0.77142857, 0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98609292, 0.97637313, 0.97206378, 0.93159504,\n",
       "            0.917524  , 0.87318958, 0.84988122, 0.81509053, 0.80549202,\n",
       "            0.78650628, 0.78347221, 0.73286702, 0.693211  , 0.63445602,\n",
       "            0.6287914 , 0.59809369, 0.56773775, 0.55206116, 0.53945611,\n",
       "            0.51697071, 0.51398609, 0.48613026, 0.48568198, 0.16192593])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8857142857142857),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.25714286, 0.25714286,\n",
       "            0.28571429, 0.28571429, 0.37142857, 0.37142857, 0.48571429,\n",
       "            0.48571429, 0.6       , 0.6       , 0.74285714, 0.74285714,\n",
       "            0.82857143, 0.82857143, 0.88571429, 0.88571429, 0.91428571,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9792458 , 0.96901928, 0.89703474, 0.89007548,\n",
       "            0.87784891, 0.8598749 , 0.83655558, 0.83133037, 0.76944102,\n",
       "            0.74516532, 0.70174256, 0.6665749 , 0.64451246, 0.63465837,\n",
       "            0.60300255, 0.57685664, 0.56307574, 0.54986501, 0.49778107,\n",
       "            0.47822428, 0.42570684, 0.39215358])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.7142857142857143),\n",
       "    'tpr': np.float64(0.8666666666666667),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.2       , 0.2       , 0.28571429, 0.28571429, 0.31428571,\n",
       "            0.31428571, 0.54285714, 0.54285714, 0.85714286, 0.88571429,\n",
       "            0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.96263225, 0.9583126 , 0.94671604, 0.94541571,\n",
       "            0.94082849, 0.93665471, 0.91851035, 0.9181478 , 0.90693456,\n",
       "            0.89122266, 0.8270235 , 0.81904699, 0.81695188, 0.80487157,\n",
       "            0.7541604 , 0.6328675 , 0.60513359, 0.41559421, 0.36176976,\n",
       "            0.33030954, 0.25781085, 0.23184983])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.17142857,\n",
       "            0.17142857, 0.34285714, 0.34285714, 0.45714286, 0.45714286,\n",
       "            0.51428571, 0.51428571, 0.54285714, 0.54285714, 0.6       ,\n",
       "            0.6       , 0.62857143, 0.74285714, 0.74285714, 0.77142857,\n",
       "            0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99348098, 0.99010714, 0.98377017, 0.97195979,\n",
       "            0.96615497, 0.91924452, 0.91517269, 0.85628125, 0.83023385,\n",
       "            0.81642169, 0.77297929, 0.76494778, 0.75707277, 0.70510925,\n",
       "            0.69933736, 0.69221737, 0.59925868, 0.58735432, 0.56472547,\n",
       "            0.52087985, 0.29400284])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8571428571428571),\n",
       "    'tpr': np.float64(0.8),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.08571429, 0.08571429,\n",
       "            0.2       , 0.2       , 0.22857143, 0.22857143, 0.31428571,\n",
       "            0.31428571, 0.57142857, 0.57142857, 0.77142857, 0.77142857,\n",
       "            0.8       , 0.8       , 0.85714286, 0.88571429, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.97932545, 0.96876565, 0.95894078, 0.94478147,\n",
       "            0.88889433, 0.86575132, 0.85901573, 0.85042659, 0.80334266,\n",
       "            0.7767604 , 0.67726551, 0.6608159 , 0.60549624, 0.59332863,\n",
       "            0.58867005, 0.58356507, 0.52186217, 0.49115131, 0.42978599,\n",
       "            0.37407362, 0.26921737])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.17142857, 0.17142857,\n",
       "            0.28571429, 0.28571429, 0.48571429, 0.48571429, 0.54285714,\n",
       "            0.54285714, 0.6       , 0.6       , 0.62857143, 0.62857143,\n",
       "            0.65714286, 0.65714286, 0.68571429, 0.74285714, 0.74285714,\n",
       "            0.77142857, 0.77142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.73333333,\n",
       "            0.73333333, 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98752377, 0.98736087, 0.98561073, 0.98505078,\n",
       "            0.98355124, 0.97429279, 0.97269073, 0.97122466, 0.967305  ,\n",
       "            0.95382966, 0.95247609, 0.89144566, 0.88413651, 0.85327196,\n",
       "            0.83030129, 0.80276518, 0.80207874, 0.78273257, 0.75231419,\n",
       "            0.73288162, 0.6728595 , 0.59672737, 0.57114749, 0.56094825,\n",
       "            0.54328331, 0.54171754, 0.12449705])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8571428571428571),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.11428571, 0.11428571, 0.4       ,\n",
       "            0.4       , 0.42857143, 0.42857143, 0.48571429, 0.48571429,\n",
       "            0.51428571, 0.51428571, 0.57142857, 0.57142857, 0.74285714,\n",
       "            0.77142857, 0.77142857, 0.8       , 0.8       , 0.85714286,\n",
       "            0.85714286, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.73333333, 0.73333333, 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99104961, 0.98267026, 0.97756265, 0.926988  ,\n",
       "            0.92340322, 0.91320966, 0.9128214 , 0.90345612, 0.89647433,\n",
       "            0.8925332 , 0.87379996, 0.85475533, 0.83824134, 0.71856016,\n",
       "            0.71771314, 0.68368152, 0.67914489, 0.65473493, 0.58083722,\n",
       "            0.53314391, 0.38787387, 0.38655452, 0.35400606])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9142857142857143),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.2       , 0.2       ,\n",
       "            0.37142857, 0.37142857, 0.4       , 0.4       , 0.48571429,\n",
       "            0.48571429, 0.51428571, 0.51428571, 0.54285714, 0.54285714,\n",
       "            0.65714286, 0.65714286, 0.82857143, 0.82857143, 0.85714286,\n",
       "            0.85714286, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.6       , 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99689624, 0.99279071, 0.96987053, 0.96805334,\n",
       "            0.92598033, 0.92444954, 0.91736337, 0.90698254, 0.88370273,\n",
       "            0.88191201, 0.88190273, 0.86654906, 0.86279813, 0.84237821,\n",
       "            0.80295422, 0.76984811, 0.69242512, 0.63445699, 0.63106462,\n",
       "            0.6003187 , 0.46244685])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.8857142857142857),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.11428571, 0.11428571, 0.31428571,\n",
       "            0.31428571, 0.34285714, 0.34285714, 0.57142857, 0.57142857,\n",
       "            0.65714286, 0.65714286, 0.68571429, 0.68571429, 0.8       ,\n",
       "            0.8       , 0.91428571, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.13333333, 0.13333333, 0.2       ,\n",
       "            0.2       , 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99512085, 0.98526837, 0.98195799, 0.97608171,\n",
       "            0.97329675, 0.96999849, 0.96939521, 0.96768025, 0.93550138,\n",
       "            0.92681093, 0.9180862 , 0.91554518, 0.81345564, 0.79721498,\n",
       "            0.77273854, 0.76439912, 0.7548557 , 0.72237406, 0.64803819,\n",
       "            0.52882411, 0.4264225 , 0.32004692, 0.30342709])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9142857142857143),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.17142857, 0.17142857,\n",
       "            0.4       , 0.4       , 0.42857143, 0.42857143, 0.45714286,\n",
       "            0.45714286, 0.51428571, 0.51428571, 0.71428571, 0.71428571,\n",
       "            0.8       , 0.8       , 0.82857143, 0.85714286, 0.85714286,\n",
       "            0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98121708, 0.97874373, 0.97805264, 0.96528955,\n",
       "            0.96290087, 0.96042029, 0.96028677, 0.95449414, 0.95266898,\n",
       "            0.92221174, 0.92169586, 0.91994779, 0.91712106, 0.91428208,\n",
       "            0.90250843, 0.89764393, 0.89207652, 0.81567193, 0.79997747,\n",
       "            0.75677496, 0.72871682, 0.70091676, 0.67705496, 0.58938763,\n",
       "            0.53034453, 0.51809384, 0.25783858])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9142857142857143),\n",
       "    'tpr': np.float64(0.9333333333333333),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.25714286,\n",
       "            0.25714286, 0.34285714, 0.34285714, 0.42857143, 0.42857143,\n",
       "            0.51428571, 0.54285714, 0.54285714, 0.62857143, 0.62857143,\n",
       "            0.65714286, 0.65714286, 0.85714286, 0.85714286, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.6       , 0.6       , 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98625177, 0.97831029, 0.97359513, 0.94010122,\n",
       "            0.93958282, 0.92704943, 0.92563565, 0.89730766, 0.89617335,\n",
       "            0.8872018 , 0.87671089, 0.87302535, 0.84871469, 0.83308042,\n",
       "            0.83014276, 0.80967803, 0.65789492, 0.56597279, 0.48139106,\n",
       "            0.39021479, 0.37652531])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9142857142857143),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.28571429, 0.28571429, 0.37142857, 0.37142857,\n",
       "            0.51428571, 0.51428571, 0.54285714, 0.54285714, 0.57142857,\n",
       "            0.68571429, 0.68571429, 0.71428571, 0.71428571, 0.74285714,\n",
       "            0.74285714, 0.88571429, 0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.98970946, 0.98847163, 0.98421544, 0.9755358 ,\n",
       "            0.96417144, 0.95120513, 0.94691045, 0.93988087, 0.91240067,\n",
       "            0.85709958, 0.84662175, 0.84643962, 0.84593559, 0.84166578,\n",
       "            0.80789452, 0.80416939, 0.80096778, 0.79701577, 0.78335787,\n",
       "            0.78119605, 0.66077712, 0.65816365, 0.39225667])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9428571428571428),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.14285714, 0.14285714, 0.25714286,\n",
       "            0.28571429, 0.28571429, 0.68571429, 0.68571429, 0.71428571,\n",
       "            0.71428571, 0.74285714, 0.74285714, 0.77142857, 0.77142857,\n",
       "            0.82857143, 0.82857143, 0.85714286, 0.85714286, 0.88571429,\n",
       "            0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.2       , 0.2       , 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99226707, 0.98972242, 0.98702196, 0.98363411,\n",
       "            0.97994377, 0.97901535, 0.97384575, 0.97292052, 0.97020798,\n",
       "            0.96880001, 0.96812719, 0.89829777, 0.89639633, 0.88327598,\n",
       "            0.88010094, 0.86729417, 0.85128889, 0.84633253, 0.84133266,\n",
       "            0.80392174, 0.80391511, 0.7106529 , 0.70256926, 0.6572258 ,\n",
       "            0.60771276, 0.40444902])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(0.9428571428571428),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.11428571, 0.11428571, 0.17142857, 0.17142857, 0.2       ,\n",
       "            0.2       , 0.25714286, 0.25714286, 0.28571429, 0.28571429,\n",
       "            0.45714286, 0.48571429, 0.6       , 0.6       , 0.62857143,\n",
       "            0.62857143, 0.82857143, 0.82857143, 0.91428571, 0.91428571,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.26666667, 0.26666667, 0.33333333, 0.33333333,\n",
       "            0.4       , 0.4       , 0.46666667, 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99457399, 0.99392088, 0.99082478, 0.98842146,\n",
       "            0.98631859, 0.98257445, 0.97922164, 0.97903515, 0.97060375,\n",
       "            0.96886751, 0.96794382, 0.96786987, 0.96210954, 0.95992277,\n",
       "            0.93243893, 0.92988767, 0.91868142, 0.91482792, 0.91073711,\n",
       "            0.9085927 , 0.83445633, 0.79325893, 0.74199596, 0.57605527,\n",
       "            0.36911364])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.02857143, 0.05714286, 0.05714286,\n",
       "            0.08571429, 0.08571429, 0.37142857, 0.37142857, 0.45714286,\n",
       "            0.48571429, 0.57142857, 0.57142857, 0.6       , 0.6       ,\n",
       "            0.71428571, 0.71428571, 0.85714286, 0.85714286, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\n",
       "            0.13333333, 0.26666667, 0.26666667, 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.66666667, 0.66666667, 0.73333333,\n",
       "            0.73333333, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99442422, 0.98588149, 0.98532454, 0.98280253,\n",
       "            0.98086921, 0.96526246, 0.94704945, 0.93936982, 0.93707552,\n",
       "            0.93517824, 0.92494014, 0.92422343, 0.91948521, 0.91945124,\n",
       "            0.89002191, 0.88785983, 0.84251679, 0.82938398, 0.75262715,\n",
       "            0.71149317])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.08571429, 0.08571429, 0.37142857,\n",
       "            0.37142857, 0.4       , 0.4       , 0.42857143, 0.42857143,\n",
       "            0.45714286, 0.45714286, 0.6       , 0.62857143, 0.65714286,\n",
       "            0.65714286, 0.68571429, 0.68571429, 0.74285714, 0.74285714,\n",
       "            0.77142857, 0.77142857, 0.97142857, 0.97142857, 1.        ,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
       "            0.46666667, 0.46666667, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.93333333, 0.93333333,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99652781, 0.99553758, 0.99379167, 0.97972376,\n",
       "            0.97891994, 0.97737221, 0.97368692, 0.97357271, 0.9728017 ,\n",
       "            0.97116091, 0.97079372, 0.9553511 , 0.9550174 , 0.95354724,\n",
       "            0.94492231, 0.94319534, 0.93350086, 0.91571806, 0.91331247,\n",
       "            0.88550507, 0.88392829, 0.84239145, 0.77904521, 0.69255869,\n",
       "            0.64717176])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.11428571,\n",
       "            0.11428571, 0.34285714, 0.34285714, 0.62857143, 0.62857143,\n",
       "            0.74285714, 0.74285714, 0.85714286, 0.85714286, 0.88571429,\n",
       "            0.88571429, 0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.66666667, 0.66666667, 0.8       , 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99397315, 0.99251785, 0.98718035, 0.98494183,\n",
       "            0.98253609, 0.97379917, 0.9634088 , 0.95197472, 0.94748893,\n",
       "            0.94317592, 0.93409435, 0.91585643, 0.89225875, 0.87822086,\n",
       "            0.87124818, 0.86303418, 0.85645838, 0.75268633])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.22857143, 0.22857143, 0.28571429, 0.28571429,\n",
       "            0.34285714, 0.34285714, 0.4       , 0.42857143, 0.48571429,\n",
       "            0.48571429, 0.51428571, 0.51428571, 0.74285714, 0.74285714,\n",
       "            0.82857143, 0.82857143, 0.85714286, 0.85714286, 0.88571429,\n",
       "            0.88571429, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.86666667, 0.86666667,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99737666, 0.99617782, 0.99472451, 0.99392837,\n",
       "            0.99238488, 0.98788068, 0.98775485, 0.98564823, 0.98498075,\n",
       "            0.98445778, 0.98050801, 0.97393933, 0.97225827, 0.97030477,\n",
       "            0.97000107, 0.96436397, 0.95866405, 0.92141328, 0.91845866,\n",
       "            0.90738337, 0.90067937, 0.89588825, 0.89304077, 0.89008846,\n",
       "            0.79293189, 0.60052706])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.22857143, 0.22857143, 0.34285714, 0.34285714,\n",
       "            0.37142857, 0.37142857, 0.51428571, 0.51428571, 0.57142857,\n",
       "            0.57142857, 0.62857143, 0.62857143, 0.65714286, 0.74285714,\n",
       "            0.74285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.26666667, 0.26666667,\n",
       "            0.33333333, 0.33333333, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.8       , 0.8       ,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99839697, 0.99625568, 0.99507637, 0.9949974 ,\n",
       "            0.99372116, 0.98732753, 0.98670581, 0.98168615, 0.9800742 ,\n",
       "            0.97952343, 0.9732052 , 0.96322414, 0.96301227, 0.95676689,\n",
       "            0.95598055, 0.94558953, 0.93853802, 0.93813903, 0.92315386,\n",
       "            0.87583879, 0.76141285])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.11428571,\n",
       "            0.11428571, 0.22857143, 0.22857143, 0.4       , 0.4       ,\n",
       "            0.42857143, 0.42857143, 0.45714286, 0.45714286, 0.48571429,\n",
       "            0.54285714, 0.54285714, 0.57142857, 0.57142857, 0.91428571,\n",
       "            0.91428571, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.53333333,\n",
       "            0.53333333, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99914375, 0.99398093, 0.99203896, 0.99174006,\n",
       "            0.99165329, 0.99065748, 0.99049751, 0.98660521, 0.98543584,\n",
       "            0.98400288, 0.983923  , 0.98268761, 0.97741666, 0.97637406,\n",
       "            0.97553579, 0.97117902, 0.96400392, 0.96361428, 0.87865008,\n",
       "            0.81673095, 0.7772224 , 0.77692814, 0.6922756 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.11428571, 0.11428571, 0.14285714, 0.14285714,\n",
       "            0.34285714, 0.34285714, 0.42857143, 0.45714286, 0.62857143,\n",
       "            0.62857143, 0.71428571, 0.71428571, 0.82857143, 0.82857143,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.2       , 0.2       , 0.26666667, 0.26666667, 0.53333333,\n",
       "            0.53333333, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99682148, 0.99444847, 0.99434666, 0.99344101,\n",
       "            0.99304037, 0.99289659, 0.99141088, 0.9900137 , 0.98603128,\n",
       "            0.9701617 , 0.96982388, 0.96586709, 0.96569046, 0.94825953,\n",
       "            0.94046264, 0.91521517, 0.88428228, 0.85590743, 0.84596412,\n",
       "            0.6989599 ])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.17142857, 0.17142857, 0.25714286,\n",
       "            0.25714286, 0.31428571, 0.31428571, 0.34285714, 0.34285714,\n",
       "            0.37142857, 0.4       , 0.45714286, 0.45714286, 0.57142857,\n",
       "            0.57142857, 0.6       , 0.6       , 0.82857143, 0.82857143,\n",
       "            1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.6       , 0.6       , 0.66666667, 0.66666667, 1.        ,\n",
       "            1.        ]),\n",
       "     'thresholds': array([       inf, 0.99682864, 0.98734025, 0.98610307, 0.98380608,\n",
       "            0.97863944, 0.97777126, 0.97659314, 0.97514936, 0.97422313,\n",
       "            0.97269478, 0.97182707, 0.9708461 , 0.96615554, 0.96073879,\n",
       "            0.96011322, 0.95846473, 0.95764737, 0.93561595, 0.89684726,\n",
       "            0.85092497])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.28571429, 0.28571429, 0.6       , 0.62857143,\n",
       "            0.71428571, 0.71428571, 0.74285714, 0.74285714, 0.8       ,\n",
       "            0.8       , 0.91428571, 0.91428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.33333333, 0.33333333,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.8       , 0.8       , 0.86666667, 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9977323 , 0.99682983, 0.99522314, 0.99418984,\n",
       "            0.99337077, 0.98599115, 0.98338525, 0.97255942, 0.97117223,\n",
       "            0.96271136, 0.95596765, 0.95578732, 0.95549418, 0.94515668,\n",
       "            0.93344271, 0.91142003, 0.90775693, 0.78875304])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.22857143, 0.22857143, 0.31428571,\n",
       "            0.31428571, 0.37142857, 0.37142857, 0.42857143, 0.42857143,\n",
       "            0.48571429, 0.51428571, 0.51428571, 0.6       , 0.6       ,\n",
       "            0.68571429, 0.68571429, 0.71428571, 0.71428571, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.26666667, 0.26666667, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.9957085 , 0.98987146, 0.98978062, 0.98869624,\n",
       "            0.98810495, 0.98702621, 0.98414246, 0.98137809, 0.97959594,\n",
       "            0.97767859, 0.97636519, 0.97095882, 0.96691399, 0.96609594,\n",
       "            0.96219838, 0.95927084, 0.95632401, 0.94790732, 0.85709419])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
       "            0.05714286, 0.05714286, 0.28571429, 0.28571429, 0.31428571,\n",
       "            0.4       , 0.4       , 0.45714286, 0.45714286, 0.6       ,\n",
       "            0.6       , 0.62857143, 0.62857143, 0.68571429, 0.68571429,\n",
       "            0.94285714, 0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
       "            0.26666667, 0.33333333, 0.33333333, 0.4       , 0.46666667,\n",
       "            0.46666667, 0.66666667, 0.66666667, 0.73333333, 0.73333333,\n",
       "            0.8       , 0.8       , 0.86666667, 0.86666667, 0.93333333,\n",
       "            0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99839349, 0.99836722, 0.99779363, 0.99601646,\n",
       "            0.99572746, 0.99466678, 0.99200974, 0.99177954, 0.99136266,\n",
       "            0.98837526, 0.98681279, 0.98612337, 0.98593984, 0.98085324,\n",
       "            0.97907699, 0.9788612 , 0.97846059, 0.97497029, 0.97444058,\n",
       "            0.94828239, 0.94296642, 0.75559828])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.08571429, 0.08571429, 0.17142857,\n",
       "            0.17142857, 0.2       , 0.2       , 0.25714286, 0.25714286,\n",
       "            0.31428571, 0.31428571, 0.57142857, 0.57142857, 0.65714286,\n",
       "            0.65714286, 0.71428571, 0.71428571, 0.82857143, 0.82857143,\n",
       "            0.85714286, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.6       , 0.6       , 0.66666667, 0.66666667,\n",
       "            0.73333333, 0.73333333, 0.8       , 0.8       , 0.86666667,\n",
       "            0.93333333, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99519542, 0.99449356, 0.99431654, 0.99241601,\n",
       "            0.99076006, 0.99062589, 0.98827853, 0.98758531, 0.98669425,\n",
       "            0.9843349 , 0.98383615, 0.97852027, 0.97823727, 0.97621883,\n",
       "            0.97518705, 0.9721197 , 0.96727361, 0.95033294, 0.94978215,\n",
       "            0.94906555, 0.93459607, 0.9125737 , 0.91045131])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.02857143, 0.02857143, 0.05714286,\n",
       "            0.05714286, 0.14285714, 0.14285714, 0.2       , 0.2       ,\n",
       "            0.48571429, 0.48571429, 0.51428571, 0.51428571, 0.54285714,\n",
       "            0.54285714, 0.6       , 0.6       , 0.68571429, 0.68571429,\n",
       "            0.8       , 0.82857143, 0.91428571, 0.91428571, 0.94285714,\n",
       "            0.94285714, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.2       , 0.2       ,\n",
       "            0.26666667, 0.26666667, 0.33333333, 0.33333333, 0.4       ,\n",
       "            0.4       , 0.46666667, 0.46666667, 0.53333333, 0.53333333,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99777188, 0.99486336, 0.99334112, 0.99187568,\n",
       "            0.991746  , 0.99025238, 0.99012948, 0.98897232, 0.9885865 ,\n",
       "            0.98128338, 0.98121784, 0.98079828, 0.9793412 , 0.97888291,\n",
       "            0.97855688, 0.97811149, 0.97698817, 0.97437649, 0.97251792,\n",
       "            0.96378421, 0.95945642, 0.95341851, 0.9507305 , 0.94506043,\n",
       "            0.93503189, 0.92144158])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.        , 0.05714286, 0.05714286, 0.08571429,\n",
       "            0.08571429, 0.17142857, 0.17142857, 0.2       , 0.2       ,\n",
       "            0.22857143, 0.22857143, 0.34285714, 0.34285714, 0.62857143,\n",
       "            0.62857143, 0.68571429, 0.68571429, 0.82857143, 0.82857143,\n",
       "            0.94285714, 0.94285714, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.06666667, 0.06666667, 0.13333333, 0.13333333,\n",
       "            0.26666667, 0.26666667, 0.4       , 0.4       , 0.46666667,\n",
       "            0.46666667, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.93333333, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99763766, 0.9922375 , 0.99207903, 0.99146789,\n",
       "            0.98994713, 0.98875326, 0.98738006, 0.98706458, 0.98701056,\n",
       "            0.98639948, 0.98483934, 0.98310668, 0.98192955, 0.97302129,\n",
       "            0.97257539, 0.96816593, 0.9672452 , 0.95955695, 0.95591929,\n",
       "            0.94154059, 0.93151568, 0.91806802, 0.89616341, 0.84453876])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.25714286, 0.25714286, 0.37142857,\n",
       "            0.37142857, 0.4       , 0.4       , 0.42857143, 0.42857143,\n",
       "            0.45714286, 0.45714286, 0.6       , 0.6       , 0.65714286,\n",
       "            0.65714286, 0.71428571, 0.71428571, 0.74285714, 0.74285714,\n",
       "            0.82857143, 0.82857143, 0.85714286, 0.85714286, 0.91428571,\n",
       "            0.91428571, 0.97142857, 0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.26666667, 0.33333333,\n",
       "            0.33333333, 0.4       , 0.4       , 0.46666667, 0.46666667,\n",
       "            0.53333333, 0.53333333, 0.6       , 0.6       , 0.66666667,\n",
       "            0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
       "            0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99741599, 0.99373233, 0.99350582, 0.99194717,\n",
       "            0.99083222, 0.99047296, 0.99033211, 0.98972752, 0.98790752,\n",
       "            0.98788044, 0.98695956, 0.9826392 , 0.98262701, 0.98074557,\n",
       "            0.97649564, 0.97335351, 0.97332033, 0.97309351, 0.97243435,\n",
       "            0.97006411, 0.96842957, 0.96840697, 0.96553324, 0.95489903,\n",
       "            0.95299257, 0.93668433, 0.91315299, 0.90800639])}},\n",
       "   {'model': LogisticRegression(),\n",
       "    'fpr': np.float64(1.0),\n",
       "    'tpr': np.float64(1.0),\n",
       "    'threshold': 0.5,\n",
       "    'full_roc': {'fpr': array([0.        , 0.02857143, 0.28571429, 0.28571429, 0.42857143,\n",
       "            0.45714286, 0.51428571, 0.51428571, 0.54285714, 0.54285714,\n",
       "            0.57142857, 0.57142857, 0.68571429, 0.68571429, 0.8       ,\n",
       "            0.8       , 0.85714286, 0.85714286, 0.88571429, 0.88571429,\n",
       "            0.91428571, 0.91428571, 0.94285714, 0.94285714, 0.97142857,\n",
       "            0.97142857, 1.        ]),\n",
       "     'tpr': array([0.        , 0.        , 0.        , 0.06666667, 0.06666667,\n",
       "            0.13333333, 0.13333333, 0.2       , 0.2       , 0.33333333,\n",
       "            0.33333333, 0.53333333, 0.53333333, 0.6       , 0.6       ,\n",
       "            0.66666667, 0.66666667, 0.73333333, 0.73333333, 0.8       ,\n",
       "            0.8       , 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n",
       "            1.        , 1.        ]),\n",
       "     'thresholds': array([       inf, 0.99862138, 0.99535125, 0.99516932, 0.99264219,\n",
       "            0.99191697, 0.99115444, 0.99059664, 0.98998973, 0.98934378,\n",
       "            0.98898741, 0.98796195, 0.98706178, 0.98704404, 0.98505361,\n",
       "            0.98372335, 0.97937209, 0.97809261, 0.97743715, 0.97742168,\n",
       "            0.97711549, 0.97636907, 0.97492945, 0.97252969, 0.97221726,\n",
       "            0.96235266, 0.94359908])}}]],\n",
       " 'roc_results': {'fpr': array([0.        , 0.        , 0.01666667, 0.01666667, 0.03333333,\n",
       "         0.03333333, 0.05      , 0.05      , 0.08333333, 0.08333333,\n",
       "         0.1       , 0.1       , 0.11666667, 0.11666667, 0.13333333,\n",
       "         0.13333333, 0.25      , 0.25      , 0.26666667, 0.26666667,\n",
       "         0.28333333, 0.28333333, 0.38333333, 0.38333333, 0.46666667,\n",
       "         0.46666667, 0.48333333, 0.48333333, 0.51666667, 0.51666667,\n",
       "         0.6       , 0.6       , 0.66666667, 0.66666667, 0.75      ,\n",
       "         0.75      , 0.85      , 0.85      , 1.        ]),\n",
       "  'tpr': array([0.        , 0.03846154, 0.03846154, 0.07692308, 0.07692308,\n",
       "         0.15384615, 0.15384615, 0.19230769, 0.19230769, 0.26923077,\n",
       "         0.26923077, 0.30769231, 0.30769231, 0.38461538, 0.38461538,\n",
       "         0.42307692, 0.42307692, 0.46153846, 0.46153846, 0.5       ,\n",
       "         0.5       , 0.53846154, 0.53846154, 0.61538462, 0.61538462,\n",
       "         0.69230769, 0.69230769, 0.73076923, 0.73076923, 0.76923077,\n",
       "         0.76923077, 0.80769231, 0.80769231, 0.88461538, 0.88461538,\n",
       "         0.96153846, 0.96153846, 1.        , 1.        ]),\n",
       "  'thresholds': array([       inf, 0.88279248, 0.84809481, 0.8012451 , 0.77175351,\n",
       "         0.72529105, 0.72396799, 0.69750671, 0.67492331, 0.5879227 ,\n",
       "         0.58531341, 0.55461517, 0.55367125, 0.46644325, 0.45209814,\n",
       "         0.44940669, 0.37904469, 0.36130677, 0.33079278, 0.31058761,\n",
       "         0.3077971 , 0.30643804, 0.26886057, 0.26371299, 0.23524845,\n",
       "         0.22967817, 0.22853979, 0.21801881, 0.2027592 , 0.20099719,\n",
       "         0.1727766 , 0.16992574, 0.14360841, 0.12646516, 0.11219609,\n",
       "         0.09904212, 0.08209747, 0.08102902, 0.02138885]),\n",
       "  'name': 'Logistic Regression',\n",
       "  'auc': np.float64(0.6711538461538461),\n",
       "  'model': LogisticRegression()},\n",
       " 'test_loader': <torch.utils.data.dataloader.DataLoader at 0x7c072ffbb7a0>}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predict_ensemble_and_evaluate import save_to_pickle, load_from_pickle\n",
    "from predict_ensemble_and_evaluate import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate import plot_roc_comparison\n",
    "from predict_ensemble_and_evaluate import make_curve_monotonic\n",
    "\n",
    "# Save the data\n",
    "save_to_pickle(list_folds_best_models, list_folds_weighted_clfs, results_original_roc, test_loader, filename='pickle/Logistic_breast_cancer_undersampling.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2ef92",
   "metadata": {},
   "source": [
    "# Load pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a43e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from predict_ensemble_and_evaluate import save_to_pickle, load_from_pickle\n",
    "from predict_ensemble_and_evaluate import predict_ensemble_and_evaluate\n",
    "from predict_ensemble_and_evaluate import plot_roc_comparison\n",
    "from predict_ensemble_and_evaluate import make_curve_monotonic\n",
    "\n",
    "# Load the data\n",
    "list_folds_best_models, list_folds_weighted_clfs, results_original_roc, test_loader = load_from_pickle(filename='pickle/Logistic_breast_cancer_undersampling.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47987f50",
   "metadata": {},
   "source": [
    "### Ensemble prediction using voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a9ef57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a single ensemble from 39 models across all folds.\n",
      "Extracting full dataset...\n",
      "Getting predictions from all models...\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n",
      "Warning: Skipping model of unsupported type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Loop through all the FPR values to generate the ensemble ROC curve ---\n",
    "\n",
    "\n",
    "\n",
    "results_tuple, prior_proba = predict_ensemble_and_evaluate(list_folds_best_models=list_folds_best_models,\n",
    "    test_loader=test_loader)\n",
    "\n",
    "ensemble_results_soft = results_tuple['soft_voting']\n",
    "ensemble_results_hard = results_tuple['hard_voting']\n",
    "misclassification_risk = results_tuple['misclassification_risk']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056832ab",
   "metadata": {},
   "source": [
    "## Load NP curve pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3873fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained ROC curve points loaded from pickle/Logistic_Breast_Cancer_NP_roc_curve.pkl\n"
     ]
    }
   ],
   "source": [
    "from predict_ensemble_and_evaluate import load_from_pickle_constrained_roc\n",
    "\n",
    "# Load the constrained ROC curve results\n",
    "constrained_points = load_from_pickle_constrained_roc(filename='pickle/Logistic_Breast_Cancer_NP_roc_curve.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32a10a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plotting all ROC curves for comparison ---\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6oAAAOqCAYAAACVfKgeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOydd1hU19aH3zOV3qUozYY9GmPvLbagMWqiMU1jism9JrnJl97Lvemm56b33ouxXDWxm9hLbKBIEQQFpMPU/f0xzsDIAAMMMup+n4cHOGWffc6sc+b89lp7LUUIIZBIJBKJRCKRSCQSicRLULV2ByQSiUQikUgkEolEIqmJFKoSiUQikUgkEolEIvEqpFCVSCQSiUQikUgkEolXIYWqRCKRSCQSiUQikUi8CilUJRKJRCKRSCQSiUTiVUihKpFIJBKJRCKRSCQSr0IKVYlEIpFIJBKJRCKReBVSqEokEolEIpFIJBKJxKuQQlUikUgkEolEIpFIJF6FFKoSiQsSExNRFMXpR6/XExsby6WXXsrixYtbu4tNwn4u5wp//vknN9xwA507dyYgIAB/f386derE/Pnz2bhxY2t3z2sYNWoUiqKwevXq1u6KW5hMJj788EOmTZtGfHw8vr6++Pn50aFDB2bOnMnnn3+O0Wh02udsO8dzhfT0dBRFITExscWP9dhjj6EoCo899liLHwtgx44dqNVqFi5c6LR89erVtb4fFEUhICCAHj16cNttt5Gent5g+0IIvv76a6ZPn05cXBw+Pj6EhobSp08f7rnnHjIzM93qZ0FBAU8//TSjRo0iOjoanU5HUFAQPXv25MYbb+T333932r64uJjw8HAGDhyIEMLt6+GKptyrkvr56KOPUBSFuXPntnZXJJJWRwpViaQehg4dynXXXcd1113H5MmT0Wg0/PLLL0yZMoU777yztbt33mI0Gpk/fz6DBw/m/fffRwjBhAkTmDRpEiqVig8++IChQ4dy/fXXn/MvSWf65b2l2b59O126dOH666/nl19+ITw8nEsuuYTk5GQiIiL46aefuPrqq0lKSqKioqK1u+sVnAsi3S7+Ro0a1dpdcbBw4UJ8fX15+OGH69zG/v1w7bXXMnDgQNLT03nttdfo1asXmzZtqnO/nJwcBg0axOzZs/npp5+Ijo5m2rRpDB8+nOzsbJ5//nmSkpJ444036u3jp59+SmJiIg888AB//vknSUlJzJgxgzFjxmA2m3nvvfcYO3YsV1xxhWOf4OBg7r//fjZv3swnn3zS+AtzCnmvSiSSFkdIJJJaJCQkCEB8+OGHTstNJpP45z//KQABiM2bN7dOB5vI/v37xf79+1u7G83msssuE4AIDw8Xv/76a631S5YsEW3atBGAmD59eiv08Mzx6KOPCkA8+uijdW6TkZEh9u/fL8rLy89cx5rAtm3bhJ+fnwBEcnKySEtLq7XN8ePHxf333y90Op04efKkY/nIkSMFIP74448z12EvoTXP3Wg0iv3794tDhw41q50//vhDAGLkyJF1bnPixAmxf/9+ceLEiWYdyx2+/fZbAYi777671jp7X129QmVmZorOnTsLQHTv3t1l24WFhaJDhw4CEBdeeKH4+++/ndabTCbxwgsvCLVaLQDxyiuvuGznv//9rwCEoiji3nvvFcXFxbW22bt3r7j88stFnz59nJZXVlaKNm3aiJiYGFFVVVXndaiL5tyrkvopKioS+/fvFzk5Oa3dFYmk1ZFCVSJxQV1CVQjbF3xQUJAAxMMPP3zmO3ee88477whAaLVasWXLljq32759u9BqtQIQ77333hns4ZnFHaF6NmA0Gh0v79OmTRMWi6Xe7Tdv3iwqKioc/0uhenafuztC9UwyZMgQAYgDBw7UWlefUBVCiM8//9yx/vDhw7XWz5kzRwCiffv29Qq4119/3fGs27dvn9O6/fv3O55vixYtavB81qxZU2vZ7bffLgDx8ccfN7h/TZp7r0okEom7SKEqkbigPqEqhBAXXXSRAMRNN93kcv3KlSvFZZddJqKjo4VWqxVt2rQR06ZNExs3bqzzmOXl5eKll14SQ4cOFSEhIUKn04n4+HiRnJwsPv/8c5f7fPvtt2LChAkiIiJCaLVa0bZtW3HVVVeJvXv3utz+9JerkydPCh8fH6FSqcTRo0fr7NuMGTMEIF5++eVm9eHIkSMCEAkJCcJsNosXX3xR9OnTR/j7+9f50lcTq9Uq2rdvLwCxcOHCBre/7bbbBCA6dOggrFarY3nNl+Ly8nJx//33i44dOwq9Xi9iYmLE9ddfX+/1KCwsFI888ojo3bu3CAgIEL6+vqJnz57iySefdOm1rCkmMzIyxPXXXy9iY2OFRqMR1113nWO777//XsyfP1/06NFDhISECL1eLxITE8W8efNcvjDbP09XPzXbrUvIXHfddQ47T0tLE1dffbWIiooSOp1OdOjQQTz44IN1elvsXp8ePXoIvV4v2rRpI2bOnCn27t0rPvzww1p9aIiPPvpIAEKn04ljx465vZ+rc9yxY4e47LLLRHh4uNDpdKJbt27ihRdecLIBO8ePHxevvPKKmDRpkkhMTBQ+Pj4iMDBQXHTRReKZZ54RlZWVLo9X81764IMPxKBBgxwDWEeOHBFCCJGeni6eeeYZMXr0aBEXFyd0Op0IDg4WQ4cOFW+99Va9L/iFhYXi8ccfFxdddJEICgoSPj4+on379uLyyy8XS5YsEUI4CyZXP6c/v1rCbmve06eTkpIi5s2bJxITE4VOpxP+/v4iPj5eTJ48WXzwwQe1PjtXPzXbbWhQ5uDBg+KWW24RSUlJwtfXVwQGBopu3bqJW265RezZs6fOa30627dvF4AYNGiQy/UNCdU9e/Y41p/+zD98+LBQqVQCEN9//329/bBaraJ3794CEHPnznVaN3fuXAGI3r17u7Rrd9ixY4cAxIABAxq1X3PvVSFs33dPP/20uPDCCx222L17d/Hggw+KwsLCWtvXtDOLxSJeeeUV0atXL+Hr6yuio6PFzTffLAoKCoQQQlRVVYknnnhCdOnSRfj4+IiYmBhx2223ibKyslrt1rSp9PR0cc0114jo6Gih1+tF586dxaOPPupSZBuNRvHpp5+KOXPmiC5duojAwEDh4+MjkpKSxMKFC0V2drbL8675nFq7dq1ITk4WERERQlEUx/1a3/NzxYoVIjk5WURGRgqNRiNCQkJEp06dxFVXXeVyMMJkMon//ve/YvDgwSIoKEjo9XrRqVMnsXDhwjq/42ra9nfffSeGDh0qAgMDhZ+fnxgyZIj47bffXO4nkbQEUqhKJC5oSKjaQ7tceVTvuusuAQiVSiUGDBggLr/8cjFw4EChKIpQq9VOL2h2MjMzRffu3QUg/Pz8xMUXXyxmz54thg8fLoKDg2u9BJpMJnHFFVcIQOj1ejFkyBBx+eWXO15qfH19xdKlS2sdx9XL1ZVXXikA8fTTT7s81/z8fKHT6YROpxP5+fnN6oP9ZSM+Pl5MnTpV6HQ6MXbsWHHllVeKCy64wOXxa7Jz507HOdTnTbWzdetWx/a7d+92LLe/aA4ePFgMGjRI+Pn5icmTJ4vLL79cxMTECEBER0eLlJSUWm3u3btXxMXFCUDExMSIiRMniilTpoioqCgBiD59+oiioiKnfewvQ3PmzBFhYWEiOjpazJgxQ0yfPl3cddddju3UarXw8/MT/fr1E9OnTxdTp051eC78/f3Fhg0bnNq97rrrHNe7d+/e4rrrrnP8vPvuu47tGhKqt99+uwgKChIJCQniiiuuEOPGjRO+vr4Oj8npWCwWkZyc7HhZHT9+vJg1a5bo0KGD8PPzc4THN0ao2sO5p0yZ4vY+NbGf43333ecQp7NnzxYjR450hFDefvvttfb79NNPBSDatWsnRo4cKWbPni3Gjh0rAgICHDbiSqzb7eqf//ynUKlUYtiwYeLKK68UAwcOFOnp6UIIIZ588kmH52zs2LGO/uh0OkdYuiuRsXPnTtGuXTsBiODgYDF58mQxa9YsMXjwYOHr6+vwOu7fv19cd911DtubMGGCkw2sW7fO0WZL2W1dQnXPnj0O4d6lSxcxffp0cfnll4vBgweLgIAA0bt3b8e2Tz/9tJgwYYIARFRUlNM51Lw/6hOqn3/+udDr9Y7ny4wZM8Rll10mevfuLRRFaVTEwSOPPCIA8dBDD7lc35BQ3bBhQ50e1ZdfflkAIiQkRJhMpgb78sILLwiwTXOw24rVahXh4eECEC+++KLb5+UK+xSJxoSZNvdeLSgoEH369BGACAoKElOnThUzZswQERERjvvFPthjp6adXXnllcLX11dMnDhRTJs2TURGRgqwhVGXlZWJYcOGOdpNTk4WwcHBAhCTJk2q1Re7TV177bUiPDxcREVFicsvv1wkJyc7BlCHDh1aa8AqKyvLcX8OGjRIXH755WLy5Mmibdu2AhBt2rQRqamptY5nf07deuutQqVSie7du4vZs2eL8ePHiy+++EIIUbdQ/eijj4SiKEJRFDFw4EAxa9YsMXXqVNG3b1+hVqtrPd+qqqrEuHHjBCB8fHzEpEmTxKxZsxzPgYiICLFt27ZafbTb7iOPPCIURRFDhw4Vs2bNcnzXKIoifvjhBzc+aYmk+UihKpG4oD6hum/fPseL7+liyR6W2qlTJ7Fr1y6ndWvWrBGBgYFCp9M5CSCLxSL69esnADF+/Hhx/Phxp/0qKytrjWA+8MADAhADBw6sNTfo22+/FWq1WoSGhtYKK3P1crVixQoBiK5du7q8Fq+88ooAxIwZM5rdB/vLBiBiY2PFwYMHXR6zLt5//32HOHLnJc9kMjlEQc0Bgpovmp06dRIZGRmOdZWVlQ4P8ukelYqKCtGxY0fHS6zBYHCsKy8vd4j+efPmOe1nfxkCxNVXX12nl/Krr76qNepvtVrFG2+8IQDRo0ePWsLGndDfhoQqIB588EFhNpsd6/bs2eN4UTvdK2S3iZiYGCdPr9lsdoQTNlao2l+ennjiCbf3cXWOgHjrrbec1q1atcoxUJSVleW0bt++fWLTpk212issLBTjx48XgHjuuedqrbcfKygoyOX+QthCHl158rKzsx0vfd98843TurKyMse1uPbaa0VpaanT+qKiIrFixQqX515X6G9L2m1dQnXevHkCEE899ZTL/pzu/XEn9LcuW9+6davQarVCURTx6quv1vJUp6eni61bt9bZ7ukMGzZMAHV6jhoSqvZnY69evWrdr9dcc40AxOjRo93qy5o1axzHsj9nDx8+7Fi2du1at8/LFVOnThWA+PTTT93ep7n36qxZsxzfHTUHP0tLS8WkSZMEIIYMGeK0T83vjo4dOzoGg4SwDabaB4979eolBgwY4NRuWlqaCA0NFYBYv369U7s1bfzSSy918p5mZWWJpKQkxwBYTUpKSsTPP//sdC8JYfO03n///QIQkydPrnXuNZ9Tb7zxhsvrU5dQtUcT1RyAspOXlye2b9/utOzee+91XK+awt9oNIr58+c7BgVOPwd7/0JCQsSff/7ptM5+vZKSklz2XSLxNFKoSiQucCVUi4qKxPLly0XXrl1djrZbLBbHaGpdL0XPPfecAJy8BD/99JPjpf/0l1JXFBQUCF9fX+Hj41Nn6M6tt94qAPHaa685LXf1cmW1Wh3n6yo02T7yvXjx4mb3oebLxieffNLguZ7OM888I8Dm7XSX6OhoAYhnn33Wsazmi+ZPP/1Ua5+8vDxHopCaXkx78pLk5GSXxyotLXWEZNUMX7N/uYeFhdXyWrnL4MGDBVArpNoTQvWiiy5y6dlbsGCByxdSu5f37bffrrWPwWBweAMbI1R9fHxcikx3sZ9jXcmzJk6c2Gi7O3jwoABE//79a62z209TX9aXL18uAHH55Zc7Lbd73Pr06eM0cFAfDQnVlrTbuoTq5MmTBVDr5bkumiNUp02bJsC96QDuYB+gcZUgqGZfaz5LrVaryMzMFM8//7zQ6XQiNDTUZbI9ux3Onj3brb4cOHDAcay//vpLCCHEn3/+6VjmakpAY7CLqn/9619u79OcezUjI0OoVCqhKEqtwVwhhDh69Kij/ZrP3prfHa4GEBYtWiTA5u1zNTi0cOFCAYjHH3/cabndpnx9fV2GMf/666+OAam6pgG4om3btkKlUomSkhKn5fZ7dcyYMXXuW5dQ9fPzE8HBwW4dv7Ky0hEV8ssvv9RaX15e7oimOH1qkf06v/rqq7X2q6qqcnioMzMz3eqLRNIcZHkaiaQe5s2b56iRFxISwoQJE0hNTeWzzz7jySefdNp2x44d5OTk0LFjRy666CKX7dlLL9Ss8bls2TIA5syZQ0BAQIN9+uOPP6isrGTo0KG0a9fO7ePUhaIoXHfddYCtfltNdu7cyc6dO4mJiWHixIke7cOMGTMa7JsnEPXUCQwJCWHq1Km1lkdGRjrOt2bJj99++w2AWbNmuWwvICCAfv36YTab2bJlS63148aNIzg4uN7+Hjp0iNdff5077riD+fPnM3fuXObOnUteXh4ABw8erHf/ppCcnOyyvm63bt0AyM7Odiw7evQoaWlpgM1mT0en0zFz5kyP99FdpkyZ4nK5q3OxY7FYWLVqFU8++SS33nor8+bNY+7cufz73/8G6r/mDZ2rwWDg119/5ZFHHmHBggWOtt9++22XbdufB/Pnz0etVtfbtrucCbs9nQEDBgBwyy23sHz5cqqqqhrZa/ewWCysWLECgJtuuqnZ7ZWXl1NeXg5AeHh4g9vbvx9UKhXx8fHcfffdxMXFsXv3bvr379/s/tT3/PIE9nO0P19amrVr12K1Wrnwwgu54IILaq1v164dEyZMAGzfM6ej0WgYP358reWdO3cGID4+np49e9a5Picnx2W/xo8fT3R0dK3lycnJhIeHU1JSwvbt22ut37VrF4sWLWLhwoVcf/31jue12WzGarVy6NAhl8dryjNywIABFBcXc+2117Jt2zasVmud227dupWysjLCwsJcPhP9/PyYPXs24Po6g+tnqV6vp0OHDoDrZ6lE4mk0rd0BicSbGTp0KJ06dQLgxIkTrFu3jtLSUm655RY6d+7seBkDHC/vhw8fdvnSX5MTJ044/s7IyACga9eubvXJfpxVq1Y16jj1MW/ePJ588km+/vprXn75ZXx9fQH48MMPAbj22mudXpqb24fIyEj8/Pzc6ltNIiIiACgsLMRsNqPR1P8IM5vNFBYWAtCmTZta6xMTE+vsf/v27QGbMLNjP+9rrrmGa665pt5juzrvxMTEOre3WCz885//5O2336735bSkpKTe4zaF+Ph4l8uDgoIAnESG/XpERETUObBS33nWRZs2bcjKyuL48eON3rcmjTkXgNTUVC677DL27t1bZ5v1XfP6zvXPP/9k1qxZZGZmut12Y58H7tCSdlsXd999N+vXr2flypVMnDgRrVZL7969GTFiBLNnz/aIiAMoKChwCMsuXbo0u73i4mLH34GBgQ1ubx/kM5lMHD58mL/++ovDhw8zZ84cVq5ciU6nc9re/gxzVxjWvB/sz7Caz7Ljx48367zt98XJkyfd3qc596pd3Nifr67o2LGj07Y1iYmJcfnctz+L6rr/7Z9lXQMm9fUnMTGRgoICp++C8vJyrrnmGn788cc694O6nx1NuafefPNNkpOT+fTTT/n0008JDAykf//+jBkzhmuuucbp3Jt7naHxz1KJpCWQQlUiqYcbbriBuXPnOv4vLi7msssu448//uCKK65g3759DsFlH92Mjo52jAjXhf1lpSnYj9OpUyeGDh1a77buvuwmJiYyevRofv/9d3788UfmzJmDyWTiiy++AGxC1pN9sAvhxmL3VBuNRnbs2NHgy+7OnTsxmUxO+zaWmqLRft4TJ04kKiqq3v0SEhJqLavvvF955RXeeustoqOjWbRoEUOGDCEqKgofHx/A5r388ssvW8TDolI1PrimvgGKhgYvXHHRRReRlZXl0qPXGBp7LjNnzmTv3r0kJydzzz330L17d4KCgtBqtRiNRvR6fb371/WZVlRUMG3aNPLy8pg3bx633HILnTp1IigoCLVaTUpKCl26dGlxjxm0rN3WhZ+fHytWrGDLli0sW7aMjRs3snHjRrZu3cqiRYu49dZbeeONNxrdbksTEhLi+Lu0tNTxUl4Xp0ehbNiwgUmTJrFu3ToeeughnnvuOaf1F110EZ999hnbt293a7Bt8+bNgM3zaRc3iYmJhIWFUVhYyJYtWxg+fLh7J+cCuzAPDQ11ex9P3atNoaH7uynPMnepea/ef//9/Pjjj3Tt2pVnnnmG/v37ExER4RiYGDJkCJs2barz/m7KPdWtWzcOHjzI//73P37//Xc2btzIunXr+P3333niiSd4//33ufrqq5t2ci5oyWspkbiLFKoSSSMIDg7m66+/pmvXrmRkZLBo0SIeeughAOLi4gDbC8XpLy/1YR+1PHDggFvb24/TpUuXRh2nIebNm8fvv//Ohx9+yJw5c/j111/Jz89nyJAhtUbsW6oPDdG7d28SExNJT0/nk08+aVCofvLJJ4Dtxa5Xr1611qenp9e5r31dbGysY1lcXBwHDhxg/vz5Hg9v/eabbwB4++23XYYjp6amevR4TcUe6n3ixAnKy8vx9/evtU1917UuLr30Un766SeWL19OXl5eg4LKExw4cIDdu3cTGRnJjz/+WEs0NOear127lry8PPr27csHH3xQa31dbcfHx7N//34OHDjAuHHjmnz8mrSk3TZE//79Hfep2Wzmp59+4tprr+XNN99k5syZjB49ulnth4eH4+fnR0VFBQcPHnQZ9tkY/Pz88Pf3p7y8nIKCggaF6ukMHTqUl156iRtuuIFXXnmFBQsWOEIlwRZOedddd1FcXMzPP/9c7xQIIQSffvop4Byer1KpmDJlCh9//DGffPIJd955ZxPO1EZBQQFAo+635tyr9ueH3cvvCvu6uqaVtARHjhypc52r7wL78/rrr792GcLcUs9rjUbD5MmTmTx5MmDz2C5atIjHH3+cm2++mcsuuwx/f3/HtavvvFrjOkskjUUOl0gkjaRNmzYOcfrCCy9QVFQE4BhR3bdvX71hhKdjnwv55ZdfOkLY6mPs2LHodDpWr17d7DDJmsyYMYPg4GB+//13srKyHGG/p3tTW7IPDaEoCvfddx9gE3Rbt26tc9sdO3bw1ltvAbbRb1devqKiIn799dday0+cOOGYK2ifawswadIkoPolxZPYQ5RdebT27t3Lzp07Xe5nH8E3m80e75Mr4uLiHJ6dL7/8stZ6o9HI999/3+h2r7rqKhITEzEajdxyyy31zr8C2LZtG5WVlY0+Tk3s17xt27YuPVufffZZs9uuK3yurrbtz4MPPvgAi8Xi1rEasoGWtNvGoNFomDlzpiPipKZNN9WO1Wo1F198MQDvvvuuR/rZt29fAPbt29ek/a+//nr69OmD0Wjk8ccfd1rXsWNHrrjiCsAWHm3//nDFm2++ye7du9FoNNx9991O6+699160Wi27du3i5ZdfbrBP69atc7n877//BhoXcdKce3XEiBGoVCp27tzJrl27am177Ngxx7O3uYMYjeF///ufy++yJUuWUFBQQGBgoNM1qu95vXz5cvLz81uuszUICgriscceIyQkhIqKClJSUgDo168fAQEBFBYW8ssvv9Tar7Kykq+++go4s9dZImksUqhKJE3g1ltvJT4+nuLiYl588UUAtFotjz76KEIILrvsMtavX19rP4vFwu+//86ff/7pWDZ16lQuvPBCcnJyuPzyyx0j3HaqqqpYunSp4/+oqCgWLlxIeXk5U6ZMYc+ePbWOYzAY+OWXX9z20oItFGn27NlYrVaeffZZli1bhp+fn8sELC3VB3e46aabmDp1KiaTiYkTJ7J48eJa2yxbtowJEyZgMpmYOnUqN954Y53t3XXXXU5zjwwGA//4xz8oLy9nwIABTqHNN910EwkJCXz77bfce++9lJaW1movNze3SS/M9mQ/b7zxhtOL37Fjx7j22mvrfIG3j/I3ZnCkudx2220APProo44XI7CFmN5///1kZWU1uk2tVss333yDj48PP/74I9OmTXPpDSgsLOThhx9m6NChGAyGpp8EkJSUhFqtZs+ePU5JswB+/fVXXnrppSa3bf88V61aVUvwvPPOO3z99dcu97vhhhuIjY1lx44d3HjjjbUGr0pKSli5cqXTsoZsoCXtti7efPNNl0mocnNzHQNMNV/y7eeQmprqCNd3lwcffBCNRsPrr7/Om2++WSvcMiMjg23btrndnv3FfdOmTY3qhx1FUfjPf/4DwOeff+50j4DtHk9MTOTIkSOMGTOm1udmNptZtGgRt99+OwDPPvssPXr0cNqmW7duLFq0CIA777yTBx54wOXnmpKSwpVXXum4Z0/Hfo5jxoxx+/yac6/Gx8dz+eWXI4Tg5ptvdvq+Ky8v56abbqKqqoohQ4YwZMgQt/vUXCorK7nlllucBr9ycnK46667AFiwYIFjGgZU39+vvfaaUzsHDx5kwYIFHu9fRUUFixYtcjmHfN26dRQVFaFWqx33kY+PD//4xz8A23ecfe472OZT33777eTm5tK+fftWTX4nkTRI6yQblki8m/rqqNr54IMPBCACAwNFQUGBY/ndd9/tSO/eo0cPcemll4rZs2eLUaNGiZCQEAGI//73v05tpaeniy5dughA+Pn5ifHjx4srr7xSjBgxQgQHB9cq/WAymcScOXMEIFQqlbjwwgvFjBkzxKxZs8TQoUMd5RWWLl3qtJ+9X3VRs+wBp+o41kVT+lBXKYvGUlVV5VQDtFOnTmLGjBli5syZjnp6gLjmmmtc1n60l5cYPHiwGDhwoPDz8xPJycniiiuucJQYioyMdFn64e+//xaJiYmOOnMjRowQc+bMEdOmTRPdu3cXiqKIqKgop33cKSHz559/Omq+durUSVxxxRVi4sSJwtfXV/To0UNcdtllLm0yNzfXqTD93Llzxfz5853qxjZUnqYuO6+rTILZbHbUO9Tr9WLixIli9uzZomPHjsLX19dRmujGG2+s83zrYvPmzY77T1EU0bdvXzFz5kxxxRVXiIEDBzpqGHfo0MGp5mFDJVrq+gzsdV9VKpUYOXKkuPLKK0Xfvn0Fp0pQ1XXPNHQvCSHEpZdeKsBW93f8+PFi9uzZomvXrkJRFPHggw/WeS9s377dUVYpJCREXHLJJWLWrFliyJAhwtfXt1YJl8WLFzuOk5ycLK6//noxf/58p/IeLWW3dd3T9jqx7du3F1OmTBFXXXWVGD9+vPD19XWU5zi9FrK9nnSXLl3EVVddJebPny/uvfdet/rz8ccfC61W6+jLzJkzxfTp00WfPn2Eoij1nsPpbN++XQBiwIABLtc3VEfVzogRIwQg5syZU2vd0aNHHeerKIro37+/mD17tpg6dapo06aN4/N8+eWX6z3GBx984Lj/fXx8xIgRI8SVV14pLrvsMtGtWzdHP12Vw2noPBuiqfdqfn6+wz6Cg4PFtGnTxMyZMx3n3b59e6e6n0I0/N3RUHmjup5ldpu69tprRVhYmIiOjhaXX365mDJliuO6Dh482Kn/Qgjx/fffC0VRBNhqt86ePVuMGTNGaLVaMWbMGDFkyBCXz6OGnlN19fXkyZOO51Tv3r3FzJkzxZVXXikGDx7s6Mcjjzzi1E5VVZUYO3aso/zO5MmTxaxZs0R8fLwARHh4uMtSeg3ZtjvnIJF4CilUJRIXuCNUzWaz6N69u4DaxcA3bNggrrrqKpGQkCD0er0IDAwUSUlJYtq0aeK9995zqlVop7S0VDz77LOif//+IjAwUOj1epGQkCCmTp0qvvrqK5d9WLJkiZg+fbpo166d0Gq1IiQkRHTr1k3Mnj1bfPHFF6K8vNxpe3dernr06OHYzp0vosb0wVNC1c6GDRvEvHnzRMeOHYWfn5/w9fUVHTp0EHPnzq1V2L0mNV9qysrKxN133y3at28vdDqdiIqKEnPnzq23RlxJSYl47rnnxODBg0VISIjQarUiJiZG9O/fX9x999216tG688IvhBC7d+8WU6dOFTExMcLHx0d07txZ3HPPPaKkpKReUbl27Voxbtw4ERoaKlQqVa2XHE8LVSFsReOfe+450b17d6HX60VERIS47LLLxJ49e8QTTzwhAHH//ffXe751YTAYxHvvvSemTJki2rVrJ/R6vfDx8RHt27cXM2fOFF9++aUwGo1O+zRVqFqtVvH++++Liy66SAQEBIjg4GAxbNgwxz3XHKFqNBrF888/L3r16iX8/PxEWFiYGD9+vPjf//7X4L1w4sQJ8dBDD4levXoJf39/h23PmjVLLFu2rNb27777rujbt6+j/q+rz7Ul7Lau81i8eLG45ZZbxIUXXijatGkjdDqdiI2NFaNGjRIff/xxrc9PCFuNzTlz5oiYmBih0WhqtdtQf/bu3Svmz58v2rdvL/R6vQgODhbdu3cX//znP2vVH24Iu9DYt29frXXuCtWNGzc6xIWrdiwWi/jyyy/FpZdeKtq2bSt0Op0ICgoSvXr1EnfddVctsVYXJ06cEE899ZQYPny4aNOmjdBoNCIgIED07NlT3HTTTWLNmjUu97vtttsEID7++GO3juOKptyrQtjqeD799NOiT58+ws/PT/j4+Ihu3bqJBx54wOX3Y0sL1UcffVSkpaWJK6+8UkRFRQmdTic6deokHnnkkVrfo3bWrl0rxo4dKyIiIoSfn5/o2bOn+Pe//y0MBkOdz6OmClWTySTeeustceWVV4quXbuK4OBg4evrKzp27ChmzJghVq1a5bItk8kk3nzzTTFo0CARGBgodDqd6Nixo1i4cGGdNdClUJV4E4oQZyDloEQikXgRq1evZvTo0YwcObJWyKek+YwZM4Y//viD77//nunTp7d2dySSRvPdd99x+eWXc+eddzqmd5xLVFVVERcXh1ar5ciRIw1mtz5Xeeyxx3j88cd59NFHeeyxx1q7OxKJ5DTkHFWJRCKRNJqdO3diNBqdlhmNRh577DH++OMPIiMjHZkpJZKzjZkzZzJ06FDefvttt2uenk289tpr5Ofn8/TTT5+3IlUikXg/sjyNRCKRSBrNHXfcwc6dO+nduzcxMTGcPHmSPXv2cOzYMXx8fPj444+dko9IJGcbr732Gv369ePJJ5/k9ddfb+3ueIzi4mKeeeYZBgwYwLXXXtva3ZFIJJI6kUJVIpFIJI3mxhtv5PPPP2f37t1s3rwZIQRt27bl+uuv56677qJ79+6t3UWJpFlceOGFbpcIOpsIDg6ulV1eIpFIvBE5R1UikUgkEolEIpFIJF6FnKMqkUgkEolEIpFIJBKvQgpViUQikUgkEolEIpF4Fef9HFWr1UpOTg6BgYEoitLa3ZFIJBKJRCKRSCSSswohBKWlpbRt2xaVyjO+0PNeqObk5BAXF9fa3ZBIJBKJRCKRSCSSs5qsrCxiY2M90tZ5L1QDAwMB20UNCgpyuY3FYiEjI4OEhATUavWZ7J5E4hbSRiXejLRPibcjbVTi7UgblXg7J0+eJDEx0aGtPMF5L1Tt4b5BQUH1ClX7NvLhIPFGpI1KvBlpnxJvR9qoxNuRNirxduw26smplDKZkkQikUgkEolEIpFIvAopVCUSiUQikUgkEolE4lVIoeoGiqIQFxcnswJLvBZpoxJvRtqnxNuRNirxdqSNSrydlrDN836OqjuoVCrCw8NbuxsSSZ1IG5V4M9I+Jd6OtFGJtyNtVOLteKokjVObHm/xHMRisXDgwAHHJGGJxNuQNirxZqR9SrwdaaMSb0faqMTbaQnblELVTaqqqlq7CxJJvUgblXgz0j4l3o60UYm3I21Ucr4hhapEIpFIJBKJRCKRSLwKKVQlEolEIpFIJBKJROJVSKHqBiqVig4dOrTIJGGJxBNIG5V4M9I+Jd6OtFGJtyNtVOLttIRtyqy/bqAoCkFBQa3dDYmkTqSNSrwZaZ8Sb0faqMTbkTYq8XZaojyNHJZxA4vFwp49e2SmNYnXIm1U4s1I+5R4O9JGJd6OtFGJtyOz/rYi8sEg8XakjUq8GWmfEm9H2qjE25E2KjnfkEJVIpFIJBKJRCKRSCRehRSqEolEIpFIJBKJRCLxKhQhhGjtTrQmJSUlBAcHU1xcXOckdSEEVVVV+Pj4tMhEYYmkuUgblXgz0j4l3o60UYm3I21U4u0UFxcTEhJSr6ZqLNKj6iY6na61uyCR1Iu0UYk3I+1T4u1IG5V4O9JGJecbUqi6gdVqZc+ePVit1tbuikTiEmmjEm9G2qfE25E2KvF2pI1KvJ2WsE0pVCUSiUQikUgkEolE4lVIoSqRSCQSiUQikUgkEq9CClWJRCKRSCQSiUQikXgVMuuvm1l/rVYrKpVKZlqTeCXSRiXejLRPibcjbVTi7UgblXg7MutvK2I0Glu7CxJJvUgblXgz0j4l3o60UYm3I21Ucr4hhaobWK1WDh48KDOtSbwWaaMSb0bap8TbkTYq8XakjUq8HZn1VyKRSCQSiUQikUgk5zxSqEokEolEIpFIJBKJxKuQQtVN1Gp1a3dBIqkXaaMSb0bap8TbkTYq8XakjUrON2TWXzey/kokEolEIpFIJBKJxDUtoamkR9UNhBCUlJRwnmt6iRcjbVTizUj7lHg70kYl3o60UYm30xK2KYWqG1itVtLS0mSmNYnXIm1U4s1I+5R4O9JGJd6OtFGJtyOz/kokEolEIpFIJBKJ5JxHClWJRCKRSCQSiUQikXgVUqi6iY+PT2t3QSKpF2mjEm9G2qfE25E2KvF2pI1Kzjdk1l+Z9VcikUgkEolE4m1kZ0NhYd3rw8KgXbuz93insFgt/JX9F8fLjxPpH8nAdgNRq86RUjzn+md46ngWYeGP7M3Mv/Nm9vzlOU2l8UgrHmLt2rU8//zzbNu2jWPHjvHjjz8ybdq0evdZvXo1d955J3v37iUuLo6HHnqIuXPnerRfVquVkydPEhoaikolndAS7+Nst9H8inx+2P8D07tNJ8IvorW7I/EwZ7t9Ss59znYb3bphK28teosFdy6g39B+LXKMv//+m48++oi5c+fSs2dPzx+gKh+O/gCx08GnxvdAfj788ANMnw4R3vv94PHrk50NAwZAWRkAZosFs8mERqtFY6+nGhAAmzd7RHj88dln9L7pJgIVBa2mDnnQjOPVdX2WpC7hwVUPcvjkYSzCglpR0zG0I/8e+28md57cpHPJzMzk4MGDdOnShfj4+Ca1cTqff/45O3bsoEePHvTs2ZNOnToRGhpa/06nfYYuqXFNf/zxR9atW0dsbCwzZ85sfN8bOJ7BYKBMUfjxvvu44dFHG9d2Pcdb0uYkDw41cDjYimV685utiVc9jcvLy+nduzdvvPGGW9sfOXKESy65hNGjR7Nz507uuOMObrjhBpYvX+7RfgkhyMrKkinBJV7L2W6j+RX5vLPtHfIr8lu7K5IW4Gy3T8m5z9lso0IIvv76a/Zl7OPrr79ukXMQQvDVV1+xdetWvvrqq5a5ToZ8OPSO7XdN8vPhnXdsv72UFrk+hYU2waFSIXQ6DEJgUqsxCIHQ6UClsq2vz3vmJlarld+/+w4fkwmD2Wxr//SfZhyvruuzJHUJV/9wNSmFKejVegJ1gejVelIKU7j6h6tZkrrEZVtGi7HOnypTFfsO7qOssox9B/dRZapyWm+xWursZ11tVhgq2LpjK2ZhZvfe3Rw9dpTUtFQMZoNjG7PVXLvBU5+hUaNg9NHW/tEoGCtKMebnUWmsZO2GtVgVK0cyj7Bn3x6qTFWu2z2FyWJy7mt+nq09F8cz+Ggpw4LaauLvjeuoNFS6325dP/l5/Bx5kqsnVXIw1IrWAgGGegyhCXiVR3XSpElMmjTJ7e3feust2rdvz4svvghAt27dWL9+PS+99BITJkxoqW5KJBKJRCKReAV79uxh78G9aNGy9+Be9uzZwwUXXODxY+zcuRM/Pz927tzZIsc4m2nR66PRYALMioLGakVrMmG1WFCDTUCezv798MIL1f8/8AB07Oi8TWUl/POfjn83d+xIaWkpABbAYLXio9dDaSlYTgk7tdomVk/n7bdtHkGA2FhMDz3Orl1QVVW9SdXPbzH0i48ZKoDUw3ze8TLiOvXh9s0PUmk0oFX5UFxVgsVqBEAAZpWFO355CP8BE1ArNg+yYqiizas3MqLn9wBY9b5YtfrTOmRBXV4CwlYqRazXY/UJcKy9s8sbjIi8jKhf3iFw318AGCPaknnDk8xY3wmLqBaGKpMBlaESq9WC0Fip1NuO9f6O99H+rUP5nxbFYkFdWcbd2X25Kr8LR6+6j6q4zgD4HYYLzDDg6jKO+9YQyIoCKKf+EZh+nYjxZzMabRkqIUALX6x/D2VHEDPj/sXVifc5do369T0C924CYEbP3zkYXK0MFYsZ7Q2nvKk1B0sUFQLhWFaqW8eOB0fx9YFqz/bROfdSFZ8EwKN7rmZX0ToANGVFjm1qXW+zieIplZhVoAiwKhBapeBJvEqoNpZNmzYxbtw4p2UTJkzgjjvuqHMfg8GAwVD9oZaUlABgsViwnLoZFUVBpVJhtVoRQmCxWBBCYLVaUavVju3s2Lc/fblKpUJRFJfLoXa9obqWq9Vqx/FPX27vY0PLTz+nhvouz+nsOye7rZ4t51RQWUBhVSGp+ak8ue5J9ufvJ/mLZHw1vra2VGq0am11O6ce6gLnY7bGcoXqB7Gr5d7QR686JwGVlZX4bvaF077DztpzaqXl8pxaaHkNG1WUs+OctKVadIU6LBssGEuM+Fb6UmQu4r7Z99G1uCuoQahre/bS/NIo1hbXWn46QaYgOlZ0RCA46H+QIl0ROqsOo8rIvXPupUt5F0f/9gXsw6Bu2JUSaYikXVV12Ki/XyX+fpW0iShk8rhNhIcXU/zxICqqNNy/MsZ21gIIBRZe7rLNmd1K6N+2bu+QnaMlGl7bEt7gdgC39iskIdjU4Hbbjvnw9d5g8io0VJpUqBWBRSjctuA6ovzMKDWed4+PPI6PpmFP69JDAazO8CeurIrHy8swVSig04OioLJa0VgsYLFgFQIwkvv+SEzR1a/z+nQTEd+WOP4/4fcD7+S3YX9+tcDwMVtYtDHV8f+fXbpAm6hT/wkqK8rQmIpQV1rh1Ne/UANqxXG85zZGUFCpZv7+HC46YTtejr+epzb+4HQ+w2IriNpRzugTBagUsAp4+qer8O2fxzFLMSAot1bYPudTl0cBFKORI6ZdPPlSEhGlfrZzs1h5fOtBlCSLzfKUKhTh/KVi1gSimM0op95/DGYLFWajY/1PPzzO5qK3uOnv/QzKPW67Rv5+WKM/QOOb69SWYhKojMLpfgbbu1J5eRkKAo3Vgr/JgmbfJnS7/uKH46tJC/bDqmhZOHoMCINDNNsRQthOUggUwFx1ErNKh4/FYhOqgAkQVgNB6U8Rl7XIsW/o2lL89trOR9dOoPat7ptirRajzlixH9JO1clj+P6+0/H/OsvfZLYJBqCw7TbKVSUoikKQofo+MFpMmGoOjggzFrVNpHpWnlZzVgvV3NxcoqKinJZFRUVRUlJi+8Lx9a21z9NPP83jjz9ea/nevXsJCLCNuISFhREfH8/Ro0cpLCxECEFpaSknTpygbdu2pKenO0aeAOLi4ggPDyc1NZWqGkNIHTp0ICgoiH379jkJhi5duqDT6dizZ49TH3r16oXRaOTgwYOOZWq1ml69elFaWkpaWppjuY+PD127duXkyZNkZWU5lgcGBtKxY0eOHz9Obm71DXf6OdmJjo4mOjpantNZfk6HDh2itLSUvXv3oijKWXFOKwpX8NnBz2zzUk6F4mSXZKMoCoqiEKwNJkQb4ther9ejVquprKx0Erw+Pj6oVCoqKiqczsnPzw+r1er0WSuKgq+vLxaLxWnASqVS4ePjg9lsxmis/kJTq9Xo9XpMJhMmU/XDWqPRoNPpMBqNmM3Vo69arRatVovBYHC67jqdDo1GQ1VVlZPoP5/OSaVSYRVWKiucXyjP5nM6Fz+n8/mcrFYrFZUV3n9OVojKj6LLb11QZ6rZFbcLjdBgVptRW9SUaEowlhvxU/wwBBhAAdUpj4qwnvo51VZ9QlgIgdVspURbQom2BI1VgyIUNFYNJdoSipVigkxBKIpia1s0LLKtVivCanvxtworPbumMmTg34SGlKLX2a5fcFA5/v4KanUkFqsKq2joFVigUmoeWwFqv6yrFFdLXaNSBCoFl+3UbF+lCKosClVmBZUiUBRQIagyK1RZFCdhamtT1GqjdttOmg1FpUIodV8D53ZBUYSTaKj221Vz+ielqjEobBu3UTBaFWq/RVdfG+tp/Ty933aOV6gR5ZpT19P2ORws0CPKodLHZh2uzk4RIFSCKq3Z0WZDn59VpQOl4VmNLq98HZe4rmPaB+Bd9d5xHeye0wZM2IrKMUBWE5PZSlVAexRTnut+uqkOXWlXAViF4vhcbMsaJzet1P35eQqvzfqrKEqDyZSSkpKYN28e999/v2PZkiVLuOSSS6ioqHApVF15VOPi4igsLHRkqJKeOnlO8pxa/pwOFR7iH0v/QW5ZLqE+oRQbirlz0J10DuuMSqUiWB9MuG/16PfZcE7n4uckz0mekzwn7zinspwyMpdnkrEsg4rcCixmC2tVa8nV5KKoFQxmA8GBwZgx061zN/654J/4hldHqAghsAorFRUVDvGsVp3q+2mv42qV2ibOfX149dVX2b17N+Fh4SgqBWEVFBQWcMEFF3DbbbehUlSUl5djtjjPpVM4dU5WZ0Hu5+tnOyerBVXpMXS7fyKs8n0UYznCDCVbojHlaykRCtbI9oigGNi6Betl06uT+AQGog4JRgjQaTXotNVCq67PCaC0vMLF56FgtToLbX8/X3RabYOfk8Fo5NWPv2Zf6hFCgwMBm7fsZEkpPTp3YMGcGSiKgtVqJcDP12Fz9dleZVUVVQYj+rR0Otx+L1UCzKe215rNaE99dooQqHx8OP7eqxg7tq++xqlphLz57ilxIjh5+y2UREdhsVpRqRQsFitKZRXxjz+DAI4czeG3hERO+Pvz7zVrMKlUWE7ZXbDVinKqj0JjC/09/t5rmDt1oLi0DCEEEV9+h/+uv1EUheP6SKam/KPGGQk0oT9xceWfTCs44Vh6/eBIjvQ5gsmv4pSKUk7ZTPVnoDb6YFUpJCx+Ar+8bgDohYGnKp/kmpv2AlCp+GBUqj18AUG2wQJtebmj3xatBpPOh4oy2xHi/1pAaNZgrin6nn6VuwE4ponkUZ+7Sbn2SoSq+jPXCwN+qurB1YrTygNpNFqE0Yq2spL5a9uSvCuC5wPnk6aJR6WC7qoc3su5gyE3VZEfUP1ZixpnqgDlPj5Y1Sp8DQaUU3ZoUasxaLVMDJnOgafnYU8rNK/iKwYYtwNw61VpHGhTfc00wkyIKOF0yWk/sn1ZqU5HlxMxvPtLFRXF7QAVLwXO55A2EYCMsU9THvM3IAi2VjtHqhQ9RlWNSDfFiMnH5g1XAK3VFvp78OGiczPrb2OJjo4mLy/PaVleXh5BQUEuRSrYRif1+tPj2W0PB7XaORV2zQfJ8ePHiYyMdGzripZcriiKy+V1ZSds7HJ5Tmf3OSmK4rDRmvt66zlVmCr49/p/U1RVRIfQDtw/7H4WLl3IoLhBdI3o6nJ/ydlLzWfo2ZhRVXLu4602aiw3krYijZTFKeTuqI5U8Q3wRXWRirK9ZUQGRlJSVIKx2IjOV0egXyAZxzIwaU10S+zWrOPv3r2b1NRUQkJC0PtUvzuFhISQmppKaWlp4+diHj8Oa9fafrZsgX4Z0L4SjvtDjJGIrpfAkBnEDBwIfn5w4ABcfTVcOge6etf3w+7duzly7AQh4eH4+vlVr9DqOXLsBJbw+KbPVW2zBzM2b7Xd/2rSaDBrtSAEKqsVHyC65wDo1at6vwFj4KobHP/6Am1dtX/J5fz222+8//77KIpCQkmJ02ohBEZ/f9tcVUAxmcBoJLpHf+jVi1j7hmOmOPbZuR72Dq9u49FHd7NhQy57tV04pL8Qi2LhYPRuMkIPgbCiUlRYlVNTllDhqw5Ap9IjhKBSU057vy68+/q/HHNUASziUv5bVR3NZcdoPEpVVcqp/2rewzaZ5uOThE4XS/CkCPw0AUCyQ8BFAW8Bxyr/cmpzx47vOX58f/WCcudjhoe3oUuPnvj59SQ0uT2KJoh7aqz3O7wHn1s1/PBbMBYXmZQVixlhqOCjSyZyPDTUeZqAWQEDxPrEcsuXHdDrE06tGenY5omqbMyiOpLDN+MAXR++HKtWi1BrTl2XSiwWWxSIyirQWq08P7A/x/1D+PcwPZ07DyU5+U7+VaNfBYY+GK01JhnXQcmWP3kh50rSQyDA2DKe1bNaqA4ePJglS5wzgq1YsYLBgwd79DhCCHJzc2nTpo1H25VIPMXZZKNmq5n7Vt7HvhP7CPYJ5rVJr1Fpbnh+keTs5WyyT8n5iTfZqLAKsjdnk7I4hfQ/0jEbTnnQFIW2A9qSlJxE4qhEHv/34xgtRgLVgbY8BQjMZjN+aj8qKyv55ptv6NWrl8uQQrf6IQTffPMNlZWV+Pn5OUWj2cOR3TqGEHDoEKxZYxOn+/ZVr/MxQ1IVBIXBRXdBxWcw5J8Q7F2C1BUeuz51YLVasVhsczHtsqtmmKVitWIBVFZrk0p4WK1WvvvuO1tbNfqnEgJOeSMN5eXoVSrbMc11Z591jeDYsW+wWisJCPCjTFfAtphtlOhLbHNQBeiEFoNitc2F1Nsy/lqsFirMFfjp9Lw89SlGdj59oFwNJNY6lz/+OEyliHR5rYUQ+PoYGD06voGBqOp2zWYzfy0/RrAIrnNrS4GJKH0IbaOs9OkTWDt8OBjQQKcyAS7mJwuzwFAhCDMHYDh1HEdY/qlpUMV5xVgsBxkyJM5F308rE7SnFCpUoLIdTwAlJdVTFVQC1EIhxOhLia8PAsHhw5sYNMiMxklIR9d5zjX5fd/PPP27hnlTzVRowdeEwyPsKbxKqJaVlXHo0CHH/0eOHGHnzp2OOW73338/2dnZfPLJJwAsWLCA119/nXvuuYfrr7+e33//nW+++YbffvuttU5BIpHUgxCCp9Y+xcasjfhofHhl4iskhCSQX5HPTRfdJGuoSiSS85ai9CJSFqeQuiSV8uPVrpuQxBCSkpPoPLkz/pH+AJhMJvLy8vD19aWiosIWXquylZWwT306fvw4ZrMZbY2w2MZgNpudjnE69R7DbIYdO6rFaU5O9TpFsXkAR4yALplQ/AuE9IY+V8NRP9Cf9j0QEQE33eR1NVSbdX3cwBgQgEGrRW80VmfePY0qnQ4REICPy7X1YzAYqKy0DRILISjVaqnUaPA1m1HXFBsGQ/XEyIAACAtzq31FMVNUlIePrw+HfA5xsN1BLCpLdcIkRUFlVTGh8wSyy7JJL0qn1FKKWlHTJbwLT415yu06qlarFZPJVOeAgKIomEwmrFar2xETZfXVPj2FELaQcfsc8lrHDwuzXbOyMqgxt71GAxi0WqeQ4ppt2NusqKhwr++nHU8Imwe1JpUaDaVarUMQ2/PwNFgT1gV/7NrF/2Xq+fAXeHSkhSOhAouH3apeNUd19erVjB49utby6667zlEkOD09ndWrVzvt869//Yt9+/YRGxvLww8/zNy5c90+ZklJCcHBwRQXF9cZT22xWNizZw+9evWqM2RSImlNzhYbfWPzG3y480NUiopFExYxLH5Ya3dJcgY4W+xTcv7SWjZqKDFwaPkhUhencnzvccdyfZCejuM7kpScRJsebVy+gBcUFDiS633++efs2rWLqVOnMmyY7bkaFBREmJuioi5qHsMVTscoL4eNG23idMMGW2kTOzodDBoEI0fC8OG2F2pzJay5BEwl0Oc5iB7TrL62Bo26Pk0gb/t2yrOyCPvgA7SZmVQZDNCzJ8VXXQVAQHw8kRde2OT2jxw5wr59+1iyZAk6nY5YRaEqJ4c+ffoQHx9PWFgY7drV8NqFhVXPE3bB+vW2j9fOkiUF/FB2H78c+aXWtnq1noeHPswNA2/AKqz8lf0Xx8uPE+kfycB2A1GrGncf2hOp1oWfnx+BgYGNanPHjh3k5uZiNpsxm80UFhZiNBoJDQ3Fx8eHtm3b0r17d7RarctphQBkZ9dbe/ZQYSGpFRWOZGzFxcUsW7YMRVGYNGkSHTt2pEePHu73/bTj7dy5k3Xr1pGTk4PBYMAvNpYe48c71rdv355Bgwa51XROTilvvbWVxx4bhepUFqZ7rrqKUCGwYiXLp4ByjYE3Xlh8bs5RHTVqVL1Fkj/66COX++zYsaMFe2Ub3QgLC2ty+IxE0tKcDTb6zd5v+HDnhwA8NOIhKVLPI84G+5Sc35xJG7WarWRtyiJlcQqZazOxmE6VxlMpxA2NIyk5iYThCah19b+oh4eHEx5uSzgXEhKCXq8nMjKSxMREj/W15jFckpcH335rE6dbtzqHh4aE2LymI0fCgAFweu6Q7F9tItUvFqJGeazPZ5IGr08zierbF/r2hfffR+TlobdYUOv1hF96qUfab9++PdnZ2fj7+9OtWzciIyNZs2YNVZ0702vGjGa3HxgYzsh2I/kt0znSsWtEV95KfoukcFvdTrWiZkjckGYdKygoyGPiyM6FF16IwWCgtLTUUdGgQ4cOjbvH2rWrV9x3OvVTk9zcXHbv3o3ZbHZbRNZ1vD69evG/Y8co1mgoKiriop49ufLKKxvXJpCeXsTYsZ+QlnaS/PwK3nhjMoqi8Nznnzu2KSoqIjQ0lDdeqKehRuJVQtVbUalUxMfHt3Y3JJI68XYbXZW2iuc3Pg/ALf1uYWqXqa3cI8mZxNvtUyI5EzZakFJAyuIUDi07RGVhtecnPCmcpOQkOk3shG+Y60SQXoMQkJoKq1fbQnoPHHBeHx8Po0bZxGmvXlBXqKKwQsYXtr8T5rhVUuR8RwE0anXddVSaSEZGBgDx8fGOpKH2ZZ7gql5XsS5jHb+m/ArAtb2v5bFRj+GjaUrA8pknNTWVTz75hODgYPz9/cnJyeGPP/4gOTm5xea0T5o0iWPHjpGQkNDwxg1QVlZGfn4+UHeyy4ZITS1g7NhPyMqyJdxatuwQBQWVRET4OW3XEonopFB1A6vVytGjR4mNjfWqbIASiR1vttHtx7bz0B8PIYRgZveZXH/h9a3dJckZxpvtUyKBlrPRysJKDi07RMriFApSChzLfUN96TSpE0nJSYQntZxHziOYzbB9e7U4rVEn2zHf1C5O3X2xPr4WKo6CNgjaTWl4+/OdwYMRERGUl5fj17dvk5In1YW9HnpCQoJDqObl5VFVVYWPT/PFpKIoPD/+edKK0rhtwG1M6XJ2fd520R4TE0P37t3566+/yM7OpkePHi0mVHv06MELL7zgkWdRZmYmYPM41xemXhd//32cceM+IS/PNm++S5dwVq26tpZIhdrlljyBFKpuIISgsLDQOU5fIvEivNVGDxUe4s7ld2KymBjTfgz3DL1Hhn+eh3irfUokdjxpoxajhcz1mRz89SBZG7IQVtuUJrVWTfzweJKSk4gbEodK48WDNmVltvmmq1fbftdMLKPXw8CBNnE6bJjbyXWcSP/M9jtuBmi83IvsDdx/P1aLhUOn5lF7kn/9619kZ2cTGxuLTqcjLCyMwsJCsrKy6Ny5s9vtHC05CtVFa5wI0gex/OrlqM5Cz7ld6PXq1Yv+/ftz8uRJsrOzycjIYMCAAS1yTHvGX09gF9rh4eFkZ2c3at9t23KYMOEzCgpsESAXXBDF//53NVFRAS63b4m0R1KoSiSSFiG3LJeFSxdSZiyjT3Qfnhz95Fn5JSWRSCQNIYTgxL4TpCxO4fDywxhKqkuVtOnehqQpSXSa0Al9UB0JV5qJSqVCo9E07+U2N9fmMV29GrZtc840GxZmy5Jjn2/aHE9b0V44uRMUDcRf0fR2JM3GarWi1+vp0KGDY1l8fDyFhYWkp6e7JVQtVgsv//kyr/z1Cnd3+gQY5XK7s/H732w2O3mca/72ZHh0S2LvZ4cOHdDpdG7Pr92wIZPJk7+g5NSzrH//tixbdjVhZ3h6ghSqEonE45QYSli4dCEnyk/QIbQDiyYsQq9pmRc0iUQiaS3Kj5eTujSV1MWpnDxy0rHcv40/nSbbQntD2ze+7ENjueWWW7jlllsat5MQkJJiS4S0erXt75okJtqE6ciR0LNn3fNNG4vdmxozAXxav27t+cxvv/3G/v37GTduHH379gUgMTGRnTt3OjyJ9ZFblss/lvyDTVmbAHj54ELwXwnlUS3a7zPFsWPHbLWJ/fyIOFUeyS5Ujx8/TkVFBX5+tUNgvQWLxeIQ2sOGDSMqyr3PZdWqNKZO/YqKClsm4uHD41m8eA5BLTTQVh9SqLqBoihER0fLkEWJ1+JNNmowG7hj2R0cOXmESP9IXpv0GkF6z2bik5xdeJN9SiSuaIyNmqvMpK9OJ2VxCtmbs6tDe3Vq2o9pT1JyEu0GtENReaG9m0w2b+natTaBmpdXvU6lggsuqBanLZFcqiIHclfZ/k68yvPtn8O0xHM0MzPTkWjHjj2pWGZmZr21O1emreT2ZbdzsrJ6gKbYVACTF8J3X4E4+zyop1Mz0ZT9Ovj7+xMREUF+fj6ZmZl07dq1NbtYLwaDgb59+5Kbm+v2fFqrVXD//ascInX8+I78+OMs/PwargXcEt/xUqi6gUqlIjo6urW7IZHUibfYqMVq4YFVD7A7bzeB+kBen/w6UQHnxsiqpOl4i31KJHXRkI0KIcjdmUvK4hTSVqRhOvUSBxDdJ5qk5CQ6jOuALkB3JrrbOEpLbXVN1661/S4vr17n41Nd33TYMAhtYe9vxleAFcIHQFBSyx7rHMPTz1GTycTRo0cBnLLLxsTEcMkll9SZcdZoMfKfdf/hnW3v1FqnVtSQMdzFXmcndqF6+rVISEggPz+fjIwMrxaqfn5+zGhkmSGVSuGXX65kxIgP6d69DV9/PRO93j25KLP+thIWi4X09HQSExNlsXqJV+INNiqE4NkNz7ImYw06tY5F4xfRIbRDwztKznm8wT4lkvqoy0ZLc0pJWZxC6m+plGSXOJYHtg2k8yWdSbokiaBYL4wYycmxCdO1a13PN61Z31R/hsL5TGVw9Cfb34lXn5ljniu8+SbW9HSKi4sJHjAA1fz5zW4yOzsbi8VCQEAAYTUSYqnVakaOHOlyn/SidBYsXsDuvN211rULasfN7f7Ljff3a3bfvIX6hOq2bdvcCo9uTfbs2YNerychIQF9I+7z6OgA1q6dR3i4L1qt+9/ZlprPGQ8hhaqbNCWls0RyJmltG31v+3v8sP8HFEXhqTFPcWHMha3aH4l30dr2KZE0hN1GTRUm0lamkbI4hWPbjznWa/20dBjXgaTkJKL7RHtXaK8Qtpqma9bYflJTndd36FAtTnv08Nx808Zw9EewVEBAB4gYfOaPfzazbBnK1q0EmExQUQEeEKp2kZWQkFArZDMjI4NNmzYRGhrKhAkTAPhx/4/cs/Ieyo3ltdqa1GkSiyYsYs/W4Gb3y1soLi6mqKgIlUpFXFyc0zq7cG0oPLq1Wbx4MSdPnuTGG2+sNzHWd9/tY8KEjgQGVovZ6GjXmX3PNFKoSiSSZvPTgZ94e9vbANw79F7GtB/Tyj2SSCQS9xFWwcm/T7L6u9VkrM7AXGUGbHOu2vZvS1JyEomjE9H6NjxP64xhNNq8pWvW2Dynx49Xr1OpoE8fmzAdMQJOe9E+41jNkP6l7e/Eq2z1VyWtSl3eQoDy8nK2b99OZGQkw8cM56HfH+Krv7+qtZ1OreOxUY9xXe/rzrkcBPZSLtHR0bW8kVFRUej1egwGA7m5ubRt27ZF+iCEoKKiAo1G4zie1WrF17fhzLslJSWcPHkSRVFqCe2avPjiRv7v/1YwalQiS5bMwdebnnFIoSqRSJrJ2oy1/GfdfwC4/sLrmdl9Ziv3SCKRSNyjOLOYlMUppCxO4UT6CUcGz+D4YJKSk+g8uTMBXuJZAKCkxDbPdM0aW33Tiorqdb6+MHiwTZwOHQohIa3WzVrkrgTDcdCFQcyk1u7N2UebNtCuHaaKCtRNqVt7GkII0tPTgerkSTWxL9t/Yj/jPxlPWlFarW06hnXk7eS36d6me7P701gqKiqwWq0EBLTcvdm9e3cefvhhl9FAKpWKQYMGoVKp8GlOqaYGSE1NJS8vj/j4eNLT0/nqq68YO3YsM2c2/J5lH4iIjo522UchBE8+uZZHH10NwOrV6Xz99V7mzu3jyVNoNlKouoF9NOJcGy2SnDu0lo3uztvN/avuxyqsTO0ylVv6NbI8guS8QD5DJd6EocTA4RWHSV2cSt6e6qy3AWEBJE1KosvULkT2jPQee83JqQ7p3b4drNbqdeHh1Vl6+/cHnRcmcxKiuiRN/BWg9sI+ejvvv4+wWqk8eRKf0xJelZSUUFpaSrt27dxu7uTJk5SWlroMawUICAggJzSHX6t+RZ+vR6Nxlguzeszi32P/jZ+2dUqzZGVlcfz4cRISElwKbU9gsVgIDAwkMDDQ5fpLLrmkRY5bk4CAAPLy8igrK8PX15eysjJWrVrFlClTGpxzWjNj8ekIIbjvvpU899xGx7InnxzNddf1blZ/ZdbfVkKlUhEeHt7a3ZBI6qQ1bDS9KJ07lt2BwWxgWPwwHhz+oPe82Em8CvkMlbQ2VouVo5uOkrI4hYy1GViMtqQfikohdnCsLbR3ZCJqnRck+7JaneebHjrkvL5DBxg1yiZOu3VrnfmmjeHkDig5ACodxMuIm6bi6jmamZnJDz/8QFBQEPPmzXP7O9g+P7Vt27Zota5DPTtHd8aabsVisTiEqr/On2fHPcv0btObcSbNw2QyOUrqhHooS3VmZqZTUimTycQTTzxBVFQU119/vctaqUajkX379nHixAkuvvhij/TjdOwe47KyMi666CJHWZz169czduzYeve1C9XExESn5Var4LbblvLGG1scy158cTx33tn8eeMy628rYbFYSE1NpXPnzjJjpcQrOdM2eqL8BP9c8k9KDCX0iOzB02OfRq2S94bENfIZKmktCg8VkrI4hUNLD1FRUB0mG9YpjKTkJDpN6oRfuF/r26jRCFu3wurVsG4dnDhRvc4+33TUKNt809jYM9+/5nDklDe1XTLoQlq1K2czrmw0OjoatVpNSUkJhYWFbg8I2sN+6ypBAzCh2wR+P/Q7uzS7AOgV1Yu3LnmL9qHtm3cizSQvL88R9uuJ0F+r1crKlSspKSnh0ksvpX379mRnZ2MwGCgsLKxzPqjZbOaLL74AYNCgQXV6XpuDv78/YBPFFouFCRMm8Pnnn7Ns2TJGjx5dpzA0mUyOObY1PaoWi5UbbviVjz7aCdimiv/3v5dw882eydQss/62IlVVVa3dBYmkXs6UjZYZy7ht2W3kluUSHxzPKxNfwVfb8MR+yfmNfIZKzhSVJys5tOwQqYtTyT+Y71juE+JDp4mdSJqSRHhSeC3v0xm30ZISWL/eJk7//NN5vqmfn/N80+D6s6muWrWKtLQ0Bg8eTM+ePVu2342hPANOrLX9nXhV6/blHOB0G9XpdMTFxZGens6RI0fcFqrDhw+nXbt29dZlTUhIoL+pP7maXGZdOIuHRj6ErpXDtoUQHDtmy8QdExPjkSiuQ4cOUVJSgq+vryMMur6MyHb8/PyIiooiLy+PrKwsunf3/FxdtVqNv78/5eXllJWVMWrUKH744Qdyc3PZuXMnffv2dblfTk4OZrMZf39/IiIiAFi/fgP33/8yhw9rgK6oVAoffXQp11zTvHDflkYKVYlE4jZGi5G7lt9FakEq4X7hvD75dUJ8Qlq7WxKJ5DzHYrKQuT6TlMUpZK3PwmqxzeNUaVTED48nKTmJuCFxqBtRE7BFOHrUlqF39WrYudN5vmmbNtUlZPr1a9R80wMHDvDnn3+SkJDgXUI1/XPb7zYjwL9u752k6bRv35709HTS0tLo1889z1h4eLhD1BZVFRGsD64lyKKiovDV+5Jcmcwt3W5pdZEKUFRURFVVFWq1mjZt2jS7PSEE27dvB+CCCy5whDi7yoi8evVqDh48yIABA7jwQlv5vfj4ePLy8khPT28RoQq28N/y8nJKS0sJDw9nzJgx/PbbbyxdurROoepKaH/77R7y8wtRq8PQalV8+eUMZszwXJ/Ly8vJysryWHt2pFCVSCRuYRVWHv3jUbYd24af1o9XJ75K28CWSckukUgkDSGEIH9/PimLUzi8/DBVxdXepjbd2pA0JYmO4zviE9JyWTkbxGqFffuqxWnaadlTO3WqTobUtav3zzdtDMaTkL3Y9nf7q1u3L2c7K1ag5OURkpUFBQVQY35i+/bt+eOPPzh27BiVlZUNli5JT09n1apVdOvWDXM7MwuXLuRfg/7F3D5znbZTqVTEx8eTmppKRkYGMTExLXFmjcLuTY2MjPRIiP6xY8fIzc1FrVbTu7fNsyiEcJmIyGg0UllZiclkcixLTExky5YtDmHYEtRMqAQwfvx4li5dyoEDBzhy5Ajt29cOxXbV/ylTkvjzTz2VlQo//TSbyZPrrqvaFIxGIwUFBR5tE6RQdQuVSkWHDh28tqCvRNLSNiqEYNGmRaxIW4FGpeGF8S/QJaJLixxLcu4hn6EST1J+opxDSw+RsjiFk2knHcv9IvzoPLkzSclJhHZoXJIVj9qo0QibN9vE6dq1kF8dfoxKBX37VovTFqq/6BVkfg9WIwR1g9ALW7s3ZzWWV17BsGEDkUKgTJniJFSDgoKIiIhAURRHdtj6SEtLY9/BfSwrX8bGPzcihOCx1Y/Rv21/ekT2cNq2plAdNGhQi5ybuxgMBocQ8pRotntTu3bt6kiYVFRURElJCSqVitgG5oPbhWBWVhYWi6VF5rfb576WlpYihCAsLIxBgwaxceNGlixZwj/+8Y9a+9jrrtZMpKTVqunUKYwRIzp5XKTakVl/WwlFUQgKCmrtbkgkddLSNvrp7k8dxb6fGP0EA9oNaLFjSc495DNU0lzMBjMZazJIWZzC0T+PIqwCALVOTeKoRJKSk2g3sB0qddOEZrNttKjINt907VrYtAkqK6vX+fnBkCHV803Ph3vBYoTMb2x/J15ly9oiaTpCYD0VJu7qSs6aNavO7L2nsyNtBz/rf6aovAjdqfByo8XIgt8WsOyqZfjr/B3bdunShbKyMrp27drsU2gueXm2UlJBQUGOJEPNpXPnzpSUlDiF0Nq9kW3btnVcn7po06YNfn5+VFRUkJOT47LUT3Px9/dHURTMZjMGgwEfHx8mTZrExo0b+euvv5g1a5ZjHqqd2bNnk59fTkmJwWm5SqUQHe35pE8tiRSqbmCxWNi3bx/du3eXGSslXklL2uiS1CW8+terANw5+E7Gdxzv0fYl5z7yGSppCkII8nbnkbI4hbQVaRjLjI51URdEkTQliQ7jOqAPrL+eoDs0yUazsqpLyOza5TzfNDKyer7pRRd5Z33TluTYUjAWgj4Sose1dm9an+xsKCyse31YGLiqg2rfr6QElcWCEAJRWIiyZ4/Tfu6K1MUHF/N09tNUqarwVzuLvfYh7TFZTU7LEhMTa5U3aQ2EEOTm5gLUmwCqsXTp0oUuXZyjw1zNT60Lex3agwcPkpmZ2SJCVaVS4e/vT1lZGeXl5fj4+JCYmEiPHj3w8/OrlWn35MmTHD9exuWX/0pZmYm1a+cRG3tmBseEEB5vUwpVN2mJlMsSiSdpCRvdlLWJx9c8DsDVF1zNnF5zPH4MyfmBfIZK3KX0WCmpS1JJ/S2V4sxix/KA6AA6X9KZpEuSCI6vPwtuU2jQRq1W2Lu3WpweOeK8PimpWpx27Xr+ehGFqE6ilHglqM7zV83sbBgwAE7NMXRJQIAtXLymWK2xn8pqRWM2I4RAWbcOhg2rez8XVJmreHz143y440OqrFUoiuIYkNGqtTw0/CFu6HuDy9DN9PR0R1mc1hKtQgjatWtHfn6+R5Io1UdjhKp9u4MHD5Kens7QoUNbpE9dunRBq9U6DUjcfffdLgfVFi9ewZdfLqGiIpAjRyK45pof+eOP61qkX2eC8/zpIZFI6mLfiX3cs/IeLFYLEztN5LaBt7V2lyQSyTmKqcLEkd+PkLI4hZytOY7lWl8t7ce2Jyk5iZi+MSiqMyz+DAabEFizxhbWW9MrplbbvKUjRth+zuX5po0hfxOUpYHaD2Iva+3etD6FhTaRqlKBxsVrt9lsW19Y6Cw4a+6n1WIWAiEEOr2+/v1OI7UglQW/LWD/if2OwRh7iHxiSCL/veS/9I6uu0TJli1b2LJlC2azudWEqkqlol27drRrQJA3F6PRSE6O7flzulC94IILSExMrFUCyL5dSyZUss+frYkrkZqWdpIPPliNSmWhrExPXFwQ77yT3GL9OhNIoSqRSGqRVZzF7ctup9JUyYB2A3h05KOoFJkIRyKReA5hFeRsyyFlcQrpv6djqqwOO2zbry1JyUm0H9MerZ97YY0e4+RJ23zTNWts9U1r1q7097fNMx050lbn9HyYb9pY0j+z/Y6dBtqAVu2KO5hMtsjtliqj63cYLjCDVasBoUVVUYpSI0zc6uOHYjayeztUnAog8MlKJeHdJwivqEQoaiz+gVhUaoQQGIXtflAEqE7bryZCCH4//g3vHH4Ag6Xy1LlaEAIQGgYGT+OWjs9SeiiQ9Yfq7n9FRQJlZVv4668MAhuY3rh7d2OujPehVqtZsGABOTk5hISEOK2z10vt0qWLk1iNj48nNDSUuLg4TCaT22HYnubAgXwuvvgjOncuBSAkJIrly+eRkBDSKv3xFFKouoFKpaJLly4yY6XEa/GkjRZWFrJw6UJOVp6kS0QXXhj/Alp16zx4JecG8hkqqUlxVjEpi1NI/S2VstzqcMjguGA6J3em8+TOBMac2YQfqqNH6b51K6rXXoM9e5znm0ZFVWfp7dsXWulF9KygJAUKNgMqSJjd2r1pEJMJBg6EHTta7hg9gfWAATADoZjRUT3f+qTBFz1w3Vz4+9SyARTyK6tP7WGmuMgfcaqOqT2CWAO19nOgK4Vx90G3H50Wa7UW1EKDWHkvP226m59cpmZyxs8vgUGDYN++LJ54woIQ526eAbVaXee83Ly8PA4dOkRkZKTTcr1ez/3333+Geuia3bvzGDfuE4zGAhQFtFpffv/9Ztq2PbMDaTLrbyvSUOYviaS18YSNVpgquH3Z7RwtOUrbwLa8OvFV/LS1Q04kksYin6HnN4ZSA2kr0khZnELe7jzHcp2/jo4TOpKUnERkr8gWedFxidUKf/9tq226di2kp+MkP7t0qRanSUleP980PDychISE1s+ubZ+bGj0W/Lw/FHrXrpYTqQpWBLUH58rxdxKq7rZmtrg5QNJmH0y9AULSa63yrwyla+rFbN+2ANf5g2tTUdEGk8kXrbaSgIBjlJbWX7KlJj6tWMK4Kfz4449UVVUxcuRI2jYilN9isZCTk4PZbHZZ17Ql2bw5m4kTP+PkySri46vw89NwySUDa4nUhIQELr30Uo8mozoTSKHqBlarlT179tCrVy+ZsVLilXjCRk0WE/esuIf9J/YT4hPC65NfJ9wvvOEdJZIGkM/Q8xNhFRz986gttHd1OhajbX6colKIHRRLUnISCSMT0OjP0KtIVZVtvunq1bbQ3przTVUq8tu3J/yyy1BGjYKz7GVu9mwv8F5WnYBjy21/J17dun1xk5YI940li/m8z3DWMYHltdYb0WFEhwq71762YKzAjzQ6EMZJrCgIN0UlAGY9+B+vtdjnwBR6V/ljrIjEaGzMgIaKkpJ4wsMPEhSU6bZQvfBC6F331FevQwjBnj17KCsrY8iQIY3ad+fOnXz99dckJCS4rGvaUqSnFzFu3CeUltoGPrp105KUFEG3bp1qbdvSGZzVajU+LTAyIYWqRCJBCMGTa5/kz6N/4qPx4ZWJrxAfHN/a3ZJIJGchJ9NO2kJ7l6RSkV/hWB7aIZSk5CQ6TeqEfxvP1EFskMJCmyhdvRr++suWHMlOQIBjvql14ECyjxwhrFcvW5IkSePJ+BqEGUL7QEiP1u5Nk3jjDbjggqbvH/rnUjo9d6MjdHz3/y2hKqY9/reCrxaE46071LFPiNmEygQfvwkVHe1Le6E7/B7cOgy0OoJPJWGymC2oNTb7VMy42A+gI7/n/YdXU+4AwF8TxD86v0jPMQM4cmQbGo2W555r3Hn9/XcCu3cfZPz4dIYObVjE+fjYROrZFCVfUFBAWVkZGo2mUd5UqE6olJ2dfUbnqSYkBHPDDX156aU/GT06gYEDM6isLHc7Y7EnCQoKqlXqxxNIoSqRSHh98+ssSV2CSlHx3MXP0SPy7HzJkEgkrUNVURWHlh8idXEqJ/afcCz3CfZxhPZGdIs4M6G96em2cN7Vq23zTWvW9ouOrg7pvfDC6jdpWT6peZgrIOt7299niTfVFRdcUF35pUn0GgL/1UOlLXlR9w3vwn/+Y3vbVsyuo20VM2hs05/pVWN5ME77CcCsmNEoGlszde0HDOMK8peu50jREd6c/CZxwfb6nhObdFpRUQmkpYEQmc27Pl6MPWtvu0bUpbUTHh6Ov78/5eXl5OTknDGhqCgKL744nk6dwrj00nheeeVF1Gp1i2dHPpNIoSqRnOd8uedLPt71MQAPj3iYIXGNC3mRSCTnJ1azlcwNmaQsTiFzXSZWs82LpFKriBsWR1JyEvHD4lFrW9hDabXa0o3axenpZSK6dq0Wp507e/1807OS7MVgLgW/WIgc0dq9aXmKimze+ago5+XBwTBrFnz0EfTqBXPnQmiozXtfVgbGOuamBgRAWJjzsrAw5/2EQGW12gZVFAWzIhCBAWhP3+8Uz457Fo1Kg1atxWq18ttvvxEXF0ePHj0aLcTi4uJQFIXCwkJKSkpafy50C9BQ/dS4uDi0Wi1Rp3/m2ARjQkIC+/btIyMjo0WFakFBBeHh1blDFEXh1lv7s337dgAiIyNbLfNwSyCFqhuoVCp69eolM1ZKvJam2uiKwytY9OciAP7R/x9M6TKlJbonOc+Rz9BzByEEBQcLSFmcwqFlh6gqqp7kF9E1gqTkJDpO6IhvqG/LdqSy0hbKu3YtrFtnKyljR6OBfv1swnTEiNpiwgXSRpuBsFYnUUqYA+dyKbPDh+Hdd+Gbb2DGDHj++drb3HILTJ1qSydsHxTZvNl5TvTphIXVroXarp3zfkLYBmVUKo5V5fOPXf+hZ1RPnqjDe+arrb4HT5w4wbp169BqtfTq1cvl9vXh4+NDdHQ0x44dIyMjo0lteDsNCdVu3brRrVu3OvevKVRbig8+2MGddy5n2bKrGTTIea7w0aNHycvLIycnh7KyMgICznxpqJZ4fkqh6iZGo7FFJglLJJ6isTa6NWcrj6x+BCEEV/S4grl95rZc5yTnPfIZenZTUVDBoaWHSFmcQuGh6hdu3zBfOk/uTFJyEmGdXHt2PEZhoU2Yrl1rq29a0zsVGFhd33TIEFu900YibbSJ5K2GymzQBkG7c3yw8+67bbYH8N13cP/9tT2hcXG2n5q0a1dbiLpDzf2EwFhVxbrsddyx7g6Kqor4s2Qfww5PZnzH8fU2YxdPcXFxTU5ol5SUREBAwDmZwd1gMJCbmwvY6qI2BbvAzcjIQAjh8WkOr7++mYULlwIwadLn7Nx5s1ON1ClTprBx40aOHj3KqlWruPTSSz16/NZCClU3sFqtHDx4UGaslHgtjbXRlIIU7vrfXZgsJsa2H8v/Dfm/M1cWQnLeIZ+hZycWo4X0NemkLE7h6KajCKttrqdapyZhZAJJyUnEDopFpW4hD5oQtvmma9bYxOnp801jYpznm2qa/kojbbQZpH9m+x03EzQt7ElvbW68sVqoGgzw7bdw881n5NBVpipu/+l2Fucsdlp+x7I7WHntStoG1p0AqCFvoTtccsklTd7X28nMzMRqtRIaGkpwcHCT2oiNjUWlUlFSUkJRURGhoaEN7+Qmzz67nvvuW+X4f968PsTHV/fTYrFgtVqZMmUKb731FitWrGDy5MlnPATYWrP+tIeQQlUiOc/IKc3htqW3UW4sp29MX54c8ySqczlUSyKRuI0QguN7jpOyOIW0FWkYSquz5Eb1iqJzcmc6XtwRfZC+ZTpgtdqKW65ZY/vJynJe3727LZx35Ejo1EnON21tivZA0W5QNBB/RWv3xjOcOAHff28TpacPWowfDwkJEBlpWz+xacmJGsuRk0e4efHN7MjeUUt8BOgCKKwsrFeo2hMFNUeoCiEoLCx0hP6eS/Mg7denqd5UsNUKb9u2LUePHiUjI8MjQlUIwSOP/MFTT61zLHvooeE88cRoJ+fCkSNH+OCDD+jSpQuhoaGcPHmSTZs2MWLE2T9fXApVieQ8oriqmIVLF5JfkU/HsI68OP5FdOpzL4xHIpE0jrK8MlJ/SyVlcQrFmcWO5QFRAXS+pDOdL+lMSI0wM49SWWnzUq1ZY5tvWlx9fLRa6N/fJkyHD7cJBIn3YJ+bGjMRfCJaty/NJIpc2r/6LPz1A5hMkJhYW4iq1bBkiS1B0hnih/0/cO/Keyk3ltdaN7nzZF4c/yLBPnV7ASsqKsjLywNsob/N4fXXX6e8vJyIiIhmiTpvwxMeZ/v+R48eJTMzkz59+jSrLSEEd931P1566U/HsqefHst999VOu5yRkYHZbEaj0TB+/Hi+/vprli5dyvDhw8/6aDkpVN1EhgJJvJ2GbLTKXMUdy+8goyiDqIAoXpv0GoH6wDPUO8n5jnyGeh+mShPpf9hCe3O25CBOhdVqfDS0H9OepOQk2vZri6JqgRed/HybKF2zxpYwpuZ806AgW42QkSNh8GDw86u7HQ8ibbSRVORA7u+2vxOvat2+eAATWsLX/QiKybbg3Xdde0zPkEitMFXw4KoH+Xrv145ldtGhU+t4fNTjXNv72gaFiN1bGBERQWBg07/zT89sey4J1f79+xMaGkqnTp2a1U5CQgKbNm2i8lR5oqZitQpuuWUx77yz3bHs1VcnsnDhQJfb1/SYX3jhhfz8888cPXqUv//+25H4ymQyYTAY0Gg0Z9VcfClU3UCtVp+TGc4k5w4N2ajFauGBVQ+wJ28PQfogXpv0GpH+0jMhOTPIZ6j3IKyC3J25ttDelWmYKkyOdTF9Y0hKTqLDuA5o/Twc1icEHDliKx+zdi38/bfz+rZtYdQomzjt3btZ802bwtluo9999x179+5l4sSJDBzo+mXW42R8CVghfCAEdT4zx2xBCgknf9RMYtd8YVuQlwclJbaBkzPMvhP7uHnxzRwuPOy0XKPR0CmsE28nv023NnVnoK2J3VvoCWEZHx/vEKrDhw9vdnveQq9evTxy//fo0YPHH38cvb55UyPmz/+Fjz7aCdhmN7z33lSuv/5Cl9sKIZw8wv7+/owcOZLly5ezdOlSx3lt3LiR9957jz59+nDXXXc1q3910RKDfVKouoEQgtLSUgIDA896F7rk3KQ+GxVC8PT6p1mbsRadWsdLE16iQ2iHVuqp5HxEPkNbn5KjJaT8lkLqb6mU5pQ6lge1C6JzcmeSLkkisK2HIywsFtt8U7s4PXrUeX2PHtXJkDp0aNX5pme7jebl5XHo0CGKa4ZNtySmUjj6s+3vxKvPzDE9wbFj8OGHtoGQe+6ptTpvyo3EWjJt80/HjoUzXK5ICMHHuz7msdWPYbTUrrk6PWk6z054Fn+d+1mt7d62xMTEZvfP3kZLlmA50xw8eJD09HS6devWbDFvn7dbUVEBgF8To0FGjUrgo492olYrfPbZdGbP7lnntvn5+VRUVKDRaIiJiQFgwoQJ/O9//2PPnj0cPXqU2NjYOvf3JKJmsjsPIYWqG1itVtLS0mQ2QInXUp+NvrPtHX468BMqRcV/xv6H3tG9W6mXkvMV+QxtHYxlRtJWppGyOIXcnbmO5Vo/LR0u7kBSchLRfaI9K8wqKmzzTVevhvXrbR4pOzqd83zTNm08d9xmIm20kWT9CJYKCOgAEYNauzfu8dJLth+z2RZOvmAB4OwtrYzvYquR2goUVxVz5/I7WXpoaa11/jp/nhnzDB1NHfFRux+2aTabSU9PRwjhEY9qu3btUKlUFBcXU1RUREhISLPbbCk2bNhAUFAQ3bp1Q1NPhMaePXvYvHkzFovFI9fop59+YuPGjUyePJlRo0Y1qY3rrutDZaWZmJgALr20a73b1iw9ZD/PNm3a0K9fP7Zs2cLSpUu58cYbm9SPxiKz/kokkkbxw/4feHf7uwDcN+w+RiWOat0OSSSSFkVYBdmbs0lZnMKR349gMVoA2/yydgPbkZScROKoRDQ+Hvz6P3HCNt909WrYssWWiMZOUJBNlI4cCYMGnbH5ppIWxGqGjK9sfydeffZkXu7Y0SZSwTag8tlncMGtrdunGliFlZ15O2st7xXVi7eT3yYuMI49e/Y0qs3169ejUqnw9/cnOjq62X3U6/XExMSQnZ1Nenp6sxMGtRSlpaVs3boVIQRRUVFE1pOEzS70POFxBhzZfhvjdbZYrKhPK/O1YEE/t/atK7R78uTJHDp0qNkJtFobKVQlknOU1emreWb9MwDc2PdGpneb3so9kkgkLcXJIydJWZzCoaWHKD9enR00tH0onZM703lSZ/wj3Q8XrBchIC3Nlghp9WrYt895fWxsdUhv7961S3xIzm5yV4DhOOjCbNl+vY3SUvD3rx22O3mybS50Tg506QJelgwo1DeUNye/yYxvZmAVNs/UTRfdxAPDH0Cn1mGxWBrdZufOndm/fz9du3ZF5aEw5oSEBLKzsz2S2daOyWSiqKiI8PBwj/RTr9czYsQIjh8/Xq9Irays9FhGZDv2zMEZGRkIIRqMWCkqquKSS77g5psv4tprGx/xVlfG4k6dOrFo0aJ6vclnA2d3788gZ1OGLMn5SU0b3ZW7iwdWPYBVWJnWdRo3XXRTK/ZMIpHP0JbAUGLg0PJDpPyawol9JxzL9UF6Ok7oSJcpXYjoFuGZ0F6LBXbssM01Xb3a9rJfk549q8Vp+/Znj5etBtJG3UAIOPKZ7e+EWeBN5c0yMuD99+Grr+Ctt2DMGOf1Gg088QQEBNi8/IoC61unq3UxMHYg/zfk/3h3+7u8POFlLu54sdP6xtpoWVkZkydPpl27dh7rY0JCAhs3biQ9Pd1jbR4/fpy0tDRCQkI8ktRIp9Nx4YWukw/VxC7ywsPDm5URuSbt2rVDrVZTVlZGYWEh4eHhdW6bn1/B+PGfsmNHLn/+eRR/fy0zZnRv8BgVFRWcOHECq9XqENquSuuc7SIVpFB1C7VaTdeu9ceISyStSU0bTTuZxr+W/wujxciIhBHcP+z+szI5iOTcQT5DPYfVbCVrYxYpi1PIXJeJxXQqtFelED8snqTkJOKHxaPWecCLWVEBGzfaxKmr+aYDBlTPN404u2toSht1k8JtUHoQVHqIm9navammpMSWNdpgsP3/zju1hSrYvKpegMVqQa1yfY8uHLCQOb3m1MrM3xQb/fnnn8nPz+f666/3mH3bBVFOTg4mk8mRQKipCCE4duwYYCuhcyapWdbFU2i1Wtq1a0dmZiYZGRl1CtVjx0oZN+5T9p0aZAwP96VTpzC3jlFRUUFmZibFxcUIIQgLC/OY0G4OMutvK2G1Wjl58iShoaEeC52QSDyJ3UZNOhMLly6kxFBCr6he/Gfsf+r8MpRIzhTyGdp8ClIKbKG9yw5RWVhdoy88KZyk5CQ6TeyEb5hv8w90/LhNmK5dW3u+aXCw83xTXw8cz0uQNuom6ae8qe2mgC64dftSk6AgmDoVvv3W9v/atXDgAHjZ4IPJYuK5Dc9xsOAgH037CJVS29bUKrXL8nGNtdHy8nLy8/MBz5SmsRMaGkpgYCBarZaioiLaNDMpWnFxMZWVlajV6nrDdFuCusJmXbFx40YOHTrEhRde2KDXNyEhwSFU+/bt6+K4RYwd+wmHD58EoG3bQFauvIZu3dy7lgEBAQAEBgayYMECqqqq3NqvpZHJlFoJIQRZWVlend1Mcn5zovwEL65+kYPGg+SV5ZEQksDLE17GRyND2SQN8/fff/PRRx8xd+5cevasOw1+UzkXnqEtfY22btjKW4veYsGdC+g31JZEo7KwkkPLDpGyOIWClALHtr5hvnSa2Imk5CTCk+oOK3MLIeDQIdt80zVrYP9+5/Xx8TZhOmKEbb7pOSriWtpGW9p+zggF223ZfnWhkDjH482npqby448/ctlll9G5cx11WYuKbDV4hw2rve6mm2xC9YILbH937Oi0OjMzk1WrVjF27FiPCjd3ySzO5NbfbmX7se2ALSP/gn4L3N6/sTZqF2GRkZFNLpPiCkVRuPvuuxsdhrxhwwZ+++03LrnkEoYOHepYbvemRkZGNssj19jP12q1Nsqjmp2dTUpKCm3atHFLqK5bt85lQqXU1ALGjv2ErCxbhEpiYgirVl1Lhw6hDfbBjl6vR6vVYjKZaNOmjdve1MrKStauXcuECRNa7Lve00ihKpGcAxwrO8a7Ke/SLrgdsUGxvDbpNYJ9vGi0W+K1CCH46quv2Lp1Kz4+Pjz55JMyVPw0WvoaCSH4+uuv2Zexj6++/IrQylBSf0sla0MWwmr74ldr1cSPsIX2xg2OQ6VphmA0m23zTdessXmeas43VRTo1atanCYmnpXzTb2Jc+YeS/8UDPkQORr8PSv0hBAsWbKEv//+G71ez2233eZ8jdLTbfNOv/kGtFrYts0217QmPXrAihXQvXstmxVCsG7dOg4fPoxOp2POnDln9DNYnLKY//vf/1FiqA6f/8+6/zAodhB9ovs0uL/BYODXX3+loKCAHj16uCXoGuMtbCw+Pj6OBEjueFStVivLly+nqqqK5cuXM3jwYFQqFUajkYIC2yBcc7ISN+XzzcvLw2AwoNfriYqKarD9srIyx5zQhpIk2a95bm6u4xgAe/ceZ9y4T8nNLQMgKSmclSuvIS6uce9riqLg5+fHN998w8GDB5kzZ06DgxFCCAoLC6msrOSrr746a55DUqhKJGc5VmHl1c2vUmWpwkfjw6uTXqVtYNvW7pbkLGHPnj3s3LkTPz8/du7cyZ49e7jgggtau1texZ49e9i2bRs6nY6tW7eyfPlykpKSUKvVbr8E5ubmOorAn05KSgo7du9AbVGz9c+tfPHnF0Rhe3GK7BFJ0pQkOo7viD5I3/STKC+3zTddswY2bLBlRrWj09lCee3zTcPcmyclcY8zcY/Z7TEmJsaj7TownoS81ba/Y6d4vPmUlBT27t2Lj48P+/fvJyUlhS5dulRvsHUrfPKJ7e+qKlvCpBtuqN1Qjx4u28/IyCA9PR29Xk96ejoZGRkeK0dSH1XmKh5b/Rif7Pqk1jpFUUgpSHFLqFqtVrKysqisrGxwWzstMf/SzrFjx3jttdfw8fHh4YcfblDwrF+/nrIymzgrKytj/fr1jBgxwiH6AgMDHeGsTaEpn2/N+qMNCf+MjAxycnLw9fUlOzu7wfaDg4MJCQmhqKiIrKwsOnXqxPbtxxg//lMKCmyfYc+ekaxceQ1RUU07b7PZ7BCediFcH4qioFarCQwMPKu+66VQdRNvmKQskdQkvyKf/Ip8fjn4C2sz1qIoCld0vwKrsHIg/wARfhFE+J3dCU4kLYsQgm+++Qaj0UhkZCTHjx/nm2++oVevXh4faT1bn6FCCJ5//nkKCwvRaDSYzWaee+452rZtS1hYGK+++qpb7Xz55Zds377daZnVbMVsMJOTk0OVqQqtVYtQBPu1+xkybgjdZ3Qnpm8zhEdeXrXXdOvW6hqSAKGh1fNNBww4p+abNpWWsFGLxcLrr79ORUUFbdu2bbF77OKLL+biiy9ueMPGUpVv86JmfgvmUlD7gKKC4gO29foI8Gne94w9oqC4uJjg4GBMJhPLli0jKSmp+hpdeik89ZRtDjXA8uWuhWod7W/YsAGTyURwcDDFxcVs2LDhlIBrOY9SSkEKCxYv4ED+gVrrEkMSeSv5LS6Ick8oaLVaJk6cSF5enlt2Y7FYyMrKAlpGqEZERDi8jA1ltrVaraxcuRIAlUrl+H/o0KGOsN/mDLDU9/nWd60CAgJISkqiU6dOLdJ+QkKCk1A1GMxUVdmewf36tWXZsqsID296SPbJk7b5raGhoQ0KbSEEK1asQFGUFvuuNxqNjj55EilU3UCtVtPxtLkOEklr88P+H3h729scKjyE2WomJjCGb/Z9wzf7vgFs9ddkWRpJfdg9PYGBgVgsFtRqdYuMtJ7Nz9D33nuP1NRU1Go1Wq0WlUrlCOWyF3Z3h6CgoFoZLQsPFZJ/JB+DzoCCgkVtQS/05Fhy2Lh8IyEJIXUL1fx8+OEHmD69OuOuEJCaaisfY08mU5P4eFtm1JEjbeG95+h806bQEjb6999/89///pfdu3ej09nKuJxt3gyO/gCp70BZKljN4NsW9v67en2nm2w/zSAlJYWDBw8SZDAwYssWck/V/nTyqmq1MHcu/PGHbf7phAlut2/3tvn6+qIoCr6+vg6vGyQ2q++uEELw9d6veWDVA1SZaye5md5tOs+Me4YAnfueNI1GQ7du3ejWrZtb2+fm5mI0GvH19W1ysqOTJ09y6NAhOnXqVOtZ525mW3D2ptrnMJaVlfHLL7/g6+uLn59fs7L91vf51uf17Nmzp1vzNJva/rhx4xg3bpzj+g8eHMfixXP4z3/W8e23lxMc3LwcIsdPDdqEhIQ4vr/rouZ3vaIoLfIcKi8v58iRIzzwwAMeac+OFKpuYLVaHUWDZTZAibcwvdt0hBC8tvk1tGotFrOFB4Y9QPdIWw0u6U2V1EdNb2pAQABZWVkIIRzzXjw50nq2PkM3bdrEl19+6Uj/HxoaihCC48ePExMTw2OPPeZ2W/Pnz3f6P3VpKqs3rmalZiUVqgpUOhUWo4WgiCAqTBWU9Cih62X1ZCzNz7eV4BgyBNLSqsVpbm71Nvb5pnZx2gKelXMFT9ro0aNH+frrr9mxYwc5p+b/Bgfb5qD5+vpSWlraYpELHid2Oqh8YP9zgBpUGujxEASdsk19872py5Yu5crt2xmYmYlGCA4ZjewfNKi2V3XhQrjjjka3b/eG2efw6XQ6Kisr2bBhA/HxnvWqlhpKuXflvfx04Kda63y1vvxnzH+4oscVTfrcG2Oj9rDW+Pj4JtmzEIL09HQKCgrQaDSEhITU6nNDmW3tfbZ7U+3t2kXVxo0bGT58OH5+fk2+5xr6fOvyelZWVrJv3z4SEhLqFclNbR9wzHsVQjjmtI4alcjIkfV7Yt0lOzsbRVGIiIigvLycoKCgOs/B/l1vT8TVEs8hIQQmk8njA3BnzxtDKyKEIDc3t0WyWUkkTSXcN5w1GWvw0fgwvet0zCYzXcK70DWiK10jukqhKqmXmiOsWq3WMVoMOEZaPcXZ+Aw9cOAAL730EhUVFQQGBjo8CqePRjcWc5WZNU+u4Y+H/yDXkku+Xz5tYtsQ0zaGAALwC/QjKCSI/Wn7OXzssOtGKittdU2zs+H66+HWW21JZnJzQa+3idJHHrGFR37wAVx7rRSpDeAJGy0uLubDDz/kgQceYOfOnRgMBoQQxMTEOF70m2s/ZxyfCCjcYgv5jU22DX4EdYXgUz/NDPtNSUlh/4ED+FksaE5d+6SMDOLNZodX1UETMsKe7g2zWCyYTCaHV+zkydpZWZvKrtxdTPhsgkuR2q1NN5ZfvZxZPWc1WRQ0xkZrCtWmUFRUREFBAYqicOLECdLS0sjLy3Paxh5S7CqzrZ2a3tSaWK1WzGYzR48exWAwUFRU1KR+nv75Ai685rU5cuQIX3/9NR999FGLtA/www/7ueGGJ3nsscccZYLs+zeXiooKjh8/jlqtJiIigtKaOQdO43Rvqr0Pnn4OlZaWYrFY6szF0FSkR1UiOUvZkrOFlIIUfDQ+jO8wni93fNnaXZKcJdhHWCsrK/Hz88NgMODr60t5eTkVFRVYLJazx+PTQoSFhVFeXo5arSY0NBSDweBYp1arqaysbPQ1Opl2kpX3reRkmm0ez7FOx+AEaHQaTAYTZswYjUbUehftm82wbBksXgybN0NJie2nsBCiouCii2DcOBg/HhpZNkLSPIxGI8uWLePXX3911DPs27cv2dnZFBYWotVqPWI/rULFUcjfZPs7+mI4trz+7bOzbTZZFyEhEBcHnPKmLluGwWBgTVISF56aU3nC35+gqirSVaraXtVGYPeGGQwGhxesuLgYRVEc93Ra2gageV5VIQTvbn+Xf6/7NyaLqdb6eX3m8cjIR9BrmpEMrZGYTCZUKlWTEkYJIThy5AgGgwFFURBCcOjQIfz8/Jyy49aV2dZOTW+qvZ2a69RqNenp6XTo0IH09HSXXtuG+lnz8zXVqPlsn6JRl9fTnURTzWn/s892M3fuT/TunUVVlYaMjIxm15t11f+IiAiioqIcUwtcncPp3/V2PPkcEkJw7NgxhBBNHnSoCylUJZKzlM922wqvT+0ylcSQRGYkzpBeVIlbmM1m8vLy8PX1dRr9VKlUmM1mFEXh+PHjmM1mtFptK/a09QgNDSUqKgpFUVxm2vT19XX7GgkhSPk1hQ3PbsBsMOMX7sewx4ax5YMt+JbZPgNhFagCVFSZqlAsthH743l5mLdsQbtqFaxcaat3ah+Z12ohPNzmQTUYbBl9e/aUIrUVeO2119i5cycAiYmJXHXVVXTs2JFbb7211j1mpzH206pk/WD7HTEYgnva5qPWFe6bnW1LzOXCg4bVCiaT7fe+fdCpE2azmYKCAvR6PXmxsaxLSmJPaCgH2rXDPzAQPVBQUNDka2SxWCguLkav12M0Gh1CyWq1OjKlVlYWo1JZsFqb/jpstpr5+eDPtURqkD6IRRMWMbnz5Ca33VSuvfZajEZjk0Jqi4qKyM/Pd0QA2BMgnV7+JDg42JFcyJ4wqCYGg8ExcGO/9vY27cLVbDajUqkoLCykqKioUfP+T/98T0ev11NcXIzFYkGjcf583Snd09T233lnGwsWLEYIKC72wWg0kJ6eTr9+/dw+t4ZIT08HoEuXLvXWdK3ru96Op55DRUVFFBUVtcigmxSqbqAoCmFhYd496ik5r0g7mcbGrI0oisKVPa+kjX8bbux7I238PTdiJzl30Wq1PPPMM7XChTZv3sx3331HUFAQTz31lMdeoM/GZ6hWq+X555+vN6QqKCiowWtkqjCx/pn1pC5JBSB2UCyjnxiNb5gvzyTV/gwQwlYzcv16gjZsQHvrrdXr2reHWbNg2DCwWODf/4aHHoKup+YLNiMhyflOc2x0woQJZGZmcsUVVzjqQwIu77GauGM/rYrFCEd/tv0dN9MW5ltf4qTCQptIVamgpjAwmWzlkex8/TU8+CBarZY777yTkpISLBYLRUVF/P7xx3SLjOTaa69FURQCAgKafI00Gg3XXHON00DTmjVrSE9Pp1evXvTt25c9e3ybJVIBtGotb05+k/GfjafUYPu8L2p7Ef+95L/EBsU2q+2auGujpaWlqFQq/P39G30M+9xU86kM4TqdDq1WS1VVFSqVqlb90MTERHbt2kVGRkYtoerr68ttt93myAT72WefYTabCQoKIjc3F19fX0aNGoWPjw9VVVWN9qq6+nxPx9fXt5ZIdTcjclPaf+mlTdx55/8c/48e3ZvQ0BSHB9RTuFt6qK7v+po09zkkhODw4cNYLJYmt1EfUqi6gUqlanKcv0TSEnyx5wsARiWMIi7YFkYlbVTSGMLDw2tlamzXrh1r166luLiY1NRUIiMjPXKss/UZ6uoaNYaClAJW3reS4sxiFJVCv1v60ee6PigqpXb7R4/a5pQuWwZHjlQ34u8PY8fCxIm28F77PD17Rt+uXauFqqTJNMdGe/bsyQsvvFDrZa+59tPq5K0CUzHoI6HNMPf302hsHv+a/5eX2wZXAL7/Hu6/H1QqQkJCqKio4PXXXycsLIwnn3zSo2WCAgMDndrr2bMn2dnZFBQUEBkZiRvlJ90iISSB5y9+nlt+u4V/9v8n/zfk/9CqPTsI4a6Nrl27ljVr1jBmzBgmTpzYqGPYvalgE8Zardbx25XXMz4+nl27dtUpxGJjY4mNtYn10NBQTpw4QefOnSkuLsZkMlFaWkpoaGid7TfE6Z+vO9gzIvv4+DQYjutu+0II/v3vdTz88B+OZXffPYSHHhrIU089RV5eHpWVlfh6oAyYxWJpVI3cln4O1ZzP3BLJEqVQdQOr1crRo0eJjY09qzJWSs5NCisLWZK6BICrLrgKkDYqcZ8//viDbt26ER0dXWudVqvl4osv5rvvvmPp0qUMGTLEI17Q880+hRDs/2E/m17chMVowT/Sn7H/GUt0n9OueUEBrFhhE6d//129XKezeU0nTrT9rmP+kcRzNNdGvdoz2lQyv7P9jrsMVI1PZORAUWx1eu3e1tmzbSHAp65zcXExAD4+PgQGBmIymaiqqmqRuraJiYmOBEElJSWA60ypdXG6R7EmU7tMpWtEV5LCkzzQ09q4a6P2sNbGlnyxe1PtczHtIb9gE61ms7mW19M+BzY7O7veawO2OZEGgwGVSkXnzp3Zu3cv+/fvp127dnW23xLUDPv1xPeREIIHHljFM89scCx7/PFRPPzwCIcXvLCwkKysLJKSmm8bVquVadOmkZWV5TRnuDWwz2e2TxlSqVQefxZKoeoGQggKCwtp165da3dFIuHbvd9itBjpEdmD3lG9AWmjEvfIyMjgww8/RK1W89xzz7kcTR47diw///wzGRkZ7N+/n+7duzf7uOeTfRrLjKx9ai1pK9MAiB8ez6jHRuFjr5lXVga//24Tp1u32l7YwfbS3r+/TZyOHg0BDdRYjIiw1ZOU4b4e4UzbqMlkYsOGDURFRbldG/OMUnoIinYBKoid1vz2/PxsNi4ETJrkFBpsF6ohISH8+eef/Pzzz/Tp04dZs2Y1/7in4evrS0xMTI3spO4L1RWHV/DWtrf4fPrn+GhczwVvKZEK7tmoPZMuuOdtO719+zxGIQQqlcopnFOj0VBZWekkSGNiYrj11lsdYrOh9oUQqNVq2rdvz/79+ykoKOD48eOEh4e7bL8laG5G5JpYrYI77ljGa69tdix74YWLueuuIY7/ExISKCwsJCMjwyNCVavV0q9fP4/OeW0qQohaYcWnz2VuLlKoSiRnEQazgW/3fQvA1b2uPqvm/ElaFyEEH3/8MUII+vXrV2fIU0BAACNGjGDVqlUsW7bMI0L1fOHEvhOsun8VJdklqNQqBtw2gF5zeqGYTNXidP16qJmYo1cvmzgdN86WHMld7EJV0iIIIfjrr7/o1q2bowaqJ/n555/5+eef6dq1Kw8++GCz23vnnXfYsmULs2fPZuzYsc3vYNb3tt9Ro8DHzdwH2dlQUeE8P9WOSmULB3aRlMaeJTQ4OJiQkBAsFku9ZT+ay/Tp0x3zCg8danh7o8XIU2uf4r3t7wHw6B+P8uzFz7ZY/5pDTk4OZrMZf3//RntUVSoVvXr1Ii0tDYPBQPfu3Wu9Y2i1WicvpEajcXhV7fVR60JRFPz8/OjVqxcJCQkUFRWxY8cOSktLmTx5ssv2WwJ3Eim5S2FhJb/+Wl1C6c03J3PLLf2dtklISGDHjh0em6e6du1aDAYDffr08Wgm4aZgt5mjR4+i1+sJCAhg/vz5zJw502PHkEJVIjmL+C31N4qqiogJjGFM+zGt3R3JWcSGDRtITU1Fr9dz5ZVX1rvtxIkT8ff35+KLLz5DvWsdTp482aj5UHUhhODvr/7mr1f+wmq2Etg2kLFPjSayMgOeeMImUmsmk2nf3uZVGj8eYutPtlJWVkZAQ95ViUdJSUnhiy++4PDhw4wePZrrr7/e48cYPXo0v/76KwcOHCA9Pb1JZURqYg+X9UhCE3MFZP9m+zvOzRfO48fh3ntt81BLSiAszO26pzU9qnYvV35+fovZ/unJderjyMkj3Lz4Zv4+Xh2a/+nuTxkWP4wpXaZ4vG/Npaa3sCkD2UFBQfTp0wer1eq2YNy5cyfLli2jc+fOzJgxo87tunbtSrt27WjTpg0BAQGMGzeO3bt3O+qS1idyPUVpaSmFhYUoikLcqTJJzSEiwo9Vq65l9OiPeeKJUVx3XZ9a29SsN9uY61oXmzZtoqCggPj4+FYXqmAr5RYWFgbYvlPtHn1PIYWqGyiKQnR0tPReSVoVq7Dy+Z7PAbiy55Woa8wZkjYqqY/y8nK+/NJWZ3fatGmOL5W6iI6O5vLLL/fY8b3RPrdt28Ybb7zBvHnzGD58eJPbMZQYWP3YajLW2l4Q2/cKYESHLPT/d61zPcmoKJgwweY97dzZNm+vvnYNBt566y127drFokWLCAkJaXIfJQ2jKAoajYbXXnuNbdu2AbbyEy2VhCQ8PJyBAweyadMmlixZwq01szu3NseWgaUC/OIh3M3wwnvugZwc298WCxQXw+lzTE9lkj2dmh5VPz8/IiMjOX78OJmZma0a0fH9vu+5b9V9lBvLa637I/2PMy5U3XmOespb2BgxpdFoHKGt9TFlivP1ioqK4t57723w+8iTFBUVERYWhk6n80hiI4AOHUI5cOAf+Pq6npsZHR2NTqejqqqKEydONGteaWlpKQUFBYB3JtCU5WlaCZVK5TLxiERyJtmQuYGMogz8df5c2uVSp3XSRiX18cMPP1BSUkJMTEyjs0B6Am+zz5SUFN544w1MJhMHDx5sslDN253HqgdWUZZRgLq8hEFhKXTfvRdlz6kNgoPh4ott4vSCCxzJY9xBr9dTVFSEyWRi5cqVHg2lkjhTVlbGTz/9xMqVK7FYLCiKwsiRI5kxY0aLDhBMnjyZTZs28ddffzFr1izvyBAsRI0kSjNAcdNmn34aUlJsgzOKYksA5iLMl4AAm7e1BjU9qmATWcePHycjI6NVhGq5sZwHf3+Qb/Z+U2udXqPniVFPcPUFV5/xfrnzHG2MUC0pKSEjI4MePXo0y8tnP1ZDmW3T09MxGo3ExsY65jGGhYVRWVnJli1b6NevX7PmN6anp1NWVkZsbGyd921cXBz33Xefy7qo7lBRYeLZZ9fzwAPD0eurJVRdIhVsSaSmTJlCYGBgs58n9vDhqKgojwltTyKz/rYSFovFEZpzJkITJBJXfLb7MwCmd52Ov865Ppq0UUldZGZmsmLFCsBWBL4xYW+ewpvsMzs7m0WLFmEymejTpw/z5s1rdBvCKtj9+lo2v7wRUVxCsPUkY9vtJ4Jy8POFUaNs4nTgQNfz9dxk0qRJvPbaa6xatYopU6ag91QtDQlgSzyzcuVKfvrpJ8rKyjAajVx00UXMmTPHI2GBDZGYmEjXrl05cOAAK1asYPbs2S1+zAYp/htKU0Clg9hGeAxjYmDpUvjHP+C666AuQRUWBjWSAQkhnDyqYBM+W7ZsadF5qnVxpGwf939+M4cLD9da1zm8M29d8hbd2rRO8quGnqNFRUUUFxejUqkcJWHqwmq18tlnn2E0GomIiCAmJqbJ/QoMDHQrs+23337LiRMnWLBgAR06dHAs/+ijjzhy5AgWi4XRo0c3uR/bt28nMzOTCRMm1CkIjx07RkREBLomZFIvKTGQnPwF69Zlsnv3cb75ZiZarXvfZwMHDmz08Vzhyfm1LUFL1FKVQtVN6iuWK5G0NAfyD7Dt2DbUKjWze7p+mZE2KjmdmgmU+vfvT8+ePVutL95gn4WFhTz//POUl5fTsWNH/vGPfzROOBcVUfnTclY/v4WsdNsXcsegEwyPPYJu+ECbOB0+3FaKwwP069ePiIgI8vPzWbduHePGjfNIuxKbSH3ooYfIzs4GbPUe+/fvz7Rp087oYMrkyZM5cOAAv//+O9OmTcPHx3U22TOG3ZsafTFoG1e6heBg+OyzRu1SUVGB+VRIcFCQ7Xj2l/CjR482mKDHcwjo8xH37HocRVPb23Zlzyt5csyT+Gk9m9G0sdT3HLWLmJiYmAYHtVQqFQkJCaSmppKWltYsoQrNy2zbv39/jhw5woYNGxg+fHiLDaaazWZee+01hBDcc889jcpPUFhYyaRJn7N5s+158fvvR0hNLaR7d/fmiJaUlPDHH39QWlrK1Vc33Rvv7UK1JTj3C9pJJOcAn++2zU29uMPFRAW0bt0sydnDhg0bSElJQafTMWfOnNbuTqtSXl7O888/T0FBAdHR0dx1113uiYKKCpun6PbbOTbscn64fS1Z6RbUipURw6yMeWM6ulVLYdEiW3IkD4ZjqVQqR6j28uXLHTUNJc1Ho9HQq1cvgoODmT9/Pk8++SQdO3Y84/3o3bs30dHRVFZWsnr16jN+fCeMxZBri74gvoFQc4PBI4e0e1MDAgIc9RfbtGmDr68vRqORY8eOud1WXl4eVVVVje+ETxFc+v/snXd8FHX6x9/bsumdJBBS6B1C772joiKiIlh+ltM7z3aeYr/zVDy98zzbeXp2FAEFK4Sm9N5DC4H0QHrPZuvM749hNwmk7Ca72U0y79eLF7szszPfyTz73XnmeZ7Pcw9MfxaTUNdJ9ffy5/1r3uefs//pdie1KRxtS2ONaqanp7f42NZjNkfZNiEhgcDAQMrLyzlx4kSLx9IQly5dwmw2o9VqHUrBzc+vYurUz21OamioD7/+eofdTipI6b+7d+/mxIkTVFVdXfNsDxaLpdmth9oysqMqI+Ph5Fflsyl1EwC3D7rdzaORaUtYLBZ8fHyYP3++w60K2hMmk4m33nqL7OxsgoKCePLJJwm4Uuil7gdgxw545hmYORPxuec58kMmP6cNoEodSPCAaG7c+hB9N/4bxYIFEOhg5MkBJk+ejK+vL7m5uRw7dsxlx+mILFiwgH/84x9MmTLF5S0xGkKpVDJ37lxAehjhitQ5u8n5GQQjBPSGoEayL778UnooYxVPagEhISEsXbq0jtCOUqm0CcXY6/gIgsDPP//Mxx9/zEUHxnWm/CAsnQk9E69aNyRqCJuWbuKGvjfYvT93MnfuXJ544gm7a+7j4+NRKBQUFBRQXl7eomNfqWzrCGq1mnHjpL6jO3bsQBTFFo2lIWpHI+0V/cnJKWfSpE85cSIPgMhIP7Zvv4vhw7s4dGw/Pz+bQm9z29RcvHgRk8mEr69vh/o9lx1VO7DKWHuSYqVMx+Gbk99gESwM6zyswdoY2UZl6mPy5Mm8/vrrth517sKd9ikIAh988AFnz57F29ubP//5z/VL+gsCHD4Mr7wiqfM+/jhs2oSuSmB9yVgOiSMRe/Sk98OzuXH7I4SO7tUq4/f29rbVbW3YsKFVjtlR8PHxsUXV3WmjEyZMICAggMLCQk6ePNn0B1yBKNT0To1d2LAq9fr18PTTkJIC110HycktOqy1r+bQoUPrLLc6qvbWqVZWVuLl5YVarSYiIsLu42+4+BkE5ly1/IERD/DjrT8SHxxv975cTWM2anUOIyIi7Bbl8vHxsaX8pqWltWhsVyrbOsqYMWPw8vLi4sWLnLenuW0zqN26xx7S0kqYOPFTkpMlld2YmEB27rybgQPtt6/a1Hbmm4P1czExMW57sNYUsuqvm1AqlZ6hxifT4dCZdKw9sxagUZVB2UZlGsIT2pq40z7PnDnDgQMHUKlUPProo3VTpkRRutFOTIRNm6RekFbCw8npO51fDwZQrVai9lYz4ekJ9L7GsforZzBr1iw2bNjA2bNnSU1NrSNEIuMc3GmjXl5e3HHHHYSEhDhc3+c0ig6BLhNUvtC5AWVwsxlee016qANw6ZLUI7hPn2Yf9siRIzbl2drnPnz4cHr16kV0LeGlxggMDGTJkiWUl5c7VOP4u57LWbf/MARJUa5ATSgf3fhvpnef7tiJtAKN2WhmZiafffYZ/fr145ZbbrF7n926dcNoNDZLXKg2KpWKmJgYLly4QEZGhsMtWHx9fRkxYgS7d+/m559/5rHHHnN4DH5+fgQFBTV4Lo7UdyYnFzJ9+hfk5Eg1wT16hLB16x3ExQU7PC4rsbGxHDp0qMWOakt7LrsSVzjQnumSexgWi4WzZ8+6NyVHpkPy/dnvqTRWEhccx4TYCQ1uJ9uojKsRBIFDhw41q5m3O+1zwIABPPDAAzzwwAMMGDBAWpiZCR9+CDfdBEuWSAIw+flS64zrr0d4930OXvcS63cHU21QEtozlAUrFrjFSQWphcPYsWMBOaraFKIocubMGYfTB909h44ZM4Y+ffo0OyIxevRobr75Znr27Nm8AWRdFlGKvgbUDdRiqtWwZg30u5zZ83//Bw880LzjXebcuXPs3bv3qnTd0NBQ4uPj0Wg0dl9LhUJhUw62Fz91IPz8HxDUkDWOfw3d4pFOKjRuo+np6eh0OgwO1g4PHz6cJUuW0K9fy5WMHY2C1zcWURTJyckhuRmR+tmzZ3P33XfXW2teVlZGaWkpSqXSLkXv55//zeak9usXzo4dd7fISYUaBzMrK6tZ80xcXBy9evWiW7duLRqHK5FVf91Iswr0ZWRagEWw8M3JbwCpNlXZRD872UZlXMnXX3/Nxo0bGT9+PA804+bUnfY5fvx4KCiAr7+WoqenT9es9PKCyZOldN9x46gqNbH12a3kHs0FoN+Cfoz901jUWvf+XM6dO5cTJ07YHWHqiKSlpfH1119z9uxZHnnkEUaMGOHQ59vyHOroudZBXwB526TXMTc1vm1kJKxbB599Bg891HCKsJ1c2ZqmNomJiRw7doxrrrmGQYMGteg4jZI7FL5ZB7kJhP3RM9u76fV6Vq9eTVlZGT169LhKCdla92hvWqsVZ0bAEhISiIiIaLYjFRMTQ6dOnSgoKODgwYP0aUGk/kqsznNUVJRdbb4+/ng+mZllGAwWNm1aQqdOfk1+pikiIiLw9vZGr9eTl5dHly6O1blOmDCBCRMaDli0V2RHVUbGQ/kt/TcuVlwkyDuIa3pd4+7hyHRwxo0bx8aNG9m7dy+LFi0iNDTU3UNqmvJy+O03yTk9dEhK9QVQKmHMGKmdzJQpcLnJfObuTLa9sA19mR6Nr4ZJz02ix6zWV4Ktj7i4ON5++2239MH1dIqKilizZg27d+8GQKPRUFxc7OZRtSGyvwcECB4CAXZEZAMD4eGHnXLoxhzVqqoqiouLyczMbJGjmlmWyfO/Pc/rM15vWDX/0vBm7781EEWRoqIiqqur613nCWmhnTt3bnGbm5tvvpkPPviA06dPU11djY+TVNQdbesSEKBlwwZJvDIkxDljsEZzU1JSSE9Pd8hRzcnJIS8vj27dujnUVqc9IP/iych4IKIosuKE1I/u5v43o1U3/QRQRsaVdO/enb59+3L27Fk2bdrErbfW38/X7ej1sHMnbNwIu3dLCr5WBg+WnNMZM6CWoy2YBQ6+f5DjXxwHILxvONOXTycoxrE0QlcjO6l10ev1/PTTT2zYsAHT5es8fvx4Fi5c2KFUMVuEYIGsddLrK1vSVFVJ3ycX1e6KomhTm62vlj4uLo79+/e3qH3KT8k/8cTmJ6gwVPBH0x9ZedNKVErPjJo2l5KSEioqKlCpVG7PuNi7dy+nTp1i8uTJ9OrluOBct27dmDlzJgMHDnSakwpNO6rbtqXTr184kZH+tmXOclBrEx8fT0pKCpmZmTalY3s4evQoO3bsYOzYsdx4441OH5cnI//q2YFSqaR79+4eq7Il0/44kXeCk/kn8VJ5sWjAoia3l21UpjWYO3cuZ8+e5bfffuOGG26wrw8prWCfFgvs3y9FTrdtk3qfWunZU3JOZ82Cep5gV1ys4NdnfyUvSWo/MOCWAYx5ZAwqr/Z1M9uesFgsbNu2jbVr19ocnb59+3Lbbbc1W2iqw86hBbvAkA+aYIisVZtpMsG990JWFqxcCXbU9TlKZWUlZrMZhUJBYD0tnqxORU5ODmaz2aEHNXqznhd/e5EvT3xpW7YrcxfvHXyPh0c7Jxrc2igUCrRa7VU2anXCunTpYutF6y4yMzM5d+4cMTExVzmq99xzDxaLpdE6YoVCwcyZMwHpQZRKpWrxOVksFvLypPm9Pkf1p5+SWbhwDX36hPHbb3cSFua6frnW1GxHH744GhEWBMEtc5krjik7qnbQ0CQqI+Mqvkr6CoC5PecS6tN0iqVsozKtQUJCAlFRUeTm5rJ9+3Zmz55t1+dcYp+iCElJknO6eTOUlNSs69xZck5nz5Yc1QZI35bO9r9ux1BhQBugZdILk+g21XOFKmSguLiY119/nZwcqaVIVFQUt9xyC8OHD29Ra4QOO4daRZS6zgfVZbVUQYBHHoHt26X38+dL9d1OENypTVlZGQD+/v71OqHh4eH4+vqi0+m4ePGi3fWXyYXJPPjLg5wtPHvVum9Pf8sDIx7AS9UylVt3oVKprrJzT0j7tRIXF8fhw4fr7RXqSLnIjh072LJlC/PmzWPMmDEtGpNKpeKFF17g4sWLV41h1aqTLFmyDrNZICkpn3/+cy+vvuo6Ma3Y2FgGDx5MXFyc3c6kyWSyzXdNfQeqq6s5ffo0ZrOZUaNGtXq7Lbk9jZuwWCycPn2a/v37X1XALiPjbLLLs/kt/TcAbh98u12fkW20Y3Po0CFKSkqYNm2aS6+/Uqlk9uzZfP755yQmJjJz5ky7fmidap/nz9e0k6mtFBoSAjNnSg7qoEGNirxYjBb2v72fk99IPSsjBkYw/dXpBHQJaNnYZFxOcHAwSqUSPz8/brzxRqZPn+6UlOgOOYfqsqFwr/Q6ZkHN8uJiOHas5n1VVU1LGididVQbirApFAri4uI4c+YMGRkZTd6ki6LIypMree7X59CbrxbGWtBvAa/NeK3NOqmiKKLX67FYLHXmXUf7g7oSa8QvMzPzKkfsq6++orS0lBtvvLHJ+kyFQoFer2fnzp2MGjWqRZE6s9mMRqO5Khr56adHuffenxAESbtg8eJB/PWvU5p9HHvw8fFhyZKGWw3WhzWjwN/fv8kWWl5eXuguZxQZjUa7hKOciaz660bkth8yrcXXSV8jiiLjYsbRPcT+NDbZRjsmer2eL774gpLLEUVr2pSrmDhxIt999x2FhYUcOnSIUaNG2fW5FtnnxYtSzWliIly4ULPc1xfz5Mms8vbm2gcfJMiOJ/ZlWWVsfXorhWcLARi8dDCj/jAKpbqDpXy2UZRKJX/4wx8ICgrC39+/6Q84QIebQ7OkHt2EjwXfrjXLw8Phhx/g9tulPsOffgrW1k5OxOqoNiYOU9tRnThxYoPblRvKeXLzk/yY/ONV63w0Piyfvpyb+9/c6hEmZ6HRaJg8eTKZmZl1zsFgMHDp0iXAMyKqkZGRaLVaDAYDubm5dRzSnJwcCgoK6hWEupJRo0axefNmCgoKOHv2LP3792/2mFauXElOTg7z58+37ee99w7w0EM1rb7uvXcoH3xwLSqV638HioqKOHPmDP7+/iQkJDS5fW1F56bsV6VS4efnR1VVFRUVFa3uqLoC+ZdZRsaDKDeU235olwx27KmbTMfk+++/p6SkhPDwcCZPnuzy42m1WqZPl1KjXNrTs7gYVq+WejXOnw/vvSc5qRqNpNT72msIGzfyUZcuJJ49y+v//CdCE1GfC5svsPb2tRSeLcQ7yJs5b81hzCNjZCe1jREdHe10J9UTuXTpEuvWrXO4J6xdWIyQfdmpi1l49fpOneC776SU3/HjnX98sD1cayzl2hoFa6w357HcY8xeMbteJ7V/p/5sXLKRRQMWtVknFSQhtSFDhhAfH1/nPLKzsxEEgaCgIId7yLoCpVLZYD/VsrIyRFGksLCwyf1otVpGjx4NwM6dO5s9HlEUSU9Pp7i42CbO9Prru+s4qY88MpoPP7yuVZxUgNTUVH788Uf27t1r1/aO1qda58bKysrmDdDDkCOqMjIexHenv0Nv1tMrrBcju4x093BkPJyLFy/anMWlS5fi5dU6KW0zZszgl19+4fz581y4cKHeBuvNoqpKEkNKTJTEkayOp0IBI0dKNadTp0rtMYDV33zDnj17UCqVLFq0qMH0MLPBzN5/7uXM2jMARCVEMf3V6fhFtLw3noyMKzAYDLzwwgvo9Xp69erFwIEDnXuAvK1gKgVtBHRqoDdjQIDLnFSQHNSuXbsSGdlAyxiga9euKJVKysrKKC0traMOLIgCHx7+kFd3vopZMF/12bsT7uaFyS/YrZqvUFgoKcnh0iVNi9ustBbR0dHcfffdGAwGu7Y3Go2IoujSSFtcXBwpKSlkZGQwduzYZu9nwoQJ7Ny5kwsXLpCTk9MsRWOrIrJSqSQ6OpoXX/yNl17aYVv/zDMTePnlaa36EMPqcGZlZWGxWBotNajdesj+1joB5OXlOc1RLSsrQ6vV2i2e6GxkR9UOlEolffr06XhqgDKtisliYtWpVQAsGbTEoYlTttGOhyiKfPHFFwiCQEJCAsOGDWu1YwcHB7Nw4UIiIyPtau7eqH0ajbBnj+Sc7tghvbfSv79UczpzphThqUViYiK//PILAPfddx9Dhgyp99il6aVsWbaF4vPFKBQKEu5OYPjvhqNspafnrsZisWA0Gp3aysFdFBYWuq2tjKfNoVqtlokTJ7J582Y2bNjQpKNaUVGB0WjEz8/PvhvKzMsiSjE3QtJJSRH7iu+Yq5k4cWKj6bwg/R06d+5MTk4OGRkZNke1SFfEoxsfZWvq1qs+E6gN5F+z/8XcXnMdGs+AAYc5fnwPotid+fPnO/TZ1qA+G/X29qafAyJXOTk5ZGdnExcX57Ka1oYiqoGBgRQWFjZZZ2klKCiIIUOG2Fqz3HbbbQ6PxZo226VLF37++UIdJ/WVV6bxzDON258ruFIkLKYRRe3S0lLKy8tRKpV07dq1we1qY42oVlRUIIpii5xwURRJSUmhurqaAQMGNCmIJav+upHWilTIdFw2XdhEoa6QTn6dmNVjlsOfl220Y3Hw4EFOnTqFRqNxWJzBGVxzzTUObV/HPgUBDh+WnNOtW6H2k9+4uBrF3gZupPbt28dXX0nK2IsWLWLChPojQinrU9i1fBemahM+oT5M/dtUuo6278e+LXDkyBG+/PJLhgwZwl133eXu4TSb7OxsVq5cyblz53jjjTfq7anZGnjaHDpnzhy2bNnCiRMnyM7ObvRG9YsvvmDfvn0sXbqUWbOa+P2oOA+lxwEl6AbCrbdKYmRffw2tWOeYlZWFv78/QUFBjd7gTpkyBYvFUqf10MqTK+t1Ukd0GcH717xP10DHv+c5Od3QaI56dFp5bRsVRZH//e9/dOrUiVmzZuHr23hbFUEQbG1a/Pxcl01ijfwVFRVRUVFBQIAkUqdUKlEoFA45M5MmTeLo0aMcP36cefPmOZzebG0DEx8fz7XX9mXp0sF8+eUJ/vWv2Tz6aMvUhJuLNT367NmzZGRkNOqo1m49ZO/85Ofnh0KhwGw2YzAYWhQJLS0tpbq6GpVK5bbUcs94dOjhCIJAUlJSk/VPMjLNRRRFViStAOCWAbegUTnWN0y20Y6FXq+3OWrz5s1rNHXOExAEgaQTJxBOnoQ334R58+DBByXBlspKiIiAJUtgxQr49lu4774GndRTp07x3//+F5CEo6699tqrtjFVm9j212389sJvmKpNdBnZhZu+vqldOakgRVMKCwvZsWNHm6xHKi0t5eOPP+aZZ57hxIkTmEwmkpOT3TIWT5xDIyIiGD58OODkevCs76T/lSPg7oegrAzS0+H66yE11XnHaQRBEHj//fdZvny5rRduQwwZMoRhw4bZHB6AB0Y8wKjoGiE3hULBw6MfZu0ta5vlpAKUlIQzbtx9TJs2rVmfdzVX2mhhYSEpKSkcOHDALiemsLAQk8mEl5eXQ61iHMXHx8f2m1RfmxpHiI6OpkePHgiCwK5duxz+fG0hIqVSwSefXM/GjUvc5qRasaf2GiTF4pCQELvTfgGbKjpIUdWWkJubC0hzkT1q6K6YP+WIqoyMB3Dw4kFSilLwVnuzoN+Cpj8g06H58ccfKS4uJjw8nOuuu87dw2mc9HQUGzYQv2YNyto3pIGBMGOGFD1NSAA7nrJnZGTw1ltvYTabGTlyJEuWXJ0iX3yhmK3LtlKSVoJCqWD4/cMZ+n9DUSjbrpBKQ/Tr14+4uDgyMjLYunUr119/vbuHZBcGg4ENGzbwyy+/oNdLbURGjBjBLbfcQlRUlJtH51nMmzePQ4cOsWfPHhYtWtTyqIZZBzlSyjyRc8HnRM26Hj2gGXWAzaGystLWZqW2A1ofZrOZX3/9laysLO644w40Gg1qpZr3r3mfGV/MwEvlxTtz32FiXEvTOBVtSnDJ6uR07drVrhZNVqcjKirK5ec5atQoqqur6eSEdPKJEydy4cIF9u/fz4wZM+yur62srCY1NRMvL6XN0VOrlcya5SRNhRZgr6M6YsQIRowY4bAiub+/P5WVlVRWVjb7GhiNRoqKigDcWrMtO6oyMh7AihNSNHV+n/kEajtg03kZu8nNzWX9+vUA3H777Z4pP5+fL/U5TUyEs2dRAF46HQQHS4q9c+bA2LGSgq+dFBYW8sYbb6DX6+nbty8PPvhgnRQyURRJ/iGZ3a/vxmK04NfJj2mvTKPzsLYhitIcFAoFc+bM4b///S+bN29m3rx5aBz4m7Y2giCwe/du1qxZY1N87d69O4sXL6ZPnz5uHp1n0qtXL3r27Mn58+fZvHkzCxfWo9DrCJcSwaID31gYfA38MA6WLgWDAT77DFppPrG2pgkICGgyUqNSqdi/fz8VFRVkZ2fb6uK7BHTh8xs+Jz44nk5+rVtf6wk4IrJTVVVl+5u3xsOgpmqPHaFv375ERUXRpUsXu3uD6vVmbrvtE4zGfIYN6+a2coKGiImJQalUUlpaSllZWb0PoEwmExcvXqRLly4Oz+sBAQHk5ua2KNMmNzcXURQJDAx0aap4U8iOqoyMm0ktSWVP1h4UCgWLBy1293BkPByFQkG/fv1QKpW2tECPoLwctmyRnNOjR8HaUkOlQhwzhku9ehF/552omoieNISPjw+dO3cmICCARx99tM4Pt0lnYscrO7iwUeqxGjMuhil/nYJPSNsXGGqKMWPGsHr1akpKStizZ0+rtChqDhkZGXz00Ue2m+vw8HAWLVrE6NGjPUbAyG5ycqT2SQ0RGurUyOTcuXN555132Lp1K9ddd539D6euHKcowrH/QVU1xI+Ci5ekca5ZIyluN9ImxtmUlpYCNOpAGC1GXt35Kgv7LyQuLo6TJ0+SkZFRR8BtZHTHVcd3xFG1RlPDwsJa5eGmxWLh+PHjZGZmct1119mVNtoQSqWSRx55xO59VFYauf76b7hw4Rw9eojs2lWG2Syg0TR/DM5Gq9USFRXFxYsXycjIYPDgwVdtk5WVxQcffEBoaCjLli1zaP/BwcH07NmzyWyFhhBFsU4E3p3IjqodKJVKBg0a1PZ+TGXaBF+dkGoNp8ZPbXZtjWyjHYfIyEiefPJJDAaD+9PUqqth507JOd2zB8y1WkQMHSpFTqdPRxEURLwgtMg+/fz8ePLJJ9HpdHWe7hYmF7J12VbKsspQKBWMemgUg5cMbpepvvWhVquZNWsWq1atIjExkUmTJrnfLurB19eXnJwcfHx8mD9/PrNnz/ao6K/dc2hODowaVVcA7Er8/eHAAac5qyNGjCA8PJzCwkJ27dpl62Ps0DhFEQQDiGZAAapXIeCdmnG2csTEGt1rqIdqakkqD/z8ACfzT7IldQvPxT8HJ1te89iWqW2jer3eJozUlKNqsVhs27ZWCqdCoeDHH39Ep9MxfPhwYmJi6Nu3b7N7IKtUKsrKyti1axfdunWjf//+9W5XVqZn3ryv2bMni8GD9SiVCpYuneRRTqqVuLi4Rh1Vq603py2Pt7d3i651cXExBoMBtVrtUOqwrPrrRoxGo9t6CMm0X4qri1l/XkrjXDK4Zcqtso12HBQKhfuutdks9ThNTJR6nlZX16zr3VtyTmfNgtpPYUXRKfap0WhsKVKiKHJ6zWn2/WsfFpMF/0h/pi+fTuRgzxaWcgVTp07lhx9+IDs7m5MnTzJo0CB3D+kqOnXqxB/+8Ad69+7doHPibuyy0eJiyflTKqG+ukCzWVpfXOw0R1WpVDJ79my++uorfv31V/sc1SvHKVqgqgI0SlB7gahy+jgdweqo1hdR/fb0tyzbsgydSQdITuvH4sfEi/Gkp6e3uOVGW8Zqo5mZmYiiSGhoaJNRs4KCAiwWC97e3q2WAqtUKomJiSE5OdmmbNtSPYX9+/ezfft2UlNT6dev31U2UFioY/bsFRw5cgmAvLyuPPXUaGbNar3WbY7QrVs3MjMzG7wmjvZPdSbWaGpkZKTbAyCyo2oHgiCQnJzMoEGDWpS+ICNzJatPrcZkMTEwYiCDIpp/cynbqIxLEQQ4fhw2boTNmyWVUCvR0TXtZGq1j6j7cefap6HCwI6/7SDt1zQA4ibFMeUvU9AGemC9bivg5+fH5MmT2bhxI+vXr/dIRxWkyKCn4rCNqtUN11hXVcE//iH1JQ0MhBdeuHqbgwdh1aqa9y+8cHXqbW6utB9ghtmMdswYRt5559X7WraMcQcP0u3SJcL695ceFF0xTrGkAgyAScAc4IMoKlGajZw4Arqyq3fpak6eLKWyEvLygrGKuVZbqvjw/DP8lr+mzraCYGFL9q/cKNxEVS5s3FiCv79zVGtPnGh6G0+hto064sRcuiQ5bp07d25VBz8+Pt7mqE6YMIH09HSMRiNdu3ZtspVOfYwdO5Zt27aRlZVFRkYG8bVaKeXmVjJjxhecOlUAQHi4L4mJS0lI8FxxtoSEBBISEupdJ4qi7Rq7qt9tQ+j1eoovlww4GpWVVX9lZNoRerOeNaelH+Qlg69WL5WRcSuiCCkpUuR040a4nDoGSDV4s2ZJDuqAAXDZdktLS0lKSnKqkMaV5J/KZ+vTW6m4WIFSrWT0I6MZeOvADv/9mT17Nps2beLkyZNkZWU12ptPxsVYLLB+veTIRkbW76impkp9S638+c9XO6qlpbZt1MDUzz+X0oqvZOVKepeWEm8ykZ+WBvoCKDsDggksJkQUiGYjCgABFKVlVCgCUIsid94FJ51z1g4xfHgpQUHw3XdB5OcDnU7Bdb+DkKvb44QI4UwomU9RcQCVlaXcdVc6eXmua6/SFrCmhTblqAqCQHBwMCaTiYiICLv3LwgCly5dorS0lAEDBjRrjFYHy+pwrVmzhoKCAh544IE6PXHtJSAggKFDh3Lw4EF27Nhhc1SzssqYPv0LUlKszpU///3vSA4e/AmTKYGRIz23jlmn05GZmUlERESdlkHFxcVUVlaiVqublfrbElQqFbGxsej1enx83K/zIDuqMjJuYn3Kesr0ZXQJ6MLU+KnuHo6MjEROjuSYJibW7ano5wfTpknO6YgRcEXUqbKykscffxyTyUT37t2d/uMqiiInV55k/9v7EcwCgdGBTF8+nU79O57aZ3106tSJESNGUFFR4XArg5ZiNBrtbkYv4wQEC+hzQZcNuiwwVYBglNJ7L22EbUcgrRoselCKICpQINZ8XgsBqkrMBjW0rqnUDEErhXGr9YGQ8ClM+QuoTFdt533uBmYFxaMStBy/0JXIyP0EBWWSl+eadM62UD0jCILd0TalUkm3bt2Ij4936GFeUVERa9asQa1W06dPH7va31yJ1Lu0RtnWGUyaNIkDBw6QlJREamoqERFdmTTpM9LTSwkPtxAX58enny7i3LkDpKSk0KVLF6cc11WsWbOGU6dOce211zJp0iTbcuv1jY6ObvVafo1G45Z044aQHVU7kdMpZZyJIAq2ljS3DbwNlbLl9iXbqEyzKSqSUnoTE+FkrfiKlxdMmCA5pxMmSO8bwN/fn8GDB3P48GESExO555576qxviX3qy/Rs+8s2MndKUYRu07sx+fnJePnLzlFtHnzwwVa9qcnNzWXVqlVUVFTw7LPPtvmotsM2WloqRU29vOqmASsU0Lkz+PpCeHj9n/Xzg9pOxpXHthjBmAudQyQRJMEIGf+FHe9D9UXJKbUSakLlq8LLoqJzFwWgBO8IUKaAUo1FqUYQ9aiVZqrxocrkh8poRiOacQ8CWm05JpWeqhnPQrffrt7EEACb32B4uAZ1YDJp6X0oKOh82VFtvPdkcxk6FIYMccmunYZKpUIQBGbNmkVWVpbdqZmOfjfDw8Nt/6yiOo5SW9nWWSJYkZGRhISEUFxczPr163nooYf44x9H8ac/bWLSJCWjR6tRqyvdljbrKHFxcZw6deqqfqptZfytgeyo2oFKpfLYmh+ZtsmuzF1klmXi7+XP9X2vb/H+ZBuVcZjKSvjtN8k5PXhQqkMFSXxl5EjJOZ06tf5UwwaYO3cuhw8fZvfu3SxcuNAmfGSvfdbXTy73eC5bn95KVX4VKi8VYx8fS7+brhbSkKHVnNTKykrWrVvHli1bEAQBhUJxVc1YW8PhOdRslnqPGgzS+6CgmnCcVgtffQWN7e/aa2HO1JqoaPnPcCkLqrOlZfp8QIRXfABr+l066C6/VHqBTzT4xsCaxXj7dgXfrtJ7n85w6gyoJoDaC4uooUz0JtivlKoqP/wC1GgQUZosfP4+6Ho49KdqMaIIh3On8U7qnwgRiq5a3ysggT/1+Q/B14eyb98nACxZMhyl0ovvvweF4hKvv25ArXZeTbq3t+SkepAQ9VXUttEJEya49FgKhYLbb7+9xfOsVdk2PT3dOQMDJk+ezLp168jIyKC4uJjHHx+Ln5+Gzp2L0OlKUKlUdisiu5va6dG1RcLcKaTUElwRMJEdVTsQRZGKigoCAgLkmyMZp2CNpi7otwBfjeOiAlci26iMXRiNsGuX5Jzu2iW9tzJokOSczpgBYWHN2n3v3r3p3r07qampbN26lQULFgD22Wd2djZ/+9vfmDlzJjfddBOIcOzzYxz6zyFEQSQoNogZr80grHfzxibTckwmE5s3b+aHH35Ap5M8piFDhnDrrbfStWvzWmt5Cg7PoQZDTa9gkKKoJlPdFk2iCKZSqMqSnE+rE6q7/N5Y0vgxVL6S4+kXAz6XHVHra+9OoLBDjdNsRiGCSrSgsIioRDMaRLwUZlDDsGFAKz7jtAgW3jnwDv/I+geCWuDKnIgHRzzIsgnL0Kg0VFVVoVINpqKigjlzpBT/wsIEOnXqxKhRFpqhx9Omsdro6dOnMZlMDBw4kJCQEJcdzxn3EnFxcezdu9epbYX69x9KYmIi1dXV/PTTT9x555387nc1Qm0pKSk2RWRPVRi30rVrV5RKJeXl5ZSWlhISEoLBYLCp7rY1R1WsPSc6CdlRtQNBEEhNTZUVVWWcwpmCMxy5dASVUsWtA291yj5lG21/5Ofns2LFCm677baW9b6zWODQIck5/fVXSZHUSrduMHeuJIzkBEdDoVAwd+5c3nvvPbZs2cK1116Ll5dXk/ZZXFzM66+/jk6n49SpU8yaOItdf9tF9r5sAHrO7cnEpyei8fXgcEc7RhRF9u/fz6pVqygsLAQgJiaGxYsXM3DgQDePzjnYPYeGhkpZBsXFUvaBIEj/mwwgCpJz6quGCy9AbhFYdA3vC8ArRHJGrRFRn8vOqG9X0ATbhMocxjrOykqUZiMaUcRsUKMRzShNFunuz99f2q4V2ZGxg1e3v4rRaESlUqHVSlHRUJ9Q/j3n30zvXtN6x8/Pj6lTp9a5+V28eHGrjteTsNrozp07KSgoICwszKWOqjOwOlo5OTlOcRq3bUtnwYJVPPNMd6qrT3HmzBkMBoPNjgBb9LYtpM16eXnRpUsXsrOzycjIICQkhPz8fBQKBcHBwVdlGHk6suqvjEw74KukrwCY1X0WEX72q/DJdCxWrFjB0aNHEQSBJ554wrEPiyKcOiU5p5s2STfVViIjpVYyc+ZAr17NvxFugJEjRxIeHk5hYSG7du1i2rRpjW5fWVnJ66+/TklJCZ07d2bx9MX8eOeP6Ap1qLVqxj81nt7X9ZYzBdzEhQsXWLFiBefPnwekvpcLFy5k4sSJbu+v12oIZqkuVJcN5mz46l7IS4XqPMjNhrJqiK0V3gtQgXfWZaEihVQvak3LrfN/V1D7uWbM0dFw4AAUF3PiCNx5FzbhpM/fvxxJDQ1t9R6qU7tNZXL4ZDZkbrB9p8fHjufdue8S6V9/D+Ta3/2qqirOnj2LwWBg3LhxrTJmT8JgMFBQILVgaQvRttDQUGJiYggPDyc5OblF+0pMPM+NN65Crzfz4otZPPigErPZzO7du+v8zlijt22lFCEuLs7mqCYkJBATE8NLL71EaWmpu4fmEciOqoxMK5JXmcemC5sAqSWNjEx9HD16lKNHj6JSqRyLIKSm1ij25uTULA8KklJ658yRCrFc6GCoVCpmz57NV199RWJiIlOmTGlwW5PJxFtvvUVOTg7BwcHM6TKHbY9vQxREQrqHMOO1GYR09+yIQXsnOzub8+fP4+XlxTXXXMO8efPwbgvSqI5i0eNlzIT8ItBfrJuiW50LXBEpCLr8L8oXFAGX60W71vp32Rn1iQaVm0S/oqMhOhpdWd0WNLoetGq675UsClvE3oy96JV6/jzuzzw8+mG7BQWLi4tZtWoVvr6+jB07tsM9wMrPzwcksSM/Pxc95HAiCoWCP/7xjwC88cYbtpIBR1m37gy33PItJpP0PZw6tSfTpoXi7+/L6NGjbdsJgmBzVNtCRBUkR3X37t22ulRrhLhTJ1nRHmRH1W7a5Q+zTKvzzclvEESBEV1G0Ce8j1P3Ldto+8BkMvHll18CMGfOnKbl9XNzpahpYiKcO1ez3McHpkyRnNPRo6EZqo3NZfLkyaxdu5ZLly5x4sQJBg0adJV9CoLA+++/T3JyMl4qL0ZUjOD8Cilq12d+H8Y/OR61t/wT5W4mTpxIQUEB06dP9/g0wyYxlV92QK+oFdVlodQX0s2gR1nkDfX5Pkpt3Uho7eiodyQ4QbndUcxmMwcPHiQgIKBNpWAbKg3MMsxi9OjR/G7s7xz6bOfOnVGr1eh0OgoLCzvczXxJiVTX3BaiqVZMJhM5OTm2SLCjfPXVCe6883ssFikF/Oab+7NixQK8vKTvXFlZGeXl5XTq1ImCggKqq6vx8vJqWclMK2K9lhcvXsRgMLB8+XJ8fHy477776vRW7ajIdwF2oFKp6Nu3r7uHIdPGqTJWsfbsWgBuH3S7U/ct22j74eeff6agoICQkBBuuOGG+jcqLYWtWyXn9OjRmuVqNYwbJzmnEydKzqob8PHxYerUqaxfv57ExEQSEhLq2Kcoinz55ZccOnQIoVqgX1E/DBUGND4aJjw9gV7zerll3DJXo1QqWbhwobuHYR+iCIYiSbSoKqvmf6uQkam8wY8qFODj36lhZ1Qb5vQ0+ZaSmJjIqlWr6NatGwMGDPCo6OKPyT/io/ZhZo+ZV60rKysjXAxnXKzjqbtqtZquXbuSnp5ORkZGh3FUDQYDP//8s00dti05qllZWXzwwQfN+uxHHx3md7/72aZbduedQ/jf/+ajVktZQUeOHGHNmjX07NmTe+65xxaV7Nq1a5vR6wgODiYwMJDy8nKOHj2KTqfDaDR6vBBUfciqv25CEARKSkoICQnpODU5Mk7nh+QfqDJWERccx/jY8U7dt2yj7YOCggJ++uknQBINqROF1Olgxw7JOd27VxJJAunmedgwyTmdPh085Mdt1qxZWCwWZs+efZV9/vjjj2zevBldoY6BhQMJJJCw3mHMeG0GQbFtSzyiLaDT6fBtLxKpoiCl4lqdz9oOaXU2WPSNf94rrEY5t9b/gncXSiotbWoOnThxImvXriUtLY1z587Rp49zs3SaQ7Wpmhd+e4Gvkr4iyDuILUu3EB1Ytw7WWnsXHBzcrGPExcXZHNURI0Y0/YF2gDWltaysDIVC0aYcVauyrSAI3HvvvXYrhP/73/t49NGNtvcPPjiCd9+dh1JZ80AmLi4OQRBITk4mLy+P2NhYZs6c2aayP6zX8/z585w6dQqQxOqa07vW3chiSm5CFEWysrKaPanKyFgECytPrgSkaKrSnrYCDiDbaPtgxYoVmEwm+vfvL9XdmEySU5qYKDmp+lo34X37Ss7prFkQ4XmiXGFhYSxZItVhWywWm30WFRWxds1ayjLK6Ffdj650pf/N/Rn72FhUXm3jCXhbwWw287///Y8DBw7w+uuvEx4eXmd97b59HoXFeFm8KOuKFN1sablobuTDSvCJqj8q6hMN6vqzDESLhayspKbn0G++gZUrYdIkKbV+6FCX1nw3RlBQEOPHj2fbtm1s2LDB7Y5qcmEyD/zyAMmFkmhOmb6M36//Pd8t+g61UrrdtFgsVFRUADRb0dTqpFmjZx0BjUbDyJEjSUtLIyAggMjI+oWnPBGrsm1WVhbFxcV07969yc+88cZunnxyi+39n/40ljfemHnVfBUWFsbAgQNJSkpix44d3HzzzURFRTn9HFzNwoUL0Wq1rFu3Dmg79bVXIrenkZFpgxTqCnl5x8tklWXRya8T1/S6xt1DkvFAjh07xpEjR1AqlSwdNgzF8uWwZQuU10pXjImRnNPZs6GNKBoCfPf1d7z3n/f4w4N/YHT8aAZlDSKnOof+fv2Z9Pwkus9o+sZFxnHUajWlpaWYTCY+//xzLl26xF133UVUVBSrV68mOjqa66+/3jkH0xdC9lrougC8w5ve3qyrt1YUXTbo84BGbngUGvCNrl9J16czKF3YxmjrVjh4UPr3ySeQlOS6Y9nBnDlz2LZtG7t27eLIkSP87ne/Y/x452bsNIUoinyd9DXP/fYcBrOhzrqDOQfZkLKB6/pcB0BFRQWCIKBUKvH396+zbWZmJlu3bmX69OmN3qhbHdW8vDz0en2r6DOUlZWRlpbmthZwarWavLw8ioqKbBHKtoQ1Cr5y5UpKSkqYO3duo9sPGhSJRqPEZBJ48cXJvPji5AYfqk2cOJGkpCR27drFnj17GDFiBIsXL/bMh3AN4OPjgyiK7Ny5k+Li4jYZTXUV8l9CRsbFFFQVsOLECqL8o7i5/81o1dqmPyTToTAZjXz5n/9AXh6zjUa6PvtszcrwcClqOmcO9OvncXVyTSEIAp999RlVhio++uAjig3FBBJIj/49mL58OoHRnpGq3F6ZO3cuJ0+eJDExEaPRSF5eHmq1GrPZjLe3N7Nnz3bOjb6hEM5/CJ0mSY6qKIKprP6oqC4bjMWN70/lW39U1Ler1O7FyVkpdmGxwM6dNe8nTnRbNNVKdHQ0gwcPZsOGDWRmZvLpp58ybty4VrtJLzeU8+TmJ/kx+cer1vlofFg+fbnNSYWatN+goKA6zpb1Jv3ChQt4eXk16mgEBAQQGhpKcXExmZmZ9O7d27kndQWCILBq1Sp0Oh2hoaFuiXaJosiuXbswGo0UFRV5bjZEA8TGxmIymQA4cOAAc+bMaXT8c+b0ZNWqhVy4UMITTzReyxwfH09MTAzJycmoVCoOHDjAbbfd1qb+PoIg8Pzzz1NaWoperycpKYlZs2a1qXNwFbKjaicBAQHuHoJMG+Vs4VmqzdVolBoW9nedKIlso22QzEzYuBHlhg3MycxkC7AAwN9fqjedMweGD3f7zXBL+P777yksKgQRSvWlJCuSWbR4EaMeGiWn+rYCgwcPxt/f39YHNSUlhejoaIYPH351HXRzMZZA/i7Q58PZf4KlWnJMzVWNf04T3HB/Ua+QVn8o0+QcqtPBokWwfTukpEipvx5A7969+e6771AqlaSmppKUlMTgwYNdftyjl47y4C8PklmWedW6/p36899r/0uP0B51lpeVlQFX16dmZGSQnp6OVqu11Z821gczLi6O4uJiMjIyXO6oKpVK4uPjOX36NKmpqW5xVM+dO0dJSQlqtZrCwkKPqUm2F7PZjMViQa1Wk5OTc9X463O8b7yxn937DwsLQ61W26L1u3btYtKkSU4bv6tRKpVUVFSg1+tRKBScP3++zV1jVyE7qnagUqno0aNH0xvKyFymUFdIoa6QY7nHWL5rOQD9OvUjvyqf/Kp8wn3DCfe1Iz3OTmQbbUMUFta0kzl9GgAVMNPLi+kTJ6KcO1dS7vVyU+9FJ6Er1FGcWsx//vkf6eYBJQICpwNO02NODwzlBnzD24nAj4djsVgQRRG1Wo0oikRFRbFs2bLmpw8KZig9AZe2QMEuqEqVRIwMRZD3G6guO78KNfh0udoJtb7W+Dd+nFbErjk0IABeekl6fekSeEAfS1EUOXjwoO36WiwWVq9ezaBBg1wWjRFEgf8e+i/Ldy3HLFxdL3x3wt28MPmFerOHgoKCGDVqFGFhYXXOYffu3ZhMJoKCgigrK2P37t3ExcU1eA5xcXEcPXrU1jPT1XTv3p3Tp0+TlpbG5MkNp6G6AlEU+eWXX1CpVHTt2pXi4mISExPp3bt3m4i4iaLInj17bO8tFkud8ZtMFu666wcGDYpg2bIJDu9fEATOnDlje61SqdiyZQsTJkxoMynS1tpOURTRarWYTKY2dY2tyKq/bkIQBPLz84mIiGgzRi/jXr49/S2v7nyVouoiALxV3pwuOM2StZK4zP3D7+f+4fc77XiyjXo4FRXw66+wcaNU22YVHFAqYcwYKXI6eTJKD7jxdQYWo4XNT23m58SfKY0qRSEqUKBAiZLiimKW37GcJfcvYfj9w9091HZPUlISly5dIjg4GB8fH1QqFRkZGZw8edKxqJvuIhTtg4I9UHQALDrQF0gpvyA5p14hkiqvYJbqRHveB73/4JoTczIOz6Ee0qMxKSmJY8eOERUVhdlsRqvVcuzYMZdFVQt1hTyS+Ai/pf121bog7yD+NftfzOk5p8HPx8fHXxUptUZTfXx8UCgU+Pj4NBlVjYuLIywsrI7D60piY2NRqVSUlZVRXFzcascF2Lp1KwcPHiQoKAiz2Yyfnx9nzpxpMxG3c+fOcebMGby9vVEoFPj7+9vGHx/fg1tv/Y7vvz8LgJ+fhj/+cbRD+9+1axdVVVUoFApUKhUKhYLKyso2FVU9d+4cBoMBPz8/QkJCMJlMTr3GoihSXV1NZWUl4eHhLrtPlFV/3YQoiuTm5naYfl0yLSOvMo+dGTsJ1AYSqA1kWOdhnCo4xfOTnqdvuNRL0pnRVJBt1CMxGKR6tsRE2L1bUvC1Mniw5JzOmAHtqKG3KIqk/ZrGgbcPUJxRTEpkCqJCRK1SI1pEVGoVZsHMubBz9LnB82+w2jqiKLJ69WqMRiMREREoFApEUaSioqLpqJtFD8WHoXCv5JzqrohcaYKletTA3hCcAPpcOPUyDHgOAi/3zNU6d55zJW1xDm3o+lZWVrokqrorcxcPrX+I/Kr8q9aNjB7J+/Pev6oVzZWcPXsWQRCIiYkhICCgTjTV2kLJy8uL6urqRqOq0dHRPPXUU845MTvw8vIiJiaG9PR00tLSWs1RzcnJ4euvv0YQBEwmExqNBm9vb6qqqtpExE0URRITEzGZTDbxLJVKRWVlJT//vJ7Nm8PZuDEVAC8vFfHxwQ7tXxAEtmzZYtvvlcvbQlTV+jcym82EhYWhUChQKpVOv8YnTpzAZDLh4+PjslIxWfVXRsbD2ZGxg79s+wvlhnLCfMN4ftLzdA3sypK1S+gb3tfmqMq0UywWOHBAck5/+02qabPSs6ek1jt7NnTp4r4xuoj8k/ns/dde8o7nAZAemI6uWodapUalVGG2mFGqlKgUKgqKC9i0axMLFixw86jbN9ZoW0BAgO1GR6FQEBAQcHXUTRSh8oLkmBbuheKjINZ6uIISQoZA+FgIHyM5o7UFjcqkiAiBfSFInudaA4eubwvJrczl9rW3Y7KY6ixXKBQ8MvoRHh/7uK0FTWNs2rSJ7Oxs7rjjDgYOHHhVNNW6T3uiqiaTiZycHLRaLZ1bIcLdrVu3Vu1JXFxczDvvvINOp0Or1RIaGmqrYWwrUVVrNNXPz6+Os+Xj48uuXUc4eLAz4Ievr4YffriVGQ4qwO/atYvKysp617WVqGp9fyNnX2NrJLukpISKioo2pWkiO6oyMk7AZDHx7oF3+SrpK0ASklg+fTnRgdGcLTzr5tHJOIXCQli7FhYskJR4rYii1KIiMRE2b4aSkpp1nTvXtJPp2bP1x9wKVFys4MC7B7iw6QIAam81g5YM4qeff0LQCagUKgRBQEREEAQUCgWCIPDpp59yww03ePzT7raKNdpWXV2Nr68vBkNN2xCVSkV1dTWrV33NoPBcFEX7JOfUUFB3J96dodNYyTkNHelRNaWtjskEGhe2vXEQu67v5agqtDwaE+UfxZ/G/onXdr1mWxbhF8G7895lQqz9dYXll9ttBQcH26KpBoMBLy8vmyosSOIyBoOh0ajq1q1b+fXXXxk5ciQ333xzC87OPgYPHsyQIUMc+kxmZibJycn06dPHIRGmyspKPvroI3JzcwFs6aBmsxmj0Wj7+/z444+MGzeO/v37ExIS4tDYXI01UmgwGPD29rZdX0EQycwsRxSN9OhRhMkUzPr1S5gwwTGRqtrRVGs2gRXre0+Pql75NzIajbZ11mvsrKhqQEAABQUFnD59GrPZTExMjEdH463IjqodKBQKQkND28QFlWl9LlZc5OmtT3Mq/xQAiwct5o+j/ohGJd3UhPuGc//w+52e7lsb2UZbgcJC+PBDmDRJclQvXJCc040b4eLFmu1CQmDmTMlBHTSozbWTsRdDhYFjnx7j5MqTWEwWFAoFva/rzYgHR6DwU1C+shylUonFYgFAVIpYBOl1bYXD1opOdDTMZjN5eXn4+Pigqx3ZF0wgGPBRGshP3oL5yBY01jsBpRZCh0uOaadx4Btrv/1qw6Hn/W0q3bc2Tc6hM2ZAYCBMngzXXCO1inIjDV7fy/j4+JCfn4/ZbAac42D/YeQf2JW5i12Zu5jabSr/nvNvh37XzGYzFRUVgCSqZLFYKCsrQ6vV1rlBt6LVaikrK7OpxV6J1fHLyMho5hk5hqO/r4IgkJKSgsFgICUlxe7+pwaDgU8//ZTCQqn+OygoyPYgwiqMBtLfJycnh8LCQo4ePcrUqVM96h7AbDZTVFSEVqtFr9cDkpOal1eFyWQBlAQECGzadDtjxjiupGwwGGz7bSjlVK/XYzAY8PHxafZ5uJL6/ka10Wq1FBUVYTab0bTwQZmfnx9Go9FW/2qxWOjWrVuL9nklrrA/2VG1A6VS6RY5chnP59e0X3lp+0tUGisJ1Abylyl/YVJc3TQTq6PqSmQbbSVMJli3Do4fh8vtPgDw9YWpU6XI6ahR0I6bdQtmgdPfnebIh0fQl0k/rNGjohnz6BjCetfUbX388cfk5eU1uJ+oqCjZSXUhGo2G1157jYrCdCg5dvnfcUkE6TKBPqAJ7nE5nXcshAwFVTPVpr0vO6ptlEbn0KwsqR0NwOHDkiK3mx1V2/W97PjVR2BgYItvbmujUqp4d967/Jj8I/839P9QOtjLtry83KZO7Ofnh1KpZOnSpVRXVzf4GR8fn3qdVJAElQDy8/NbNSXXXrKysmzOh16vJysryzbmhrBYLHz11VdkZWXh5+fH/fff3+B5lZWVcfbsWdRqNSaTidLSUo+Kqmo0Gh5//HGqqqQ2VXl5ldx99w+kpUlZR6GhvqxceRtjxjT+N2kIHx8fHn74YUpqZzFdQVhYmMc6qXD136g+/P39nfI9tlgsmM1mFAoFZrMZLxd0FnBF5Lr93k05EUEQyM7OtvtpmEz7x2gx8ta+t1h9ajUAgyMH8+r0V4nyj3LLeGQbdRGFhdK/vDx45RXJOf34Y/D2lpzRceOkVOAJE6RldtDWGrVbEUWRjO0Z7H97P2WZUi/EkO4hjH5kNDHjrk4h6tmzJz0vpzvL9tmKWIxQchQK9xBWuJewSkmoBAUQCmgCIWx0Ta2pd4Q7R+sxNGqjO3bUfe8hNW+uUL29UHyBY7nHuKn/TfWuj/CL4N5h9zZr39YeqoGBgba/cUBAQLPr5fz8/AgPD6ewsJDMzEz69vWc2mhBEDh//rxtvhdFkfPnzxMTE9PgHCiKImvXruXs2bNoNBruuusu4uPjqa6uJiUlBVEUqaqqwu+yOrw1EqdQKFAoFKSnpxMcHOzy35eKigpSU1Pp3LkzERGNzx/BwcG2nrmVlYXk5iqpqPAmOjqAjRvvoE+flmVgdO3ala5du7ZoH+6m9t/IVYiiyMXLmV9W+8jPz6dLly5OtRdZ9ddNiKJIcXEx0dGNq9nJdAwyyzJ5euvTJBcmA3DnkDt5cOSDdglJuArZRl3E2rXwwQeQni6p+AJcrrEiIAASEqSUQDsxm828/PLLjB07lhkzZrik55grKDhdwL639nHpyCUAfEJ9GPHACPpc3welqmnHU7ZPFyKKUJVRSwTpMAiGWhsoIXhATdQ0aEBdESQZoAkbHTwY7r8ftm+H/Hwppb8dsubUGp7e+jRGi5FuId0Y1nmYU/dvdVRr35QXFxcTEhLS7JvluLg4CgsLycjI8ChH1RpNtSq4CoLQZFT1zJkzHDx4EKVSye23324TkRIEgbKyMluLEbPZjMViobq6GoVCgVqtRqPRUFxc3CpR1X379nHq1CkSEhKadFRr06dPOJs3L+Wee35k9eqFdOvmOdHf9k5paSnFxcWo1WoEQXCZvciqvzIybmbj+Y28svMVdCYdwd7BvDT1JcbFjHP3sGRcxQ03SK1ljEbQasFshr/+Faw3ROGOPQ1OTEzkwoULFBQUMGHCBNuTcU+lMq+Sg+8dJGW9lPao8lIxeMlghtw5BC8/56cNydiJqVLqZWrta6rPrbteGyFFS8PHQvhoKYoq03wGDapxTsvLoY08YLKXKmMVT299mm9Pf2tb9uAvD7J56WYCtc6zndLSUkCquQTJcf3iiy8ICgrijjvuaNaDu7i4OA4fPkxmZmbTG7cSV0ZTrTQVVe3Xrx+zZ8/G39+f/v3725ZrtVr69u2LIAikpaURHx9PWlqaTYTKmhptMplaJararVs3Tp06RWpqKpMnT3boWIMHR3LgwL1tMquorSKKIunp6VgsFry9vRFFEaVSiV6vb7UofEuQHVUZGTvQm/X8Y88/+P7s9wAM6zyMl6e9TISfnDbXrvn2Wzh1ShJReeYZ+MtfJCe1GU/ui4uL+f777wG49dZbPdpJNVYZOfbZMZK+SsJilASQes3rxcg/jMQ/sgMrv7oLUYDyszVR05ITQK0UK6WXVF9qjZr6d2+3Il5uJ7B9Of0n80/yu59/R1pJWp3lWWVZ/Hvfv3l+8vNOO9aVEdXCwkJUKhX+/v7Nzi6xRiczMzMRBMEjSguujKYCjUZVk5KSyMnJYfbs2UyfPv2q/anVajp16oTFYuHSpUuo1Wpby5ra9butFVWNjY1FpVJRXl5OcXFxg+nnhw9f5OOPj/LOO3NR1cq88WSnqD1ijaZqNJo634/WjMK3BNlRtQOFQkFUVJT85eqgpJak8vTWp7lQfAGFQsG9Q+/l3mH3olJ6zlN12UZdwObN8Mkn0uvnnoPujvV3u5KVK1diMBjo2bMn48ePd8IAnY9gETi77iyH/3uY6hJJ4KTz8M6MfWws4X2bX0sk22czMBTVOKaF+8FUWne9X9xlx3QchA4DlX010jL109FsVBRFPj32KX/d/tereqMCLBm8hCfGPeHUY14ZUe3RowcPPPBAvarF9hIZGYlWq8VgMJCbm0sXN/eovjKaemXN3pVR1dTUVFauXInZbCYiIoJhwxpOt1YoFERGRpKZmYnZbEar1dpU1a3rzWazy6NkXl5exMTEkJ6eTlpaWr2O6u7dmcyb9zXl5QYMBjMffTQfpbJjfLc8CWs0tbXsRVb9dRNKpZKoKPeI5Mi4l5+Sf+Lvu/+O3qwnzDeMv039G6OiR7l7WFch26iTOXdOip4CLF0K8+ZJokr33+9wui/A6dOn2bdvHwqFgrvuussjnvrXRhRFsnZnsf/f+ym5rMgYFBvEmEfHEDsxtsU/PrJ92oFgklR5rc5pxbm669V+EDZKck7DxoCve2/I2xvt2UYtogVi9oNfPlRFUGrszd0/PMGmC5uu2jZAG8AbM99gfp/5Th9HZWUlULdGVaPR2BzX5mBVa05JSSEjI8MjHFWTyVRvX0/r/yaTCUEQKCgo4PPPP8dsNjNgwAASEhIa3bdSqSQyMpK0tDTUanUdp8OKWq2murra5aJ93bp1Iz09ndTUVEaMGFFn3datqcyf/w06nfQA5Pz5EvR6M76+ntOHuKNgrWtuLXuRVX/dhMViIT09nfj4+DYjfiLTMnQmHX/f9Xd+SfkFgFHRo3h52suE+oS6eWT1I9uoEykpgccfl8STxo6FP/5RWh4eLjmqDmI2m/niiy8AmD59epPtCVqbonNF7HtrHzkHcgDwDvJm+O+G029BP5Rq5/zoyPbZAFVZlx3TfVB8ECxXtOkI7F9Taxo8CNwo2NbeqddGjUaoroYWOFLuZn3Keh458CwsuQAK6UZ14W4RXy9vvNV1o/BDOw/l/XnvExfsmjnq97//PTqdzultMeLi4khLS7M5wu5ErVYzbtw4qqurOXv2LBaLhe7du9dpkeLr60tlZSX/+9//qK6uJi4ujsWLFzd5k2+10YSEhEbVVa9M8XQF3bp1IykpiZiYmDpOzs8/n2PhwtUYDJKtzZrVg3XrbpGdVDehVCoZPnw4JtPVWRNWnGkv9TnDLUX+1bOTxnqVybQvUopSWLZ1GRmlGSgVSh4Y8QB3JdzlcM+41ka2USdgMsFTT0FuLsTGSi1pWjiBb9q0iZycHAICArjppvpbPriDqvwqDv7nICk/S20PVBoVA28byND/G4qXv/OFkmT7BMw6KD5UEzXVZddd7xV62TEdJ4kgeXlu3VB75Cob3b4d7r5bUveeNAnuuw88uJbrStanrGfJ2iVUGw1g9gWVAbwqMQgiRr2eYO9gm7P6+5G/56nxT6FRucahsDozrqjNnzhxIlOnTnVqz9iWEBgYiJeXF0qlEpVKRWxsbB1HQKfT8eGHH1JWVkZERAR333233WOvqKjwiAd+gYGBLFmypM6yNWtOsXjxWsxmyYm+/vo+rFq1EK1WdjXciVarRavVunsYzUa2HhmZy4iiyLqz6/jHnn9gtBiJ8IvglWmvMLTzUHcPTaa1+Oc/4cgR8PWVXrdQOKWkpIR169YBcMstt+Dv734hIpPOxPEvj3PiyxOY9WYAeszqwaiHRhHQpXn9DGUaQBShIgUK91wWQToOorlmvUINIUNqak0DesqtYzyJ7dtBEKQ54cQJ+P3v3T0iu7EIFp7d+iwGiwFfVSB6r1JQGZEa6oKISIWxgi4BXXhn7jtM7TbVpeMpKSnhX//6F506deLhhx926r6t0cqKigq0Wq3TI7bNwRrd9fX1reOkmkwmPv/8c/Ly8ggMDOSee+7B19fXXcN0Gl98cZy77/4BQZDSnW+9dSBffHEDGo2cQSPTMmRHVUYGSZ7/lZ2v2Gp2xseM569T/0qwd7B7BybTeqxdK6n8KhRSJLVbtxbvctWqVej1enr06MHEiROdMMjmIwoiyT8mc+g/h9AVSeIlkUMiGfvYWCIGyurVTsNYIokfWaOmxuK6632iodM4yTkNHQHqtn+T2m7ZsaPm9fDh4AEPmuxlf85+LpRcwFfti86ou+ykSiioqUV7bfprLndSQRJSMhgM6PV6l+z/448/Jjk5mbvuuqtOaxd3YY3OBwTUPPwTBIGVK1eSlpaGt7c399xzj0errdrLf/5zkN//fr3t/f/9XwIffnhdHaVfGZnmIjuqdqBQKIiJiekwaoAdjTMFZ3h669Nkl2ejUqp4aORD3D74do9P9a2NbKMt5OhR+Pvfpde//z04yam84YYbqKioYOHChW4VUMram8X+t/ZTfEFymgK7BjL64dHET41vFZtp1/YpmKHspNTPtHAflJ8BajU9V/lIDqm1dYxfjNuGKtMwV9moKMLf/iZFVbdvh8mT3TtAB8mvysciWlApVRiF6qvW+3v5YxEtmGtH+F3IlYq/zibwcvZLRkaGRziq1ohq7SyaX375hZMnT6JWq7nzzjvp3Lmz3fvT6/WsXr0ak8nEwIEDnT7e5mI0Wvjf/47a3v/xj6N46605ssJvB0VW/XUTSqWywT5RMm0XURRZfWo1b+1/C5PFROeAzrw67VUGRQ5y99AcRrbRFpCbC08+CRYLzJoFd93ltF1HRUXx5z//2Wn7c5TiC8Xse2sf2XulWkhtoJZh9w6j/839UbViSla7s0/dRSjaJ0VMiw6Auaru+oDeNbWmwYNB5f5URJnGucpGFQrJObU6qI2I13giEX4RqBQqLIIFP3UgZXoDqA2gkN5rVWoMFkOr9QIvLy8HXOeoxsfHc/DgQTIyMlyyf0cQRdHmqNaOqA4cOJDDhw+zYMECevTo4fA+i4ulB42epBrv5aUiMfF2pkz5nPnze/Pqq9Pb5wNJGbuQVX/dhMViISUlhV69erm9gF3GOZQbynlp+0tsS98GwJT4Kbww+QUCtW2zmbtso81Er4c//UlS+u3dG55/XrpBbePoinQc+uAQyT8kIwoiSrWSAYsGMOzeYWgDW19Uoc3bp0UPxUcup/PugaorboY1QTXqvGFjwLv5PWdl3EOTNupBzoE9jI4eTY+QHpwrPoc3gWD0AmMAKMx4aVXozOX0CevD6OjRrTIea0S1dmsaZxIbGwtAVlYWFovFrfOMXq/HbDajUCjw9fXl9OnTxMfH061bN5566qk6CsCOIIoiBoMBi8XiUc5qp05+7Nt3DwEBbVewR8Y5yKq/bsRVdRUyrU9SXhLP/PoMlyouoVFpeHT0oywasKjNPwWUbdRBRBFeegmSkyUlzzffhGbeQHgKZr2ZEytOcPzz45iqJTn6btO7MfqPowns6t6HMG3KPkURKlNr6kxLjoJgrLWBUmoXY601DewriyC1A9qUjTaBSqnilemvsGTtEnTGclD6gqAChYjOUo6Pl5aXp72MStk6Dp2rU387deqEr68vOp2OixcvEhPjvhR7azTVz8+P5ORkvvjiC8LDw/n973/fYuGkxtrStAaCIPLPf+7h/vuHExRU095IdlJlXIXsqMp0GARR4KsTX/HuwXexCBa6BnZl+fTl9OvUz91Dk3EHn38OmzaBSiXVp0ZFuXtEzUYURM79co5D7x+iqkBKQ40YGMGYx8YQNaTtnlerYiqvJYK0Dwz5ddd7R9XUmYaNBI2skCzj2czrNY8VC1bwyI/Pcr76ch9VUUWMbx/emv8y83rNa7WxlJWVAa5zVJVKJTExMSQnJ5OZmelWR9UqpFRVVcV3332HIAjExMQ0O5LqKZjNAvfe+yOff36cH35IZuPGJfj5yWUNMq5FdlRlOgSl+lJe/O1FdmftBmBWj1k8O/FZ/Lyc39NNpg2waxe89570+sknYdgw946nBeQczGHfv/ZRdK4IgIAuAYx6aBTdZ3Zv81kCLkUUoOxUTdS09BRQK1qh9LosgnS51tQvrl2khcvYwfnzEBbWpnqmNsS8XvPwGzWbKbfvB798qIrgo69GM7lX66bGWh1VV6X+AsTFxZGcnEx6ejrjx4932XGawt/fH5VKxS+//ILJZKJPnz4sXLiwTc/HRqOFJUvWsmbNaQD27ctm9+4sZs1yrNZWRsZRZEfVDpRKJd27d/eomgAZ+zl66SjP/vos+VX5eKm8eGLcE9zY98Y2/aNxJbKNOkBaGjz7rJTeedNN0r82SElaCfvf3k/mzkwAvPy9GHrPUAbeMhCVl2fVgXqMferzpWhp4V4o2i9FUWvj370mahoyFFRyOltHoY6NLlsGe/fC4MGwcCHcc4+7h9ciVAoVZI2r9b51j28ymWzpsK52VAEyMzNddoz6sFgsXLx4kaKiIhISEvD29mbHjh2YTCa6du3KkiVLWlwzq9FomDJlCnq9vtXrb/V6MzffvIaffz53eSxKvvlmocNOqrV+UaFQuP+3QMYlyGJKbkKhUNikz2XaDoIo8Nmxz/jg0AcIokBccByvTX+NXmG93D00pyPbqJ1UVEjiSVVVMHQoPPGEu0fkMNXF1Rz+8DBn1p6RhJJUSvot7Mfw+4bjHezd9A7cgNvs02KU6kutUdPKC3XXqwMgbJRUaxo2BnwiW3+MMh6BzUZ1Ojh4UHqQdfy4NE+0QYwWI+vOrGNat2lAJ7eOxRpN1Wg0Lk1/jY2NRalUUlJSQllZmcvSjK+koqKC7777DoVCQbdu3fj0008pLi4mPDycu+++G6225Q+81Go1Q91gi1VVRm64YRVbtqQC4O2tZu3aRcyd6/h91IoVKygpKeHmm28mOjra2UOV8QDk9jRuwmKxcPr0afr37982FSs7IMXVxTz363McyDkASOlPyyYsw1fTMiEDT0W2UTsQBCmSmpkp1aP+/e+g0Thl1yaTCbVa7dIovdlg5uTKkxz95CgmnSSUFDc5jjGPjCEotnVuyJpLq9mnKIIu83JP071QfBgEQ60NFBA0oCZqGjQAWklMRsazsdnopUuoTKaaFW2sf6qVPVl7eGzjYygUCuK1w2DkHDj4e6D1M4l8fHyYP3++TQnXVWi1WiIjI7l06RJZWVmt5qgGBwcTHR2Nv78/X3/9NRcvXsTf35977rmnTnualtLav/NlZXquueZrdu/OAsDPT8PPPy9mypR4lx9bpm0iq/66EVf88WVcw4GcAzz363MUVxfjrfZm2YRlXNv7WncPy+XINtoE774Le/aAVgv//CeEhjpt15988gklJSXcddddRDlZlEkURM5vPM/B9w5SmSulz3Xq14kxj42h8zD7G8a7G5fZp6kSig9KjmnBXtBfqrte2+myYzoGwkaDl2c79TLuw2KxwIQJsGoVbN8OO3fCuHFNf9AD2XxhMyC1NEkuPww9gYN/cMtY/Pz8mDBhQqsc66abbsLPz49QJ87v9h535cqVpKWlodVq+b//+z+X9I5urd/5oiIdc+Z8xaFDFwEICtKyYcPtjB3bfJGqW2+9FVEU0TjpAbFMx0B2VGXaDRbBwkdHPuLjox8jiiI9QnuwfPpyuod0d/fQZNzNhg3wxRfS6xdfhD59nLbrlJQUdu3aBdS0JXAWl45cYt9b+yg4XQCAf6Q/I/8wkp5zeqJQtp8aa4cQBSg/W1NrWnKcOiJICg2EDq2Jmvr3kEWQZOxHq4WJE6V/bRRRFNmUuqnuwtSZ7hkMcPbsWXJycujRowfx8fEuPZa1n6ooii49zpUYjUZKS0tRKpUsXbqUrl27turxnc0bb+yxOanh4b5s2rSEoUNb9mDUGSnQMh0P2VGVaRfkV+Xz3K/PceTSEQBu6HsDT4x7Am+1Z9bsybQip0/D3/4mvb77bpg1y2m7FgSBzz77DIDJkyfTs2dPp+y3LLOM/W/vJ31bOgAaXw0JdycwaPEg1NoOOG0biuqKIBlL6q73jZWc0k7jIGQYqNt2GwgZmZZwpvAMOeU5dRdecN685yinT59m3759TJ8+3eWOanV1NStWrODSpUs8/fTTrRK9q6ioICAggPvuu4/MzEyn/Q64k5demsrJk/kcOXKJLVvuoH9/99Y5y3RcOuAdj+MolUr69Okjq5R5KHuy9vDCby9Qqi/FV+PLMxOfYU7POe4eVqsi22gDFBVJgklGoxQhefBBp+5+69atZGZm4uvry6JFi1q8P32ZniMfHeH0mtMIFgGFUkG/Bf0Yfv9wfELbrvPlsH0KJig9IaXyFu6BinN116t8JREka9TUt4vzBy3ToWhPc+imC3WjqRHeMRQVOi+LxFFc3UO1Nt7e3ly8eJGqqiouXrxoUwJ2FUeOHGHt2rUsXryY/v37u9RJbU0b9fJS8e23i8jNrSQ+Ptjlx5NpH8iqv27Ey0tuauxpmAUz7x98ny+OSymdvcN689qM14gNinXzyNyDbKNXYDTCn/8M+fkQHy9FVZ04iZaVlfHtt98CcPPNN7dI1dZitHBy1UmOfnwUY6URgNgJsYx+eDQh3dt+L0ewwz512TV1psWHwKKruz6wX02tafBgUMo/XzLOpb3MoVc6qqNCZ3HGDSJKVoqKihBF0aWtaawoFAri4uI4ffo0mZmZLnVUz507x5o1a7BYLFy4cIH+/fu77FhWXGWjZ84UoFYr6dWrpq7W21stO6kybkf+pbcDQRBISkpi0KBBsqKqh5BbmcszW5/hRN4JABYNWMSjYx7FS9U+bjQcRbbRKxBFSdX3xAnw94c335T+dyKrV69Gp9MRFxfHtGnTmjlMkdTNqRx49wAVFysACOsdxphHxxA9qv3I99drn2ad5JBaU3p1WXU/5BUqOaXhYyURJG3riqPIdCwEs5mM998nbvFiVJ3abppjXmUex3KP1Vk2MnQWn7tnOECNo1pcXNwqx7M6qunp6Ux0Ua1xTk4OX375JRaLhSFDhnDNNde45Di1cdXv/LFjucya9SXe3mp27rybuLhgp+1bpmMhCELTGzmI7KjKtDm2p2/nr9v/SrmhHH8vf16Y/MLlXnEyMpdZswZ++EGKoC5fDrHOjbKnpKSwY8cOAO68885mpbvknchj37/2kZeUB4BfJz9G/H4Eva/p3T6FkkRRSuEtPiil85YcA9Fcs16hguAhNbWmAb1A0fbTMGXaCKdPE/Pyyyhffx0GDpTmjWHD3D0qh9matrXO+wBtAAOCxrhpNFBVVYXpcrsfV6fhWrEeJyMjA1EUnd4Sp6ioiI8//hiDwUDPnj1ZtGhRm00Z378/mzlzvqK0VA/AE09sZs2am908KhmZGmRHVabNYLKYeHv/26w8uRKA/p3689qM1+gSINenydTi0CH4xz+k13/8I4wd69TdC4LAF5cVhCdOnEivXo41Pi/PLufAuwdIvdxAXeOjYfAdgxm8ZDAan3Ym228shcL9KAp20z1zC8pcY902jj5dIHzc5ajpCFD7uWukMh0cxeUHT4giJCWBC1qLtAYbL2ys835q/FTUSvfNK8nJyYBUu9a5c+u004qOjkapVFJeXk5paSkhIc4rn6isrOTjjz+msrKSLl26cMcdd7TZdivbt6dz7bUrqbxcbjJ2bFc++ui6Fu1TFEUuXrxIWloaI0aMwNtbFrSUaRmyoyrTJsguz+aZrc9wuuA0ALcPup2HRj2ERtU2fyBkXMTFi/DUUyAIMG8eLFni9EPs27eP9PR0fHx8uOWWW+z+nKHcwJGPj3Bq1SkEsySU1Gd+H0Y8MALfcF+nj9MtCBYoS6qpNS0/A4goRFBbdKAKgbCRl53TMeAbI7eOkfEM9uypeR0fD60U/XMm1aZqdmbsrLNsZveZ0DoZt/WSkpICSL1UWyvqqNVq6dKlC9nZ2WRkZDjNUTUYDHzyyScUFhYSEhLC//3f/7VZR2zjxvPceOMqqqulrJapU+P58cfb8PdvWfmUQqFg69atFBcXExERQe/evZ0xXJkOjOyo2oFSqWTQoEFtNrWjrbMldQt/2/E3qoxVBGoD+cuUvzApbpK7h+VRyDYK6HTw+ONQVgb9+8Ozz7rECRozZgw6nQ6NRmOXiqXFZOH0t6c58tERDOUGALqO6cqYR8cQ2rMd1F1WX5Ic08J9UHQAzFf0kg3oBWFj0IaOlvqbquVeejKeh+LTT1Hu3w87d0Jo2/xe7srchd6st71XKVVM7z6dk25yVAVBICtLqj13ZlTTHmJjY22OakJCQov3Z7FYWLFiBdnZ2fj6+nLvvfe2SECvOTjrd/77789yyy3fYjRaAJg3rxfffnszPk7K6Onbty/FxcX4O1kXQsbzkVV/3YjRaGyzT87aKkaLkTf3vsm3pyVl1SGRQ3h1+qtE+ke6eWSeSYe2UUGAv/wFzp+XbjL/8Q9wUXNxpVLJjBkzmtxOFEXSf0vnwDsHKMuS2jOE9ghl9KOjiRkb45KxtQoWAxQfvuyc7oWq9LrrNYEQNqZGode7E4giRr0e7w4qdibTBtBqMY4ahfekSW02yn+l2u/ILiMJ9g52z2CQ6lMtFskZioxs3d/t+Ph49uzZQ0ZGhlP2d+rUKZKTk9FoNNx99910cpPgVkt/51euTGLp0nVYLCIAN93Uj6+/vgkvL+eJM40aNcpp+5KRkR1VOxAEgeTkZFlRtRXJKM3g6a1Pc65I6p94d8Ld/G7E71DLLSnqpcPb6CefwK+/gkYjOakREW4dTv6pfPb9ax+5x3IB8An1YcSDI+gzvw9KVRuLeosiVKVd7mm6F0qOgGCstYESggfW1JoG9btKBKnD26eMx9PWbVQQBbakbamzbGb3mW4ajURAQAAjR45k165dhLZylDr2soDexYsXMZlMLa4jHTx4MPPnzyc0NLTVRKGupKU2euxYLrffvhZR8lFZsmQwn356PWp1G/tNkvFYZNVfmQ7B+pT1LN+1nGpTNSE+Ibw05SXGxjhXEEemHbFtG3zwgfT66adh8GC3DaXiUgUH3j3AhY0XAFBr1QxeOpghdwxB49uG6qlN5VIar9U5NeTXXe8deTliOk6qOdUEuGecMjIyAJzIO0FeZV6dZbN7znbTaGqYNm1as9t3tYSQkBDmzp1LdHR0i1R/9+zZQ0FBAddddx0TJkxw4ghbnyFDInnqqfG89tpufve74bz//jUo26PCvEy7QnZUZTwGvVnP67tf58fkHwEY3nk4L097mU5+bbennYyLuXABXnhBen3LLTB/vluGYaw0cvTTo5xceRKL0YJCoaDXNb0Y+fuR+EW0ASVbUYCy01LbmMJ9UHoSqPVkVOkFocMvO6djwS++zaZHysi0R8J9w3l49MNsTt3MmYIzdA/pTveQ7u4eFmVlZfj7+7d6lFqhUDB16tQW7ePEiRP88MMPiKJIt27dGOzGh6DOQKFQ8Oqr0xk9uivXX9/H6W17ZGRcgeyo2klbTAVqS6SWpLJsyzJSS1JRKBTcN+w+7h12L0q5j6LddDgbLS+XxJN0OhgxAh57rNWHIJgFzqw9w+EPD6O/3Ieuy8gujHl0DOF9wlt9PA6hL6ipMy3aL0VRa+PXrcYxDR0GqpbV/HY4+5RpGxw4ADk5MG5cm7bRroFdWTZhGcsmLCOrLItLlZfcPSQA3nzzTfR6PX/605+IaOWSjLy8PHbt2oWXlxfXXedY25XU1FS++eYbRFFkzJgxDBo0yEWjdAxHbFQURS5cKKFnLdE+hULBDTf0dcXQZGRcguyo2oFKpfKYSaq9IYoiP537ib/v/jsGs4Ew3zBemfYKI7qMcPfQ2hQdzkYtFli2TLrB7NIF/v53ULfedCaKIpk7M9n/7/2UZpQCEBwfzOhHRhM7IdYzn1RbjFBy7LJzugcqL9Rdr/aHsFE1rWN8opx26A5nnzJth88/h3XrUAGDxo6Fb79194haTExQDDFB7hdsMxgMVFdXA7S6Qi5AdXU1+/fvx9/fn2uvvdbuefnSpUt89tlnmM1mBgwYwA033OARc7oj86goijz++Eb++9/DbNy4hIkT2167JZm2hyse9smOqh2IokhFRQUBAQEeMVm1F3QmHct3LmfD+Q0AjOk6hpemvkSoT9tsDeBOOpyN/vvfUiTExwfefBPsaBPjLArPFrL3X3u5dFiKWPiE+DD8d8Ppe0NflJ4kSiGKoMuCgj2XRZAOg0VfawMFBPWvqTUNGgBK10SUOpx9yrQNBEFqRwOIgMnHBw0gW6hzKCuT1M69vb3dokgfHR2NSqWisrKS4uJiwsLCmvxMSUkJn3zyCXq9nm7durF48eImW244Q6zJHuydRy0WgQcf/IWPPjoCwLXXruTChYcJby/9umU8FtGq1OVEZEfVDgRBIDU1tc2qAXoi54rOsWzLMjLLMlEqlPx+5O+5Y8gdcqpvM+lQNvrTT/D119Lrv/4VevZ0yWEEQaCiosLWK7Uyr5KD7x0kZb3UwF7lpWLQ4kEk3JWAVwubpDsNcxUUHaypNa2+WHe9NrwmnTdsFHgFt8qwOpR9yrQdsrOhqsr2Nrd3b6IFQbZRJ1FaWgpgV79pV6DRaOjatSsZGRlkZGQ06ajqdDo+/vhjysrKiIyM5M4772zSAbVYLOzbtw+tVsvQoUNd6rDaM4+azQJ33fU9X32VBIBSqeCtt2bLTqpMqyCr/sq0eURR5Lsz3/Hm3jcxWoxE+EXw6vRXSYhKcPfQZNoCSUnw6qvS6/vvBxeqSe7YsYOvvvqKRQsWEZwWzIkVJ7BcbpDec25PRv5+JAGd3ax2KwpQnlxTa1p6AkRLzXqFBkISapzTgJ6yCJKMjJXYWDhzBg4dQvztNyqGDXP3iNoV1ohqcHCw28YQGxtrc1SHNXJ9TSYTn376Kfn5+QQFBXHPPffg69u0c1d1+UGHIAioXVh+YjAY+OmnnygqKmLAgAH1OqpGo4XbbvuOtWvPAKBSKVixYgG33jrQZeOSkXE1sqMq02pUGit5ecfLbEmVer1NjJ3IX6b8hSBv9zxtlWljFBTAn/8MJhNMmQL33uuyQ1VWVrJq1SpKL5Wy7bVtxFVJ9T1RQ6MY+9hYOvV3oxK1oViKlhbuhaJ9YCypu943VqoxDR8nKfWqfdwzThmZtoCXF4wbhzh6NKakJHePxmHSS9NRKVQeUZN6Je6OqALExcWxc+dOMjIyGt3u22+/JSMjAx8fH+699167nevKykqABtNxy8rKSE1NJTIyki5dujg8fiuCIJCVlWWr+b2S6moTN920mg0bzgPg5aVi9eqFXH+9LJwk07aRHVU7cUd9RXvidMFplm1ZxsWKi6iUKh4e9TCLBy2W69WcSLu2UaMRnngCCguhe3d46SVoom6ouYiiyMf//Jis41n4Gn2JIYag2CBGPzyauMlxrW+zgglKk2pqTSuS665X+Uq9TK1RU9/o1h2fnbRr+5Rp85hMcP58MGVl0JYyf98+9xa/5q0mzq8fI0NnMqnTjcT69amzzYkT7hmbNaLqbkcVIDc3F4PBgFZbv3r5xIkTSU9P55ZbbiEyMtLu/VdUVADg7+9f7/qjR49y7Ngx+vfv3yJHVaPRMGfOHHJycq76DaqoMDB//jds25YOgI+Pmu+/v5VZs3o0+3iuQKfTIQgC3t7eLo0+y7QvZEuxA5VKRd++8lOp5iCKIitPruTt/W9jFsx0CejC8unLGRAxwN1Da1e0axsVRXjlFTh1CgID4V//AjtSsppD0bkiflr+E4lJiYiIjPQbyYQHJ9Dvpn6oNK1496rLqdU65iBYdHXXB/SBTuMkxzR4EChdL+TREtq1fcq0eUwmGDdOxdGj8e4eimMoLPDgFvCBoqIzHMk8w39/7QRH+zT92VbAE1J/g4KCCA4OprS0lKysLHpeoWlw4MAB+vXrR9euXfnzn//ssANVO6JaH927d+fYsWOkp6cjCEKTwkwNoVar6devH/369auzXBBErrnma3buzLw8Di9++WWxR6r8rlmzhpKSEm6++Waioz3zgapMy5BVf92EIAiUlJQQEhLS7EmmI1JuKOcv2/7CjowdAEzrNo3nJz1PgNbNdX3tkHZtoytXwi+/SBHU114DF/zAVRVUceg/h0j+MZltbAMFDBswjIfffhhtYMv6h9qFWQfFhy87p/tAl1l3vVcIhI25HDUdA9q2pYzdru1Tps1z/DgcPeruUTSDzkfAp7justSZTX6stZIbPCH1F6SoamlpKRkZGXUc1QMHDvDtt98SGhrKo48+6nDWh8ViQaeTHiI2FFGNjo7Gy8sLnU5HXl4enTt3bvZ51DePKpUKHnxwBLt2ZRIc7E1i4hJGjZKdQBn3IIspuQlRFMnKynLrU8G2xom8Ezy99WnyKvPQqDQ8NuYxbu5/s5zq6yLarY3u3w9vvSW9fuwxGDXKqbs3VZs48eUJjn9xHLPeTDrpVAVUERUTxRNvPuE6J1UUoeJ8TdS05BiIppr1ChUED7lcazoWAvtAG1bEbrf2KdN2Wb4ceveGiRPR6yPcPZrm0WNT3feFfaEsttGPDB0KQ4a4cEy18ISIKkiO6vHjx+vUqZ4+fZq1a9cCkJCQ0KzSBGs01cvLCy+v+pXfVSoVcXFxpKSkkJaW1iJHtaF59LbbBmGxiAweHMngwfanLcvIOJsO0Z7mvffe44033iA3N5chQ4bwzjvvMKqRm9O33nqL//znP2RmZhIeHs7ChQtZvny5XA/VihTqCll7Zi0L+i0g1CeUL49/yXsH30MQBWKCYnht+mv0CfeMVCSZ5nHy5Ek+++wz7rrrLgYObCUFwawsePppqdfh/Plw663N3pWuUMeZtWfot6AfvuG+iILIuZ/PcfD9g+gKpSfiwQOCyRfzCRQDuWnRTYSGOhC11BdC9lrougC8w+vfxlgGRfulWtOifWAorLvep0ut1jEjQe3XzLOVkZFplLw8eOcd29vIO/5GbGw3Bg78mpMnF/PUU9MYPNiN47OTPx7eTFatqoCFCbNY0ojGnLe35KS2QstPRFHkrrvuorS0lJCQENcfsBF69OjBsGHD0Ol03HvvvcyaNYvjx48jCAIjR45k9uzZzdqvtT61obRfK927d7c5quPGjWvWsWpTVWUkMLCuSN6SJZ5vsDqdjsLCQnJzc+XUXxm78ShHddWqVTz++ON88MEHjB49mrfeeovZs2eTnJxMRMTVTzy//vprli1bxieffMK4ceM4d+4cd911FwqFgjfffNMNZ9AxKdQV8uHhDxkSOYSXkl5iT9YeAGb1mMWzE5/Fz0u+4W7LiKLIN998w6FDh/D29uZvf/ub6yPjVVXw+ONQXg6DBsGyZS1qq6Ir1HH4w8PETYqj+EIx+9/aT1FKEQABXQIY/fBodubsxLDFQOfOnZkzZ45jBzAUwvkPodOkGkdVsEDZSSliWrAHys8AtZ42qrwhZHhNralvjNw6RkamNdi5s87byvj+9O//Bd7epfTvv4aBA6cwYYJnZzCkl6aTt/8ctQN5902byfDm6/U4FYVCQffu3d09DAA6d+7MokWL+MMf/kBpaSnr1q0jOjqa/v37s2DBgmb/nlkjqg2l/VqJj48nMjKSbt26tahOFSAjo5Lrr/+A55+fzL33tp12SqIoUlhYiMFg4OjRowwbNkzOsJOxC49yVN98803uu+8+7r77bgA++OADfvnlFz755BOWLVt21fZ79uxh/PjxLF68GJAmg9tuu439+/c7fWxNPTHr6OhMOp7Y/ATVpmq8VF48Of5Jru9zvTwRtSKustGkpCSOHTuGr68vx44dIykpicGuDDcIAjz/PKSlQadO8MYb0EBalSNYDBZ2vLyDwrNSJFMboGXovUMZcPMAqvRV7HhMqqW+4447mq9IaCiArDOXRZAOgLmy7nr/njVR05AEULX8vNoK8hwq4zEUFkqCbDod+Pmx31SKj08RoqjAx6eIkye3MmlS07We7mTzhc113of7hjO081A3jeZqMjMz2blzJ7GxsUycONHdw2Ht2rUUFkpzv8lkQqPRcPvtt7dI/KUpISUrPj4+3Hbbbc0+jpWTJ/O5997dFBbquf/+nwgL8+HGG/s1/UEPICMjA51Oh0KhICcnh4yMDOLj4909LJk2gMc4qkajkcOHD/P000/blimVSmbMmMHevXvr/cy4ceNYsWIFBw4cYNSoUaSmprJ+/XqWLl3a4HEMBgMGg8H2vry8HJCK4i0WCyA9CVQqlQiCYMu3jo+Ptzld1u2sWLe/crlSqUShUNS7HK4uOm5ouUqlQhTFepfXHmNjy+s7p8bG3tQ55VXkUaiTJv2Pj35MRlkGRouR7sHdeWzMYwzrPKzOk8O2cE5t+ToBtknfYrE47ZyUSiUrV66kvLycyMhIysrKWLVqFYMGDUIURdec0wcfoNyxA1GjQfj73yEkBJpxTvpiPbpCHWaTmb1v7KUktQRTtQmNr4aec3qScF8Cod2l9F4/Pz9eeeUVDh48SL9+/eoco8FzMhYj6PMRRRHFhY9QVFyAvXejUHsjioBCDdoQxLDRED4GZadxWDRhdccuim3e9uz9PlmjK/aea1s4p/Z4nTrEOd13H9x1F8qjRxGysti/4UcUCgFBUKFUChw8uBaLZRpX4knntDm1rqM6rds0REFEVIp1z7XWGFvzOl28eJHjx49jMBiuSndtbdsTBIGff/65zpiLi4tt59Lc69S3b18qKyvx9fWVfgdceE7Hj+cza9aXFBXpARg0KILRo7vY9uHJc4RSqWT37t14eXnh7+9PVVUVu3fvJi4uznX3EfK857ZzcjYe46gWFhZisViu6l8VGRnJ2bNn6/3M4sWLKSwsZMKECYiiiNls5oEHHuCZZ55p8DjLly/nr3/961XLT506ZUvfCA0NJTY2luzsbIqLixFFEb1eT1xcHF26dCE9Pd1WmwAQExNDWFgYKSkp6PV62/Lu3bsTGBjI6dOn6xhQnz598PLyIumK5uKDBg3CaDSSnFzTJ1GlUjFo0CAqKipITU21Lff29qZv376UlJSQlZVlWx4QEECPHj3Iz88nNzfXtvzKc7ISFRVFVFSUw+f0/m/vsyZ1DSIi6ZXpKBQKdCYdWcVZ/Gn9n7gp/iZuir+pTZ1TW75O586do7S0FG9vb1vKlTPOSRRF9u/fj9FoJCcnh9DQUA4dOkRSUhLR0dFOPyf/ffuI/+gjlGo12XfeSZEgwOWxOnpOpn0mDn94mNLsUswVZgB0pTqCfIM4v/k8Vdoq4m+Kr3Odxo0bV2c/jZ6TMRHT6XexmAxojZmIgMJyEYXaFzNaCgOvJS/sd6BQEuMbQ5h3GClnz7Y727Pn+ySKImFhYXTu3JlTp061i3OC9nedOtw5jRpFYnExlZXSQ1elUkAUFVRWFvHLL78QG1sjTORJ51RpqmRvlvQA32SSRNh6KXqRlJTkMdfp1KlTmM1mgoKC3G57R44coaqqCpBupBUKBcXFxXz55ZfMmDGjxdcpNzfXped05kwlDzywi/JyKcgyYEAwb789nOrqQiDIc75PDZxTQEAAqampKJVKTCYTCoWClJQUMjIyCAgIcPv3yVnXyZPmCHedkyv64ypEV0g0NYOLFy8SHR3Nnj17GDt2rG35k08+yfbt2+tN5922bRu33norL7/8MqNHj+b8+fM88sgj3HfffTz//PP1Hqe+iGpMTAzFxcUEBgYCVz/lsFgsnDp1ioEDB6LRaNrsUw5XRFRP5p/k2d+epUBXwGfXf0bv0N6AlIYU7hveps6pLV8no9HIqVOnGDBgACqVyinnJIoif/nLX9i+fTuCIKBQKIiIiKCyspJRo0bV+8CnRed07hzKe+8FgwHF7bdjefjhq8boaER1/9v7ObvuLGa9GYVSwfTXphPRPwJRFPEO9cY3vKYfq8PXyRpRLdiD8vRrYCxCHP4uyrDhWAQLaMOlf/WdazPPyRNtz55zss6hgwYNuuqJa1s9p8bGLp9T2zgngIceeoiCgsLLx1UAUmSsU6dw3n77bdtnPemcfkz+kT9s+IPtvZfKi6QHkvDV+HrMdfr22285fPgws2fPZsqUKU2ek3V5fWNvyTkJgsDDDz9MYWEhSqXS9hmz2Ux4eDjvvPMOarXaY79Pv/6axo03rqaqSnogkZAQyubNyIuLSQABAABJREFUdxES4nvVuV45Rk84J1EUWbVqFSkpKQQGBqJQKBBFkfLycnr16sWt9Ygkevo5WcfYVuc9V55TaWkp4eHhlJWV2XyqluIxEdXw8HBUKhV5eXl1lufl5REVFVXvZ55//nmWLl3KvfdKMneDBg2iqqqK+++/n2effbbOD4wVrVaLVnt1ywmVSnVVrULtz9dOsWyopsGVyxUKRb3L6zvH5ix3dIyRAZFEBkSy/+J+vNXe+Kp96depH33D+9q9H087p7Z+nazHrr1NS87pxIkTHDt2jNDQUFtmQUVFBYGBgRw7doxTp07VW6varHMqKYEnnwSDAcaMgYcfbvH1SP8tnfMbzqP2VjPigREc++wYEf0jCO/bgCqvo2P3DkfpHQ65G0HtDYIfirDhENSXhqqe2qvt2bPcGsloT+fUnOXyOXnOOW3evJmioqLLN3/WmzMFCoWSoqIitm3bxsyZdWtVPeGcNqfVTfudGDeRAO+ABre34qrrVFVVRXV1NWFhYbYHUdayqqCgILfa3q+//mq7xtbtrPdzRUVF/Pbbb8ycOdMjv0+//HKOm25ajcEgOSAzZnTjpZf6ExLiW+dznvJ9qm95eno66enp+Pj42MajUCjw8fEhPT2drKysemtVPfmcrLTVea+x5S09p4a2awkeI2vn5eXF8OHD2bp1q22ZIAhs3bq1ToS1Njqd7qo/ivUP7CGB4nbPibwTAPhofJrYUqYtIYoiq1evprq6Gq1WS0REBIIgUFVVhSAIVFdXs3r1aud8z8xmeOopuHQJYmLg1VehgYnVXlK3prLnDUl9esQDI+g+w4Xqk6XSdwCV/B2QkWkrCILA2rVrL0cPFJfnMhFRlG7W6q73HEwWE7+m/Vpn2czu7hV+OnXqFCtWrGDz5hoH2hN6qNa+htZrav3nydcYYN26M9x44yqbkzp/fh++//4WfHw8Jr7UJKIosnv3bgwGA0qllPZr/adUKjEYDOzevVu+X5dpFI+y+Mcff5w777yTESNGMGrUKN566y2qqqpsKsB33HEH0dHRLF++HIDrrruON998k6FDh9pSf59//nmuu+66Bp8gNAeFQkFoaKhLioTbMqIokpSfhFqpZumQpYT7NhypknEtzrZRs9lMXl4ePj4+6HRSoz6NRoPBYKCkpAR/f3/y8/Mxm81oWtqU75//hCNHJBXOf/4TWpgucunIJX57/jdEUaTfTf0Yes9QqouqGX7/8Dqpvk7BYoSys5JwUo8ltlRfmbrIc6iMR7B/P6xYAZMmYRg5kqqqKpRKJaIoUGOaCkRREtipqqrCYDDg4+M5D6EOXjxIuaG8zjJ3O6oGgwGVSkXnzp0B6d6gtLQUkCKq7hyX9RrX54x66jUGiIkJwsdHg8lk4JZbBvDllzeiUrWtedRisVBWVoZWq8VoNF61XqvVUlZWhsVicUlto0zr067FlABuueUWCgoKeOGFF8jNzSUhIYHExESbwFJmZmadCOpzzz2HQqHgueeeIycnh06dOnHdddfxyiuvOHVcSqWyjqiCjERWeRZl+jJ8Nb48N/E5NKpW6CIuUy/OtlGNRsNrr71Wp5A/KyuLd955B6VSybJly4iNjW25k7p2LaxZI/UPffllaGHfveLzxWx8fCMWo4X4KfFMeGoCCoUC33Bfht8/vGVjrY/ysyCawLsT9H9K7oPaAPIcKuMRbNoE330H332Hj1LJy4mJFBqNnDoFn376DRpNFVlZ45k3L4P4+ACuv/56j3NgNl3YVOf9oMhBdA7o7KbRSEycOJExY8bY3ldXV9scE3c6qj4+Pvztb3+jqKiowW06derkcdcYYMSILqxfv5ivv07i7bfnolJJ975taR5Vq9UsXbqU6urqBrfx8fGRndR2hCtSfz3OOh566CEeeuihetdt27atznu1Ws2LL77Iiy++6NIxCYJAdnY2Xbt2dclFaKsczz0OQL/wfrKT6mZcYaNhYWGEhYXZ3sfHx7N9+3aSk5NJTk4mISGhZQc4dgxef116/eCDMGlSi3ZXmVvJhoc3YKw0EjkkkmmvTEOhdLHjaE37DRokO6mNIM+hMh7B9u01rwcPJnbgQGKR2qkajRtQKKCysis63TEuXaokNDTUbUNtiN5hvRneZThHLh1BFEVmdZ/l7iEB1HloaU379fX1xcsJPbBbQlxcHHFxcW4dg71YW9xYGT8+lvHjaxzTtjiPBgQEyD20OxCuSKNvG5buZkRRtInJyNSQlC9JXg+JGuLmkci0lo3OnTsXgJMnT7bsWLm5kniS2QwzZsDl9P7mYig3sOGPG6jKryKkWwhz/jUHtbb+53AZGRnO+ztZHdWQq0WlZGqQ51AZt2MyQWwsXG5Dx+TJdVanp0/j/Pl55OUNJTQ0GpPJxL59+9ww0MZZPGgxP932E8cfOM6/Zv+LG/vd6O4hXYUn1Ke2JURR5KWXtvPHP25odI6U51EZT8cVtulxEVWZtsPxPCmiOihikJtHItNaDB06lMcee4yEhITm1SIUFsKqVfDbb1BcDL17w4svtigaaTaYSXwskZK0Evwi/Jj7zly0gVcrewNkZ2fz/PPP07dvX/70pz/VqwBuN6JY46gGy46qjIy9lJSUcP78eXr27ElISEjrHFSjgU8+kRzWo0fhip7tOTk1oo19+07ixImV7N69m0mTJrW8xMEFhPuGc8vAW9w9DBu1r6mXlxd9+vSpk5EjUz+iKLJs2RZef10S//Pz0/D3v7u35lhGxpOQHVWZZlFprCS1RGoKPDhSvknvKCiVSoYNG9b8HRQUSLWoYWEQFQX/+Ae0oD5IFER+ffZX8o7n4eXvxdx35uIf5V//tqLI559/jiiK+Pr6tsxJBdDngqEQFCoI6t+yfcnIdBBEUSQ9PZ2ioiLUajXBwcGtKw6j0cCoUVct7tJlH2q1nkuXhhMbO5iMjPWUlZVx7NgxRo4c2Xrja4NceU0TEhLo3kK9gY6AIIg8/PAG3nvvoG1ZZGT9v18yMh0V2VG1A4VCQVRUVJtRWmsNTuWfQhRFugR0IcxXfmrqbtqMjX7/PZSXQ6dOUn1qly7N3pUoiux6bRfp29JReamY/eZsQns0XFO2f/9+zp49i0aj4fbbb2/2cW2USqnvBPQBlXfL99eOaTP2KeNySktLKS4uRq1WU1xcTGlpaZNRVUEQ6lUNrQ+NRtMs1f9u3bai1ZZRUtILpTKA8ePHs379enbu3MmgQfZlDalUKqdHX41Go111XwqFouUP367AYrFgMpma3O7Ka3r27FkCAwMJDw93+pjaCxaLwH33/cSnnx6zLfvPf67hgQdGNPgZeR6V8XTaveqvp6JUKomKinL3MDwKa9qvHE31DDzaRgsLpX/nz8PHH0vL5syR2tGcPQvh4dI/Bzn68VHOrD2DQqFg2svT6DysYeVLvV7P119/DcD8+fPp1KlTs06lDiXSd4BgOfW9KTzaPmVaDWvkzWKx4O3tjV6vJz09vcmoqtFo5ODBgw2ur82AAQOcIoI0evRotm7dSm5uLhs2bCDcjjkqOjra6ZHEtLQ08vPzm9zO39+foUOHOvXYhYWFnDt3rtFtRFGkuroahUJhu6br1q2jtLSUW2+9tWUZOO0Uk8nC0qXrWLXqFABKpYJPP72eO+5oXO9DnkdlPB1XiHzJYkp2YLFYuHDhAhaLxd1D8RiS8qRokuyoegausNHCwkJ27txJenp6y3a0di0sWQL33w+XLkl9Un/9VVq2ZIm03kHOfn+WQx8cAmDcn8fRbVq3Rrdft24dJSUlREREcM011zTrNK7CGlGV61ObRJ5DZaAm8qbRaFAoFGg0GltUtSlUKpVd/5yFj48Poy6nCJ85c8auY7vkJk2pRKVSUWoutb1urWND0393URRtfTCt17SkpASLxeLW1jSeil5v5qabVtucVLVayapVC5t0UkGeR2U8H1fYphxRtZPa/SQ7OoIo2BR/ZUfVc3C2jZ45c4YPP/yQ/v378/TTTzd/RwsWwNChkqPq6wtaLTz3HPTtK613MJqasSODna/uBGDo/w1lwKIBjW6fnZ1NYmIiAEuXLnVOap5FDxXJ0mtZ8dcu5Dm0Y1M7mmr9DqpUKkwmU5NRVW9vb8aNG9f8gz/8sKQwPnmy9M/OqNT48ePZvXs3paWldOvWjc6dW79faa9evYjrHseA9wcQ5hPGzO4zmd1zNqOjR7u8LVxkZKStj319iKLIsWPHMBgMtl6YSqUSnU6HQqGQHdUrqKoycuONq9i8WdL30GpVfPfdIq65pneTn9Xr9axevZqysjIeeOABpz6UkZHxZOSIqozDpJemU2msxFvtTa/QXu4ejoyLyM3NBWh5qlF4OJw7ByqV5Jz6+Ej/W/854Kjmnchj69NbEQWR3tf1ZsSDDdfzgHQj9eWXXyIIAkOHDm1571crZadBtIA2HLzlVCwZmaa4MpoKOBxVbRYGA/z8s1Qf/9hjNb2b7SA0NJSBAwcSFBRka7niDvZk7aHKWEVmWSYfH/2YW769hVJ9qdvGY6W+a2oymRAEAYvFIrdRuYKKCiNpaaWApO67fv3tdjmpIP2WFRUVyQ/8ZDocsqMq4zAn8qSWHAM6DUCllJ/qtVcuXboE0PIogijCd99Jr2c1vzl9aXopiY8mYjaYiRkXw6RnJzVZuL9//35Onz6NRqNh6dKlzT721YOplfYrC1vIyDSKNZpqNptRKBRYLBbbP4VCgdlsJj093TWOzcGDoNfXvJ8yxaGP33DDDTz11FP0tWaAuIHNFzbXeT+s8zA6+Tmhzr4FNHRNrY6Ul5cX2dnZsrNai6gof7ZuvYPBgyPZtGkp05ooWZGRkZFTf+1CoVAQExMjK61dxuqoymm/noMrbNTqqLY4onroEGRmSmm/N9wA3t4Op/tWFVSx4Y8bMJQb6NS/EzP+PgOluunnbHv2SL3prr32WucIKFmxCSnJ3wF7kOfQjo1VcEetVtdbw6RWq6murkYURefbiI8PzJ0Lu3ZBZSVMmODQx/39/RFFkZSUFCoqKlpdHEgURTalbqqzbGZ39/fZbOiaVlZWAuDr6+u6a9qGiY0N4ujR36FUNu9v4uXlJf89ZTwWWfXXTSiVSrlxdS1kR9XzcLaNCoJAXl4e0LSjWlZWxpYtWwgODmb69OlXb/Dtt9L/11wDsbFSraoDGCuNJD6cSMWlCoJig5jz7zlofOyrzXr00UfZtWsXY8aMceiYjSKKUCYLKTmCPId2bJRKJcOHD2+01YlGo3GNINDw4ZLauNkMycnQDEXgc+fO8fHHH+Pr68uAAQNateXKmcIz5JTn1Fk2q0fzM1OcRUPX9ODBg/j5+dG9e3eGDx/uMpGntkB2djl//es23n57Lj61frNqO6nZ2dmkpqYyfPhw/Pz8Gt2fQqFArVZ36L+pjGcjq/66CYvFwtmzZ2WlNaDcUE56aToAgyLlthyegrNttLi4GJPJhFKpbDISeezYMb7//nt++OEHzGZz3ZWFhbBtm/R6wQKHx2ExWtj0xCaKUorwCfVh7jtz8QnxsfvzSqWSSZMm4eXl5fCxG0SXDcYSUGgg0H3pgG0JeQ6V0Wq1+Pv7N/jP5c6fWg0DGhdea4hevXoRFhaGTqfj8OHDTh5Y42y6UDeaGhMUQ5+wPq06hoao75rq9Xrb70ZH7qGallbCpEmf8r//HWXhwjUYjfXPfTt37uTIkSOkpaU1uU9RFNHr9fI8KuOxuMI2ZUfVTvS1a1w6MNa2NLFBsQR7B7t3MDJ1cKaNWoWUIiIimlQXHDduHEFBQZSUlLBv3766K3/8ESwWGDwYejkmvCUKIr+9+BsXD11E46th7jtzCYwOdGgfLsFanxrUF1ROdIDbOfIcKuPJWCwaLBYN9ZVUKpVKJk6cCEiOhSAIrTauKx3VWd1neXTqp1V0Kjg42L0DcTJGo5GLFy/aJWaUnFzIxImf2oSTkpMLKSzU1bttv3796N+/f5O9fzUaDZMnT6Zv374eff1lZJyN7KjKOISc9tsxcERISaPRMHOmVDO1YcOGGvEMQajpkbpwoUPHF0WRvf/aS+rmVJRqJbP+MYvwPo7VtbqMUuk7IKf9ysjYj06n48SJE2RkZLh7KPWyb9+TbN/+CpWV0fWuHz58OL6+vhQVFXH69OlWGVNeZR7Hco/VWeYJab+NYXVU21trmvLyci5cuMD58+cb3e7EiTwmTfqMnBzJoe3XL5wdO+6mS5eAerdPSEhg1qxZdOnSpdH9qtVqhgwZQnx8vOyoynQoZEdVxiFkR7VjYI2o2qv4O336dDQaDZmZmZw5c0ZauGcP5OZCYCDUV7vaCCe+PMHJlScBmPKXKUSPqv/m0S3IjqqMjMNUVFRQVlbWem1eRFF6WOYktFoto0ePBqSoamuwNW1rnfcB2gDGdHVivb0LWLp0KY899hh9+nhGerKzsEZS/f39G9zm4MEcpkz5jPz8KgASEqLYvv2uBp1UGRmZppEdVTtQKpV07969wxewWwQLpwpOAbKj6mk420arq6tRKBR2K/76+/szadIkQIqqAjUtaa67DhyoVUpZn8L+t/cDMOaxMfSc09P+gbsasw4qLj9RD5ZrtO1FnkNl7LnRdyopKVLJwQMPwDffQBMO8pgxrzN58nP4+19scJtx48ahUqlIS0sjKyvL2SO+io0XNtZ5PzV+KhqVfUJy7sLX15fOnTvj6+vr7qE4FauacUP2u3NnBtOnf0FJiVTiMGZMV3777U46dWpcIMkR5HlUxtNxhW3Kqr92oFAoCAz0gNo4N3Oh5AI6kw5fjS/dQ7q7ezgytXC2jd5///3cfffdDvXAmzNnDr/++ivHjh0j5+hRonftklbcdJPd+8jel832v24HYNDtgxh8u4c9ECk9CQjgHQXeEe4eTZtBnkM9B5MJjh+v21q0NaiqqkQQICMjgIsN+4JOI/LH7cTmFsO3P8K3P5L0znD0MfWno544ASqVCZXKiELR8JwXFBREQkIChw8fZseOHdx+++2uGj56s56dGXUjt57QlqYxqqqq+M9//kNISAh33313u3GoRFG0PWgJCLg6Orp58wWuv/4bqqslMcEpU+L58cdbCQhwrpiUPI/KeDpyexo3YbFYOH36NP37929SWKY9Y037HRQxCKWiffwAtRdcYaMajWNP7qOiohg6dChHjhwh8T//4R5RhFGjpJY0dlBwuoDNf96MYBHoMbsHYx7xwBQ3uS1Ns5DnUM/AZILRo+Ho0dY9rlIp8NprVajV8Oqr/hQWuv6YX7IdaywrlyhGLm48M2P8ePv2O3HiRA4fPkxSUhLFxcVNiuA0l50ZO9Gba54mKBVKpnWb5pJj2UtGRgaRkZF4e3vXu760tJT8/Hyqq6vbjZMK2JR2lUplvZHit98+YHNS587tyXffLarTjsZZyPOojKcjq/66EVkOXK5P9XQ8wUbnzp0LosjuEycoA7ujqeXZ5SQ+moip2kT0qGim/GUKCgcaop8/f56SkpLmDdoRSuT61ObiCfbZ0Tl+vPWdVICoKB1qtYBer6KwsH4nx9n8yHx+4RrKCWQ7kwH755MG/DAAunTpQq9evRAEgQMHDrR8oA1wpdrvqOhRhPiEuOx4TVFdXc3333/Pf//7X3S6+hVsS0tLgfYnpGRN+/Xz86vXAf/mm5uYMCGWBQv6sW7dLS5xUq3I86hMR0OOqMrYjS2iKvdPlWmAPn360M3Xl3SzmTMBAYyZPLnJz1QXV7P+ofVUF1cT1juMmW/MRKWx/2mxwWDg3XffpaqqiieeeMJ1Ih6iUBNRDZEdVZm2h7s6BMXGSjf6WVkBOOIwtoQ1LGINi1Bhxo8quz/Xr5/IkCGNbzNz5kyGDx/O4MGumQcEUWBL2pa6x3Rz2m96ejqiKBIeHt5g/Wl7bU3TVH21n58X69cvxsdHg1otx39kZJyJ7KjK2EVxdTHZ5dmAlPorI1MfCoWCu00m/ICIm28GdeNTjElnIvHRRMqzywnoEsDcd+bi5edYb9KffvqJoqIiQkNDiYuLa8Hom6AqE0zloPSCAMd6wsrIeCLvvSfpDbkavb4CkwkGD/Zn8WLXH68uaqDpCN+6dSJlZXqeeUagqaqH+Ph44uPjEUWRsrIyp0cQy/Rl9A7rTXF1MSaLCYDZPWc79RiOkpaWBkC3bt0a3Ka9R1St9amffXaMmTO7E12rr7ez61FlZGQkZEfVDpRKJX369GlXNReOkpQnRZK6h3QnQCtLrXsaHmOjmZl0S0oChQJuvLHRTQWzwOanNlNwugDvYG/mvTsP3zDHlCJzc3P55ZdfAFiyZEmDtVNOwdqWJmgAKD1bedPT8Bj7lKnD4MEwYYLrj3P0aCWVldCvXwDhHtIO+Uq2b1egVGrx8rLPRouKivjyyy+pqqpi2bJlTq0ZDPEJYdXCVZQbytmWvo2jl466VcDQYrGQnp4ONO6otsceqqIo1lH8/fvfd7Fs2Vb69g1nx467nKrq2xTyPCrj6bjCNmVrtxMvL8eiPO0NuT7V8/EIG7W2pBk/HhrpwSqKIttf2k723mzU3mrmvDWHoFjHbm5EUWTFihWYzWYGDhzIiBEjWjLyprH1T5UzCpqDR9inTKsjCAJVVVLqbau1pmkmjihWBgYGUl5eTllZGUlJSS4ZT6A2kPl95vPilBddsn97uXTpEkajER8fn0ZblrXH1F+TyYS3tzcqlYq//30/y5ZJvW3Pni1kzZrTrT4eeR6V6WjIjqodCIJAUlISghObh7c1kvKlH2LZUfVMPMJGDQb46Sfp9cKFjW564N0DpKxPQaFUMOPvM4gY6Hirl6NHj3L8+HFUKhV33HGHS2TR62BzVJsoYJO5Co+wTxm3IIoicXFxREREoHWgn3KzKS+HZgjOiKJI9f+zd97xUZTpA//O1vTeIIEk9JYA0qUKIopdrIDeed7Zz1P0LKfn+bOc3tn1LOedd2cBAcUughSlhx4IBJIQUiG9l022zPz+GHZJSGGTbE3myycfdmdn3veZnXffmed9msFg9xjVarVceOGFAGzdurVLpby8DYPBQEBAAAkJCZ1aTKyuv71JUdXpdIwfP54vv6zluefOlgt68cV53HvvJJfKosyjCp6OM8am4vqrcF7MopmjZUcBJT5VoRM2bZIfEmNi4MwDXHscWXmEQx8dAmDWU7MYON2+8jUtMRqNfPLJJ4Bcv7VfJ9Zbh2Cqg/qT8mvFoqqgYDdqtZoBAwa4rsNnnoF162DmTJg//7yLZj1h2rRp/PzzzxQWFpKTk8OgQb2zvvjQoUMZMmQIJpOpw31EUaSmpsamsNfW1vaKmp+iKHHvvT/wz3/ut217881LeeCBKW6USkGh76BYVBXOS2ZFJs3mZoL0QcSHODFZjYJ388UX8v/XXQcdrLpnb8hm16u7AJh07ySGX9W9DL3ff/895eXlhIaGcs0113SrjS5RfUT+3zcW9M6pm6igoNBDJAm2bIHqatm746uv7D70oosuYtKkSV1Srvz9/ZkwYQIgW1V7M4IgdOp22tDQgMViwWKxkJeXR35+vgulcw5ms8ivf/21TUkVBPj3v69UlFQFBReiWFQVzos1PnVM1BhUgrK20ZvZt28f69evZ/z48SxcuND+A7Oy4PBhUKvhqqva3eX0/tP88vQvSJLEqBtGMe72cd2SsbS0lO+//x6AJUuWODeBkhWr22+o4varoOCxZGdDUdHZ93aUx7IydepU/P39bZld7WXmzJmkpKSQnp5OWVkZkZGRXTq+JWbRjEpQeeV9VhAEZs+eTU1NDSqViiZ31UJyEEajhcWL17BmzTEA1GqBTz65lltuUTxqFBRciffNhm5ApVKRlJTUZzOtWRXVsdHKQ7qn4qgxmpuby/HjxykuLu7agdYkSnPm0F5az4qsCn5a9hMWk4XEuYlM/+P0bseUnjwpu+COHj2ayZMnd6uNLqMkUuoRfX0OVXARERHwyitw5ZUQHAyzZtl96J49e6isrLQlfrKXyMhIRo0aBcC2bdvOs3fn/JT9E+P/OZ6H1z/M+hPraTQ19qg9VxIQEMDll1/OddddB0BTU5NXx+1+9FGqTUnV6dSsWXOj25VUZR5V8HScMTYVi6qdGI1G11huPBBrIqWkaOUh3ZNxxBgtKSkB6DSzYxsaG2HtWvn1okVtPq4rquPH3/+IscFIzPgY5j4/F0HV/cRHU6dOtZVIcHoCJQBJhJozrr9KIqVu05fnUAUXERICixfLfxZLhyEI7bF582aqq6sZNmxYl2MrZ82aRXp6Ovv27WPBggX4+3evZMlP2T9R1lDGZ0c+47Mjn3FR4kUsv255t9pyNQUFBZSUlNC/f38EQUCSJJqbm732N//b317Anj2nWL48ja++uokFC4a4WyRAmUcV+h7KsowdiKJIRkZGn8y0VtZQRlFdESpBxZioMe4WR6EDHDVGT58+DdC15ETr1snK6sCBMKl1FsSmmibW3r+W3PJcDgUfYvYLs1Hrel5vMDo6mujo6B63Yxf1J8HcAGpfCBjsmj57GX15DlVwE2q1HFRoJ5Ik0dTU1K0xmpiYSFxcHGazmV27dnX5eACLaGHjyY2tts1NmNutttzB4cOHWb16Nfv27bMpUt7s/isIAu+/fwV79vzOY5RUZR5V8HScMTYVRVWhU6zW1CFhQ/DT+rlZGgVnIkmSzeXXbkVVks66/S5a1OrB0NxkZv1D66nOq+aI/gjVkdWkHExxtNjOx+r2GzwaVD1XshUUFHoXgiAwa9YsYmJiiIrqeqktgANFB6g0VLbaNn/wfEeI5xJalqbxRkW1vLyRgweLWm1Tq1WM6UbpNAUFBcehKKoKnXKoWC4jopSl6f1UVVVhNBoRBIGIduJM2yU9HTIyQKeDK66wbRYtIpv+tImSwyX4BPpwy323oNKqWLdunfetBldZ41OVGsIKCgrtk5yczEMPPURycvfmiZ+yf2r1fkTECAYGd710l7uora0FIDg42KaoGgwGd4pkN0VFdcye/T/mzv2YQ4e6mJ9BQUHBqSiKqp2o1X3TkmK1qI6NUWLzPJ2ejlGrNTU6OhqNxs7wdas19eKL5eQlyJbZ7S9tJ29rHmqdmgWvL+DSRZcSEBBAeXk5+/bt65GcLqdaUVQdQV+dQxVcxLFjclyqm1CpVIiiSGpqKitWrOhyIqENJze0ej9/kPdYU+GsRTU4OBhfX1/AOyyq+fk1zJr1P9LTy6iubuLXv/7GI5JANTY2cvDgQfbs2dNquzKPKvQ1FEXVDtRqNUlJSX1ugjBajBwrl7PeKRZVz8YRY7ToTFkHuxMp1dbC+vXy6+uvt20+8K8DHP/qOIJKYO4Lc4kZF4Ner2fevHkA/Pjjj92W0eUYq6HxTD1AJeNvt+mrc6iCi6iqkhfLRo+G3/0O9u/vchOCIODr69ujMWo0GlmzZg2pqalkZGTYfVxudS6ZFZmttl0y+JJuy+FqRFGkpqYGgNDQUK9x/T1xopKZM//LiROyy3VCQghr1tzomiR956G2tpYtW7awd+9eLBYLzc3NfP3112RmZnqEfAoK7eGMe7yiqNqBJEnU1tZ6xCqbKzlefhyTxUSobyhxQXHuFkehExwxRrusqP7wAzQ3w5AhkCQrcce+PMb+D+SHxOmPTSfxokTb7hdffDEajYYTJ06QlZXVbTldSrXsUYB/POiC3SuLF9NX51AFF7F9uxwvX1srz0uN3SvrYrFYejRGfX19bSWzulKqZkN2a2tqhF8E4/uN77Ycrqaurg5RFFGpVAQEBNgU1ebmZjdL1jFHj5Yyc+Z/yc+XFexhw8LZuvXXDBoU6mbJZKKiovDz88NkMnHq1ClEUaSgoIDc3FxlHlXwWJwxNhVF1Q5EUeTkyZPeF1vXQ9JKzpSliUpSVvA8HEeMUWtpGrsSKbVMonT99SAI5G7JZftL2wG44LcXMGrRqFaHhISEcOGFFwKw1lrOxtOxKqqK22+P6KtzqIKL2LLl7Gu9HrpRX1mSJIxGY4/H6IwZM1CpVGRlZdmyqJ+Pc91+5yXOQyV4z+OZ1ZoaGBiIWq3G19eXSZMmMWXKFDdL1j4HDhQxe/b/KC6uB2DMmCi2bv01AwZ4zmKkSqUiISEBkGuHa7VaFixYwMiRIxVFVcFjUbL+KriUQyVyIqXkaOUhvS8wfvx4ZsyYYatR2ikHD0JuLvj6wmWXUXK4hE1PbEISJYZfPZwJd01o97DLLrsMgP3791NaWtppF+Xl5ezcudO9N+Vq+TegKKoKCh7Mww/Da6/BVVfBZZfJyqqbCA0NJemMh4k9VtXa5lpSCltnQ/cmt184q6iGhIQAspLl4+PjkQvcu3YVMHfuR1RUyImeJkzoxy+//Iro6AA3S9aWQYMGAbKiqlarGTZsmK1OrYJCX0FRVBXaRZIkWyIlRVHtG8ydO5e77rrLPkX1iy/k/y+7jKpSI+seXIfFaGHgzIHM/NPMDm+kcXFxJCUlIUkS69at67SL5cuX895777FixYqunopjEC1Qc1R+rSiqCgqeS79+cPPN8P778O673WpCq9WiVqsdogTMmjULgNTUVJsS1xE/5/yMWTTb3uvUOmbFz+qxDK6kZWkaT6a0tIEFCz6lpkZ2SZ4+fQCbNt1GeLhnlt4bOHAgarWa2tpaKisrz3+AgkIvRFFU7cQac9FXKGkooayhDLVKzajIUec/QMHtuGyMVlbC5s0ANMy5nB/v/5Hm2maixkQx76/zUKk7n1asVtW0tLQO3UTS0tLYt2+frT6hW6g/AZYm0PhDgB3Ku0Kn9LU5VMG7eOSRR7j77rvp379/j9saMGAAiYmJWCwWdu7c2em+67PXt3o/Y+AM/HX+PZbBlTQ1NaFSqQgO9hzX2faIivLnr3+Vk/pdfPEg1q9fSnCw585LOp2O5ORkpk6dik6nA5R5VKHvYWcNir6NWq1mxIgR7hbDpRwukUtyDA8fjo9GmRg9HZeO0W+/BbOZ5mFj+PGtLOpL6gkeGMylb1yK1ld73sPHjBnDAw88wPjx41Gp2iq1JpOJjz/+GIBLLrmEAQMGOPwU7KLqjNtvcBJ4UbyYJ9IX51AF78LRY3TmzJnk5OSQkpLC3Llz0bfjjmyymNics7nVNm8rSwMwf/585s6di9lsPv/Obub++yfTr18Al18+DB8fz38Enj17dqv3yjyq4MkoWX/dhCiKVFRU9KlEIFZFVSlL4x24bIyKInz5JRZR4KeKiVSeqMQv3I+F/1iIT4h9CxqCIDBp0qQOa7WuW7eO4uJigoKCuO666xwpfdewJlIKVdx+e0pfnEMVvIu///3vPPbYYxQWFjqkvVGjRhEREYHBYGB/B+Vy9p7eS21zbatt3hafCvLvW61Wt6uMu5u8vOo22xYtGuUVSuq5KPOogqejJFNyE5IkUVBQ0KcyrVkVVSU+1Ttw2RhNSUE6dZrN5ckUlWvR+mm57O3LCOwf6JDmKyoq+PrrrwG45ZZb8PNzY+yQkkjJYfTFOVTBBeTnw7594ABLntFopLGx0WEPWiqVihkzZgCQmZnZ7j75Nfn4an1t78dEjaFfoB1Z1z2Ml156iWeffZbi4mJ3i9KKDz7Yz9Chb7NmTbq7RXEIyjyq4Ok4Y2x635KSgtNpNjeTUSEXK1cUVYWWSF+sYWfJYHKEBFRaNZe8egnhw8Id1v7y5csxGo0MGzaM6dOnO6zdLtNcCYbTgADBY9wnh4KCQsesWgWvvw5BQTBjBrz3HmjPH37gKiZMmEB4eDjDhg1r9/Obx9zMtSOuZXv+djac3MDQsKEulrDnWCwWamtrEUURX1/f8x/gIl5/fRfLlv0EwC23rGH//nCSkqLdLJWCgkJXURRVhTakl6VjES1E+EUQExDjbnEUPIDG8kaO/W8P5jX5HK2Kg0GhXPTsRcROinVI+0eOHOGtt96isrISPz8/brvtNvem4K+WPQoIGARazytboKDgqZhMJiRJsiV/cSrW+qm1tZCT41FKKoBer2f48OFYLBays7PbVVj1Gj3zBs1j3qB5PeorPz+fTZs2MW/ePAYOHNitNgoLC/Hx8SE8PLzd+TcrK4uvvvqKa6+9lqFDZaW6vr4eURRRqVQEBjrGs6YnSJLECy9s489//tm27Q9/mMKYMVF2HV9cXExpaSnR0dFER3uWYltQUMAvv/xCSEiIrcaqgkJvR3H9tRNPmIBdRUu3X6Vel/fgzDHaWN7Izr9vZ3/pQPDzY9oTsxl8yWCHtC1JEp999hmHDh2isrKSefPmER8f75C2u41VUVXcfh1GX5pD+zJFRUXs3r2bEydOOLej2lpITT37/pykM92hveRuPcVisfDqq6/y73//m/z8fHmbaGFnwU6+Pv41Owt2YhEtPepDkiS2bdtGdnY227Zt67b73ebNm/n000/bvXaSJLF27VqOHDnC2rVrbX1UVVUBEBQU5JTvrytIksSf/rSplZL6zDOz+fvf59v9LFNTU0NNTQ1NTU3OErNbSJLEjh07KC0tZceOHYr7r0KfQbGo2oFarWbwYMc8lHsDSnyq9+HsMVq09xR1FUZCdDD2hmEkLXZckq20tDQOHjyIRqOhqanJM7Ia2hRVJZmYI+hrc2hfpr6+HsD5bqBBQXJ86pYt8t/8nmXLFQQBvV7v8KyVarWa+Ph4ysvL2bp1K2FTwnhy05NkV2VjkSyoBTWDQwfzwrwXWDh0Ybf6yMvLIzc3F71eT25uLnl5eV22uJnNZoKCgqirq2s303pmZibHjh3Dx8eHY8eOkZmZyfDhw6mtlZNBubuGqihKPPTQOt56a49t28svz+eRRy7sUjvW8etpC2vWa+zr69vta6yg4GyUrL9uQhRFiouL+0SmNUmSOFyqKKreRk/GaHV1NSUlJVgsrVf1G8sbKUsvY88/9rD1qfWARERQM4Pumk/58XIayxt7LLckSaxevRqz2UxCQgJ+fn5899137l0tFk1Qcyb5RuhY98nRi+hLc2hfp66uDoCAABe4zMfEwE03wbvvwrRpPW7OZDI5ZYxaa0F/k/4NS9YsIbMyE71aT6AuEL1aT2ZlJku/XMrarLVdbttqaTOZTAQEBGAymbplcdNoNFxzzTXcddddbWp1SpLEmjVrqK2txdfXF5PJxLp165AkierqagC31lC1WETuvPO7Vkrqu+8u7LKSarFYaGyU72suGb920vIa6/X6bl9jBQVn44z5U7Go2oEkSRQXFxMZGeluUZzOqbpTVBmq0Kq1jIjwAMuWgl30ZIxu3ryZr776itmzZ/Pb3/7Wtn3Hyzs48K8DWJotYGxGq7JQrY/hqzu+B2DCnROYcOeEHsmdlpZGamoqgYGBtoLxqamppKWlkZzspoWS2kwQjaANAr/uxXoptKYvzaFdxWSCQ4fAFZ6Ghw87t32j0YjRaAQ860HfHiRJwmw2O+Xhv1+/fgweMpiPCj/CYDQQ5BuERiU/fqnUKoJUQdQaa3lq81MsGLwAtcp+q0RLS5sgCD22uLVXNiwjI4PDZwZPXV0dAQEBNqtqTU0N4F6L6t13f8+HHx4EQKUS+M9/ruJXvxrX5Xas1lS9Xu+aGGs7sV5jHx8fzGYzPj4+ilVVwSNRsv4qOB2r2++IiBHo1J4zUSs4D2tJgZgYOXFW5YlKUt5MIX9bPoH9A9FqYeDpFPLqw5n1l8uJmD4cAL+InpWOsVpTjUaj7SHH19eXuro6Vq9eTVJSkntipFvGpyox2gpOxGSCKVPg4EF3S+IYrA/6fn5+TnEB82aCRgdRdboKSZSoMFTgo/FBr9aj1+gRBAE/jR8nKk+w+9RuLhxgnyWwpaXNWspLp9NhMBjYsWMH8fHxPZ5DrdZUSZJQqVTo9Xr0ej0NDQ2sW7eO0NBQwL0W1ZtvHsMnnxzGYpFYseI6brhhdLfacak3gJ20vMa+vr40NTWh0+mora112DVWUPBkFEVVoRW2+NQoxe23r1BUVARAqF8oW5/fSsa3GUiihMZHw+gbR3OBeQ91H+eTRzwR04cTMSLCIf22tKZab7SCIBAYGOheq6qSSEnBRRw65F4l9RwPzx7jiQ/6nkC9sZ51peswCSaQQJAEmsxNNJmboBlCfELQqrRYzBZKG0rtbvdcayrgEKtqSzIzMzl+/LhtbrbGbvr7+3Ps2DFbPKs7Larz5g1izZobEUWJK68c3u12rAstnjR+XXGNFRQ8GUVRtQNBEAgLC+sTq1ZKIiXvpLtjVJIkik4X0VjeyL4/78O3WU6Akjg3kcm/n0xwtC8s/DN1AA58ELFaUw0GA35+fjQ3N9s+U6vVGAwG91lVlURKDqcvzaFdwZ2JRcePh7EODsF2WSKaL7+EuDj5JBxUkmbu3LkUFRU51DJoFs18lvYZr+x6hVO1pxAQkGjtGqdRadCpdZgtZtSCmih/+8qoWC1tzc3N6HQ6TCaT7TOVSkVzc3OPLW6SJPHNN9/Q3NyMWq1GpVLR1NSESqWy9ZGbm0tISIhLLapNTWb0enWr87r88vZr1XYF60KLpyRS6ugam0wmh11jBQVH4oxxqCiqdqBSqbpdl8ybaDQ1cqJSTkuvKKreRXfGqCRK7P98P6ePnkYyS+jQETUmiqkPTSVm7Jn6uevWQXU1fjH9mXD9rB67+1oxm82UlJTg6+trS17REl9fX0pLSzGbzWhdWRuxqRSaSgAVBHfPfUyhLX1lDu0p77wDrnAi8PGRlVRH/rQkSXKNRcpkgscfh/p6CAiARx6BO+/scbMXXti1xDudIUkSG09u5Pltz5NVkQWAVqVFrVJjFuU4WEEQCNAG4KfzAwkazY0MDx/OlNgpdvVhsVioqalBr9fb4oJbotfrqampwWKxtBt3ag9ms5nc3FxUKpUtcZJer7dldNbr9fj5+bFgwQLCw8O71UdXqaw0cNlly1m4cAh/+csch7VrMplsJWk8xaLa0TW2KqyOuMYKCo7EGSWqlJFtB6IoUlhYSFxcnNvrhDmTo6VHESWRmIAYIv2VpCfeRFfH6Kk9p0h5I4WMzAxERIL1wVzyl0sYNH9Q6xWxNWsA8LvxCibcOclh8mq1Wl566SXbCnZ7BAUFuVZJhbPW1MAhoHGMUq7Qd+bQnpKcDDNmuFuK7tEykZK/v7/zOkpNlZVUkP93kKfHzp07KSoqYv78+QQFBXW7nUPFh3h267PsKtjVarsgCATqAqluqkZAwEfywVfri9liptHciF6t5/m5z9udSEmj0XDrrbdiMBg63MfX17dHCozRaCQwMBCdTkdMTAzFxcVMmjSJOXPm2PYJCAhwmTW1tLSBSy75hEOHStiz5xRhYb78/vf2Kfbnw7rI4uPj4/r7Tgece42t2dNjYmJs82hPr7GCgiNRsv66CUmSqKysJDY21t2iOJW00jRAsaZ6I/aO0aqcKna/uZv87XLheYOPAf8gfybMmsDgS86pc3nypBxAp1LBNdc4XObw8HCXrcLbTdUZRVUpS+NQ+soc2pexPuj7+/s7N5FSWlrr97NnO6TZzZs3U1JSwqRJk7qlqBbUFPDi9hf5+vjXHe5zxbArmBQ9iZd/eZlKqZI6Yx1qQc3w8OE8P/f5LtdRbRkz6gxSUlKQJImEhASGDh1KZWUlISEhtt9xaWkpW7dupV+/fkycONFpcgCcOlXLxRd/wvHj5QBER/szZ06Cw9r38fEhPj7e4xbSWl5ji8Viy56uJCtT8ESUrL8KTuVQ8SFAUVR7I4ZKA/s/2M+xL48hiRIqtYqRi0biE+FD7s+59I/r3/agM9ZUZs+GKPviprweq0U1WIlPVVDoCmFhYUycOLFVrKRT+M1v4PLLYetWyMiA6Gjn9mcHq46s4tGNj2KytH/uSdFJPD3raaYPnA7AvVPvZfep3RTXFRMTGMOU2CldKknjCkRRJCUlBZDrwFqT7rXk9OnTbNu2jcTERKcqqjk5Vcyb9zE5OdUAxMUFsWnTbQwb5riFTl9fXyU8QUHBA1EUVQUARElULKq9EHOzmbQVaaT+NxVTo/wQFT87nikPTCEkPoTdr+0GzpamsWEwwA8/yK8XLXKlyO7DYoTa4/LrUOU3oKDQFayZSK3xi04lOhpuuMH5/djJuJhxWERLm+2xQbE8MeMJrhlxDSrhrKVOrVJDAWRuzWTGb2Z0S0nNyckhODiYsLCwHsneESqVinvuuYe9e/cyduzYdhVVe2qoWiwWamtrsVgsRER0PWN8RkY5F1/8CYWFtQAMGhTKpk23kZDQcZ8KCgq9B0VRtQNBEIiJienVWdXya/Kpba5Fr9EzLLzn2fMUXMu5Y1QSJU6sP8Hed/ZSXyy75EWMiGDqQ1PpP+Gs9dT68NFGUf3pJzn+KzYWJk92zUm4m9pjIJlBFwa+iouqI+kLc6iCdyMIAhqNpltjdHjEcG4ZcwvL05YDEKQP4oEpD3DH+DvQa/TtHpOVlUVlZSXbtm1jURcXA81mMxs2bKCxsZHrr7+euLi4LsvcGZIk0dDQQFhYGAsWLOhwv+rqaoBOXaUNBgNHjhxBq9V2WVE9fLiE+fM/obS0AYCRIyPYuPE2+vf3jKy8rkaZRxU8HSXrr5tQqVRtH+R7GdayNKMiRqFRKcPC22g5RosOFJHyRgpl6WUABEQHMOm+SQy5dAiCqvUk8uSTT1JcXNzW5cnq9rtokRyj2hdoWZZGeRBwKH1hDlXwfrRabacxirXNtQTp21fKHrnwEb7P+p4bRt3AQ1MfItQ3tNO+Zs6cydGjR9m/fz8LFizoUqbZ48eP09jYSGBgIP37txO20UMyMzP56KOPmDZtGldeeWWH+1ktqqGhHZ+r1cJuMpkwm812J/7Zv/808+d/QlWVnIl33LgYfvppKZGRTkzU5eEo86iCp6Nk/XUTFouF3NxcEhISem0Ae1qJ7PabFK3E5nkjFouFozuPUvRVEXlb8wDQ+mkZd/s4khYnodG3/1MPCQlp67aVni7/abXQyUNKr6P6TJKWECWRkqPpC3OogncjSRLNzc1YLG1deLMrs3l+2/McLz/Oll9vQafWtdknOiCavb/bS4DOPoUzMTGRuLg4CgsL2bVrF/Pnz7frOFEUOXDgAADjx4+3PRhmZGRQXl5OYmJij5XXrVu3Yjabz5sYxWpR7Szrr1qtRqvV2sq/2KuQh4T44OMj37emTInlxx+XEBrqArdyD0aZRxU8nfbmz57SR0wlPaezMhq9gUMlciKlsdHKQ7q30VTdxK5XdvHzvT+TtzUPQSUwctFIbv76ZsbfPr5DJbVdysvhiSfAbIZ586CTlfJehSRBlfwbIERZrHEGvX0O7U1UVVWxd+9eqqqq3C3KWQoL4W9/g5QUuZaqEzi3tEJ5YzlPbHyCOR/NYf2J9eRV5/G/1P91eLy9SirILnKzZs0CYNeuXXYnocrLy6OyshKtVsvo0WdrPWdnZ7N3715KS0vtlqE9ioqKyMrKQqVSMeM8tZLsiVEFOaMuYKtTag+DB4exceNtLFo0kg0bbu3zSqoVZR5V6GsoFlUF6prryKnOARSLqjdhMVo4suoIBz88iLHeiGSRiLswjmkPTiN0UDcVzLw82LQJEhL6ThIlAEMRGCtAUEPwKHdLo6DgNiRJIjc3l4qKCjQaDSEhIZ4RE7d5M7z5pvzn7w8bN0J8vFO6MpgMfLD/A/6x9x80GBtaffZ6yuvcNPomgn16Xjs0KSmJkJAQqqurOXjwIJPtyAdgtaYmJSWh15+Nf42Pj8fPz69bCYtasm3bNgDGjBnTaaIms9lsK0l0vjqqPj4+1NXVdUlRBRg1KpIvvrixS8coKCj0LhRFVYEjpUeQJIm4oDjCfJ2TQVDBcUiSxMkNJ9nzjz3UnZZXV8OGhjH08qHMumVWz1yCtmyRrYtxcTBunGME9gas8amBw0HdfvITBYW+QHV1NWVlZbbat9XV1Z3GILqMLVvOvtbrYcAAh3chIvLdye/48McPKa4vbncftaAmsyKTSbGTetyfWq1mxowZfP/992zdupVJkyZ1uiggSRKjRo3CaDQy7pz5uaV1tbvU1tZy8OBBAJu118qMGTMYO3asraZnbW0tkiSh0Wjw9+88btQap2owGDrcZ/Xqo3z55TE+/fQ6NBrF2U9BQUFGUVTtQBAEBgwY4Bmryk5AKUvjPZQcLiHl9RRK0koA8I/0Z+I9ExmycAjVNdXdG6Pl5fLf1q3wwQfytjFj5BqFABER8l9vxqqoKmVpnEJvn0N7C1ZrqslkQhAEzGYzubm5nmFVPRMPCcCsWQ5P8pavymd96Hq+2PtFu+Vi9Bo9d15wJ/dNvq/DhErdYfLkyWzYsIHS0lIyMjIYMWJEh/sKgsDIkSMZOXKkw/pvyc6dO7FYLMTHx7dJsBccHNzKctoyPvV8Y+N8rr//+18qd9zxLaIoodGo+Oija1CrFWX1XJR5VMHTUbL+ugmVSkV4uOMKS3sa1oy/SVGK26+nUltYy+63d5OzSXbR1vpqSb4tmeSlyWh9tQDdH6NffAF//StUVMjvAwLg55/hl1/k93feKf/1ZmyJlBRF1Rn09jm0t1BdXU1lZaUtQY9Go/Ecq+qaNVBaCtu2yR4fDiK9LJ1ntzzLVv1W0IOa1kqqIAjcMOoGHp3+KP0DHZ9h18fHh8mTJ7Nt2za2bdvWqaLqTJqbm0lJSQHaWlMB9uzZQ1paGklJSUyePBlRFImJibHrd92Zovruu3u57761LfZVHks7QplHFTwdJeuvm7BYLGRlZTF06NBel2lNlESbRXVsjJJIydNorm3mwIcHOLrqKKJZRFAJDL9qOBPvnohfhJ9tv26P0fp6OHAAgoLkv+nTYf9+eOopsD4w9XZrqtkAtWesx4qi6hR68xzaW7BaU1tmbVSpVJhMJs+xqkZFOSx2vqiuiL/t+Bufp39uy257bvmUmfEzeXrW04yO6rlbbWfMmDGDHTt2IIoiJpMJrVbr1P7aY//+/TQ2NhIeHt6uG3FZWRkZGRm28ihDhgxh2bJldrXdUlEVRdH2MPvyyzt49NGNtv0eeGAyr79+KSqVYjFsD2UeVfB0nJH1V1FU7aSrSQC8hZyqHBqMDfhqfRkcOtjd4iicwWKykP55Ogf+fYDm2mYA4qbGMeUPUwgf2v6KapfHaH4+LFsGubkQHAx/+QsMHAhLl8pKqptW9l1O7TFABH0U+ES7W5peS2+dQ3sLVmuqVqtFFEUkSUIQBLRaredYVR2EJEn86utfcaT0iG1bfX09oiji7+/PmJgxPDXzKeYkzHGJch4aGspjjz3mku+3oaGBwsJC4uPjbQqkKIps374dkOu72mMVqampQaVSERAQcN7vSKfT2do0Go3o9XqeeeYXnn12q22fJ56YwQsvzHX/YoiHo8yjCn0NRVHt41jL0oyJHNNuXI6Ca5Ekidyfc9nz9h5qCs4UUx8UytQHpzLgQgcmD0lJkcvQ1NXJVopXX4WRI+H4ccf14S20LEujPCQp9EGs1lSz2Yxer0eSJCRJslm/PCpW1QEIgsDD0x7m9m9ub7XdV/Tlz5P/zD2z7nH5/TA0NJTm5mb27dvHmDFjzptJt7tkZ2ezefNm+vfvz403yhl1TSYTw4cPx2w2M2HCBLva+frrrzl69CjXXnst06ZN63RfQRCYNGmSzVL8xz9u4NVXd9k+f+GFufzpTzO7eUYKCgq9GUVR7eOklchuv0pZGvdTerSUlNdTKE6Vs036hvky8Z6JDL9qOCpHJZaQJPjsM/a8/jq5ksTYoUMZ/o9/gDXuJSJCjkft7e6+LVHiUxX6OJIkYTAY0Gg0WCwWm6Jqfa3RaDAYDDYra2/gksGXMDVuKimFKfjr/BknjGNg9UCuGnSV2xZtV6xYwbFjx6ipqWHhwoVO6ePkyZMAJCQk2Lbp9XquvvpqrrjiCrtdSlsmU7IHnU6HKErcf/9a3ntvn237668v4MEHp9onvIKCQp9DUVTtQKVSMWjQIKcECbubw6VyIqWx0Up8qruoO13Hnnf2kL0+GwCNXkPS0iTG/WocWj/7YpXsGqNGI7z4Inz3HQeAHcHB+NxwA8NbJmewKqp9BUlSMv66gN48h/YGVCoVEyZMwGQyAXDkyBGampoYPny4rRyJVqt1z/V77z054+/s2TBpEnQhfvNA0QH8tf4Mjxje5jNBEHh69tOsOrKKZdOW8cEbH1Cpq3TrGJ08eTLHjh1j9+7dzJs3r1WdVEdgMpkoKCgAYNCgQQAUFBSwZ88eZs6cSVRUlN1t1dTIHj8hISF2H2MwmNi37zQgO6/8859X8Lvf2WfBVVDmUQXPR0mm5CYEQSAoyHHp6D2FmqYa8qrzAMWi6g6M9UYO/ucgR1YewWK0IAgCQy8fyqR7J+Ef1XldunM57xitqIA//hEOHwaViqIRI0CS6OeEWoReRWMBmKpB0Mo1VBWcQm+dQ3sTer3ephhpNBrUajV+fn4EBAS4V7BPPpHj6N9+Gy66CJYvP+8hedV5vLj9Rb7N+JYZA2ew6vpV7VqCx8WMY1zMONt7tVrtVovxyJEjiYiIoLy8nL179zJjxgyHtp+fn4/FYiEoKMiWPXbr1q0cOnQIs9nMTTfdZFc7JpOJ+vp6wH6LKoC/v45165Yyf/4nLFs2lSVLlMXBrqDMowqejjPmT2VZxg4sFgtpaWlOyWblTqzZfuND4h1aF06hc0SzyNHVR1l5zUoOfXwIi9FC/4n9ufbTa5nzzJwuK6lwnjGang633iorqYGBSG+9RdGZJBr9+vXr6el4N1a33+CRoNa5V5ZeTG+dQxWcTH6+rKRamdq5i2iVoYq//PwXZv1vFt9mfAvA9vztbM7ZfN6urO7P7hyjKpWKmTPlWM3t27cjiqJD28/JkcubJSYmIggClZWVpKXJc6C1X3uora0F5AUNPz+/8+zdmrAwX3bv/q2ipHYDZR5V8HScMTYVRdVOeuPEYK2fqrj9ugZJksjbmscXN33Bjr/voKm6iZCEEBa8voDL37uciOE9iwttd4yuXw+//a1cfzAxET76iNqRIzEYDAiC0CVXr15JtTWRkvLQ5Gx64xyq4GTKy2HMmLPvZ89ud7dmczPv7X2PaR9O418H/oXJYmr1+fPbnscsmp0pqcOYMGECfn5+VFZWcvToUYe27e/vT0BAgM3t11oSZ+jQofTvb3+NWGt86vmSa9XXG7n//rVUVhpabddolEfP7qLMowp9DcX1tw9jVVSTohS3X2dTfrycXa/vomh/EQA+IT5MuGsCI68dicoZN21RlGO7/vtf+f2MGfD88xAQQHGGXDM0PDwcna6PWxGVREoKCm3QarVYLBb3J0664AL46SdZYd2xo7XSilwH/Jvj3/Di9hcprC1st4kgfRA3jrrRVivV09HpdEydOpXNmzezdetWkpIcd3+eNm0aU6dOtVmP9+zZA3TNmgr2xafW1DSxcOEKdu4sYM+eU2zceBtBQY6NuVVQUPAsKoQK+J1j21QU1T6KRbRwtExerR0bo1hUnUV9ST1739lL1tosANQ6NUmLkxj363HoApykJDY0wFNPwbZt8vtf/Qruuw/OBLkXF8tZha2F2/ss5gaokxNYEaIs1igoWBk71sPuCRERcPXVrTbtKtjFs1uf5VDxoXYP0aq13D7udh6c+iAhPiEuENJxTJ8+na1bt5KXl0dubm6rDL09RRAEBEFgz549NDc3Ex0dzfDh54/Pj4qKYuTIkURFRZ034295eSMLFnzKgQPywmxmZgUnT1Yxbpzn3HOsWa61XUjOpaCg0DkVQgU4OB+noqjagUqlYvjw4b0q09qJyhMYTAYCdAEkhCS4W5xeh6nRROr/Ujn86WEsRtlVZ8hlQ5h07yQC+wU6vD/bGD11Ch5+GHJyQKeDp5+GSy9ttW9Rkfzw0OcV1eqjgAg+/cAn0t3S9Gp64xza1ykpKcFisRAWFobPmZh3V5BVkcXz255nQ/aGDve5avhVPDHjCeJD4u1uVxAEfHx8PGKMBgYGMn78ePbu3cu2bdscqqiC7D66Y8cOQLam2mM5nzRpEpMmTQLgq6++AtpXVIuL67n44o85erQMgIgIP376aalHKakAJ06coLq6muHDh3tNCIwyjyp4OkrWXzfS21wkrW6/Y6LGoBKUSc9RiBaRjG8y2Pf+Pgxn4nJixscw7aFpRI5yrjKkS02FJ5+E2lqIjIRXX4VRo9rsZ7WodiUmqVeilKVxKb1tDu0Rp05BZSV+2dDSmdUvGwgGwsIgNtbh/XVIR/11clzV8ePUqNVoZ85sq6g6oz9jLS/lL2dF7ndYxPbj9KbETeHpWU8zvt/4jvvugIsuuoimpiaPyao6c+ZM9u7dy9GjR6mpqelSdt3zkZaWRnV1NQEBAYwfb993VVVVRW1tLUFBQR26/ubn1zBv3secOCFfw379Ati48TZGOfne11UkSaKurg6gy8mg3I0yjyp4GuVn/hkxslx7/qzsXUVRVO1AFEXS0tJISkqyuxi2p2NVVJOjlYd0RyBJEoW7Ckl5I4Wqk1UABA8IZvIDk0mYk+DcWC9JQvrsM5r++lf8fHzkOK5XXpHd5dpBcf09g1VRVeJTnU5vnEO7zalTMHky1NeTbIbtLT7yvxf5rhwQAHv2OEZZbdFfh7TXXyfHScAQsxmLnx/irl3ywpgT+wNAL/HNEgOW/uG2MAYrg8MG89TMp7hk8CXdnmunTJlCWloa/v5dz7ruDGJiYrjssssYMmSIQ5VUSZLYunUrABdeeKHdrq87d+5ky5YtzJ49m8svv5ypU6cSHR1t+/zEiUrmzfuY/HxZiY2PD2bTptsYPDjMYbI7iqamJiwWCyqVyqsUVWUeVfBEPmItn/EWAFXaKoe3ryiqfZTDpYqi6igqMitIeSOFU3tOAaAP0jPhzgmMXDQStdbJNxOjEf72N4RvvkEQRaSFCxGeekp2++2AmJgYTCZT31ZUJbFFIiUlPlXBhVRWysqYSoWo1dDc4iNfLSCY5c8rKx2jqLboD007t3xzB/11cpwkSUiiiLqxEV1jo+P7U6tlzxCt1vYXajLzwH4Nz8eINkU13C+cR6Y9wuKkxWjVPYs13L17NydOnCAxMdGhimFPuOiiiwBobpZHibXObU+orKyktLQUrVbLtGnTutVGZGQkkS0WJ9LTy7j44o8pKpIXGYYODWPjxtsYONAzvsdzsVpT/f39FTdaBYVuYsHCXexgB1fyFLOZgIG9hr3cxm0O7UdRVPsglYZKTtWeQhAExkSNOf8BCu3SUNbAvvf2kfldJpIkodaqGX3zaMbfPh69K7IbVlbCH/8Ihw6BIFC2dClxjz7a/sNhCx588EHny+bpNOSBuQ5Ueggc5m5pFHoBJpP8U2xq6nw/v2xINoOo1WBCS8uiKZIGEIDmZli+HKzu+bNnQ3sJjj75BKrOrGCPHAnz57fdZ8cOuT3rvODr23Yfg6F1f/fee/YzjUZWFkVR3g9AkkClQpCk1hZMk0lux9qf9dhzaWg429+IEXDJJa37E0WwWOS/piYICQGNhjuOaPnf9dGUSw3cNeEu7p10L4F6x8T8b968mZKSEubMmeMxiipASkoK69atY86cOcyZM6fH7YWHh/OnP/2J/Pz8blmPjUYj//rXvwgODua6665Do9Hw3nt7bUrq6NGRbNx4GzExAT2W1VnUn7HaBwR4rowKCp7OZ3zGekqppR+ZxLMEHTWWGof3oyiqfRCr2++g0EEE6JSJuquYDCYOf3KYQx8fwtwkP2YOmj+IyfdPJijWRfFNx4/LSZNKSiAgAPH556kKCCDO3eUkvAWr22/waFAp06BCzzCZYMoUOHjw/PuOQXb3bQY6rez54YdnlbyAgPYV1Q8+gOwzmatvuKF9RfWXX2TPC6NRbq89RdViad3fXXe1v8+ZB3wBwM+vrZut2Sy3Y+2vI0XVaDzb3/XX2xTVI+EWXplcx8vb/Yg06GWFVxBkDxGzGb1F4N1xTxJ3wRxiAvqGR4haraaxsZEdO3YwY8YMNOdZiOyM6upqTCYTkZGRjBgxoltt1NTUkJWVhU6n44YbbgDg9dcv5fTpenJzq1m/fikREZ7tTmtVVAMDHZ/YUEGhN2MBaoAw4HquZx33MYLp/IkhTutTeUKzA5VKRVJSUq9xEbHFp0Ypbr9dQRIlMr/PZO+7e2ksl93dopOjmfrgVKKTo89ztAPZsAGeeUZ+iIuPh9dfRzVgAEmi2GvGqNOpUuJTXUlvm0PP5dAh+5TUc9FhtL02o0YQvCzuzEELY6dqT/G3Qy+xZlEjkiDQb6KRFzecaVurbdXPxNDR4AQlVRAEfH193TpGf/75ZyIiIhg5cqRNIR03bhzr1q2jpqaGw4cPc8EFF3S7/c2bN5OSksKCBQuYN29et9pomUjJulCh0aj47LNFGAwmgoNdlwG6O0iS5LUW1d4+jyp4Nr+QzzJqGctoPkTABx8+4d8InJ2fI4VI+AB4xXH9KoqqnRiNRpem4HcmSiKlrlO4u5Ddb+ymIqsCgMD+gUx5YAqJ8xKdmyipJaII778P//mP/P7CC+GFFyAwECSpV41Rp6Nk/HU5vXl8ns/dtz0EJEI5m3iiUR2IVusHJgcK5gQkkF1/gZ7OfLVakX/47uWD/0zH2Hw21vXToQbu2KtjSLlGnt9chHTmvNxBRUUFhw7JNWEHDBhgy6ir1Wq58MILWb9+PVu3bmX8+PHt3nPS0tIoKSlh2LBhDBw4sM3nDQ0N7Nu3D4DExMRuy1lTU0NtbTMREa3zIOh0anQ6z19oaWxs9MpESlZ68zyq4JkYsXAPu1hNMCJqmikll2gGQSslFSBcCod/oSiqrkYURTIyMnpFpjWTxUR6WTqgKKr2UHWyipQ3UijYWQCAPlDP+DvGM/rG0ahdeVNubIQ//xm2bJHf33Yb3H+/LalIbxqjTsdUCw058utgJUbbFfS18fnOO5DcwfTqly1n9/XTgKruzEZB9u613fIFATZvhqTzJPratu38wjz5JPz4o+w+21GGV53u/P1ptRAdjSRJmEwmVBaLvHjWEl9fuZ0ZMzrtzxToxyf/+T2vFaym0pAn+5O1wCLAu1PVvLYr9Pzn5yAkSaKpqQnx3HNyEYGBgcyaNYuampo2ZV+mTp3K5s2bOX36NNnZ2QwZ0tbNrqCggMzMTCIiItpVVHft2oXZbCYuLq5HimphYTknT1Zx4IDAokWVHpnVtzNaWlNdtsjsIPraPKrgfn4hnzvIpxj5dz6MMlYQxaAO9nfG/Kkoqn2MrMosjBYjQfogBga3vZkpyBgqDex7fx/Hvz6OJEqo1CpG3TiKC357AT6udm06dQoeeghOnpQf/p56ChYudK0MvYnqI/L/fgNA710PWQreQXKyrKu1SzDynVdl4Yx9Uv7PYpGDXc2dRq52n47abWd7Xl4eQn4+AyQJ4ZzPJVFEsFjkREpd7E9CYm1cEy9MbiI3/R/tJn4LMML9ab787qgPWFqYl531vXgIOp2uQ7def39/Jk6cyK5du9i2bVu7impnmEwmdu7cCcCsWbO6raBVVhrIyakGoLxc5K23dvPmm5d1qy130dDQACjxqQoKndHaihqGjmb+gJmnmYWqx740XUNRVPsYh4pl16Lk6GSvW010BeYmM2kr0kj9XyqmRvkhKXFuIpPvn0ywO1Lt79sHjz4ql2qIiIBXX4XRo10vR2/CVj9VKUuj4AbCwmTzaV0dtHQ9FAQ5yRDIn4c5aBHF2l99/dn2z+Wc/srKyrA0NxMXEIDQ0NDqOEEUUVuz/QYFtZWzg/72R1n4v2nN7Iu2yOd6TpydRq3l1ix/HtoFEU0q2vWBduT34mXMmDGDlJQUjh07RklJSasapufj4MGD1NfXExwcTNL5rPQdsGtXoU1JBZg2bSivvrqgW225k8TERPr166c8/ygodED7VtTRjCTCLfIoiqqd9BY3i7RSuXak4vbbGkmUyPoxi73v7KWhVF5xjRwVydSHptJvfD83CCTB55/DK6/I7nWjRslKaovadefSW8ao07Epqu1kUVVwGsr4PENsLOzZI5eX6oiwMMfUUO1Bf8bISOo3bSLoHEum6jzHndtfbsMp/prxb74v3gqcUcxVqlaK6mVDLuPJWU8y6Ba9674XLyMyMpJRo0Zx9OhRtm/fzqJFi+w6TpIktp1xEZ8xY0a3fodvvpnCypVHiI+3yuLHU0/NR6PxvqQ+1qRZ3ooyjyo4C0+yorZEUVTtQK1Wd3sV0tM4VHLWoqogc3r/aVJeT6H8eDkAATEBTL5/MoMvGYygcsOP02SCl1+GL7+U3y9cKMeZdVLsvTeNUaciiWddfxWLqstQxuc5xMa6VuHqZn9S//7QnZqiZ/pbkbaCJ1KewGQxtevme0G/C3h69tNMjp0sbwjFbYqoVYHxZEVg5syZHD16lP3797NgwQK7stZmZGRQUlKCXq9n8uTJXe7zr3/dxpNPbkavD6G0NIArrqgiLi6IsLDuxQ9LkkR1dTUGg0GxbHYRZR5VcBbtWVGXM5pRXbSiOmP+VBRVO5Akibq6OgIDA716Ui1tKKWkvgSVoGJU5Ch3i+N2qnOr2f3WbvK25gGg89cx7jfjSLolybWJklpSWQmPPSbXuhAE+P3v4dZbz1sGwp4xarFYPPohzCXUZYOlEdR+EOi8ul8Krektc6hC1xgfMx6LaGmzPT4knidmPMGVw670mPGg0WhQqVRuzfx7PhITE4mPjyckJASTqbVrdP/+/REEoU0iJqs1dfLkyV2yJEqSxJNPbubFF7cD0Nys4dFHZ9HUtAGA4O4sYJzh6NGjSJJEWFiYksG2CyjzqIIzSCGFX7GPcuaixcgfMPGXblpRnTF/KoqqHYiiyMmTJ70+01paiez2OzR8KH5a70vL7igMVQYO/OsA6V+kI4kSgkpg1PWjuOB3F+Ab6kaXoMxMWLYMiovB3x/++leYPt2uQ+0Zo//+9785fPgwN998MzNnznSk5N5DjfwbIGQMCN7ntuat9JY5VKFrjIwcyY2jb2TlkZUABPsEs2zqMn417lfo1LrzHO1aHnnkEdLS0ujXzw2hHnYiCAJ33313u7+hcePGMW7cuFbbTp8+TVZWFiqViul23kus/PJLrk1JBXj88QQiIo5RWAjDhw/vtoIpCAI+Pj4YDAaampoURbULKPOogiORkDPNJ5PMaF6hmkT+zZQuW1FbomT9VegRNrffqL7p9msxWkj7LI3U/6RibJCTfMTPimfKA1MISQhxr3CbNsFf/iIXZBw4EF57DRISHNpFcXExtbW16HSe9YDoUqrk34Di9qvgdkQRqs7WUcXfH7z0ob26qZoQn5B2P3t0+qP8eOJHFo9ZzANTHiDYxw1J6XoRarWauro6du7cSVRUFOPHj+9w35CQEBYsWEBdXR1hXUxCddFFiTzzzGyeeWYL//jHZSQklLFly1Fmz57N5Zdf3qNzsCqqBoOhjQVYQUHBuRix8BhHMZPEmwj44ccn/JMwwtrURfUEFEW1D9FXEylJokT2T9ns+cce6ovlGmoRwyOY+tBU+k/s717hRBH+9S/5D2DqVNmSGhTk8K6KiooAiImJcXjbXkO11aKqJFJScDMNDa3rlr7wAtx+u/vk6QYZ5Rk8v+15siuz2fLrLWjVbeumxgTEsO93+/DX+btBQvt55ZVXKCoq4uGHH2bAgAHuFqdTUlNT2bRpE1FRUYwdOxaVqq13iCRJ+Pn5MW/evG738/TTs1m4cCiTJsXyww8/AJCVlUV1dXWPFEyrFbWpqanbbSgoKHQdCYk7eJyvWEI01dxAKLOAcMLdLVqHKIqqnXi7e4rRYuRY2TGgbymqxanF7HptF2XpZQD4R/kz6b5JDL1sqHsSJbWksVG2ov78s/x+yRJ44AHopktPZ2O0vr7eVj+uzyqqxipozJdfh4xxryx9EG+fQxXOUlJfwss7X2blkZWIkuzq9fGhj7njgjva3d/TlVSQa42KoujRMapWJk2axIYNGygtLSUzM5MRI0bYPpMkidTUVHJycqitrWX+/PnE2pGgqrnZzKFDJUyefHZfQRCYNKn1sadPn2bDhg3ccMMN3ZbfGiurKKpdR5lHFbqD1c1XQOByJpDKF9zAdcyke0nRXImiqNqBWq1udSPwRo6XH8csmgnzDaN/oJutiC6gpqCGPW/vIWdzDgBaXy1jfz2W5CXJaHw8YNifPi3Ho544AVot/OlPcOWV3W7ufGPUak0NCwtD30n24F6N1ZrqnwBax1usFTqmN8yhCtBgbOD9fe/z7r53MZgMrT57LeU1bhh9A0F67/xtWWMnvSH2z8fHhylTprBlyxa2bt3a6rdVXl7Ozz//THFxMZIkMXHixPMqqo2NJhYtWs3PP+ewdu0S5s5N7HT/nrrrKhbV7qHMowrd4RfyeYMg/kwIE4AbuZH5zHeKFVXJ+usmRFGkqqqK0NDQdl1svIHDJXLtyOTo5F6dLa65tpn9/9pP+ufpiGYRQSUw4poRTLhrAn7hHpJAav9+ePRRqKmB8HC5FE1yz6zc5xujVkXVkxOFOB2b22/f8SjwFHrDHOpwfHzkOslWJk50nyznYM0oa7UumkUzK4+s5JWdr1DaUNruMRqVhqyKLCb0n+AyOR2N2Wx2SjIQZ3DhhReybds2Tpw4wenTp+nfX16AliQJX19fRFG01V7tjLq6Zq688jO2bJGz39988xfk5PwBf/+Ocxk4SlE1GAxIktSrn0kciTKPKnQFIxbuZRerCEaNEV8CWIEGFSqnufoqyZTchCRJFBQUeHXQf0tFtTdiMVo4+vlRDv77IM11zQAMuHAAU/4whbDBXUsi4VS++EJWTC0WGDkSXn0VoqJ63Oz5xmhxcTHQh91+oUUipd75G/BkesMc6nC0Wli82N1StItWq8VsNgOwIXsDz297nqyKrHb39dX6cs/Ee7h74t0E6M5f19NTkSQJk8nkFa6/AKGhoSQnJ5OamsrWrVu5+eabAYiIiKCmpgZBEJg5c2anCk1VlYHLLlvO7t2nAAgM1LFmzY2dKqnQs9I0cFZRtVgsmM1mtNq2sc0KbVHmUQV7Obcu6hAK+T8iEAhxar9KeRqFbiFJ0tmMv71MUZUkiZxNOex5ew+1p2oBCBsSxtSHphI3Jc7N0rXAbJYV1DVr5PcLFsDTT4OL3HD7fCIl0Qy16fLr0N71G1BQcAbZ9dm88sMr7C3e2+7nKkHFzWNu5o8X/pHogGgXS6cAMGvWLFJTU0lNTeWyyy4jODiYo0ePUllZib+/PxMmdGzdLi1t4JJLPuHQoRIAQkN9WL9+aZuY1PboqaKqVqvR6XQYjUaampoURVVBwUG0tKKKhKHFyAMYeYbZ3aqL6gkoimofoKi+iIrGCtQqNSMjRrpbHIdRcriElDdSKDks32j9wv2YeO9Ehl853P2JklpSVQWPPQYHDoAgwP33w223ya9dhNWi2mddf+uywNIEmkA5RlVBQaFdCmoK+NuRv/Fz0c9otdp23TLnDZrHkzOfZESEEi/nTuLi4hg0aBAnT55kx44dLFy4kG3btgEwderUDvMRnDpVy8UXf8Lx4+UAREX5s2HDrSQnd7zg0DKe1BEWPR8fH8xms83NXEFBoWeca0UdRimfMprRRLpZsp6hOLnbSWBgoLtF6Dbb87ZT1lhGfHA8eo33JdJpLG9k/wf7aSxvBKDudB0bn9jIN7/5hpLDJWh8NEy4cwI3fXUTI64e0WUl9ciRIzzyyCMcOXLE8cJnZXHkllt45MABjuj1cn3UX/3K4UrqiRMn+Oqrrzhx4kSbz0RRpKREVub7rEW1bAc0lUHgYBCUae9csrKy+Pvf/05WVvvunT2ls/Gp4DlIksSvv/k1vxT/0u7nY6LGsPqG1Xxy7SdtlFRnjyFn09DQQFVVFXl5eU5pPz8/n//+97/k5+c7tN1Zs2YBsHnzZh599FEyMjJQq9VMmzat3f1zc6uZNet/NiU1NjaQrVt/3amSClBbW0tTUxPFxcUOOYfRo0dz4YUXdrm+a1/Hm59FFZyDEQu/ZTtXUkPxGSvqw9Szn9ler6SCoqjahVqtZvDgwV6RDbA99p7eS3ljOYkhnWfy81Ssimp1XjUpb6awetFqTm44iSAIDL9qODd9eRMT7pyA1q/r7kOSJLFy5Ur27dvHypUrHetfv3kz0u23s7K8nH2CwMpJk5BmzHBc+2eQJIl169aRk5PDunXr2pyDKIrcfffdXH/99URERDi8f6+gch80l4PfQHdL4nFIksTatWs5cuQIa9eudXiMyfnGp4LnIAgCj0x7pM322KBY3r7sbdYtXceMgW3nMGePIWcjSRLl5eUYjUa2b9/ulN/Atm3byM7OZtu2bQ5tf8SIEYSFhdHU1ITZbMZisZCcnNyue67RaGHevI85ebIKgMTEELZtu53hw89/X9BqtTZl1RHXWKPRKEmUuoi3P4sqOJ5fyGcku/iMMETUDKWMHcTyLJPd4uqrZP11E6IoUlpaSlRUlFdmWsuoyABgaPhQN0vSPUSziKHSwA/3/oBkkW+OsZNjmfrgVMKH9SxzWVpaGqmpqfj6+nLgwAF27drVbpZEf39/u+JoTCYTDXV18Omn8PHHpAMHtFp8g4JIPXGCtLQ0knuY4fdcMjMzOXbsGDqdjmPHjpGZmcnw4cNtn2s0GiZPnuzQPr2OWvk3QNDwzvfrg1jHj16vJz09nYMHDzJkyBACAgJQqVTU1dXZ9VCq0+nw8fGxxZ1ZOXHiBEePHkWj0XD06FFb+4DD+ugMV/QhSQGACq22DkGQMBigtvY8fZjNiHPmIJ3Jktj8wAOYr7vOrecREBDApUMuZXjgcI7VHMNP48e9F9zLrWNuxUfjQ31dfbt9HD16lPT09DZjyJ3n0ZU+8vLyqK+Xz+3kyZNt5tCekpeXR25uLnq9ntzcXPLy8khISHBI2yqViuHDh1NSUoIoimg0GgIC2k9qpdOpefnl+dx44+cMHRrOxo23EhtrXzkhf39/Wzxpe/cZBefj7c+iCo7lQ1byEMOwtIpFneXWWFQl66+bkCSJ4uJiIiO9x4Re3lhOeWM5DaYGjpcfB0AjaGyvI/wiiPDzXOtaY3mjzdV34xMbaShpQFAJhA0JI/m2ZAbPH4x/ZM+KyEuSxOrVq6mtrbVlH3zqqafo379/m5XeZcuWMX78+PO2efTAAV59/HGoq0MCTut0NFgsaOrr0ev1rF69mqSkJIetJEuSxKpVq6iqqiIwMBCTycS6desYNmyYslrdVC5bUZvLobHgzEYJauTfAPoI8PHc34ArkCSJb775hpqaGkC+ybz11ltERkby3HPP4ePjwzvvvENlZeV525o5cyZXXnklR44cYeXKlbb2y8rKMBgMCIKAJEm29gVB4Nlnn+1xH+fDFX1cdtmzgA8TJ76Dr28lX30FGzacp485czDn5dFkkOuRrlu1in3p6S45jyJVEXpJT5jU2u3S2seQwiE0mZuYXDGZktMlvPL9K7THzJkzueKKK/j888+prq5GpVK1GkPtzUGeds2t5VG0Wi2+vr5YLBaHzqGSJLFjxw5MJhPBwcHU1NSwY8cO4uPjHdK+KIocPHgQQRAQRRGVSsXevXu5/PLL21VmrrtuJGvW3Mi0aQOIirLvHipJEikpKUiShF6vV+4zbsIbn0UVnIcWI+F8TTDzWM4oj3DzVbL+KtjNl8e+5IP9H1BpqKSkoQS9Ws8/9v6Df+z9BwB3TriTOyfc6WYpO+bYl8fY/8F+zAYz1bnVWBeIGssbSXktBVO9iQl39qxen9WaCrLLm1qtxmAw0NTUhJ9f65qrdt2Mi4oQXnoJoa4OAENEBIaaGtRqta391NRUh1pVMzMzOX78OIIg0NDQQGRkpLLabaXwSzjxgRyb2lwOGj841uKBe8id8l8fZu3atRw8eBBJklCpVKhUKpqamjAajbZ9rNvPR8vfiHV/6+/J+l4QBFv7vr6+DunDXlzRhySpkCQVggCdHdpqPjnzWhAEu/rryXlUC9Xs1OwkR51DnCWOq0xXIbSz+t5f6I9/oz9+fn6d9iUIApmZmRQUFLSSq71r7MjzsBd7+jAYDFRXVxMSEmJTWh05h1qtqSqVisbGRnQ6nUOtqtu3b7dZg7VaLaIoUl9fz/bt25k1axZFRXX069c6rvHqq7uWBCszM5OKigrUarXNWqvcZxQUXIsRC+9Rz0UEkwwsYQlxbGOOm62ozkaQvC2YxMHU1tbaVjmDgtp3gbFYLKSlpZGUlOQ1sQHljeWU1Jdw39r7KKwtBOCVS16xJcDwFovqztd2krs5F1OjicvfvZyIEbLMfhF++EX4naeVjpEkiT//+c/s2bOHhoYGBEEgLi6OyspKJk+ezHPPPde1leKDB+HRR+UMv2FhSH//O3/+4gv27NmDj48PlZWV+Pr6IghC99rv4Bzeeustdu/ebVvFio2NpaqqiuTkZB544IG+vdrdVA6NhbD3LmiuAEEN4/4GQWce0vqwRdVkMvHdd9/x7bffYjAY8PX1JTw8HJVKRWVlpUPGj3V8Hj582BZDZ/0t9LbxuX07zJx59v22bXDecPSmJnjmmbPvr7oKLrzQGeJR0VjBa7te45PDn2AWzbbty69bzkWJF7XZf9++fRgMhg5jHa2ce42tVnNvucbOHqOSJLFixQqysrJsXjvBwcE0NTUxdOhQFi9e3KP2RVHkmWeeob6+vpVCLooiAQEBzJx5G9deu5q//e1i7ruve+Ef3n6NexPe+Cyq4BhOc5pFrOc4s5hNIqtR0Xm1Y/dQVVVFWFhYpzpVV1Gc3O1AEATbBO0tRPhFcKruFPXGeqL8owjWBzMiYoTtz5OVVJAVUX2QnpJUOauvb5gvESMibH89UVLhrDU1MDCQfv36ER4ejkajITAw0Gb1tJuvvoK775aV1OHD4eOPSVOpbO3rdPJ0Yjabu9d+B1hjC60KMMg3Mn9/f9tqd5/GJwLqs0CyyEmUtIGykhp85q+PKqllZWW88847/PLLLzQ1NREQEEBUVJTN8u+o8WMdn/7+/jaPAke27/X4+MBLL539c4KS2mRu4u3dbzPtw2n8N/W/rZRUgOe2PodFtHS7/XOvMeBV19jZY9RqTW05RwuCgK+vr82q2hNaWlPPpa6ujoce+hcNDSbuv/9Hfvyxe9mYvf0a9ya88VlUwTH4448v36Aln0soxFMrDztjbCqKqh2oVCoGDhzoVcHrkiTx6eFPAbh0yKVeObEdWXkESZSISopC4+M4L3VrbKrBYECtVqNSqdDr9RiNRpv77+rVq8/va282w9//Di+8ABYLzJ8PH36IFB3dqn1JkhBFkebmZlQqlf3tn+cc1q1bR3Nzs22FG6CxsRGVSkVzc7OSYVUSIXeF/Dr2CvfK4kGcOnWKU6dO2dwQAwMDMRqNtj9HjJ+W41OlUtlciR3VvkLnWEQLq4+uZvp/pvPi9hepN7ZVZkJ9Q1mavBSJttdg9OjRTJgwocOkPND+NXbkGHI2zh6j1thUa/uiKCJJEmaz2db+jh07ut2+KIps3LgRwHYPsP6JooQkwfjxjYDI1VcPZ+7crmf99/ZrDJCbm0tRURFms/n8O3s43vgsqtB9Uijl7TMzdDDBvMGzHGY09zLQYx19nTE2lRhVOxBFkcLCQuLi4rxmgkgtTiW9LB2dWsctY24hJiDG462oLWmua+b413LSm7G3jqW2sLbHVlQrZrOZkpISfH19aWxsbPO5r68vpaWlmM3mjjP91tTAY4/Bvn3y+3vvhdtvB0HAbDK1aV+r1dpilOxq345zqKioQK/XI0kSGo0Go9FoS1qj1+upqKjoUR9eT+lWOYmSJhAGXg/aANndtw/S3NzM9u3bmT17NuPGjaOmpob169dTWVnZbibVno6fluPT2r7FYrG5qynj03lszdvKs1ueJb2s/cRMOrWO313wO34/5fcE6dt3zWoZW7p//360Wi3Dhg3Dx8fHtr29a9wST7/Gzh6jFouFmpoa2yKoVVE1mUy29mtqarBYLGg0XX8Ua25utsndUlG0KqoAvr4CN900kk8+uQGttuuuor3hGhcUyIn0wsN7ViHAE/DGZ1GFrmPEwr3sYhXBxKBnMMEsBJJxbMUIZ6Bk/XUT1niM2NhYd4tiN1Zr6uVDL2do+FCvK01z/OvjmBpNhA4KZchlQxxqEdZqtbz00kvUnUl61B5BQUEd33izs+Ghh+D0afDzg+eeg9mzHde+neewbNkyGhoaANk9a/Xq1QwbNowbb7wRkEs0eOLDg8vIlX8DDFgE/gP7bOKkkpISPv30U0pKSjAYDFxxxRXMnj2bsWPH2sZPe/Rk/Jw7Pi0WC5mZmQwbNsymCPT58elgjpUd4/ltz/Nzzs8d7nP9qOt5bPpjxAbZdy8zGo2tFttacu41bg9PvsbOHqMajYZbb70Vw5mszmvXrqWsrIw5c+YQHx8PyAsC3VFSrcc+8MADVFVV2bZ9/30m77671/Z+/vzRLF9+I2p195Qab7/GVrdovV5vC8HxZrzxWVSha/xCPneQTzFyRvYw0pnINDdLZT9K1l8Fu8ivyWdr/lYAliQvcbM0XUc0ixz57AgASUscV8qlJeHh4d1bYd2yBf78Z2hshNhYeO01GDzYce13gZCQEEJCQti9ezeHDh3iggsu4De/+Y2SZAGg+ihUpYKggfib3C2NW5AkiX379vH1119jMpkICgpi5MiRts+t48dZtGzfYrHYHrCU8elYKhoreGHbC6w+uhpRan81e8bAGTw9+2nGRI3pUtvWxTY/P792r5uzx5CzcfYYDQwMJDBQzrgbFxeHr68v/fr1IyoqyiHtx8XFERcXB8Arr+zkT386AsjW8N//fjJvvHEpKlXP7p/efI2timpnLuwKCp5ASyuq2Kou6tQ+H6OpKKq9kM/SPkOSJGYMnEFCSIK7xekyJzeepKG0Ad8wX4Ze5iGWYEmC//4X3n1Xfj9xIvztb9BJRkxXUV1dzenTp0lMTMRsNrcqC3HkyBHMZjODBw+2PTD1CXKXy//3WwA+7q8t5mqam5v58ssvOXjwIADDhg3jpptu6ltjwNMxGOD++8++X7IE5s7tcjOCIPBD1g/tKqnDI4bz51l/5qKEi7q14Kc86DuOiy++2Gltv/LKTv74x7OFex9/fDp//es8r8xN4Uis41eZ9xQ8mXOtqEMp85i6qJ6AoqjagSAIxMTEeMWkX9NUw7eZ3wKwJMn7rKmSJHH408MAjL5xNGqdB1hfmprg//4PNpx5ELjxRli2DLrpsuUsMjMzeeaZZ5gzZw4LFiwA4MsvvyQrK4v77ruPqVOnullCF2EogmI5yQgJ3vcb6CmnT5/m008/pby8HJVKxYIFC5g9e7ZbY5q8aQ51GWYz/Pjj2ffnrWfTPmG+YTww+QFe2PaCbVt0QDR/vPCP3Dj6RjSq7s9TfelB35vH6Pz5gwgN9aGqqonnn7+IJ5+c5W6RPAKrR0BvWWjx5jGq0JaOrajeWxfVGWPTs560PRSVSkVMTIy7xbCLNcfW0GxuZlj4MCb2n+hucbpM0YEiyo+Xo9FrGHX9KHeLA8XF8PDDkJEhK6aPPQbXXutuqdrF19cXs9ncquRBcXExgNeMX4eQ+xkgQvhkCBrmbmlchiRJpKSk8N1339nqNS5ZsoSEhAR3i+ZVc6g3cscFd/Df1P9S3VTNvZPu5e6Jd+On7VnyOUmSet2Dfmd48xgdOzaGdeuWsmfPKe6/v3v1UnsbJpPJlgCqt4xfbx6jCq2RragFvc6KqmT9dRMWi4Xc3FwSEhI8Or7KaDGy6ugqAJYmL/XKVbe05XJ90aFXDMUnxOc8ezuZQ4fgj3+EykoIDZVL0Ywf716ZOiEkJITy8nIKCgqwWCwYDAbbg2afubmZ6qHwa/l1wlK3iuJqtmzZwtq1awEYOXIkN954I/7+/m6WSsZb5lCXolLB2LFn30d0nJE6tTiV13a9xmsLXms3e7uPxod/XvFPBgQPIMrfMfGPRqPRlqHWU8aRM/GmMWqxyG7eLZMkTZ4cy+TJSpIdK1ZvAB8fH49N9tRVvGmMKrSPBQt/ZgNvE4tIaK+worbEYul+Xe6OUBRVO+ksg6unsO7EOioaK4jyj+KSwZe4W5wuU51XTd5W2RqYvMTNabi/+QZefFF2zxs2DF59Ffr1c69M5yEgIIC6ujqam5spKSmxZesMDQ1tVVaiV1P4NVgaIWAQRHhPprye0NDQgL+/PxMnTiQlJYXp06czY8YMj1uo8oY51KX4+7d2/W2H/Jp8Xtz2It9kfAPA67te54V5L7S774T+ExwqnvV6+fv795mHYm8Yo0ajhaVLvyQoSM8HH1zZ42RJvZXe6rbuDWNUoWNqqWU77yHwJ4bS1CusqM5GUVR7CZIksTxNTiBz85ibexSX5C6s1tT4WfEED3R+kqJ9+/ZhMpkYMWIEoaGh8kaLBV5/HVaulN/PmwfPPAMt6gp6KoIgMGDAAE6cOEFeXp5tZSs6OtrNkrkI0Qx5n8mvE5aAhylqjkaSJLZv38769eu56667GDBgAA8//HCvsR70ZWqaanhz95t8ePBDTBaTbfsnhz/hjgvuYFDoIKfLoCRS8jyamszccMPnfP99JgAhIT688or3LUq7gr7ktq7g2ZgQ2YaKuUAoofwfD1JALUu5uFdYUZ1NX8963GvYfWo32ZXZ+Gp9uWbENe4Wp8sYqgxknrn5Ji91jTX1+++/59133+XEiRPyhtpa+P3vzyqpd98NL73kFUqqlYEDBwKQl5dni0/t5+GWYIdRvAmaSkAXBv0uc7c0TqWxsZGPPvqI7777DqPRSGpqKtC21qWCd2G0GPnnvn8y9cOpvL/v/VZKKoBZNPP+vvedLkdlZSUFBQVA77NIeSsNDUauuGKFTUn18dEwd26im6XyXHx9ffH391fGr4JbOcFJRrGNe6jFmpf7Ii7iNuYrSqqdeJ/ZzQ1YLVWe5krXkk8PfwrA1cOvJkgf5GZpuk76F+lYjBYiR0YSM9758ZSSJFFUVAScUeROnpQz+RYWyorps8/CRRc5vN/6+noaGhqcYuUMDg62Jc7Jy8vDz09OptInFFVJglz5N8DAG0Dt/cXdOyI3N5cVK1ZQXV2NRqPhiiuuYNo0z3Zz9oY51J1IksS3Gd/y4vYXya/Jb3efIH0Qf5jyB34z/jdOl+fkyZOAHDbgrTU0u4onj9GamiYuv3wFO3bIiwf+/lq+++4WLrpIUVQ7IjExkcTE3vX9ePIYVWifjfyEkSoaCcfCKHq7fVDJ+usmVCoV4eHh7hajQ05UniClMAWVoGJx0mJ3i9NlLEYL6Z+nA5C0NMklk3Bdbi6NeXkIoaFEZWbK7r2NjdC/P7z2GgwZ4vA+Dxw4wOuvv05iYiLPPvusw9v39/cnPj4egIqKCqqqqoA+kkip6iDUHgOVDgZe725pnIIoimzdupV169YhiiIREREsWbKE2FjPT6Di6XOoO0kpTOHZLc+SWpza7udatZZfj/01D059kFDfUJfI1NjYSF1dHVFRUbaazL0dTx2jFRWNLFjwKfv3ywurwcF6fvxxCdOmDXCzZAquxlPHqEJrTiCiQsUg4Df8hire4mqiGdXLlVRQsv66DYvFQlZWFkOHDvXIpBLLD8uxqRclXET/wP5ulqbrZP2YhaHSQEB0AIPmOT/2CqA4MxPKywnXaNA99phskbvgAjmzr5MsCFFRcjbO4uJiJElymEIeExNDcnIykiSh0+mIjo6mpKSktcW4t5NzxpoaewXoXPMw70rq6+tZtWoVGRkZAIwbN45Fixah1+vdLJl9ePoc6g6yS47x/H9+zXrTcXmDXt+mNvNVw6/i8RmPkxCS4DK5RFGkvr4eSZLIz89n6NChbq3B6yo8cYwWF9czf/4nHDlSCkBEhB8//bSU8eN715xeU1NDQ0MD4eHhXjOnuQNPHKMKZzFi4T528Q0xzGMQn6BCh44neMTdorkMJeuvG7HW4/I0yhvL+fGEnDlyabL3leOQJMmWRGn0zaNRaVzzQFRUXAxGIzGFhfLD4fXXwyOPtHlQdCTR0dEIgoDBYKC2tpbgYMckjBo7dixjxowhLe1MMqr4eE6fPo0oiqhUKiI6KXvRK2jIh7Jt8uuEJe6VxQmcPHmSFStWUFtbi0aj4ZprrmHSpEle5/7lqXOoO/go9SOe2vQklqqisxtVgm3+mRw7madnP80F/S5wuWwFBQWIolz+pLm5mYKCApunRm/Hk8ZoYWEt8+Z9TGZmBQD9+gWwceNtjBrV+zKE5uTkUFdXh06nUxTV8+BJY1ThLHJd1PwzdVGNGMinkQS8LxDP8+jyU3lubi7ffPMNO3bsID09nfLycgRBICIigpEjRzJ9+nSuuuqqXhcb4KmsProas2gmOTqZpOgkd4vTZQp3FVJ1sgqtn5aR1450bmfl5fKfKFL073+DxUKMKMJvfgOXXALV1Z3WMuwpWq2W8PBwysvLKSoqcpiiWlFRQXl5OdXV1QBMnz6diRMn0r9/f5qamtA4Ufn2CHKXAxJEzgT/3vdAXVtbS21tLVFRUSxdurRvuHL3cibHTkaUxDbbE0MT+fOsP7Ng8AK3LESIong2udwZTpw4wYABA/qEVdWT0OvVqNXyGBg4MJhNm25jyJAwN0vlHHx9famrq8NgMLhbFAWFLmG1oq4kGJGwFnVRR/YBR1/XYPf3+P333zNnzhyGDBnCsmXLSE1NJS4ujosuuojZs2fTv39/UlNTWbZsGUOGDGH27Nl8//33zpS9z2MwGfgi/QvAO62pAIc/PQzAiGtGoAtwcgKcL7+EpUvhxhspKSwEoJ8gwMcfy9u//NK5/XPWDdeakdcR7Nu3jw8//JBjx47Z+khISECn0xEU1MvX84zVcOo7+XWid/4G2qO2tpZvv/0Ws9nMuHHjuOmmm/j973+vKKm9hJGRI7lx+LW292HNKl4Ivo5ffvULlw651G3W8oKCgjYWm6amJlsGYAXXERnpz8aNt3HppUPYtu32XqukArY634q1UMGb+IV8RrKLFYQhomYoZewglmeZrGT0dSB2mVqmTp3KoUOHuPrqq1m9ejUXX3xxhw/AtbW1bNiwgS+++IIbb7yRsWPHsmvXLocK7WpUKhWDBg3yuBXl7zO/p7a5ltigWOYkzHG3OF2mIrOCU3tOIagExtwyxvkdXncdzJoFGzdS9MEHIEnE3HUXXHyx/LkLXGRjYmJIS0tzqKJqJSwszDZGv/76a44fP86SJUsYMKAXJ90oWAOiEYJGQKjr3SSdQWZmJitXrqS+vh61Ws3ll1/OhAkT3C1Wj/DUOdTZVBmqOkyA9Oicp1if/zO3jb2N+ybdR6DevWU0rNZUSZJs2wRBQJKkPmFV9cQx2r9/ID/+2PvCGc5FUVTtwxPHaF+kfStqM88wq88rqG5LpnTRRRfxzTff2FVSIygoiEWLFrFo0SKKi4t58803eyykuxEEweMsU6IksuLICgAWj1mMSvC+ievwctmaOujiQQT2c8FDWkQEREQgrllDqUYDFgv9JkyAESOc3/cZrBZVa6IjR+Lj42OzxFRWVlJZWUleXl7vVVQtRshbJb9OWApeFrN5LhaLhQ0bNvDzzz8jSRL9+vVj0qRJ7hbLIXjiHOpMcurT+ccXz1JQW8Avv/oFrbptfdt+gf3Yf+d+fLWekVXXak21KqdwttSA1aram2NV3T1Gd+8u5IUXtvHZZ4vw9++95bXaQ1FU7cPdY1QBtlLA7eSdiUWFoZSxnFGMpvfFjncHZ3gD2aXdvPjii92q+xgTE8OLL77Y5eM8DYvFQlpamlOyWXWXbXnbKKgpIFAfyJXDr3S3OF2moayB7PXZACQtcW1sbd3RowRIElqVinAHxYnaizMV1aKiItsYtT5Q5uXlObwfj6FoHRgrQR8FMRe7W5oeUVNTwwcffMDmzZuRJIkpU6Zw//332zJFezueOIc6hYAiuPRBlqXOZ2veVnKqcmw1rtvDU5TU9qypgO291apqTbLUG3HnGN2yJZeLL/6E777L5JprVtHUZHa5DO7EWgKpqampV4+xntJn5lEPxIKFu9jG5VRTfMaK+jB1HGCWoqS2wKuy/ubk5PSqhEqeNjFYH34WjVyEn9bPzdJ0naOrjiKaRWLGxRA12oUP40YjwSdP8pYkYbrpJlQuVgSsMYalpaVYLBaHpphv+ZDZ6xVVSYLcMwpA/M2g8t6EUceOHWPVqlU0Njai1+tZtGgR48aNc7dYDsfT5lBH0miugxn/gAkfgKaZlvreq7teZdGoRQTpPdcSIooiJpOpzWq4tYyWIAiYTCZbJvHeijvG6Lp1J7j22rPKqcUiYjb3LWVNq9WiUqkQRZHm5uY+U7u3O/TmedSTWcUqfuQUIpczlFKWM1pRUF2Ew5/uDh8+zEsvvcQXX3yB0Wh0dPMKQHpZOgeLD6JWqblx9I3uFqfLmBpNHFsjJ/5JXprs2s5PnACzGcLD0T7+uMvdRcPCwtBqtZhMJsrLy7vlqWAP1niy6upqampqHJZh2GMoT4H6k6D2gwHXnn9/D8RsNrNu3Tq2bt0KQFxcHIsXL+795YR6ESaLiU8Pf8pz+16FKZXt7qNVa8mpymFszFgXS2c/Go2GCy+80JZ11WyWlSa1Wm1TXv38/Hp/BnEX8/XXx7nxxs8xmWTFdOHCoXzxxQ34+rZ1Fe/NCIKAj48PjY2NNDU1KYqqgkdgAWqAMGARi/iRexnKDJ5idp+PRXUlXbrrHD16lPfee4/s7GxCQ0O54YYbuPZa+SHxwIEDPPXUU6xfvx6tVsvSpb0nA6enYbWmLhi8gCh/73MNzPw+k+a6ZoIHBBM/y8UxT2cy4zJqlFtiGlUqFcuWLSMkJITw8HCn9aPX64mJieH06dPk5eWRnOziBQFnY7Wmxl0DWvcmoeku69evtymp06dPZ+HChWi1fesB1VuRJIl1J9bxwrYXOFl1EqOp7T7+On/un3Q/d064s30XX7MZtm8/+37oUIiNdZ7Q5yEoKEiJf3Mhn32Wxq23foXFIpvfFy0ayYoVi9DpHOdl401YFVWDwUBoaPsJyBQUXMVO8rmfGsYwhv8ioEfPx/wbQVFQXY7dimpKSgpz585tFey+atUqXnvtNcxmM4899hiBgYH88Y9/5A9/+IMtFq83oFKpGD58uEe4PBXVFbHx5EbAO0vSSKJE2vI0AMYsHoOgcvGPPj1d/n+kk2u2dsKYMc7JcBwZGdlqjMbHx/dORbU2Cyp2AyrZ7dfLqKysJCwsjNmzZ5ORkcEll1zitDHhKXjSHNpT9p/ez7Nbn2Xvqb3t7yCqubTfUl6/YRmR/p24hhkMsHjx2fcvvAC33+5YYRXsxpVj9MMPD/C7331ncxFfujSZ//73ajQa7/99dBdfX19UKpXi2toJvWke9WSaaeZPPEIWj9FEKTlEMwQUJdUO3Jb1F+DZZ5/Fx8eHr776ipkzZ5KTk8Ptt9/O008/jcFgYNmyZTz55JO9z8XwDDqdZ2ThW3lkJaIkMjl2MsPCh7lbnC6T+0sutadq0QfpGXaFG+S3WlRHj3Z9307m3HjX+Ph4du3aRX5+vpskchK5y+X/Y+aBX3/3ytIFTCYTP/zwA3v27OGee+5hwIABPPjgg33mocNT5tDukludy4vbXuS7zO863unEAtj2JHd/M4RIf9fJpuAYnDlGs7OzaWxsZMOGCh5+eItt+113TeDddy9H5epFWw8jISGBxMREt9UQ9ha8fR71ZMqBCECPnodYwnJ+4S/cxhB3C9bHsfsJaffu3dx3330sWLAAPz8/Ro8ezWuvvUZdXR0PPPAAf//733utkiqKImlpaW7PRldvrOer418B3mlNBTj8qVySZtT1o9C6Og6nuVmOUQW3WlSdRXFxMaIo8q9//Ys333zTprieOnUKk6kd30RvpKlMzvYLkOA99QXLy8t555132LlzJ2azmZMnTwLOWX30RDxlDu0ukiRx+ze3d6ikDg0cD6u+hG/+C5XKY4034uwxum/fPjZu3ERKSoZt27JlU3nvPUVJBXkuVJTUzvH2edRTMWLhd2znIurYcWbbVVzFKpYpCZO6iDPGpt0W1erqaoYNa20Bs76fO3euY6VSaJevj39No6mRxNBEpsZNdbc4XaYkrYSSwyWotWpG3+gGi2ZGBogihIdDZO+ZfGbMmMHYsWPJyckB4NChQ9TU1HDFFVcQEBBAfX09p06dIiEhwb2COoL8z0EyQ8hYCPEOd9nU1FTWrFlDc3Mzfn5+3HzzzYxwYe1ehZ4jCAKPTHuE3333u1bbBwYP5E8z/0Ro6ZXMKuziQ7afH6xff/Z9f+/xDlDoOv3798fX15drrhlPRcXPzJgxgGeemaMoZwoKbmQL+fyGfIoJQ0Mx69EzHZ3i5utB2K2oSpLUxrXQ+t5arFnBeZhFM58d+QyAJUlLUAneZ4mxxqYOvnQwfhFuKKnj5kRKzsLf3x8fHx+Ki4sxGAzU1NQA8oPRJZdcglarJbI3KOZmAxR8Ib9O9HyPApPJxLfffsvu3bsBSExMZPHixb3W86S3s3DoQib2n8i+0/sI9gnmwSkPcvv429GpdWwv60aDajUkubaGtIL7mDlzpu31unVL0Gr7ZtIkBQVPwIiF+9jFSoIRz9RFvZ96nlEcfT2OLmX9Xbt2LcXFxbb3jY2NCILA559/Tmpqaqt9BUHgoYcecoiQCrDp5CZK6ksI8w1j4dCF7hany9SdriNns2zxS17ipsQ+HpBIyRns3r2bffv2ERwcjL+/HBgXHByMr68vU6d6n+W9Q059B6Za8IuDqNnulqZTSkpKWL58OcXFxQiCwNy5c7n44osdWjdXwfHsLNhJhF9Eu/H/giDwl9l/4YesH/jDlD8Q7KMsOCh0jsUi8vTTP3PnnROIjw+xbVeUVAUF99HSigowlDKWM0px8/VQuqSorlixghUrVrTZ/s9//rPNtt6kqKpUKpKSktwWTyZJEsvT5AQyN4y6AZ3a+4Lp0z5LQxIl4qbGETYkzD1CtLSo9iKqq6vJy8tj2rRplJaWAtiybhsMBrZv305paSmLFy/2XjczSYS8M3NP/GLwYI+C1NRUWx3pgIAAbrnlFoYOHepusdyKu+fQ85FZkckL215gQ/YGZifM5rNFn7W734T+E5jQf4KLpVNwBY4eo2azyK9//TXLl6exenU6W7f+mn79vLOUloJn4OnzqKfTvhXVyLPMUuqiOgi3Zv21xr/1VYxGo9tcnA8WHyS9LB2dWsf1o653iww9obmumYxv5AQSSUvc5OrW2Ag5ObwJGDZu5Oa4uN4Rs9kCi8Vi83iIjo4GZPf8TZs2IYoiCxcu9N76dKVbobEQtEEQe6W7pTkvRqORoUOHcvPNNxMYqDycgnvn0I4obSjllZ2vsCJtBaIkJ4HYkruFLblbmJ3g2VZ7BcfjqDHa3GzmllvW8NVXxwHIyali377TXHnl8B63rdAW6UydH69diO0CnjiPegNbKeB28mxW1CGUsUKxonoFdiuq8fHxzpTDoxFFkYyMDJKSktziuvfp4U8BuGLYFYT6ep+icfyr45gaTYQNDiNuapx7hMjIAEnimFZLQ3Z2r7yhlZWVYTabAYiJiQHkVPb9+/ensLCQvLw871VUc+XfAAMWgcbXvbK0Q1FREVu2bOH6669n3Lhx6HQ6RowYoax8n8Hdc+i5NJoaeW/ve7y37z0aTY1tPn9267P8NPAn1ConyiqKcCaeHJCTK+n1zutPoVMcNUYNBhPXXbeadevkDPM6nZrVq69XlFQn0tTUxMGDBwkODmbUqFG98v4OnjePegPtWVHvo5nnFCuqU3Br1l+Qy1989NFH5OTkEB4ezqJFi7jgggscLpTCWfJr8tmWvw2AxUmLz7O35yGaRY6sPALI1lS33UCOHaMeaDjzIGi1OLqLqqoqW6mSq6++2mHtlpSUAHIiJSvx8fE2RXXcuHEO68tlVB+FqlQQNDDwRndL0wpJktizZw/ffPMNZrOZ8PBw5s+fz6he5l7eW7CIFlYdXcXLO1+mpL6k3X0i/CL41dhfOV+YhobW9ZxfeAFuv935/So4jbq6Zq66aiW//JILgK+vhq+/vplLLhnsXsF6OXV1dVgsFkwmU69VUhW6joTE73iCL7kFEbViRfVSuuT6O3nyZCorK21uFn/729/4+OOPWbzY+xQob2H54eVIksTMgTNJCElwtzhdJntDNg2lDfiF+zHkUjdmU0tPpwjA15fQ0FC3u87U1dWxcuVK/Pz8uOqqqxxyc5Ukyeb6a7Wogqyo7tixg7y8vB734Ras1tR+l4KP59xgmpqaWLNmDYcOHQJg+PDhTJs2rcvtWEQLu0/tprShlCj/KKbETnGuJa+X0973qRJUbM7ZzPPbniejPKPd43w0Ptwz8R7umXQPAboA+zo7dQoqK/HLhpbFkvyygWAgLAxiY9s/rqAAWtY3LiyENDkzeofHKXgsVVUGFi5cQUpKIQCBgTp++GExM2f2XW80V1FfXw+ghFkoACABAiAgcDkT2c/nXM3VihXVS7FbUX3mmWeoq6vjzTffZO7cuZw4cYI//OEPLFu2jJtvvrnXu7i5w82iuqma77O+B2BpsueX4zgXSZJI+1R+8Bp902jUOjc+fKenUwzg42NLNOROrIpkY2Mj9fX1DrnBmkwmmpqaUKlUREVF2bZb3fZPnz6N0WhEp/OiZFyNp6F4k/w6YYl7ZWnBqVOnWL58OeXl5ahUKi699FJmzZrV5XlwbdZantz0JNlV2VgkC2pBzeDQwbww7wWvzO7dGa6YQ9v7PvsH9ifSL5Lsqux2jxEEgZtG38Sj0x8lJiCm3X3a5dQpmDwZ6utJNsP2Fh/534t8dw0IgD17Wiud1uPq6uDMAzYAf/0rvPKK/Lq94xScTnfHaFlZA5dc8impqfJCYWioD+vWLWXyZOX6uYK6ujoAAgLsXGDyYhSX387ZSgGvEcCThDIJuJ7rmcc8wgl3t2gK3cRuRXX79u3cdddd3H///QCMGjUKjUbDlVdeybFjxxjd0oWpl6FWq0lyQ727NelraDY3MyJiBBf08z4X66L9RZRnlKPRaxi5yI0lYerrIT/fZlH1BEVVp9MRHh5ORUUFRUVFDlFUo6OjufTSSzEYDGg0Z3/aISEhBAUFUVtbS2FhIYMGDepxXy4jbyUgQvgUCHJ/5lxJkti1axfff/89ZrOZkJAQFi9e3K3EXGuz1rL0y6U0W5rx0/ihVqmxiBYyKzNZ+uVSPr3u016jrLpiDj33+1QJKmqNtWRWZJJZkUmITwg+mtaeFBclXsRTM59iZGQ35qfKSnluUakQtRqaW3zkqwUEs/x5ZWVrhdN6nFotK6RWNBp5m7mD4xScSk/G6CefHLYpqVFR/mzYcCvJye4NL+krSJJEQ0MD0PsVVXc9i3oLu9jFrRyghkvQEcgqNKhQKUqqC3HGQordimpBQUGbeNQLLrgASZIoLy93uGCehCRJ1NXVERgY6LL4B6PFyKqjqwDZmuqNcReHPz0MwLArh+ET7EZX2+Ny5sUif39Qq1u5xbqTmJgYm6I6bFjbuo1dRRAElixZ0masCIJAfHw8aWlp5Ofne4+iaqqHwq/l1x5gTW1sbOSLL77gyBE55nr06NHccMMN+Pn5dbkti2jhyU1P0mxpxl/jj8FswCTKbqCSJNFkbuKWNbcwa+CsDn/70f7R/Ouqf7X72ZfHvuR/qf/rslwgu8CuvmF1u5+ty9zIXze9RVfzJUjIWalfGLsGvbrtXHCsdi8f5TzXDWllHhnxT/6Y+iQGYzN+6iAki4AZEaPZhHRGgtrmOgSLHkEQSPAfxa8Tn2Zc6CwqMmB7+97AneKXDclmELUaTKIaDUZUiJjRIGl0su+Z0Xj2gO3bITNTdvE1mcDfH3w7SAzW8jgFl9CT+/xDD00lJ6eKr746zsaNtzFiRISTpFQ4l8bGRiwWC2q1ultzsTfhjmdRb8Dq6ptMMqN4hSoSeI4IBLw0eaQXYw0NdSR2K6pmsxmtVttqm/W9xWJxrFQehiiKnDx50qWZ1tadWEeloZIo/yguHnSxS/p0JFU5VeRvz0cQBJIWu3kFMD0dgOIzq62eoqj269ePo0eP2uJKe0plZSWiKLY7Rq2Kam5urkP6cgmFX4GlEQIGQUTXYz8dzdq1azly5AhqtZrLL7+c6dOnd/thYfep3WRXZaMSVFQ2VSLRenKXJIm65jp2Fe7qsG7ygOABHbZfXF/MvtP7uiWbn7b9hz2TCe5ZVkHuiO61C3DxwxKY2/lgUDVc2/12d7y9G67LBrMfTaL1mqhA6w862b3WIlioPB0EvzxHRfoi9ks9m8vHILv7NgPBVOFz5sSa8AHauWZffw0rVshfpNEoK6oKHkNP7vOCIPDmm5fx5JOziInp3VY9T8Man+rv79/rlTd3PIt6MkYsPMFRjCTxJgL++PMx7xNBBIISi+oW3J71d9++fa2S0NTV1SEIAtu3b6e6urrN/tddd12PBeyLSJJkK0lz85ib0ai6dJk8giOfyVangbMGEjww2L3CpKcjAiVnbmKeoqha5eipohoTE0NycnKnsacDBw4EID8/H0mSPP+GLpoh9zP5dcJScJO8oihSVFREbGwsl156KWVlZVxxxRUMGNCxkmgPpQ2lmEUzRouxjZLaqn/J8ZN+dzl0CHJzgRHulqQd/MpBsIB4zsObyQ+0jSCIYNHAz89BumMzR6sQ0bTQvjWY3TVcFVzEkSOl1NQ0MX36QNs2lUpQlFQ3YI1PVRIp9S1a1kXtRzU3EMosIFLJ6Nvr6JIG9MYbb/DGG2+02f7MM8+02SYIQq+3tDqLlMIUTladxE/rx7UjrnW3OF3GUGUg8/tMAJKXJrtZGiA9nUrApNOhVquJjPSMicyqqBYVFfWonbFjxzJmzBjSrBlD2yE2Npbhw4czYMCAdr0jPI7iTdBcCrowOduvG2hoaGD16tVkZmZy7733MmDAAO655x6HtB3lH4VGpUGr1lJvrO9wP5XgOUnqmprcLUEnNEaApAaVBcSW35kATSGACBoT1DmnjrMRHQISWkyoEVFrAdN5D1PwQvbvP82CBZ9iNFrYvPlXTJzY//wHKTgNq0W1t8enKsi0Vxd1KSeZyQR3i6bgJOxWVH/++WdnyuHxuLKcidWaes2IawjUe98qYfrn6ViMFiJHRRIzzs3Wy5oaOH0aDXDt9dfT2IFrrDuw1jotLS1FFMVuZ86uqKigvLwcg8HQ4T5arZY77rijW+27HEk6W5Im/ibowPXVmeTk5LBixQpqamrQaDRUVFT02IrakimxUxgcOpjMykx8Nb4YzAZ8Nb5o1VpbjGpMQAzPXfRch8pqZyVU5iTMIVjfPU+GTj04Tk+En14G4Fe/hoEDO961JaIoUllZyaKfVOjbWSMpaxrFweqXuy7sGaZdfhEPHBhMYWMmfuqgVh4DkqSh0VLLAL/h/Gv5FNQOsnb6ZcvZfUWtCkkTigBYkNBqO3A6e+EF+L//gyNHYP58xwih4FDOd5/fsSOfhQtXUFsrp876859/5scf3R8/31cRRdGWSKmvWFTdXVrPnbS0ogIMpYzlSl3UXo/dimpiYiKRkZH4dpT8oRejVqsZMcI1/m5ZFVnsPrUblaDi5jE3u6RPR2JuNpP+uRwTmrw02f0upseOARAycCDXeVi93/DwcDQaDSaTiYqKim5bevft28emTZuYPn16p0p4XV0dWVlZqFQqxo0b102pXUDVQag9Bio9DFjk0q5FUeSXX37hp59+QhRFIiIiWLp0qW1RwVGoVWpemPeCLUttoC4QH40PFtFCo6URP60f717+brez/o6KHMWoyFEOlRmAqsHyH/DbyTBjhqMajuVaevbA/2ac/H02WWrxU5/NomwwN+Kr0/PGVc8ze6gDF6mCke+ggplWmqnVC9h8TjCuXi//+fnJruznfk4Hx7kBk8mEJEloNJpeX3rOyvnu85s353DllZ/R2CibymfOHMiqVde7SjyFDhg5ciT19fV9QoFz5bOoJ9GeFfV+jDyr1EX1OJxhCLL7DpSYmMhXX33lcAG8AVEUqaiocEqQ8LksT1sOwNzEufQP9D6Xoqy1WRiqDATEBJA4L9Hd4tgSKTHSjeVxOkClUpGYmMjgwYNpcoBfZUNDQ6djNCcnh5UrV/LLL7/0uC+nknPGmhp7BehCXNZtXV0dH374IevWrUMURS644AL+8Ic/9FhJ3Vmwk+e2tM1ou3DoQj697lOGhQ1DlETqjHU0W5oZHj68V5WmAdfMoS2/z2ZLs/O/z7AwubyMKMrJkc79E0X587AwxxznQg4dOsTu3btt8X99gc7G6A8/ZLJw4XKbknrJJYNZt24pQUF6V4up0AKVSkVYWBgDBw50/6K4C3Dls6insJUCRrKLFYQhomYIZewglueZrCipHohbkyk5I+WwtyBJEgUFBYSEhDi1n7KGMtadWAfIJWm8DUmUOLJCTqI05pYxqNQesBJ/xqLKKCdYlxzA008/7bC2ampqOv2dxsfHA3LypubmZvR6D3zIasiDsq2A4NKSNCdOnOCzzz6jrq4OrVbLNddcw8SJE3v08GMWzbyR8gavp7yOJEmMiBjBDaNvaLXPwqELWTB4AbtP7aa0oZQo/yimxE5BrfIM93RH4ao51KXfZ2ws7Nkj1zvtiLCwtrVQu3ucglPpaIx+8UU6ixevwWSSH8Cuumo4q1dfj17vfUkOFbwbV82j7qKccr7kS67jOoIIbWNFvY9mnlOsqB6NW8vTKDifz9M/xyyaGRs9ljFRY9wtTpcp2FVAVU4VOn8dI67xEPcUD7aouprg4GBCQkKorq4mPz+foUOHuluktuSukP+PnAn+dgZA9pBffvmFH3/8EUmSiI6OZunSpURHR/eozaK6Iu5dey+7C3fbtj2x6Qkm9J/AoNDWdWzVKjUXDriwR/0pnMWl32dsbPcUyu4ep+BSPv74ELff/g2iKD983XTTaD755Fq02t61kORp1NXVUVdXR0BAAEFBQe4WR8FFlFPOB3zACEbwOFkcQi5LN4QyViixqH2WLpm8+oJrhbswmAx8kf4F4J3WVIDDnx4GYMS1I9D5uz4BThsqK6GkRI4H6+VxHUajEZPp/GlGrVbV/Px8Z4vUdYzVcOo7+XWi634DISEhSJLEpEmT+P3vf99jJXVD9gbmfTyvlZIK0Ghq5KPUj3rUtoIHUl0Nzz9/9u/oUXdLpOAAsrMr+c1vziqpt98+juXLr1OUVBdQUlJCdnY2FRUV7hZFwQ344Yc/P+BPCQ9Sx0FmKUpqH6ZLFtUHH3yQJ5980q59BUEgOzu7W0J5Is7OKPdd5nfUNtcSFxTH7ITZTu3LGVRkVnB672kElcCYmz3EGmx1+01IkBOY9GJKSko4deoUl112WaeW0vj4eA4dOkReXp4LpbOT/C9ANELQSAgd79SuMjIy2LlzJ7feeivjxo0jNDTUpsR3F6PFyPNbn+ffB/7d5jONSsPjMx7n7ol396gPb6bXZuWsq4N33z37fswYGD3affIodJuWY3Tw4DDef/8Kfve777j//km8+eZlqFTKYr0rsCbt7CyTfV/F0+dRM11TLMop5xSn+IbD5JKEBBRSyP38ijB0jGSw4urbx+mSohobG0usk92V3nnnHV5++WWKi4sZO3Ysb7/9NpMnT+5w/+rqap588km+/PJLKisriY+P54033mDhQsclzlCr1QwePNhh7Z1LaUMpf932VwAWJy32qNqJ9tBY3sj6ZesRzSJDFw4lwFOKnlstG73c7ddkMmEymWhoaKC6urrTfVtaVLtSEicrK4uvvvqKa6+91jkuw42n4fhroNJBwlLZCu5ArPJfffXVnDx50lZua8eOHcyePbvHSmpOVQ53/3A3aSVta9kOCB7AuwvfZUL/vlvnzdlzqIL7yc/PZ9OmTcybN4+B9tYt8iBOnTrF1q1b0Wq1Nvl/+9sLGDkyggsvHNBjj7ItW7Zw4MAB5s+fz5gxHrKY66FYM/g6Islgb8KT51ET8B9gC/A/wF6fujWs4XGeo5YVaFETx0ye53nb53ee+afgHTgj62+XFNVHHnmExU4s8bFq1SqWLVvG+++/z5QpU3jjjTdYsGABGRkZREVFtdnfaDQyf/58oqKi+OKLL4iNjSUvL8/hgeaiKFJaWkpUVJRTUvX/mPUjWZVZjIkcw5XDrnR4+86mPKOc3J9zCUkMIXlpsrvFOYvVotrLrRs1NTWYzWaMRiPffPNNp0mA+vfvj1arpbGxkfLy8nZ/V+ciSRJr167lyJEj6PV6HnjgAceHARR+JSdSCpsIMfMc2rRV/rS0NPLy8vDz80MQBKZNm8aFF/Y8lnFN+hoe3/Q4DcaGNp9dMewKXrnkFYL0fTvOytlzqFtRqaBlHJ2m76V+kCSJbdu2kZ2djU6nY/HixV4VKmSVPyMjs43806f3XOmWJIkdO3ZQVVXFjh07GD16tFd9P66mpaIqSZLyXZ3BU+fRDOAZIOvM+6+AS4BA2lcysskmkURUqFjEIsop51N2EMRcmjjCUzzFCORwrQgiXHAGCo7CrVl/XcFrr73G7373O26//XYA3n//fX744Qf+85//8Pjjj7fZ/z//+Q+VlZXs3LkTrVauIp+QkOBwuSRJori4uNt1Ls/Hd5lyXN4lgy/BV+t9dWqzfpSnp8hRkUSO8pA4AknqM4mUcnJybNbR1NRU0tLSSE5uf8FArVYTFxdHTk4OeXl5dimqGRkZpKWloVarOXz4MD/++CMDBgxgwIABhISEcPr0abtiifR6PcOGDcNsNnPMuogAIJrh8FqoDgP/mXD07GeO6KOgoIDU1FRbvVpfX19uvfXWDr8je2kwNvDk5idZfXR1Wzk0ep6d8yxLk5cqD1k4fw51K7GxcPy4u6VwK3l5eeTm5qLX68nNzSUvL88p92JnkZuby/796TQ2mjlyJNOh8hcUFJCbm0tdXR2CIFBUVOR134+rsSqqFosFs9lse77r63jaPGq1ov4HsAAhwOPAW8DLwEeA1UwgIpJCCp/yKbvZy3w+YgmjSCKCR3iEx9CSRSa3UsWIM/8UvI9enfXXaDSyf/9+nnjiCds2lUrFxRdfzK5du9o95ttvv2XatGncd999fPPNN0RGRrJ48WIee+yxDs3Pzc3NNDc3297X1tYC8oRosVgAOb5WpVIhiiKSJGGxWJAkCVEUUavVtv2sWPc/d7tKpUIQhHa3lzeWU1pfSkFtAanFqQAMDh3MsbJjiKJIhF8EEX7ySpJarbb13xK1Wm2T8Xzbzz2n88l+vnOqK6mjoayBk+tPkr5aVgijk6MpOVoCgF+EH34RfrZVv/Zkd+o5lZcjVVSAIHBCrSa4tJSwsDA0Gk2XrlN7srvtnNq5HoBNIdNqtRiNRlatWsWYMWM6vH7x8fHk5OSQk5PDBRdccN5zXb16NXV1dTa5Vq9eTWRkJLfccgtjx45lz5497Ny5s41c5xIREcHDDz+MwWDgk48/AlMNmGrB0giiBNIIOFEMmn+dOWkNN92yhHHjxtndR3h4OI888ojcxyefIEkSZWVlGAwG2zlGRUUxZsyYNt9NV67TsfJj3LP2HrIr28bhDw0fyjuXvcPIiJG277M7cwR44tg7O6/Kfdl3TtY51DqfetY5cd7t3nedenZO1msliqLt887OdceOHZhMJoKCgqitrWXHjh3Ex8fb2vGEc+pIdrPZwquvrkKjacZkgurqRjZs+IU77rjNLtnPd07btm3j2LFjWCwWm8w7duwgLi6u1SKWMvbOyg5n72cNDQ0EBwd7/Tk58jq17MNd55QBPCNJnDgzhi+SJB4XBMKAN8/sZxFFGmlmnXodK6QV5JJLM/0o4i98RiQnBPhUFNFKGkBCxCKneBXoFdfpfNt74zn1aotqeXk5FoulTcbN6OhojnewWn3y5Ek2b97MkiVLWLt2LSdOnODee+/FZDLxl7/8pd1jXnzxRf7v//6vzfajR48SECDHVloLSBcWFlJZWYkkSVRWVlJWVkb//v1tq6NWBgwYQHh4OFlZWa1iKgYNGkRQUBDp6emtBtDw4cNZk76Gt3a8Ra2plormCnzVvvxj7z94a/dbNDU1sShhEYsSFqFWq0lKSqKuro6TJ0/a2vDx8WHEiBFUVVVRUFBg2x4YGMjgwYMpLS2luLjYtv3cc7ISExNDTExMl89p4+sbyfh3BuYGMwD6ID3HvjrGgRUHAEhYlEDCogSSkpIwGo1kZGTY2nDJOWVk0NzcjCE2luf+9jcaGhp46qmnGD9+fJeuk06nIy2tddyh286pnetUV1dHcXExTU1N+Pj44Ovry969e9m9ezfTpk1r95wGDBiARqOhtrbWdm4dnZNOp7OdjyAIqNVqjEYjYWFhiKJIWloa9fX1BAYGotVqCQoKwmAw0NjYaGtHr9cTEBCASjKSt/NttNV7iVOVotI1o9I0I09xAqKgRxJyUKnVqASBRu1AiouLbX30798fnU5n+01aCQ4ORq1WU1lZiVarJS0tDaPRSHx8PFVVVRQWFqJWq/Hx8UGr1XLy5ElSU1NbPRR15TqJksgDex/gZN1JLBZLq4l50fBFvHbFaxTlF5FWdHbcdGeO8LSxJ+97NkeBPA6D7Ton68OVKIqkWz0dPOCcHDGXe9p1csQ5gZzIJisrC51O1+k5BQYGcvLkSQRBoKmpCUEQyMrKIi8vj8DAQI85p/au04gRo7jjjk8ICqpAowGdTiAw0IfS0lMcO3asVSb17l4na0Z2tVptm4dyc3PZvXs3/v7+Dj8nbx971nMyGAwYjUaOHz/OpEmTesU59fQ6VVdXU1lZydGjRxEEwS3nZAJSRo3iE62W+qYmAi0Wbi8pYWp9PSFJSTQZjTyXkUG1uppVIRvYFLoJc4AZsyhSbboCk/AbwsQwQiQNt/lBfVUVhWeuU5W6imujryUiKsKrr1NvHHv2npPGCaEvgmSnnTYvL4/IyEj8nJQ99fTp08TGxrJz506mTZtm2/7oo4+yZcsWdu/e3eaYYcOG0dTURE5Ojs2C+tprr/Hyyy9TVFTUbj/tWVQHDBhAZWWlrV7Xuascoihy6tQp4uLi2rXI9cSi+vaet9mYsxGTxcQbl77BiIgRHm9RLUsvY/2y9dSdqkNQCSTMTaBgewEzn5xJ2LAwwAMsqv/8J9KHH1J/6aXck5sLwAcffICfn5/HrkZZLfb2XidJknjmmWfYunUrFosFf39/IiMjKSsrY/LkyTz33HPtyt6elaO9c5IkiXfeeYf9+/djMpnw8/MjLCyMyspKkpOTuf/++zmXNufaVIpQtgVV6S9IlQdAaiGPPhIh/AIsIZMAM6r0lxBH/QkheAQqQYVFGwr6s/EpXb1OgiDw9ttvc/jwYcLCwhAEwbbolJyczH333dfKotGVsZdels4VK6/AZJEfZAN0Abw07yWuGXGN166Enm+O2LLFwpw5Zy2qW7aIzJpl3zmJosjp06eJi4vjXJQVa887p8rKSsxmM8HBwTa3y47miFWrVpGVlUVQUJDtN1ZbW8vQoUO5+eabORdPuU5NTWZuuWUNZvNh4uPNmM0QHu5HVFSITf6bbrqp23OEIAgIgsCKFSs6/H5atq+MvdayZ2VlUVpaysCBA23J7rz9nOzZ3tk5mc1mCgsLiY2NtR3vynPKAJ4VhFZW1EclibAW+2eRxXJpOeuF9ZiRDRkhwjiM0hPUkYAaFdMkiT9JEv166XXqy+dUU1NDeHg4NTU1DquBbJfq+9lnn3HzzTd3OdZKkiRWrlzJLbfcct59IyIiUKvVlJSUtNpeUlJCTExMu8f069cPrVbbys135MiRFBcXYzQabSvBLdHr9ej1+jbbraudLWk5EbSMJ+nIrbgr2yP9I4n0j6S4oRgfjQ9alZaRkSMZEdG+X77VmnUuLa1CPdluj+ySJJG2Io09b+1BtIiEDgrl4pcuRlAJFGwvIHJkJBEj2g98b699p57TsWMIQGlsLEJeHqGhobZFFkdcv462d+ecysrKeOWVVzAYDLz11lud7t+y7cOHD5OammrzBLAquYGBgZ3GqgqCgMlk4tSpUwQGBhIeHt5u+xkZGRw7dgy9Xo/ZbLYd6+/vz7Fjxzhx4oTN+tLqnAyFULIZSn6GmrM1JQWAoGEQfRFEz4WAQSAIsjNpzXEQQB0yCoLl30BHuePsvR5W+f39/Vs9DFrlz87ObiO/vWMsKSaJZ2Y/w5Obn2RszFjeu/w9EkISuiyjI7e7eo5oOT+eb3+1Wn3ezMqecE6u2O4N59RyTuhs/9zcXHJzc/H19bXJIwgCvr6+5ObmUlBQ0G4spruvU0ODkWuvXc2RI1lcfrmF5mYV8fFhBAfLsZFW+QsLC9vI3xXZz/f9tNd+Xx971u3WpHdGo9E2f3v7Odm7vaNz0mg0bvk9iWp1m1jUx4D5Z65Ly/jTPezBWk0mibFE8zBbGIlJEP6fvfMOj6Jc+/A9W7LpPSEBQui9gyBKkSICIgqoCIIFFcXPyvHYewM9iu1YjiKKFAuKgkqRpggKSO8lgVAC6b1tdnfm+2Oym4QkkLJlkrw3Vy52pz67++6785unEQTMAq6TJEfDmYb4OTXm1+QKj2q1yoY98sgjtG/fnjfffJOTJ09ecvu4uDhef/112rZty6OPPlotQ7y8vOjTpw/r1693LJNlmfXr15fzsJblyiuvJC4urpz6P3bsGNHR0ZWK1Noiy7KjnYczySjM4GzOWQDNF1Eqyi5izaw1bH1nK7JNptXwVkxYPEE7xZPKopQWUjpfUgH6wpByLREQEMC5c+fIzMwkLy+vWvsoisJ3333nyL00GAzIskxRURF6vZ7CwkK+++67CnfA7CxbtoyPPvqInTt3Vnn81atXYzabHV5Ym81GcXExOp0Os9nM6tWr1eMrCuQcheMfw+ab4c8JcOy/JSJVguAe0OFRGLwCrlwCbe+BgDZOb0FzMfuLi4sdfxXsryV39LyDd655hxW3rCgnUgUVcdUcqgkKCmDFitK/xERPW+QWLBYL33//PUVFReh0OkebLIvF4viObdmypU7fMVeQk2Nm1KjFrF0bT8+exXh5QUxMED4+OgoKCsrNEXWxX1HUSr/2Oai+vD9awdvb2/HbI1Dx1Dy6DPgMVaQOA74Dri6zPoccHuMxtrMdHToCeJVw1nOaz1lHZyxIXFGy3zgQXVEbMB7LUT1x4gTvvvsub7/9Nk899RQtW7akd+/etGrVipCQEBRFITMzk5MnT7Jjxw7OnDlDWFgYDz30ULWFKsCsWbO4/fbb6du3L/369ePdd98lPz/fUQX4tttuo1mzZsyePRuAmTNn8t///peHH36YBx98kOPHj/P666/z0EMP1eKtqBp7uKCze8jaey62CWnDte2vdYT6ao2kvUmsf2o9+Sn56L30DJg1gE4TOznucvqG+9JnRh98w10TFl5jkpIgKwsMBpJK7vZER0d71qaL4O3tTUhICJmZmSQlJdG2bdtL7mO1WklOTsbHx6dckQ57g3QfHx9SUlKqrJgYExPDzp07OXXqVJXHT09Px2QyUVxc7BCq9rwIk8lEevIprAfexpj+BxSVCbWX9BDWT/WcRg4BU9XeGQemcGg7o1yob10oa39lvfhMJhPp6ekXrSiZnJfM/3b+j6cHPY1BV3GqlCSJSV0nOcXeho6r5lBNkJ4O991X+vyjj9RKwA2cLVu2OPLFzWZzhYgrk8lEdnY2NpvNJXfZa4OiKEyY8C2bN59Gp4OQEAgN9cfLC0cuqSzLSJJUZ/ttNhvZ2dmOOfRCtPj+aImIiAgiIiJqHMnXkPHUPDoBtT/qeFSBmk46P/EnN3ADAMEEczM3AzCJSUwgGntmpR/wL+A6hEBtDLjixlK1Zkc/Pz+eeeYZnnjiCX7++WeWL1/OX3/9xbJlyxxGSZJEmzZtGDJkCNdffz3XXXddjUuKT5o0idTUVJ5//nmSkpLo2bMnq1evdnjDTp8+Xc7NHBMTw5o1a3j00Ufp3r07zZo14+GHH+aJJ56o0Xk9xb7kfQBc1uwyZvTRXkNjRVbYs2APOz7egSIrBLUIYsScEYS1Ly887EJVM9hbn7RtS1JqKqBtoQqqfTURqkajkTlz5pCbm8tvv/3Grl27iIqKYtq0aY6wjcDAwCq/g/bwoTNnziDLcoXwDaPRyKxZs8jPzyc1NZXTp08TFOBP2/ACSNsGGf/gr8vAmFhy90zvDeFXqOI0YiAYA2r2BniXCFUnUdb+qvD396/y/dl4ciMPrX6I9IJ0vA3ePH7l406zTSCo7yQmJrJnzx7CwsIYOnRopbnHoN4w05IIkySJF14Ywl9/ncHX18j990+lXTs1j8pms3Hs2DHat2/vmEPrYr/BYGDatGmOm4eVobX3R0sIgeo5jgKLgedRRYIR+BBVaOaTzw3cQCGF5drIPMIjjv3bAYeAbsAbwKWb4AkEVVOjGdJgMDB+/HjGjx8PqBO7vVJVaGholXHQNeGBBx6otEgLwO+//15h2YABA9i6dWudz+sJ7EK1e5O69XN0BYUZhWx8fiNnt6qhyW1HtWXQ04Mw+taDfmb2qqKdOzuKalWV56wVoqKiOHToUJVFwCojLCyMsLAwYmNjiYuLo0mTJrRs2bJa38MmTZrg5eVFUVERycnJlQr54OBggv29aKY/Qk9pO6T+CYklws8PMARA5CA13zT8clWsaojg4GCCS0K/q4vFZuGNLW/w0T8fOZa9t+09roi5goEtBjrZQoGg/mGxWFi7di2gVo+0t7iqLwwaFMvPP08mKsqfLl1KL6FtNhvJyclERkY65VoG1LSOgIAa3rQTCDxIMfAgkAG0Bm5D5jCH6VLSEdUPP4YwhHOccxRLupA5wFngMoQXVVB36nQrT6/Xa6bxsCuRJImoqCin3uGzylYOpqqFZrQmVM/tPMeGZzZQkFaAwWTgisevoMO4DvXnDmeJUFU6diTp11+B+uFRBWokVC8kICCg2p+RXq+nRYsWxMXFcerUqfLvjyUHUjZB0gZI30pihsyRZG/C/Wz0aB1WUgxpKIT2gUpCYusrp7JOcf/K+9l9fne55YqiMHvzbH6Z/Ev9+Q5oDFfMoZohKgrK9vgN12YKh7PYsmULWVlZ+Pv7M3jwYE+bc0lSU/MJD/ctN/aGD29dYbsGPUYFDQJ3jFEv1FDd37AisYqbWUACCSxlKa1oBcDzPI8XVdeBaVryJ2h8uGJsNpyrTBei0+mc7pE7ln6MYlsxgaZAWgS1cOqxa4siK+yat4td83ahyAohrUMYPns4oW1CL72zVihTSCmzeXNHYYxwjV882sdX2X5UNSUgIKDKymyVYReqp0+f5vKebSDld7VSb/oOoDQh/mxBc9bEB9OlS1d6DJ0FUvXPUV/4+ejPPLb2MXLNuRXWjWwzkneueUdcwNYBV8yhmsFohEoqcTZEZFl2RFGNGDECb29tRVFcyJEjaQwf/hW33dad118fftHvcIMeo4IGgSvGqAW1mm9HYAhq/ukJlrKX7/mDLED1op7ghEOoXkykCho3NbkGrS5CqFYDm81GQkJCtcMqq4M97LdbZDd0Grjwz0/NZ+NzGzm34xwAHcZ14Ip/X4HRpx6E+pbl7FnIywMvL86XXERFRkZqPg+orFCtLGe0OqSnp2Oz2ao9Rls28QFzOgm7l0Hgm+VXBrQrKYY0FA6lw5Fl4B3R4ERqoaWQF35/gUX7FlVYZ9QbeX7w80zvNV2I1DriijlU4H50Oh3jx4939BXXMnv3JnH11QtJTS1gzpwttGgRxMyZl1W5vRijAq3j7DF6FHgROA54k88Q3mMDP2NB7Q/elKbcwi1cz/X44Vfn8wkaPhf2cnUG2r561xC5uRU9LXVBS/mpZ7eeZeNzGynMLMToY2TgUwNpN6adp82qHfZCSu3b06lrV955551qt3zxJBEREY4WBhkZGbXyAJvN5otvoCiQe1z1miZvpEVGPJijSDNDfrGEX2S30rBe3zIXoVJGjW2pDxxNO8p9v97H0bSjFda1CmnFJ9d+Qrcm3TxgWcPE2XOowDNIkqR5kbp9eyLXXLOIrCy14nevXlHceGPnS+4nxqi2KSoqorCw8KKF8Bo6zhijdi/qfBSyySePs/jyAWvYBkB3ujOVqQxhCPoqO5oLBO5BCFUP4fCoevBCWLbJ7PhkB3u+2ANAWLswhs8ZTnBssMdsqjNlCinZQ361HvYLas5oZGQkSUlJJCUlOc9mRYas/SXidAMUnnOs8jXpiQgNJLXAl9OxT9OpR+X9ihsaiqLw9YGveXbDsxRZK7aumdhpIrNHzMbfy98D1gkEgrqwadMpxo5dQm6u2hLm8subs2rVrQQHaztMWXBp0tLSOHnyJGFhYXTufOkbD4KKHAWew8YuckgnAxNbiOJLvMhjOFczhSl0Q9ygFWgHIVQ9QEp+Ckl5SegkHV0ju3rEhrzkPNY/vZ7kvckAdJrYiQGzBmAw1fMhYReqnTp51o5aMHbsWGRZpmnTmpUhiIqKonv37nh5leSNyBbI2KkK0+Q/oDi9dGOdV2kbmchBtMxZQ8auXWTmN46m6jnmHB5f+zgrjq6osM7H6MPs4bO5ucvNHrBMUG+xWCAxsfR5eDj4i5scnuC33+K54YZvKCxUq5FedVVLVqy4hYAAk4ctEzgDuzdRVFKuOaVeVMijmBSOEcVXRHOAG7iBSUyiqSiBJNAgdVIlZrOZXbt2kZKSwpVXXlkvPFe1QZIkYmJinJantj95PwBtQ9via/R1yjFrwqk/T/H7C79jzjHj5efFoGcH0ebqNm63w+nIMhw5oj6uh3dbhwwZUqv9evToQbfO7cg9+Ru6Ay+qbWSsZcKdDf4QMUgVp+EDwODjWDV69GhuuOEGTYdR2WQb2xK3kZKfQqRfJP2b9Uevu0g4UmIiZGRgU2xsy9hPijmDSFMo/UO78cL+t1lxbi1ckAPcOaIz/xv7P9qE1uJ7UHK+KgkNBTc3aHcqJa/PNx7K3lbzjQeCqPr1lXlfJFmmZW4u0oEDpe99fX9fQH2Nhw7BlCmly559FoYNUx83hNfoJGqbe19dli8/ws03f09xsZojNXp0W3744WZ8qllnwdm/8wLnY0/j8W+kN4JqO0Z/4xSvYSS/RIheiw8WfuMyruR63hT5pwKnoamqv++//z4vvvgi2dnZAKxdu5Zhw4aRlpZGx44defPNN5k+fbrTDPUkOp2OsLAwpx3PU/mpNouN7f/dzv7FqlCO6BTB8NnDCWwe6FY7XMbp01BQAN7ejaMKpyUHUjaTHreWjDO7CTYVIgWUJLJ7hUKTq9RiSGF9QVf5xZr9Bz83Nxc/Pz+XXkjWhpXHV/LM+meIz4zHptjQS3rahLThteGvMabdmIo7JCZCv36sjMjkmSvNxAfJ2HSgl6FNto7Hdnix7nIr6c3DHILpzp538vyQ5zEZauF1KTkfF8uD9veH7dvrp2Ap8/q6W2FzmVV+96P+glT2+i54X3RA8IXHrs/vC5S+xpwcyM8vXf7UU2olYKj/r9EJZGZmsmXLFhRF4brrrnPJOX766Qg33vgdNpsaGTJhQieWLJmAqQYRQs7+nRc4F4vFQlGRmqrRWIVqTceoBXidJN4kFwkDfYnkGQxcDcBzLrJS0JhxxTVkrY74xRdf8MgjjzBq1Cg+//xzFKU0bDA8PJxhw4bxzTffOM1IT2Oz2Thy5IjTqlntT1GFojuFau65XH6++2eHSO02pRvjPh/XcEQqlIb9dugADbVqY1EanP4B/vk/2HA17H+edX/u4b31Afy0PwS5xWToPw+GroYuT0PEgCpFqp333nuPV155pU49XF3ByuMrmbpsKscyjmHSmwjwCsCkN3Es4xhTl01l5fGVFXfKyGBlRCZTRxdxLETGJEsEFEuYZIljITIPDjMz5aAeZJkg7yDmXz+f14a/VjuRWnI+8vJU0evlVfFPp1PXX8zjqmXKvD7Z6IWZ0j/ZeJHXd8H7onh5YdHrURrK+wLlX2NZjMaG8xqdgKIoxMXFceLECZcVK+rVK4qmTdVw0KlTu/PttzfWSKSC83/nBc7F7k319vbWdASQK6nOGDVjZj/7UYD/A36mCd740ZVkPia9RKQKBK5BM1V/3377ba6//nqWLFlCenp6hfV9+vTh/fffr7NxWsJ+J6+uFNuKOZymVqbtFumehPWTG07yx8t/UJxXjCnAxJAXh9BySEu3nNut2Cv+1sOw34tSkFhaDClrP1Amn9S/DScK/EnOy+ZgThuUDo/UWKTb706fOnWKZhrx/NhkG8+sfwazzYy33pt8Sz5KyetWFIUcaw5TfpjC8FbleyMqWVmsH11InheYbGCUJVAUdDYItEGOl8LS9hbmdHmY4YPvoFlgyev9+2+Ij1cf+/vDDTdUNOrYMdU7ZufmMrmsBoMqUGQZylZf1unUZWVRFFi8uPR5r17QpUvF8/36K2Rmqo9btIDBgytus3UrxMVd3O7jx2HbttLnN90EpguEeXo6rFpV+nz4cIiOLvf6FMWIkcLSZZJRfd3FxeWPtXIl7N2r5m56eZV6F6H0sdWqrv/lF9X+8eNrZ3dGhno+O8OGQWU53ovKtB/q0QO6VTL3rlqlvg8AMTFQWSj+9u3qOABITS19TcHBpdvYRSpUfG8aIaGhoQwZMoSYmBiX5RbGxgazYcPtzJu3i9dfH45OV7vwM2f9zgucj12oNvb81KrGaDrpLGUp3/M9FiysYhVj8eUEEouIYXRJD1SBoL5RK6EaFxfHQw89VOX60NDQSgWsAI6kHcFisxDiE0LzQNeW+LcV29j67lYOfncQgCbdmjDs9WEERDfQib4eF1Iqh6JAXnxJMaTfIfdY+fVBXUvbyPi1gD9nA7tqfboWLVpw9OhRTp06xRVXXFFhvV6vx8fHp7RYkxvYlriN+Mx4dOjIMmdVWK8oCjnmHP449Qde+lK7iosLyTEBCth0jo0BkABfC8QFy7T1b1EqUgG+/x6+/lp9HBtbueDbuhWefLL0eWVhjDabGgpqp6qLqscfL3389NOVC9X33oMDB9THY8dWLlR/+KFU9MbEVG73tm3lzzd2bEXBl5hYfpuvvy4vVEsIpPS1KRZ/MFQyJt5/H3buhKIi9b33rSQP32xW18+dq4bpVyZUt28vb9OYMRXtPneu/DaLF1cuVJ94wjEOeOKJyoXqBx/Anj3q41GjKheqP/4ICxaoj0NC1P8lSU03EFRJr169nH5Mq1XGYCj1ZrdtG8qcOSOcfh6BNrB74xtr2G9VHOc4S1jCalaTSzQyTWhDDmc4w3V04CogUNRNFdRjajV6g4ODSUtLq3L9oUOHiIqKqrVRDRlHW5rIbi4t2pB9Opv1T60n7aj6OfW4rQeX3X8ZOoO2chCdhs1WrwspociQfbDUc1pwtsxKHYT2KRGnV4F3pFNPHRsbC6ge1cro27cvffv2deo5L0VKfgpW2UqRteii3xNZkS/6/EL0sipgU8yNOxxTIKivKIrC889vZPfuJJYtm4SXVwNN8xCUQ3hUS5GR2cY2FrGI7ahRPnl0J51naU0gCwkhuKT/aQNK7hI0UmolVMeMGcOnn37K/fffX2HdwYMH+eyzzxpMISVQk4Nbt27tlCRhu1Dt0aRHnY9VFXFr4vjztT+xFFjwDvZm6MtDibkixmXn0wQnT6peGl9fNUyyPiBbIWMXpGxUBaq5zM0fnReEXV7SRmYweAVd8nA+Pj61GqMtWrRAkiQyMjLIzc3VxIVApF8kBp0BPy8/CiwFVW6nk3QXfX4h9sJKkaZQp9gpqD56g7irX1/IzMwkLi6Otm3bEmL3HGsARVH4179+4513tgIwdeoyvv32xovezFIUpVo3hZ35Oy9wLqmpqWRkZGAymfDza7wVai06CwfbH+Q13Wuc5CQAOnQMZzgTuZXXaE071OghgcATuGL+rNWVw6uvvkr//v3p2rUr1113HZIksWDBAubPn88PP/xAdHQ0zz//vLNt9RiSJBEYWPf7UoqiOAopdWvi/PxUa5GVv97+iyM/qp7F6N7RDHt1GH6RjWBit4f9duwIOh3z588nODiYkSNHaitUyGaGtK2q1zT1T7Vyrx29L0QOgibDStrI1Kx1kdForJWX3tvbmyZNmpCUlMSpU6fo2rV8b9/t27ezbNkyOnfuzG233Vbj49eG/s360yakDccyjuFr8KXAWoCv0Re9pEdRFIpsRUT5RfH8kOfLiVP5zGle+uMlkv0UfCwlC0smTgWFAi+FDhk6+ode8P17+WW1rUiZ7SswaVL5cN/KBL3RCJFlPN5Wa+V5igcPlj728am4HtRwZHthgqqKh7z4oho6fDG7b7oJrr229Hllc1nnzuVtquI7k0LpawvzBrBW3Oi772D/fhg9Ws3XRL1wKjc2fX3V/NaffoILxpuDG29Uw33tBFVys6Zjx2rZ7QihhqrDdL/55tLv97PPloYaHz6svsYGhqIoJCQkkJ6ejsFgIDg4WBMtW2RZ4f77f+V//9vpWDZoUItL2nbixAl2797NZZdd5ogeqQxn/c4LnIuiKJw4cQKbzYbNZkPfUAslXgRH/qn0PVm+WQB4E0A7ZvEKfWhe0nbmS9SOYQKBp9BMe5qmTZuyc+dOnn76ab799lsURWHhwoUEBAQwefJk5syZ06B6qtpsNg4dOkTnzp3rNEkm5SWRmp+KXqenc4Rzw1MzT2ay/sn1ZMRnIEkSve7qRe97eqPTN5K7w2UKKRUWFrJx40YARo0a5UGjSrDkQupm1Wua9hfYyhRD8AqByCGqOA3tC/ra54Hm5ubW+oc8Nja2SqGqKAqyLJer7u1q9Do9rw1/janLpmK2mQnwCsDb4I1NtlFgLcDP6MfHYz8u36Lmxx/hTC7R672ZOroIs17BoCiOcN8Cg4LJJvHqXyb0My94j/z81L+LYTJVzJG0Y61EsFW1XJJK8xsvRnUumutqtx2D4eI2Wa1ICpR716yAVMnrCwxU/yRJFX4WCwpgtdkw6PXq3X6bTV0fGFi5AHWW3XZc8X7bb1TU5LOvB2RlZZGRkYHBYCAjI4OsrCyPe1WtVpnp05ezcKEakSRJMG/eOKZPv3Tu665du0hMTCQqKuqiQtVZv/MC55KVlUVeXh6mkrlAC+PR3WxhC/OYBwr45fox1n8W/+iuZT8GNgH2Ls5CpAo8jWaq/gJERkYyb9485s2bR2pqKrIsExER0WDDZpzx5tu9qR3COuBtcF7xjWO/HGPznM1Yi6z4hPow7NVhNOunjeqtbsPuUe3c2dFmJTAwEN/KCrm4A3MGpPyuitP0f0Apc9HqHaUK0yZXQUhPKOMRtFqtJCQkkJqayoABA2p0yroIydjYWLZt28bp06drfYy6kF+cj59XecE1pt0YFk1Y5Oijmluci17S0yGsA68Oe7W8SP39d3j4YSguZozZxKJVlPZRNajhvh0ydLz6l4kxqSEQ6qTQ39BQ1YuXl1d1hVd/f+edz92UeX06azFlZaPOQmkf1Qtf34Xvi6Kgk2XV62u/41qf3xdokJ+93ZtqNpuRJAmdTkdCQoJHvarFxTamTPmBH35Qb0bq9RILF45n8uRLRyUlJSWRmJiITqejZ8+el9xetKbRFvbxaLPZ8Pb2pqioyOPj0dXIyGxlK1asDEYtoDeKUWxkI1fLY9ic3Z6lAS2QkQgGRDUYQUOnVkJ1+vTp3HvvvfTv3x+AiIiIcuu3b9/OJ598wvz58+tuYQNib9JewHltaSwFFja/sZnjvx4HoFm/Zgx9ZSi+YR4SZ57CYlFbWQB07kxSidiKrqRqqUspOFciTjdA5l7KtZHxa1VSDGkYBHYovVi/AIvFwksvvQRA9+7d3ZaPY/c0nDlzBqvVisGN+YSbT2/m3l/u5ZNrP2FQ7KBy68a0G8M1ba5hW+I2UvJTiPSLpH+z/uh1ZTweJ0/C3XerHqySvp1jHp7LNX16sS1jPynmDCJNofQP7aZ6UkNDwVlteJo1U6vTXqxXpjPP527KvL59u+D2O0pXLfgIevem8td3wfsi22wcP36cdu3alXqr6vP7Ag3ys7d7U+0YjUaPelULCy3ceONSVq5U53cvLz3ffXcj11/fsVr779qlVkNv3769tlJABNXCPh7taS2eHo/uYCUreZEXiSGGgQxEhw4vvJjJO7wgyewNK8IXGAY8CdSf22ACQe2o1dXol19+yYgRIxxC9UJOnjzpyFkVlGL3qPaIqnshpfTj6ax/aj1ZCVlIOom+9/Wl5x09kWrZP65ec+KE6tEICIBmzUgq6XXp8srTigL5JyFpg+o5zT1afn1g59I2Mv4tq3VIHx8fgoKCyM7OJjk5mdatWzvf7koIDw/H19eXgoICzp8/T0yMe4pvfX/oe2atmYVVtnLXirv46ZafKoTF63V6roip2DbHQWwsTJsG//uf+nzKFLjjDvSSxBX0dJ3xdpo1q1dipMaUvL6CbCiT6UlBG+Bi99zKvi82G0WgtoVpSGGVDeizL+u9sqPX6zGbzR7xYuXlFTNu3Nds3JgAgI+PgR9/nMQ117St1v45OTkcL7mB2adPH1eZKXARZcejsSRnXK/XY7FYGpRXNZ10UkihE2pbvWEM43/8j0EMwowZAz7MB+YDNkkiwGbjJVlmVEOaRwWCi+ASt8m5c+fwqapASD1Ep9PRoUOHOoU1F1mLOJquCpm6eFQVReHIj0f4662/sBXb8Iv0Y9irw4ju7WbvoZYo2z9Vkhyhvy4RqooM2YdK2shshIKyobI6CO2lek0jrwKfJrU6RXR0NNnZ2Zw7d65aQjU2NpYDBw7UaYxKksStt95KaGgooW4IVVQUhQ+2f8CczXMcy/KK87h12a2sunUVUf41+Ox0OnjhBYiIUHuG/uc/VXqsBZ7BGXOowPkoikJ6ejrh4eHlclPtYlWn03nMiyVJatgvgL+/F7/+OoXBg6vOMb2QEydOoCgKMTExFaK+KkOMUW1xoTcVaFBe1bL9T2OJ5Wu+RkLCF1+WsxwdOo4BLwL2TupDgUcMBpqKMSrQKB6t+rt8+XKWL1/ueP7pp5+ybt26CttlZWWxbt06LrvsMudYqBG8vGpf5AbgUOohbLKNcN/wml2El6E4v5g/X/uT+N/iAYi5IoahLw/FO7iRN5svU0gJ1LwkcGLor2yDzF0l4vR3MKeUrpOMEN6/RJwOUosj1ZGoqCiOHDnieB2XYvz48VxzzTWOu861pV27dkDdcl2rg1W28sz6Z1i4b2GFdZc1vYwQ71q+h/ffD/fdV3X1W4FHqescKnA+x44dY9WqVfTp0wd/f3+sVitGo9HR0sVmsyFJkiN33p1eLD8/VZzedNNSXn11GP1qWHehZ8+eREVF1cheMUa1gd2barVaMZlM5bz8nhqPzsCef1q2/ymAL75kk00wwQDY0DEP+BywAcHAE8AIQBZjVNDIqLZQPXToEEuXLgXUiWLbtm3s3Lmz3DaSJOHn58fgwYOZO3eucy31ILIss3//frp161braoD7k9Ww3+5NutdqYk09nMr6p9aTczYHnV7HZf93Gd2ndm+cob4XUqaQkqIoDoFXJ4+qrRjSt6riNGUTWLJL1+l9IWKgWgwp4kowODeP1C6wk5OTq7X9vn372LFjB0FBQUyePLnWYzQzM5Pvv/+e3NxcZs2aVatjXIoCSwEzf53J2vi1FdbN6DOjQruZSlGUqj2mQqRqEmfMoQLnk5Ki3nQzGAwUFhZiMBiwWq3lhGrZ9dXtSeosgoK8+e23abXevya/AWKMagdFURzjsbICV54aj7XFjJmVrGQJSyr0P53CFLqVyaE4BzxGqRe1bC6qTYxRgcaRZdnpx6y2UH3qqad46qmnANW1+/nnnzNlypRL7CWwszdZLaTUvUn3Gu2nKAoHvzvItne3YbPY8I/yZ/jrw2nSvXZhpQ2O4mKIi1Mfd+pEdnY2RUVFSJJEZNl+ltXBmg+pW0p6nG4BW2HpOmNQaRuZsH51aiNzKewXV/YQ5kuRlZXFqVOnaNWqVZ3O6+vrS3x8PLIsk52dTVBVbUNqSVpBGrf9eBt7kvaUWy5JEi8MeYEZfWZc+iA2G/zf/8HIkTBhglPtEwgaG4MGDaJVq1ZER0djtVqxWCwkJydz5swZQkNDy6UeGI1Gl4bFnjqVxUMPrWbevOuIiGgEvb8FVaLT6ejTpw8Wi6XKbVw9Hp2Bo/8p35NFFqB6T8cznklMomlJ/9OyBAO5lHpRr3abtQKBNqlVjqorFHNDRlEURyGlmghVc46ZP175g4SSYhItr2rJkOeHYAq8RG/BxsTx42q11+BgiIoi6aiaBxweHl69UFhzhuoxTd4I6dtBKfPDaIpUCyFFDYPgnqBzzx1Mu0c1KSnJrXeMTSYTUVFRnDt3jlOnTtG9e81uqlyMk5knmbJsCqeyTpVb7qX34oPRH3Bdh+sufRBFgaeeghUr1L/0dLjnHqfZKBA0Rpo3bw6ohWpMJhOpqano9Xr8/f3dVik3Li6DYcMWcOZMDiNHZrNx4+0EN/aUlkaOyWRy9E6tb8QTz2IWs4pVWFCvKaKJZjKTuZ7r8aP8jZhTQAygA3yBt4AIREVfgQBcVExJUJ7E3EQyCzMx6o10DK9eWf3k/clseHoDuedz0Rv19H+kP11u7lIvwlzcSpmwXyQJSZLo3LnzxQsCFSaVFkPK3AOUufHiF1vS43QoBHbySFGeiIgIJEnCbDaTmZnpluJGdmJjYysI1djYWMaNG1drO3ae28ltP91GZmFmueVB3kEsuGEB/Zr1q96BVq+GRYtKn8+eDWPGNJiqqwKBFigu6QnrrnzNgwdTGDFiIUlJeQAUFFjIzy8WQlVQb/mZn1nBCgC6051buZWruAo9FW92LwA+Ah4HJpYs6+AuQwWCekCtheqqVauYO3cuu3btIjs7u9ICLA2lebZOp6Nbt261DjPZl7wPgI7hHfG6RMioIivsW7yPf/77D7JNJrBZIMPnDCei06WrFjZK7IWUOqml3Tt06OAIUS9H3slScZpzuPy6wE5l2sjULXzWGRgMBpo0aUJSUhJJSUnVFohRUVF1DoWKjY3l77//5tSpUs9nVFRUrfN9V8etZuavMzFbzeWWNw9szpKJS2gbWr1WEwCMGqWG/X74odri5NNPhUitJ9R1DhW4D7NZ/a66Q6ju2nWekSMXkp6upll06xbJ2rXTaNLE/T1PxRgV1AZ7/mk72tGVrgBMYhLJJFfIP60ME2rBpP2UCtWqEGNUoHU8WvW3LD/88AM333wzXbp04ZZbbuHjjz9mypQpKIrC8uXLadeuHTfccIOTTfUsxcXFeHvX7g6vXah2j7x4KGVRVhEbX9jImS1nAGh9dWsGPzMYL39R5a1KynpUy6IoqiC1i9P8hDIrJQjppRZDajIUfLTX2icqKork5GQyMjKqvY8zbgzFxqrtHxITE7FYLBiNRhITEzly5Ajh4eH06FH9HsBf7P6CZzc+W+EmVrcm3Vg4fiGRfjXMIZYkeOYZtQ1NUBCMGFGz/QUepS5zqMB9REZG4u/vj5+fa/NE//77DKNHLyY7WxXGffs2ZfXqWwkL83XpeS+GGKONjwLgJ6A3UL14t/J8xEcsZjGDGcxc1CKi0UQzm9mVbm8BUgD7LdabgVjg8mqeT4xRQWOjVkJ19uzZ9OvXj82bN5OZmcnHH3/M9OnTGTZsGAkJCVx++eV1LuyiJWRZ5ujRo7WutOYQqhfJTz2/+zwbntlAfko+ei89Vzx2BR3HdxShvhejqAhOnFAfd+qk9jjN3KMWQ0reCEVlquZKBrUIUpNhEDkYTNrO/rj77rvx9fWtUcuZ1NRUZFmuUzXA0NBQ/P39ycvLIzExkZYtW3L27FnWrFlDly5daiRUMwozKojUoa2G8r+x/8Pfqw4ekxnVKLok0BR1nUMF7sMl/acvYOPGk1x33dfk56v5ewMHtuCXXyYTFOS5C3AxRhsf24GXgSSgJzCvGvvEEYcRI7GoN3UnMIGNbKQvfVFQkKj6ms3eF7UA+BrwQc1LHVBNe8UYFWgdj1b9LcuhQ4eYPXs2er0eg0E9hL06W8uWLbn//vt54403uO2225xnaT2lwFJAXIZalbZbk4ohIIqssOfLPez4ZAeKrBAcG8zwOcMJaxfmblPrH8eOAVbobITk/8H+P8CSVbpe76O2j4kcqv5vdH84WW1xdsXd6iJJErGxsRw8eJBTp07RsmXLWh9r1oBZnM05y7cHvwVgctfJzBkxB6O+muK7sBB8fGp9foFAoD1WrjzOxInfUVRkBWDEiNb89NMk/Py8HFEcAoErKQDeA34os6ywim2htP/pYhazjW2MZCSv8zoAscTyEz+ho+qQRwvwBaV9UYOAk0DnKvcQCAR2aiVUfX19HfkrwcHBmEymcq00mjRpwsmTJ51jYT3nYMpBZEUmyj+qQqhjYUYhG57bQOK2RADajWnHwCcHYvQVP9QXxVqgto858CHceBxCfCFxubrOGFjSRmYohPUHff2sGuhJygrVuiBJEm9e/SYpBSn0ie7Do5c/Wv0IgYQEtf3MU0/BTTfVyQ6BQKAdfv75qEOkXndde7777ia8vHRs2LCBw4cPM23aNAIDAz1spaAh8xzwR8nj7sC+Krarqv+phISM7BCnFxOpdi9qZX1RBQLBpamVUO3QoQOH7LmBQM+ePVm4cCFTp07FarWyZMkSWrRo4TQjtUBtwyyqCvtN3J7Ihmc3UJhRiMHbwJVPXEmH60SttyopzirTRmYbyMVQeA4MstpGpsXNqjgN6e22NjJaISkpiaysrBrls14M+3f31KlTlRZJqwlGvZEFNyzAoKvBVJOaCpMnQ1ISPPyw+nzmTI9UYBY4DxGqJgD473/HkJVlRlEUFi4cj9GojovMzEwsFgu7d+9myJAhHrFNjNHGwb2oHs2nUb2dD16wPoMMlpb8q27/0wupzItq74tal18yMUYFjY1aCdXx48fz/vvv89Zbb2EymXjmmWe4/vrrCQ4ORpIk8vPzmT9/vrNt9Rh6vZ5u3S5eua0qLhSqiqyw87Od7J63G0VRCGkdwog5IwhpHeI0exsMhcmQ8ruac5qxm3JtZHxj4JgN9oTAcx9B50GestLj5ObmkpOTg9FodMqPWExMDCNGjCA2NrZaQvVw6mFsio2ukV0rXV8jkQrwzTdQ1pv7yy9w993gpnYZAudTlzlU0LDQ63V89dUN6HQSen2pJ6pPnz6cOXOGAwcOcPnll7u9h6YYow2X7ajCdFLJ8/bA96j5oX+X2S6OOJawpNr9T6vCVV5UMUYFWscVN1JqJVQfe+wxHnvsMcfzsWPH8vvvv7Ns2TL0ej3XXnstQ4cOdZqRnkZRFHJzcwkICKhRcSNZkdmfsh9QhWp+Sj4bnt3A+V1qmHTHGzpyxWNXYPAW7Wwd5J9SvaZJGyDnUPl1Ae1Le5zqouBfV4Gih85dPGKqVoiKisJoNBIcHIyiKHUuwGU0Ghk5cmS1tt18ejPTl0/H1+jLz5N/JiYopk7nBuCBB9T81HffhdatYeFCIVLrObWdQwX1nw8/3M6gQbF0797EsczuRS1LbGwsoaGhZGRkcODAAfr06eNOM8UYbaAcAe4H9EAvVJEKlAvWzSOPfzjMLcx0LOtOd6YwhaEMrbT/aWW4yotqR4xRgdapaxReZThNIQ0aNIhBg0q9WvYvU0NAlmVOnDhR40prp7NPk2POwUvvhc8RH3546QeKsoow+hoZ9PQg2o6qQQ/JhoqiQM5R1Wua8jvknSizUoKQHmoxpCZDwbdMuM2uXeq+TZpANfuMNlQKCgqw2WycP3++zlV/7Zw5c4Z//vmH0NBQfKooaPT9oe+ZtWYWVtlKXnEety67lRWTVxDsHVy3k0sSPP44NG8OgwZBmCgsVt+p7RwqqL8oisJrr/3Jc89tJDLSjz/+uIOOHcOr3F6SJHr37s26devYvXs3PXv2dOtYEWO0YdIRGAEEA82r2CaTDKykEYyOYQxjClPozsXbCV6IO3JRxRgVaB3NVP29GCkpKbz77rt8/PHHZGZmOvvw9Yp9yfuQbBL9dvRj7cdrAQhrH8aIOSMIauGZqq6awNFG5veSNjKlhbiQ9CVtZIaqRZFMVYiUSvqnFhcXu6VJvdaw9zx15vctPT2drVu30rx5c/r3719unaIofLD9A+ZsnlNueVxGHO9ufZcXr3rROUZMmeKc4wgEAreiKApPP72eOXO2AJCSks/q1XEXFaoAHTt25K+//kJRFLKysggTN6kENaQA+BS4HbAnVL1OqQfVnn86nvFEoha4DCMMb1ryEz9VK/+0MraiilRne1EFgsZOjYRqSkoKX331FfHx8YSEhDBx4kRHeE5iYiKvvfYaX375JUVFRVx11VWusLdesffQXrov6k54Rjj4QZebu3D5I5ej92qEd8JsxZCxQxWmKb9DcRlRpfeG8CtUcRoxEIzV8MTbhWqnTgAUFRUxY8YMQkJCeOONN0RD7Dpib0tz7tw5iouLHcutspVn1j/Dwn0LK+xzXfvreGrgUzU/WUIC1KENjkAg0A6yrPDII6v54IPtjmVvvXU1jzxy+SX3NRgMTJgwgZCQEOExEtSYf1D7op4HkoHZJcvLhvk+y7NsZzvFFPMgD+IFxOJHGzrVWKJaKb2IvhXIBSYjKvoKBM6k2kL1yJEjDB48mPT0dEcM8ptvvsmiRYuQJIm7776boqIiJk6cyL///W+355e4mpoKn1ObTpHzXA6BuYH4R/tz9etX02pYKxdZp1GsBZD2txrWm7oZrPml6wwBEDlYFafhl6titSYcPqz+X+JRTUpKQlEULBZLoxWpOl3VJfJrSnBwMEFBQWRnZ5OUlISPjw+KQeGuFXexNn5the3v7XMvzw15Dp1UQxuWLIEnnoA331Qr/QoaLI31e9mYsNlkZsz4mfnz9ziWffTRGGbOvKzaxwgPv7jX1ZWIMVo/ubAvajQwgdL+p53oREiJf3USkyiiiB70AKAPsKqG57Pnom4EFgBeqDmw/1fH11EdxBgVNDaqLVSfe+458vLy+Oijjxg0aBAnT57k0Ucf5ZFHHiE7O5vrrruOOXPm0Lp1a1fa6xH0ej0dO3as1rY2i43tH2xn7+K9WHOt5DbNZcLCCbRo27Da9VRJcXaZNjJb1TYydrzCVGHaZCiE9oGaVoO1k5MDZ86oj0s8qklJSYBaWKihYLPZ+OKLL0hOTuZf//rXJX+g/P39neqFaNGiBfv37ycsLIwHHn+A23+6nT3xe8ptI0kSLw55kXv63FPzE6xZo+aiyjL8619qG5qHHnKO8QJNUZM5VFA/sVhs3HbbT3zzzQEAdDqJ+fPHcfvtPT1rWDURY7R+UtaLCnAjcC9mfmclb5b0P72Xe7kH9TdqMIMZQt3aHxWiiuJ0YC1wbZ2OVn3EGBVoHY9W/d20aRMzZ87k3nvvBaBz584YDAZGjx7N7bffzhdffOF047SCLMtkZmYSEhJyUa9VTmIO659aT+qhVAothST2T8Q8ztzwRWpRippvmvI7pO+gfBuZ5qXFkIK7Qk09bpVx5Ij6f9OmEKTm+tqFanR0dN2PrxH0ej27d+8mJyeH8+fP06rVxT3yxcXFyLLsNM9qy5Yt2b9/Pzvid/DMyWc4lXFSFZUleOmM/LfHU4z1uhz271eLWjVrVvFAiYlQWY/XH38Esxl0OtDrwWZzit0C7VHdOVRQNzIyMggJCXF7RdCiIiuTJn3PihVHATAYdCxZMoGbbnJNRfaioiIkSXJqCxsxRusXlXlRHyabeL7hpgv6n5at2ivVMnPUiuo1lYBA4HkgHzUX1V2IMSrQOh4tppSenk737uWroPXooYZOjB8/3rlWaQxFUThz5gzBwcFVbnNi3Qk2vbKJ4vxiTIEmzJPNnFBOMKbZGPcZ6k7yT5cUQ9oA2QfKrwtoV1IMaSgEtFWruDqTC8J+oWF6VEEV3jk5OSQlJV1SqBYVFTm1NHiLFi1I0iUx/+x8ipVC/AoLHT/xQWaJBWt86Jf0TOkO/v6wfXt5sZqYCP36QV5e5Scxm8Figf/7P3jkEafZLtAW1ZlDBXUjPT2dJUuWEBsby+jRozEajW4796pVxx0i1WTS8/33NzN2bPtL7FV7du7cyZ49exg0aFCF65LaIsZo/eFCL+pQ0jHxP57j5xr3Pz0AzAVaoFbtrQx7Rd+pgP2K7so6vobaIMaoQOt4tD2NLMsVfvjsz/39/Z1rVT3CVmzj77l/c+h7tbhPkx5NGP76cJ7Y8QQkQrfIBtKcWVEg97ga0pu8AfLiy68P7l4a1utbVRF4J1FJxd9z584BDcujCqrwPnr0KOfPn69yG71ej06nc/od1v2F+1nhvQKrYkWvKEiKApJETJ6exWsDaJutV5NzAKxWVYxmZJQXqhkZ6nKdDgyVTDd2b+r06c6/oSEQNBJkWea3337DZrMhyzKGyr5rLmT8+E7Mnj2cV17ZxIoVtzB8uOtSgBRFcVQ69/X1ddl5BNqjvBdVwZtMoviMjSx1bFPT/qe5wD7AXMm6C/uizgNGUb44k0AgcC01+jXbsWNHuTy53NxcJEli8+bNZGVlVdh+woQJdTZQy2Sfzmbdk+tIP5YOQM87e9L33r6ghwOpqpexR1QPT5pYNxQZsvarwjR5IxSeK10n6SG0b2kbGe8I99l1gUdVUZQG61G1v56LCdUePXqQlpZ2SY9rTdhyegv3rrwXRa+oMU8ldMswsHBjCJFFetDLYK8ILMvqzYwLsVjUP6NRDe01GlVhWhZZrrjMSVgssHcvFBW55PCNin37PG2BoCp27NhBcnIyXl5eDB8+3O2hvwBPPjmQKVO60cLFrdckSeLGG2/kzJkzxMTEuPRcAu2QDtwBnEMhmyx8WYnMRyRgRleH/qcdgLeBC90tVfVFFSJVIHAvNRKq7777Lu+++26F5S+++GKFZZIkYWtAOWcBAeVbphxfdZzNr2/GUmjBJ8SHq16+ipgB6o9mXEYc+cX5+Bh9aBPSxhPm1h7ZouaZpmxUQ3uLy+QW6kwQPgCaDIPIgWAMdL99WVlQ4j2lpKhATk4OhYWFSJJEkyZN3G+TC7ELVbsQvxjOzNfq37w/w1sN59fDv2Kzqt/jIad1zPszED+pRFRarZCdrT5WFKjMu1FYqKrEoiLVYxoYCD4+TrPzYlgs0L8/7N7tltMJLsGFc6jAOaSlpbF161YArrrqKre8zykp+ezefZ5rrmlbbrmrRaodnU5HbGys048rxqh2CQXaAgrgxysUsAlffLmBKdzCLbXufxoK5UorXehF1VpfVDFGBY2NagvVjRs3utIOTaPX62nTRhWc1iIrW97cwtGSfJzoPtEMe3UYfhGlORD7klXXQ5eILuh19aAXnLWwpI3MRkj9E6xl8gkN/mobmcirVJFqcI/IqBK7N7VFCzUnklIRFxYW5ta8LHdgD2W2t9+5mKckLCzMaRXXDDoDn4z9hFGpo9iVuIte+a2Z92scfr4S1JO3eO9eIVJdSU26JJSdQwXOQ1EU1q1bhyzLtG7dmk4lVdBdSWJiDsOHf8WJE5msWDGZUaPaXnqneoAYo9pjB2DgBOtYxsM8zPMYMSHxOyPIou8l809rSlVeVK30RRVjVKB1PFr1d8iQupXzrs/kpeSxfcF22g9pz99v/U3miUwkSaL3Pb3pfXdvJF158bA/eT8A3Zs4p8hDnSlKg7PLoPkE8C7pUWfJgZTNalhv2t8gl8nQ8AqFJlepntPQPqDzrDLZsGEDS5YsYcqUKQxLSFAXlslPtYfFNrT8VIDIyEgkSaKoqIjs7OxKiygMHDiQnj17kpeX59Sqv75GX5ZNXcbclXPxX3YSneUYVlmuWRiGBxHhvq6jVy/oUYOsBlmWSUlJITIy0iXVKk+fPs369esZPnw4LVo08CrrZZAkieHDh7Np0ya3hPyePJnJ8OFfcfJkFgAPP7yagwfvx2BwfUCkqz9jV49RQc34AvgQhSy2Esw3dKYzY0pKGY3BeUUqU4G/UfNej6BNL6odMUYFWsejVX8bM3kpeex8fyfHlhxDp9fhG+bLsNeG0bRv5aEme5P3AhoSquY0iPsUgrqpLWSSN0LGDlDKhGb7NC0phjQMgrs5p42ME5BlmaVLl5KVlcXSpUu5yttbzREpI1S7du3KzJkzG2RhDaPRSHh4OKmpqSQlJVUqVP38/PD29ub8+fNOr7gW7htOb3qzJ/8giqJQbDajN5nUH28vL4goyU2256JeiL8/+Pmp2xqNHi2Y9OGH4KQCoY0ab29VpNYkeMGeRx4R4fxcdkVR+PPPP4mPj8fLy4spU6Z4JEfTU0RERDBx4kSXn+fo0TRGjFjI2bM5ALRuHcKaNVPdIlLd8Rm7cowKqocZM+tYxyhG0Rc9OiR605GWjKA1rinQFYdaQdiO1ryoZRFjVKB1PFr1t7FiKbDwzwf/YE4z4xfkR4srWzD05aH4hFYeAptVlMXp7NOAhir+pmyEvATYfg/oy8Tr+bdRhWmToWpLGQ1e3K1fv5709HQkSSI9PZ31hYVq37IyIW7h4eGEh4d7zEZXEx0dTWpqKufPn6+02fe2bdvYsWMHQUFBdOtWszF3MOUgL/z+Ap9e9ymhPpX/NG/atAmTWfW4KxYLtqIiDJUVRKps/Oh06p8sV94n1WqtuMxFdO8OAwe67XQCN3Hq1CkSEhIwmUwkJCRw6tQpWrZs6WmzGhT79iVz9dULSUnJB6BTp3DWrbuNpk3dky8nPuOGTQYZLOZHvmI3ClvxwYdhDONHoBm9gd4uO3dkyf964BW050UVCBo7QqhWQUFaAQVpBRz87iAn158EIPaqWC67/zLyU/JRZAXf8IoePHvYb2xwLEHe7iksUSlFaaontTgLDr8NtkKwFYF/OwjvD9GjILSn5+yrBrIss2zZMkcogaIofK/TMVyS0HXo4GHr3Ee3bt0ICAio8i5qVlYWp06dqnHV302nNnH3irvJK87j9p9uZ+lNS/E2lE88VBQFo9FIrtFIkdGIj9WKraCg1KtaFn9/CL1A7IaGqsvz8korBFdnP4GgGiiKwpYtW7BYLAQFBZGdnc2WLVuIjY1tVF7VuqIoCnv27MFoNNKxY8dy7W3++SeRa65ZRGamGkvfs2cUv/02lYgI5+UGXso28Rk3TOKJZzGLWUoCZ7gDK33pRwE21JuazS6xvzNoA3wHhAMeKA8pEAgugRCqVXB42WF2froTFCjOLca/uT+JWxP5ceuPAPSZ0Yc+M/pU2G9/Skl+aqSHYwzPLlPDfa0FUHhezTM1+EPhGThzBkyhmheqdm8qqLlYiqKQbjCwIDaWOxtgmG9VjBo1qlrb+fj4VPvCbenBpfzrt39hlVWP5s5zO/m/X/+PT6/7tFwBsGPHjpGfn48tKooPpk4lMDOTO3/9FZ3JhFGvh4cfhmuvVTcODS3fQxXU59u3q/1Uq6Ky/QQNCkmSCA0NdbqwsHva7GPfx8dHeNxqgcViIS9PLaJXNvdt8+bTjBmzmNxc9SZT//7NWLXqVkJC3FdUz12fsavGqKA8Cgp/8zeLWczf7CGZyWRxIz740Al/5vIZPavR/9SZuK7rr3MRY1SgdVwxNoVQrYJOEzoRO1gtf592JI1Nr25i8LODCe+ohphW5k2F0oq/Hs9PbT4BIgZD0m9w9H2w5EK3FyGwJHTUpO1QWbs3tbJ49zVAwPffc/311ze4Kr91ISQk5JIFFhRF4f1t7/PGljcqrMsoyqDQWoi/l79j29WrV2OxWAgNDSVfkrCWtMAxm80Y/PyQmjWDS4UbN2smhGgjR6fTOb0ATllPmz0/3cvLi8LCQuFxqyHFJdEORqPRMYfk5pq5/vpvHCJ1yJBYfv55MgEBzmuDdSnc+Rm7YowKSjFjZhWrWMISTnCCfDpznjl404qWhDINXx4CGs8t6JojxqhA67iiyJcQqlXgG+7rEKOyLFNcXExo+1CHUK0Mm2zjQMoBQANC1Ttc/Tu/Ss1Llc2qSA2qmOOoRezeVJ1O5xj4ktWKVVGwAuvWrePMmTNMnTpVFBYoITMz86JVf62ylafXP82ifYsqrBvXYRzvjXoPk6H0IvTYsWMcPnwYPz8/x8WgJElIOh1WqxWb1SomEEG1kGWZs2fP0rx5c6f9kF3oaQOEV7WW2IWql5eXY1lAgIkFC25g/PhvGT68FcuWTcLX1703Bt35GbtijAoghxy+4RuWspRMMpExkck9WLmeWEJpgRfPA5d52tB6gBijAq3jiqq/tR7pp0+f5r777qNDhw6EhoayadMmQG0+/tBDD7G7ATUwVBQFq9V6yWpWcRlxFFmL8PPyo1VIzfIFXUb+KfV/nfvugteVsrmp9pBfRVFUEVbyGWRnZ5OYmMjy5cs9bK3nOXnyJKmpqcTHx1c5RvOL85m+fHqlIvW+vvfx0bUflROpdm+q2WxGp9NRXFxMcXExBbLMxt69Wd25M39dcQVKDYs3CRoniqKQkZHhtIqAdk+bfXxaLBYsFkvpOC0oYMuWLS6pQNgQqUyoAowd254NG25j+fJb3C5Sq/qMLRYLOp0Os9ns1M/Y2WNUoGLDxpd8SSaZeDMI+JYQ7qYJUUzGi28RIrW6iDEq0Dqaqfp76NAhBg0ahCzL9O/fn7i4OKwl1TvDw8PZvHkz+fn5fP7550411lP4hvvScmLLKsN97djb0nSL7IZOI+1dyD8NkgFa3Kz5cF87ZrOZ/Px8dDodsiyrA19R1OI9koROp8NgMNClSxduuOEGABISEoiOjsZkqj+C3FlkZmaSl5dHbm5upetT81O57afb2Ju0t9xySZJ46aqXuLv33RX2sVqtpKenYzKZKLqgIenqktZAkZGRDOjSBRF8LXA3NpuN7OxsTCaTQ2SBOnfYhU12djY2m61cYSBB5djfw4yMigXPBg2Kdbc5QNWfsR2TyVSvP+M00ljGMiYwgXDqx2/zpbDnn+5gBw/xEAAhhHAXD/IXl7GXNuiQiAbhRRUIBNWiVrP7448/TnBwMFu3bkWSJCIjI8utv/baa/n222+dYqAWqK5QtVf89XjYrx3ZCgVnQWeAjo+oocD1AB8fH1555RVHIaWlS5eSk5zM2BMnaBIVBY8/TkREBDExMQCkpKQwb948QkJCmDp1Kk2aNPGk+ZriROYJpvwwxdEyyY6X3osPx3zIte2vrXQ/o9HIrFmzyM/Pr/LY/v7+IkdY4BEMBgPTpk2jsLCw3PLi4mJWr15NmzZt6N27d70UMJ6guLiYlJR85s3bxz//FPGvf13haZOq/IzL4uPjU28/4zTS+JRPGczgBiNU00jjUR7Fho2ruZpOdGI38BOTOV+yzY0gclEFAkG1qdUMv2nTJp5//nkiIiIcYqIsLVq0IDExsc7GaQVJkoiKirpk0YZ9KRoppGSn8Bwgg94HTPUrjzM2NpbYWPVO/tq1a7ElJdHJbKZlly7Qu3xPtYKCAry9vUlOTub999/n+uuv57LLLmt0hVRMJlO517zj3A5u/+l2Mgszy20X7B3MghsWcFmzi9/PDg4OJjg42BWmChoZ1Z1Da0JAQAABARX7eN51112N7rtfV7ZsOUlGRjbZ2VYee2wtl1/enCuv9HzRlqo+Y1fgijF6Mc5xjlxyySbbLefLBPYAVwJeF9+02mSQwd/8zbWoNzwjiOAGbsCEiTDCAFCA8yC8qE7A3WNUIKgpmqn6K8uyowJfZaSmpjaoEEydTkdUVNRFt8kozCAxJxFJkuga2dVNll2C/AT1f98WUN8nNvtd9ZKwU1C9AJs2bSIqKoqHHnqIpUuXcvToUb7//nvi4+OZMGFCgxqHl8JkMjkKLKw6vor7V96P2Wout01MUAxLJiyhTWgbT5goaKRUZw51FuIirvooisKLL/5Obu45YmK8yc628vTTA7niihhPm+Z23DFG00hjL3v5nu9ZxzrOcY6pTKUvfRnAAMYwhra0deo5FWAt8HTJ82eBG+p4THv/01WswoKFTnSidUmTl6d4inQokanQG5iNKpCFF7VuuHMeFQhqgyuKfNXqiL179+bXX3+tdJ3VauWbb77h8ssvr5NhWsJmsxEfH4/NZqtyG3tbmtYhrR3tPTxOfkm4p59ncoycij1PslMnx6KkpCQWLFjAf//7XwIDA7nzzjsZPXo0Op2O3bt3895773Hu3DkPGew+AgIC0Ol0KIqCzWZDVmQ+2flJBZHavUl3fp78sxCpArdTnTlU4F4UReHf/17Lyy9vIihIvWc9fXofXntteKMU++4Yow/wAOMYx1d8xTnO4YUX5zjHClbwFE8xghHcz/0sYxmZZF76gJcgA3iCUpEKYK5i20uhoPAXf/EADzCJSaxgBRYsdKUrhag3kotRRekNwNky+16NEKnOQMyjAq3jirFZK6H61FNPsXr1ambOnMmBA2o7luTkZNatW8fIkSM5fPgwTz75pFMN9TRVFaqx4+ifGqmRsF+AggYiVIuKwGYDgwHalt5tTkpKAiA6OhpQ7+QMHTqU++67j+DgYNLS0vjvf//LX3/91SCq5NlsNpKTkx2Fy+xERUVhNBrx8/MDQCfpmD9uPi2DWzq2GdZqGD/c/AORfuXzyWtMbi6MG1f6t3p13Y4naDRcag4VuA9ZVrj//l95++2/Adi1K4fw8HAefNDzuamexNVjdCxjaUUrbuZm3uRN2tCG93mfZ3mW4QwnmGC2s53XeZ1ruIb7uZ9TnKrxeRTgN+AmYAOgB2YAfwOTangsM2Z+4icmMYmHeIitbEWHjuEMZz7z+ZIv6UIXQA3RSwAKgS01tlpQHcQ8Kmhs1Cr0d/To0Xz55Zc8/PDDfPrppwBMnToVRVEIDAzkq6++YvDgwU41VOvYhWq3Jhpq12FvTePn+VyjOmGfmFu0gDLtE86fV8szXBgK07JlSx5++GGWLl3KoUOH+Omnnzh16hS33HJLvfYUPProo2RmZvLqq6868ndBzdGVZRmzufReeZhvGEsmLuG6r6/jmjbXMGfEHAw6JxQdsdlgx47S56mpdT+mQCBwG1arzPTpy1m4UP3NkiS47rr+TJvW+xJ7CmpCAgnMYx796Mc4xgFwC7fQj360pz1HOMIXfMGVXElH1P7miSSyruTfYQ6zk52EEOI4ZhxxhBFWbtmFZABzUAUqQHvgxZL/a0IGGSxlKd/zvcO764svN3ADt3ALTWkKQAEgAT6ono8XgERELqpAIHAOtb5ynTZtGhMmTGDt2rUcP34cWZZp06YN11xzjduKH2gFi83CodRDAPRo0sPD1pTBIVTruUfVLlRble9Na/eoVpaz4efnx+23387mzZtZuXIlsbGxjp6s9VWshoWFkZmZyfnz58sJ1cTERCwWC5mZ5UPFWga35LepvxHlL4ovCAQClYcfXuUQqXq9xFdfjWfKFA3dYG0g/MVfrGY1+9jHWMaiQ4cBA+0vIhmb0YzbS/4lkshBDhJIoGP9a7zGQQ4yhzkMY1i5fe25qG8A2ahe1LuAO6FGLcTOcpb5zHfknwJEEcVkJnM91+NPaWrTP8ArwEDg8ZJlTUv+BAKBwBnUSqjaL/b9/PwcfSwbMpIkERMTU+XF/rH0YxTbigk0BdIiSCPeS2sBmNPUx74asam22IVqm/K5lReG/l6IJEkMGjSIjh07Eh4ejizLLFiwgLZt2zJw4MB6J96io6OJi4tzvG6Ac7nnKFbUHoPe3t4VXlN0QOXvTa0xGGDEiNLnzZo59/iCBsml5lCB+/i//+vHt98eJCfHzLff3sj48Z0uvVMjoK5jNIEEcsihO2r6z0QmcoxjTGYyukqyrMIJZwYzqmxN06zkn51iirFhQ0FxnANgC1uII4OdXM1feAOVe1EXoIbjTgBGXeR1ZJPNClYA0IUuTGUqwxiGHr1jmwLgfeD7kuebgQcQeaiuRsyjAq2jmaq/zZo146abbuLmm2/myiuvdLZNmkOn0xEWFlblekd+apPu2plA7IWUvELBqJHiTrVBlkuFauvW5VZVFfp7IRERamueAwcOcPjwYQ4fPkx8fDw33XSTI6+zPmB/nXahejDlIFN/nIpRMtLOux2hoaEuqbhWDn9/+Oor155D0OC41BwqUDGbzaxdu5YBAwa47P3q3DmCtWunkZycz6hRzq0wW5+p7RhNIIHP+Zw1rKElLfmGb9Chw4SJF3mxyv3sQrW6eOHFV3xFKqnlxO1XfMs3TMLKSfzxZjyZvEwrIi8IDz5MPmsooDvegBr1ZsbMKlaRTTa3czugitPpTGcgA+lGNyTKX9PYvaj2MoWiL6r7EPOoQOtopurvkCFDmD9/PoMHD6ZFixY89thjbN++3dm2aQabzcaRI0eqrGZVVqhqhoYS9nv2LC0LC2lnteJTppBSbm4u+fn5ADRp0qRah+ratSvjx4/HYDBw6NAh3nvvPRISElxhtUuwe47Pnz/PplObGP/teJLzkolX4olrEUdwcLCoBijQJJeaQwUqf//9N3Fxcfz666/IsuyUY+bkmLFayx+rV69oIVIvoKZjNIEEnuM5buZmVrEKGZkYYsgjz6V2RlC+J/pALqMzBzFxigj+xQ5mMLakEFPZ6sEDScbEC3QixbHvAQ7wKq/yKZ+W6+d6P/fTne7lRGoBau7rTFSRGg18DDyJEKnuQsyjAq3jirFZK4/q119/TWFhIb/88gvffvstH3/8Me+88w4tW7Zk0qRJ3HzzzfTs2dPJpnqWInt7lErYl1JSSClSQ3k+BSVCtb6H/R4+zKSsLOjatVyYqd2rGBoaWu1eqZIkMWDAAGJjY1m0aBFpaWl88sknXHPNNQwZMsT13sg6Yheqf2X9xRfLvsAql1b/PWg4yJa8LdzFXZ4yTyC4KBebQ+tKZmYm+fn5BAQEEBQU5LLzuJq+ffuSnZ1Nnz59nDIfpaUVcM01i+jSJYIvv7wBnU4jET8apTpjtKwHVUa9ATCEIdzDPY6iSK7EnovaHOgMTGMaU4DTnGUT17OWtRzhCNtL/s1hDn3pS0c6YuQPTtEXUNNoetObIQyhN70xXiSTVXhRtYMr51GBQIvUupiSj48PN910EzfddBP5+fmsWLGCb7/9lnfeeYc33niDdu3aceTIEWfaqklS8lNIzktGJ+noEtnF0+aU0lB6qB46RLLBgK1VK8LMZocotYf9VpWfejGaNm3Kww8/zLJly9i9ezerVq3ixIkT3HzzzZouBBYREcGR4CMcDDxIkC2oXJh5U7kpPUwaKuQlELiR3bt3s2/fPi6//PJ63cPb39+f66+/3inHSkrKY8SIrzh4MJVdu84TFeXPm29e7ZRjN0Y8LVDtfAO8DbQGFgFeqIWTWtGcViWFmM5ylvWs52d+5ghH2MQmfuM3znOet3mbnvTEF1/CCedt3q7yXBfmokYDzyMq+goEAvfhFBeSn58fkydPZtGiRfznP//B39+f48ePO+PQmsce9ts2tC2+Rg3dX2woQvXwYb4MDeXdc+cc4hTUlixGo/GS+alVYTKZuOWWW7jxxhsxGo0cPXqU9957j/j4eGdZ7lSsspVn/3iWw6GHgfLhFZeHXM515uvw0fl4yjyBQKAhTp/OZtCgLzh4UG0fFR3tzx139PSsUfWUykJ8hzCERSzibd52q0gFGA1EAiOAqvzjzWnO7dzOSEaiQ0c++aQSicwwUvBlOtOZylSWsazK8/wD3EKpSL0R+BYhUgUCgXupc2PFgoICVqxYwXfffcfq1asxm820adOGhx56yBn2aQKdTkfr1q0rDcWyC1VNtaVRFMhPUB/X5x6qsgyHD0NAAHh7l1s1atQoRo4cSXFxca0PL0kS/fr1o0WLFixevJjk5GR+++037rvvPu0UxQLyi/O579f7WH9iPTqdDlmWHflrM/vOpG9hXzae3OieYkoFBfD666XPr78eLhOXLoKLc7E5VOBc4uIyGD78K06fVnMOY2ODWL/+Ntq0CfWwZdqmsjH6C7/wMi971IOaAawEbkUVpsHAMsD7IvvYmcAEBqP2tH8JhRUoTKaQ+1GLCFZVcVgBPqE0F1V4UbWBmEcFWscVY7NWQrWoqIhff/2Vb7/9lpUrV1JQUEDLli156KGHmDRpEr169XK2nR5FkiQCAwMrXWcXqt2aaCg/tTgDbAWADnzqcfuQhAQoLISgIKgkD1Wn0+HtXZ2f64sTFRXFAw88wKpVqxgyZAiSJHHs2DGioqKq/NzdRWp+Krf9dBt7k/YCoNfrsVqtyLLMK0Nf4a7ed7FmzRqg8vY0Tqe4GObPL33eoYMQqoJLcrE5VOA8Dh1KZcSIrzh/Xi3o065dKOvW3UaLFvU3b9dd2MeojOwoItSPfhgwMIABbheoF/ZFjQRGlqyr7q9eeMk/9XEqkEpToulYRadTBVUMS6ji9FtE2xktIeZRgdbRTHuaiIgICgoKaNq0KTNmzGDSpEn079/f2bZpBpvNxqFDh+jcuTN6fWkvsWJbMUfS1DxcTVb89YkGvZdnbakLh9UwV/xd317HZDI5egKnpKTw1Vdf4eXlxS233EL79lU3aHclJzJPMOWHKZzOPu1Y5mXyws/bj3evfpcbe9xYbvvz589js9nKjVGBQAtUNYcKnMfu3ecZOXIRaWkFAHTtGsnatdOIiqrH7cncyFnbWV5JfwWfcB/e1b0LQCSRLGd5hUq7riYDtcLuhpLn7YGWLjxfAfABqgB+uGRZLPC4C88pqDliHhVoHc1U/b3jjjuYNGkSAwcOdLY9mqWyN/9I2hGsspVQn1CaBWjIc9lQ8lMPHVL/d/MdREmSCAsL4/z583z++ecMHTqUq6++2qU/DDbZxrbEbaTkpxDpF4le0jN9xXQyCzPLbRfmG8aCGxZwWbNSL+bAgQPp0aMHJ0+erN7JEhMhI6Pq9aGh5Sosl9vv1Ck1FNtOSgrs33/x/QQCXPMDJlDZtes8w4d/RVaWWhG0T59o1qyZSliY8IVVFwsWtvhtwVvy5jSnaYGaNuNOkXqhF1UP3AXcCRepyVs9/PAlnAj8K/HH7gWWohYtuQmq8LcKtICYRwWNjVoJ1Q8++MDZdtRL7OGY3SK7aSqnscH0UN29G1JTK+SnupqIiAgeeOABfv75Z7Zu3cqGDRs4ceIEU6ZMITg42OnnW3l8Jc+sf4b4zHhsig0UsMgW/L388TaUvvaYoBiWTFhCm9A25fb38/PD29vb0bLnoiQmQr9+kHeRXn/+/rB9e3nRWdV+L7+s/lW1n0DgBoYNG8awYcM8bYbHaNkymJiYQLKyirjiihhWrpxCUJB75836xilO8Q//cCNqZEoLWjAtdRpjY8bSQu/+2g6VeVFfLPnfGfjiRwR+2G8z2sN8AQYA01HzUIVIFQgEWqJaQnXTpk0ADB48uNzzS2HfvqGyP0X1JPWI0lAhJSgjVOtxISWbTQ39TUsDL/eHLxuNRiZMmECbNm344YcfSEhI4N1332XSpEl06tTJaedZeXwlU5dNxWwz42vwRa/TU2wrprCokKyiLIK9g/E2eNO9SXe+Gv8VkX6RFY6xbds2duzYQVBQEN26XSJXOiNDFZs6HRgq+fpbrer6jIzygrO2+wkEApcTGurD2rXTeO65jcydew3+/vU45cPFnOIU85jHGtagoNCXvrQsCay9JusaOsa4t4qvK72oVfEP8C7wDmruK8D9LjqXQCAQ1IVqCdWrrroKSZIoLCzEy8vL8bwqFEVBkqQGE6Kg0+no0KFDuWpWiqKwN7nUo6opCkpCf33rsUf1xAmwWFRh5O2tFvHxAD169KB58+YsXryYs2fP8sUXXzB48GBGjRqFoTLBVgNsso1n1j+D2WYm0CvQ8Z3yNngT5hNGemE6ucW5jG43mk/Hfoqfl1+lx/n777/5559/6NSpU/UrrhkMYDSqlZXLvrc6nbqsLBYLbNig/m80qttUFgbtws/IYoG9e6Emvc737XOZOYIaUtkc6ilkWebEiROcPHmSESNGaCsapgbIsoJOV2p7kyb+fPrpdR60SNuUFaj2Kr6DGYyupEufJ8aoq72olbEMNR8V4H/Acy48l8C5aGkeFQgqw2NVfzdu3AiAV4lny/68MeF1gVfvfN550gvS0ev0dI7o7CGrKkG2QcFZ9XF9DP1NS1P/1q9XVYm3t+qpk2W1CrC/P4RXXlLfVYSFhTFz5kxWrVrF5s2b2bRpE2fPnmXGjBl1+lJuS9xGfGY8vgbfChfLRr2RQFMghdZC7u1zb5UiFaCwsJDi4mLMZnPNjbBaITu79Hll+cBmM7z6qvp5FBWBJIGv+3LfLBbo31+NBBfUXy6cQz1FcXExq1evxmq10qFDB1q0qH+RJ0uW7Oejj/5h1apbCQioWBFdUEpVAnUGMypU8XXnGN0NPIb7vKh2Ekv+n0hp4SRB/UEr86hA4C6qJVSHDBly0ecNHVmW2b9/P926dXMU1NmfrIb9dgjrgMmgoQuFwnOgWEFnAm/3Vip0CsuWwccfw8mTqkIJC4Njx1QP3lNPwZ13wowZbjfLaDQybtw4WrduzdKlS+nevTs6nQ6r1Vprz2pKfgo2xYZeV3mRJm+DNxbZQkbhRQoflSE3NxdZlhtcNcC9e50jUt2c6iwoQ2VzqKfw9vamS5cu7N27l127dtU7oTpv3i5mzPgZRYGxY79m9epb8fFxtbypf9REoIL7x2gL1LBfd3hRAUduajSqF7Wfi88ncD5amkcFgsqQL4zIcwK1usIeNmwYzzzzDMOHD690/caNG3nllVfYsGFDpesbAvawX021pYHSsF+/FiDVw/CQ66+HjRtVkerjA1YrUydNwhIbS3RoKDT1bKmHrl27Ehsbi7+/P7Is88UXXxAREcG1116L0Vizi8VIv0h0kg6bbEOnr/hZ2WQbeklfaV5qY6Im4b5V0asX9NBYKrnAc/Tq1Yu9e/eSkJBAeno6YWFhnjapWrz33lYeeWSN43nnzuGYTHVLQWho1FSgugsF2A/YrxjCUENvW+J6LyrA3UAXYCCiL6pAIKg/1OoX7vfff+fuu++ucn1KSgp//PFHrY2qD9gLKWlOqNb3ir+LFsHx4xASAs8+C888Q7P+/aGjeoFx7tw5Pn7uOVq1asX06dM9YmJASXuW+Ph4jh8/zvHjx0lISGDq1KmE1yAsOdeci9lqpthWTKh3aLnwX0VRKLAW0CGsA/2bubBHsZdX+VDqC3NWQQ3zXbIEJkxQt/dw6NGHH0L3GnztvL1VkVrD+wiCBkxwcDBt2rQhPj6e3bt3M2LECE+bdElef/1Pnnmm9Obvv/41gP/85+p6m2PrCk5wglu4RVMCFVSR+gzwG/AWcFXJ8nZutCEAGOnG8wkEAoEzqPWt2Iv9OMbFxTku5hsihZZCjqYdBTQsVH3rVzgbACtWwNdfq49fesnhPf3h99/J27aN0aNHk5iYSEJCgiYuztq2bcv06dP59ttvOXfuHO+99x4TJ06kZ8+el9x357mdzPx1Jv5e/mQVZZFemE6QKQi9To9NtlFgLcCkN/HqsFerDA2uE1Zr9ZfrdBAVVVpoqbJtqjqeC+jeHRpRC2eBi+jduzfx8fEcPnyYgQMH4q3R2HBFUXjmmQ3Mnr3ZseyFF63DVNsAAQAASURBVIbwwgtDNDEPepo88vDHH4BWtKI73QkkUBMC1Y6E2vZFD5zzsC0CgUBQn6i2UF2wYAELFixwPH/11Vf57LPPKmyXlZXFvn37GDNmjHMs1AA6nY5u3bo5CuccTjuMrMhE+EXQxK+Jh627gHx76G8986ju2wezZ6uPZ8yAYcPUokozZhCXnEx6bi5Dhgxx9AqNjo72oLGldOzYkUceeYQlS5Zw8uRJlixZQnx8POPGjasyFPh4+nGm/TiNImsR3gZvgr2DyS3OJd+SjyRJ6CU9HcI68OqwVxnTrvLv0datWzlz5gxXXXWVY1lAQMClizsZDGpBqry8qqv0+vtDaGj5ZaGhtdtPIKDiHKoFmjZtSr9+/Wjbtq2mReojj6zm/fe3O5a9+eYI/v3vK+t8bLPZTFxcHN7e3rRp0+bSO2iMAgqYwxz+5E+Ws5xAApGQ+IiP8KLmUR/OHqMZQAHQvOT5DGAU0NYpRxc0RrQ4jwoEZfFY1V+AgoICUlNTHc9zc3MrGCRJEn5+ftx33308//zzzrNSAxQXFzsuZvYlq30vukd2194d7frYQzUlBf79bzUvdehQsIeVh4erovWNNxyb2oVqVFSUJyytlKCgIGbMmMG6devYsGED27Zt49SpU9x66600aVL+Rsb53PNM/mEyWUVZjmXeBm8md53M5G6TSStII9Ivkv7N+l/Uk7p69Wri4+Np2bKlY1m1ktg/+QQiItT3eMyYyqv3hoZW7IXarBls3672Sa2KyvYTCEooO4dqAUmSuOKKKzxtRpXYbDL33fcL8+aVVhL78MMx3H//ZU45flFRERkZGfVWqHrjzXGOk0sum9nMGNSberURqXacMUbL9kVtBnyB6kn1QohUQd3R2jwqELiaagvVmTNnMnPmTABatWrFe++9x7hx41xmmJaQZZmjR486Kq05hKrWwn6thWBOUR/XF49qcTE89hikp0ObNmrI70XuyJw/fx7QjkfVjl6v55prrqF169Z8/fXXJCUl8cEHH3DDDTfQt29fALKKspj8w2TO5ZYP/hraaijvjnoXo776SZTR0dHEx8dz7lzpsfLz8y9e9ff8efjxRzVM97PP4PRp+OKL6r/IZs2EEBXUigvn0MaAoihs2LCBdu3a1aqysKJAenohADqdxOefj+OOO3o6zb7iksgIk0lDVesvwilOsZjFPMIj+OKLDh1P8iQmTE4J8XXGGL2wL2qTkmX1sP6+QIM0xnlUUL9wRdXfWvloT5482WhE6oUoiqJdoVpwRv3fGAzGSvphag1FUftzHjqk9u+cO/eS/Tm16FEtS7t27Xj00Udp164dxcXFbNu2DVmWKbQUcvtPt3Ms/Vi57XtF9+Kz6z6rkUiF0teflJTkCPm95F3WL74on0t6++01OqdAIKg+hw4dYv/+/SxfvpyCgoIa728w6Pj664lcd117liyZ4FSRCqVCVet9GU9xiud5npu4iWUs43u+d6zrQQ9N5KEqqIWSbkIVqXrUUN8FCJEqEAgEdaFaHtXTp9W8R/tdYfvzS1Hf+tNVhzM5Z8gqysKoN9IhvIOnzSlPfQv7XbIEVq5UPahz5lzSW1dYWEhubi6gXaEKaq7oXXfdxaZNm+jRowcyMrcsvoVtqdvKhcu3CW3DwvEL8TXWvFlAWaF67bXXYjKZ8PHxufhO//d/6g2Bzz9Xqyo3sn7IAoG7yM3NdVS+HzBgAL6XuAFXFSaTgeXLb3FJionZbAa0K1RPcYrP+ZzVrC5Xxbc/rqmCng2k1KI0+IVeVHf1RRUIBILGQLWEasuWLZEkicLCQry8vBzPL4XNZquzgVrBHmaxP1ltS9MpvBNeeo39wBfUo0JKf/8N772nPp41C/pduv14WloaACEhIZrP0dDpdFx11VUoisLMn2ayIUG9jPH29sZoNNLEvwlfT/yaUJ/aFR8qK1RbtWpFQEAAKSkpF98pKAgeeEDN+z13DrSWXy1o0DSWUDVFUVi7di3FxcVERUXRu3fvau2Xm2tmxoxfeO21YbRuHeJY7qo6CFr1qFYlUF1ZxXcfcJdOR1GrVqxDDdmtDvZc1CxUL+pdwJ24py+qoHHSWOZRgcBOtYTq/PnzkSTJUcXU/ryxoNfr6datG4B2w36h/vRQPX0ann5abXUybhxMmlSt3exCVcve1AuZs3kOy+OXo9frsVqtFBYWYpSNfHnLlzQPbH7pA1SB/T3Iy8tjy5YtbN68mSuvvLJ6P2JeXlCmCJNA4GrKzqENnQMHDnD69Gn0ej0jR46sVhXEzMxCRo9ezLZtiWzdepZNm+4gJibIpXZqTah6QqDaaQcokoTJ15dMLi1UM1AF6vqS58KLKnAHjWkeFdRPXHEjpVpC9Y477rjo84aOoijk5uYSEBDAvpR6IFS13EM1Px/+9S/IzYVu3eDJJ6vt2atvQnXernl8sP0DJEnC19eX4uJirEVWhucMZ/Wi1YRPDadZLYsTmUwmQkNDycjIIDs7G1CreCqK0qhuIgnqB2Xn0IY+PrOysgC48sorCa1Gu6aUlHxGjlzI3r3JAOTkmElNLWg0QtUTAlUB/gQGohbq8AFQFGyyjKLTXfQ3aR1qqG8WwosqcC+NaR4V1E8URXH6MZ3a8Ka4uJj8/HxnHlITyLLMiRMnyC3KJT4jHoBukRq7q6Uo2u+hKsvw3HNw8qTaIuU//1G9e9WkRYsWDB8+vF7cUVx+ZDnPbyzfosnb5M37I9+nU2An0tPT+fDDD9myZUutv9j2ysc5OTkAZGRkuKTimkBQV+xzaGMYn4MGDWLSpEn07NnzktsmJuYwZMiXDpEaGenH77/fTu/erq9qroWqv1as3Mu9rGQlMjKDGcwiFjGXuS71oj4FzAKWllkWgZq3e6kxug9VpLYHFqIWTRIiVeAOGtM8KqifaKbq7zfffMOjjz5abtlLL72Ev78/wcHBjB8/nry8PKcYqCUOph5EVmSiA6KJ8NNYLT9LFlhzAQl8YzxtTeV88gls2qSK07feUvukVoOWLVvSrl07evbsyR133MFllzmnj6ArySis2G/0P1f/h9uuvI1HHnmELl26YLVaWb58OQsXLqxVVVC7Z9kuVCvl77/V9j9xcTU+vkAgqB3R0dGXDPlNSMhi8OAvOXJEjRRp3jyQP/+8k27dqpshWXtsNpujhoS7PapnOOPwnBowcCu3uk2g2umN2te0urcIy87O96OK3AWIUF+BQCBwNbUSqm+//XY5z+lff/3FSy+9xDXXXMOjjz7K6tWree2115xmpFbYn6IWUtKcNxVKw359okFrRZ4A1q6F+fPVx88+C126VHvXSZMmcc8999Ckiesv4JzFnb3u5KNrP3K0nXly4JNM7jYZAF9fX2677TbGjRuHXq/nwIEDfPjhhzUuPtasWTMiIiIunhPw0UdqdeXBg+Guu1SvtkAg8CjHjqUzaNAXnDiRCUDr1iH8+eedtG8f5pbz22w2wsPDCQ4Odmtxljd4g4lM5Hd+dyybylSXC9QU4HCZ5zeielNvucR+mcATwIOAfeb0BqYgvKgCgUDgDqqVo3oh8fHx3F6mB+OSJUuIiorixx9/xGAwIMsyP/zwA7Nnz3aaoZ7G29ub/SdUodqjSQ8PW1MJ9rBfLeanHjsGL72kPp46FcaMqdHuycnJ2Gw2wsLC6k1zeoAbOt5AqE8of576kwf7PVhunSRJDBw4kNjYWBYvXuwohGQ2mzEajdUqwHL11Vdz9dVXs2bNGtavX4/BcMHX+ehRWL++9LnJpLYCEgg8gNYrdbuLAwdSGDHiK5KT1Zu9HTuGs27dNJo1c1/vay8vLzp16uS289kJIggZmX3sYxjDAJBwXa6dAvwMvA0EAd8Avqh36CurDqC7IO+vCPgbMAMHAQ3eohY0MsQ8Kmhs1Eqoms3mcl+W3377jdGjRzsulDt37sxHH33kHAs1gF6vp32H9hzYdgCAbk00+HOl1R6qmZlq+5miIrj8cnjwwUvvcwFffvkl6enp3H///bSsZ9VqB8cOZnDs4CrXx8TE8Oijj+Ll5YUsy3z11Vfo9XomTZqEn59fjc4VGRlZ3jtiNkP//rBtm/r8vvtq8xIEgjqj1+vp2NF1HrO8vDzMZjM+Pj617lnqLn755ZhDpHbv3oS1a6cRGVmz73p9wF4kaSxj6YfafmwKUxjCEDrhepGcArwK/FXyvBWQiypUK0OSJLx9fDCXWRaNWs23GaCxrumCRoir51GBoK64IkKnVu6VVq1asW7dOgB27NhBXFwco0aNcqxPTk7G39/fORZqAFmW2ZOwh1xzLiaDifZhGsxMcfRQbelRM8phtcITT0BSEsTEwOuvg+gBVgGTyYQkSZw/f56TJ09y5MgR3n33XU6cOFGj4+Tn55dPZO/eHX78EX75Ra203F2DlaoFjQJZlklPT3dZEZDt27ezcOFC9u3b55LjO5MnnriSWbMup1+/ZmzceHuDE6mnOMXzPM9N3MRKVvI//udYF0igy0WqAqwAbkIVqV7AQ8B8Lt52RlEUrFYrDygK28ssH4YQqQJt4Op5VCCoK5oppnTvvffy3Xff0b17d0aOHEnz5s0ZO3asY/2WLVvoUoMcRK2jKAp/HPkDgC4RXTDoauWIdi1a9Ki+9Rbs2gW+vvD22xDovtA2d2G2mpm+fDrbzm6r87GaNWvGAw88QEREBNnZ2Xz66aesX7++2l/87OzsyisI9+6tClWBwEMoisKZM2dcUroewGg04uPjUzH8XYNIksRbb41k48bbCQ318bQ5TuNCgWqv4vsYj7nNhhTgYeBlIB81VHcJcBvVu9gpLi6mGDVEWCDQGq6eRwWCuuKKsVmrX/UHH3wQb29vVq5cSZ8+fXjiiSfw8VF/cDMyMkhKSuK+BhZmeDznOKDRQkqKDPln1MdayVFdtgy+/17tR/fqq9C6tactcjo22cYDKx9gddxqNiZs5JNrP+GattfU6ZhNmzbloYce4scff2TXrl2sWbOGEydOcMsttxAQEFDpPnv27CEhIQGLxVKncwsE9ZVBgwYxaNAgT5tRKb/8cgw/PyNDh7ZyLFN7KzeMcjxV9UG9h3vcEuIL5XNR81G9qPcBU6n+3fiWisJZReEuReFu0aNSIBAINEGtbz/fc8893HPPPRWWh4aGsmPHjjoZpUXictT2Hj2iNFhIqTAJFAvovMAnytPWwO7d8MYb6uOZM9WKsw0MRVF4ZsMz/Hr8V0D1rN614i7eG/UeEztPrNOxTSYTkyZNom3btvz0008cP36cd955h8mTJ9OuXbsK28fExLBv3z7CwtxTMVQgEFSPpUsPMmXKMkwmPWvXTmPAAI22DqsFWhCoUDEXtRvwAtCyhseZqyjsiI/nii5dEAkqAoFAoA3qHCd16NAhTp1Sw05jY2Pp3LlznY3SGjnmHM4XncdkMtE1squnzamIPezXNwYkD1d1TUqCxx8Hmw1GjIA77/SsPS5i7t9z+WrvV+WW+Rp96RDunGwmSZLo27cvMTExLFq0iOTkZObNm8ewYcMYMWJEuYR1Ly8vJElSiy9ZrbBhAwwbJvKBBZqiqoiAhsqCBXuYPn0Fsqxgtcp8+eWeBiFUtSJQAVYCb1B7L2pZvICmNSxgJxC4m8Y2jwoEtVY1y5cvp02bNnTr1o2xY8cyduxYunXrRtu2bVmxYoUzbfQ4h9MPYzKZaB7YnFCfUE+bUxFHfmqsZ+0oKlJzITMzoX17eOEFNfS3jthsNoqLizUT2vrV3q94+++3yy0z6o18cf0XTr+R0aRJEx588EH69euHoijExcVV2Obs2bNYLBYKCwvRr1kDt9+uerEXLFA/E4HAw+j1etq0aePWnp3OpKY9jj/66B/uuGM5sqzm60yf3pOPPrrWFaa5nXnMK5eDupCFzGWu20UqQAa1y0WtjPo+RgUNHzFGBVrHFWOzVh7VlStXMnHiRGJjY3n99dcd/dgOHz7Mp59+yoQJE/jll1/KVQKuz2w+vZmk3CSGxA7xtCmVU+DZHqrHjx/nx2XLGJ+URLujRyE4WC2e5OOcQiFxcXFkZmby4osvsnjxYqccs7b8euxXnlr/VLllkiTx4ZgPubLFlS45p5eXFzfeeCNt27YlNjYWvV7Pvn378PLyomPHjhQWFmK1Wtm5YwepP/5IBMDJk/Dyy3D99SD6rgk8jCzLpKSkEBkZWa0ewVrBarWyefNmjh07xrBhw/jzzz8ZPnw4LVpUPde+9dZf/Pvfax3PH3ywH+++Owqdrn7mPZ7iFF54EU00AHdzNwUUcDd3u12cKkAmYL9dPAUIAUZTh7vuJdTXMSpoPIgxKtA6rqj6Wyuh+sorr9C9e3f+/PPPcr0ex40bxwMPPMDAgQN56aWXGoxQ3XV+F+mF6TQPaO5pUyrHgxV/FUVh5cqVHNi9G1NWFg/p9UhvvgnR0U45vs1mIzMzE51OR05ODjabzWN3E/868xf3r7y/QlWz14e9ztj2Y6vYy3n07NkTgNTUVL777juKi4u56qqrsNlsKIpCRH4+HDmCEhCABDB5snrTQCDwMIqikJSUREREhKdNqRF6vZ6zZ8+Sn5/PmjVrOH/+PF5eXkyZMgXpgmgRRVF4+eU/ePHFPxzLnnzySl5/fXiFbesLS1jCu7zLKEbxMi8DEEssb/P2JfZ0PunAS8BZ4GvAhCpOneWnrq9jVNB4EGNUoHVcUfW3Vrdk9u3bx+23315OpNrx8/PjjjvuqBf97KqDrMgcT1cr/mqyfyqUEaot3X7qY8eOcXjPHryLijhsMnHszjvVdihOYt68eUiShKIo6PV65s2b57Rj14RDqYe4c/mdWGzlw49nDZjF7T1vd6stwcHB9O3bF4Dff/+d3bt3I0kSKb6+PDZ4MIdHj4aICKik2JlAIKg+kiQxePBgBgwYQGZmJiaTiYSEBEddBjuKovDEE+vKidRXXx3K7Nkj6p1IVSi90OhJT2RkCihw5KN6CiNwHEgC9nvUEoFAIBC4i1p5VL29vcnIyKhyfUZGBt71PNwwrSCNtII0ErISyDZnIyFRaC3kSNoRAMJ9wwn3DfewlYDNDEVJ6mM3e1QVRWH199+TlZODpNPh6+XFarOZ9orilIszm83G6tWrHY91Oh2rV6/m7rvvdrlX1Sbb2Ja4jZT8FGRF5qXfXyLXnFtum6ndp/KvAXXsT5qYCBf5LhEaCs2alVtkNBq54YYb6ODvz/qlSynOykKSJGRZRpIkvk5P56X589HVg56SAoHWiYmJ4c8//8RqtRIUFER2djZbtmwhNjbWMc/t3p3E22//7dhn7tyRPProAE+ZXCvsRZKCCOJfqPNaZzqzjGW0wDNpJRmoob0SEAi8VvK81cV2EggEAkGDoVZXssOGDeO9995j1KhRDBhQ/sd427ZtvP/++4wcOdIpBnqKZYeX8enOT8kqyuJ83nl8DD7M2TLHsX5GnxnM6DPDgxaWUFDSP9UYCMYgt5762J49HN61CxQFSafDFBLC4cOHOXbsGB061L367bx581AUBVmWURTFEVIwb9487r333jofvypWHl/JM+ufIT4zHqtspdhWjCRJBHgF4G1Qb8CMaTeG2cNn102QJyZCv36Ql1f1Nv7+sH17BbFKYiKdbr+dtpmZlRaZsv32G7qQkMr3FQjcjCRJhIaG1jvvIsCpU6dISEjAx8cHSZLw8fFxeFVbtmwJQO/e0Xz55fXceedyPvroWmbM6ONZo2vAhVV8vfDibu4mCPX3xBMitWxf1H8D9sQK58XqVKQ+j1FB40CMUYHWccXYrJVQffPNNxkwYAADBw6kX79+DlFy9OhRtm/fTmRkJG/Y+2jWUyZ0msDg2MHkFefxW9xvLNy/kGcHP0vH8I4A2vCmAuSXKaTkxslLsdlYPXcuFpsNSa8HLy9M3t7k5OSwevVq2rdvX6cBW9abqtPpHB5DwKVe1ZXHVzJ12VTMNjM+Bh/MVjOyogrlrKIsgr2DuarlVXw45kP0ujqePyNDFak6HVTm/bRa1fUZGRXFZkYGSl4eZpsNWyVFFWSbDUNeHlJl+woEbkan0120AJFWURSFLVu2YLFY8PX1BdTiZoWFhRW8qtOm9WDAgBjattVgZfhKuFibGbtI9QQX9kX9DTUP1dW/bvV1jAoaD2KMCrSOK4p81UqotmrVin379jF79mxWrVrFt99+C6h9VB9++GGefPJJIiMjnWqouykb2utr8OWbfd/QPrS9Q6hqhvwE9X83t6Y5Nns2h3Ny8AOKvLxQUAeon5+fU7yqdm+qoigOoQq41Ktqk208s/4ZzDYzgV6BFFoLsSpW9dyKmrtlkS18Pu5zTAaT805sMIDRWH6ZLENxsSpW//wTYmIqFEayWixIsoxOkrDpdMhlbwzIMlaLhQuO6nEsFti7t2ZdcxpIunujRpZlzp49S/PmzetVtcoLvamg3jH29vbm0KHj5byqQL0QqVrqg1qWsl7UC/uiuuMWbH0do4LGgxijAq2jiaq/NpuN1NRUgoODeeedd3jnnXecbpTWUBQFq9XqkmpWdcbuUXWjUFVWrmT15s2YfXzwDgtDzs8HRcFsNmMwGDCbzXXyqtq9qWW9qI5zlwhXV3hVtyVuIz4zHl+DL5Ik4Wv0RVZk8i35SJKEHvVch9MOc0XMFU47b6VYrZBbkhP7wgswYAD06uVYLZcIUR+rFQCzwYBZry8nVq1WK3pZrnPbBmdhsUD//rB7t6ctEbgbRVHIyMigWT3y7tu9qWazGS8vL0eIvSwrnDmTi6KYWbjwF5599v/qRSieVgUqVPSidgVewL25qPVxjAoaF2KMCrSOR6v+KorC008/TUhICM2aNSMwMJDx48dftKhSQyHcN5yJLSdqJ9y3LPYequ4qpHToENZXXyVdr8fk60uRweDwfJrNZoqKijCZTKSnp2MtEVE1JT8/39FyRZKkcp4MnU6HoijYbDby8/Od+cpIyU/BptjKhfT6e/kT4BWATtIR7BOMrMik5Kc49by1obi4uNxzk9VKoNmMoYywVxSlwnaeZO9e54jUel6nTVBPsNlsZGdnYzKZKC4upri4mKKiIs6ezaS4uBiLReLw4bOkpuZe+mAe5DSneZ7nuYmbWMlKZGQGM5iFLGQucz3uRV0B3IwqUr2Ah4D5iIJJAoFAIKiBR/XLL79kzpw5NG/enFGjRhEfH8/y5cuRZZnly5e70kaPo2mham9N4+sGj2p6Ojz2GMbiYma1bk3+44+DTsd//vMfAKZMmeK40+fv74/xwnDWahIYGMhLL73EyZMnWbx4MYqiEBoayqBBgwgNVUPrWrRoQWBgoHNeVwmRfpHoJT022YZOX3oPx9foi4/RB6vNil7SE+nn+bB2b29vZJMJyWZz5Cbb9Hq8AwJAkpCsVvRWKzoNqbqahPtWRa9e0KNH3Y8jEFwKg8HAtGnTKCwsBCArq5C77/6ZgwctgBF/fy++/PImIiOdOw85m2McYyUrAe14UEEbXlSBQCAQaJtqC9WPP/6YXr16sXnzZnx8fAB4+OGH+fDDD0lLSyM8XIMizklIkkRUVJT2wruKs8GSrT72i3HxuYrh3/+GlBRo2ZLg2bMJ9vcH1OIiAE2aNHFaSEqvXr3o1asXY8eO5bXXXqOwsJAhQ4aUywdzNv2b9adNSBuOZRwjUBdY/vNWoMBaQIewDvRv1t9lNjgwGiEkRH3fFyyAThUvLHU6nbqNwYCCOk6NOl1pPpcLcgWcyYcfQvfu1d/e21sVqbW8/yHwIJqdQy9BQEAAAQEBJCXlMXHiDxw4kA7oCQ/35aefptKrV7SnTazAaU5zlrNcgZqeMIxh3MzNXMd1mhCo9lzUuUAe5XNRPZmmUF/HqKDxIMaoQOt4tOpvfHw8zz//vEOkAtx///188MEHHD9+vEELVZ1OR1RUlKfNqIjdm+rdBPQu9JwpCrzxhlrVxt8f5s5V/3cDXl5e+Pr6OrwarkSv0/Pa8NeYumwqOcU5+Bp80etUD2uBtQCT3sSrw16te7XfC7FaobBQLapkMJRWb5ZltSJwTEzl8a6SpG4jy0igZtDabKXH1Djdu8PAgZ62QuAONDuHVoOzZ3MYPvwrjh1LByA62p91626jc+cID1tWkZ3sZCYzCSaYFazAG2906Hicxz1tmoM84L8l/2vJi1qfx6igcSDGqEDruKLIV7WPmJmZSURE+R9muzgtckZMn4ax2WzEx8djs4sArVDgpkJKS5fC8uWqaHr9dWig5dEVRWFU21EsmrCI9qHtMdvM5BbnYraZ6RDWgUUTFjGm3RjnnTA0VBX8xcVQUAA5OWormoICdZksq+tDK6kkat+3pDqwYjZjKypCMZsvva9A4GY0O4deghMnMhk06AuHSG3RIohNm+7UlEg1Y3Y87kEPooiiK13JpWLubG5uLsePH+f8+fPuNBGl5A8gAHga7eWi1tcxKmg8iDEq0DquGJs1qvrbmMMNcnM1WDDD7lF1pVDdsQPeekt9/MADcEXV1W7tIcD1lc2nN/Pcxue4r+99/HXXX+xO2k1KfgqRfpH0b9bf+Z7UZs1g+3b4739h3rzS5d9/XyowQ0Mr74Nq37ekmJlss3H8+HHatWtXWgm5qn0FAg+gyTn0IsiywvXXf0NCQhagtp5Zt24asbHBHrXLzmlOM4957Gc/S1mKoeTfEpbgT+URL/n5+SQlJREaGkp0tHvClu25qGOBkSXLrnLLmWtOfRujgsaHGKOCxkaNhOqTTz7J7NmzHc/tyvnuu+/Gz8+v3LaSJLF3714nmCioEkchJRd5OM+dgyeeUL1zo0fDtGmVbqbX67HZbPj6+rrGDjfx8Y6POZZ+jFlrZjF782yevPJJJneb7NqTNmumhuvaEy9btoQhQ6q/r12I2mwUAXTrBk5s2SMQNFZ0Ool5865jxIiFtGgRxLp104iODvC0WQ6BWrbNzHa2O3JSqxKpAGaz6n11503FX1ALJsUBQ0FzvZ0FAoFAoF2qLVQHDx5cqUc1MtLzFVAbLa7soVpQAP/6F2Rnq4V8nn22NHfyAuw3LFzVCqVly5aEhoaWy492NodTD/N7wu+O56n5qdgUN4XXvPkmPPkk7Nyp5qoKBAJN0L9/c9aunUbbtqGEh3v2RlxlArWmVXztc7SrhaoCjqJutwFngWkIkSoQCASCmlFtofr777+70AxtI0kSMTEx2gp9VmTX9VCVZXjxRTh+XA0fffttMJkuuVtBQYFz7Shh0qRJLjluWT7Z8Um552G+YdzY+UaXn9dBaChcfXWtd9fkGBUISnD1+MzMzCQ/P5+AgACCgoJqfZwjR9Lo0CGsnJ2XX97cGSbWGmcIVDuuFqr2ir6/AB+iClMD8LxLzuZcxBwq0DpijAq0jker/jZmdDodYWFhnjajPEXJIBeDZACfps499vz5sGGDWoH2rbegml5zV1T7AkhOTsZmsxEWFoapGoK5ppzPPc+PR34st+zOnnfibdBOD9JLockxKhCU4OrxuXv3bvbt28fll1/O5ZdfXqtjrFkTx/jx33LvvX2YO/caj18MOlOg2nGlUE0BXgO2lDxfAUx0+llch5hDBVpHjFGB1vFo1d/GjM1m48iRI9qqtGYP+/WNAcmJH+Mff8AnJd7Fp56qVqNLe/GewEDXNL7/8ssveffdd11WqfLz3Z9jlUvbuZgMJu7oeYdLzuUqNDlGBYIStD4+f/rpCOPGfUNhoZV3393GokX7PGZLGmk8z/PcyI2sZCUyMoMZzEIWMpe5deqFaheqzrzhp6CK0ptRRaoXakXfG5x2Bveg9TEqEIgxKtA6Hq/625jRXAseR8VfJ4b9xsfDc8+pjydNguuvr9Zu9oEpy7LzbHETueZcFu5bWG7ZLV1uIdSn/rV10dwYFQjKoNXx+fXX+5k27UdsNrWBysSJnZg0qavH7DFg4Hd+dwjUunhQy6IoitM9qhd6UbsAL6KdljM1RatjVCCwI8aooLEhhGp9xdk9VHNyYNYstYhS377w6KO1OEQOISEhdTblxx9/ZN++fYwaNYr+/fvX+XgX4+sDX5NrLi33LkkSM/rMcOk5HWRnw//+p77fffuCizzSAoGgcj7/fBf33PMzSkmTz2nTujN//vUYDO4LNjrNadaxjulMByCYYJ7kSVrRyikC1U7ZYndGY93KGtlzUecCeahe1PuAWwFRc1wgEAgEzkII1fqKM3uo2mxq1dnERGjaFObMUfNTPcShQ4eIi4uj0MUVcC02C5/t+qzcslFtRtEqxE3+gJ074d13S5//+CO4WJgLBAKVDz7YxkMPrXY8v+++Pnz44bXodO7LTc0jjylMoYgietCDPvQBYAxjnH6ust7UuuTfNjQvqkAgEAi0ixCq1UCn09G6dWuXFQuqFc7sofree7B9O/j4wNy5EBxc92PWEqvVyokTJwBo27atS8/16/FfScxJLLds5mUzXXrOcuzYUfpYktQ2QLVEk2NUIChBa+NzzpzNPPXUesfzWbMu5623RrqlgFIqqUQQAag9T8cxjiSSCCbY5ecODAzEUMubkA3di6q1MSoQXIgYowKt44qxWSehmpiYyKZNm0hJSWHixIk0b94cm81GdnY2QUFBjiI79R1JklxWKKhW2IqhsKSwUF09qj//DEuWqI9feglcLA4vxZkzZyguLsbX15emTZ1czbgMiqLw8Y6Pyy3r07QPfZv2ddk5K5CUpApURYGOHesU+qu5MSoQlMHV43PYsGEMGzasWtu+//62ciL1uecG89JLV9VJpO4BvgEmAz2q2KZsFd8FLHCE9T7GY+jcUNcwICCAsB49+AzoB1xbw/1/Bl4uedwQvahiDhVoHTFGBVrHFTd7a/XrqCgKs2bNolWrVtx6663MmjWLY8eOAZCXl0fLli354IMPnGqoJ7HZbOzfv187ldYKzwIKGPzBqw45oQcOwOuvq4/vuQeqeaHnSuLi4gDVm+rKu4Z/nfmL/cn7yy27v+/9LjtfpcydC4cPqzcKnnyyTofS3BgVCMqgpfE5cWInWrUKBmDOnOG8/PLQWv+4FqF6GO8B1gHfV7LNaU5XqOL7N3871rtDpNr5EfgVmF+LfUcBHVEr+s6nYYlU0NYYFQgqQ4xRgdbRTNXf//znP7z33ns88cQTDB8+nKuvvtqxLigoiAkTJvDDDz/wyCOPOMtOj6OpiaFs2G9t716kpsJjj4HFAlddpQpVDWC/4eHqsN8Lvaktg1syss1Il56zUgID1fffCWhqjAoEF6CV8dmsWSDr19/Ghg0nueuu3rU+zh5UD+PpMsvKvkJX9EGtDTZKQ3PvAHYBt1djvxRgMaow1aOG+n5Fw+5pp5UxKhBUhRijgsZGrYTqZ599xm233cbrr79Oenp6hfXdu3dn1apVdTZOUAX5daz4W1ysitS0NGjdGl5+GZzgvQwICKjzMewe1Xbt2tX5WFVxNO0oG05uKLfs3j73otc1jFB1gUBQitUqY7HY8PEprXTbqlUId91V+2iULOD/ADMQCXQF7DOKVgSqDCwt+fsS8AcCgC+qsa8VuBs4B4SgClxo2CJVIBAIBNqjVr87Z86c4YorrqhyvZ+fHzk5ObU2SnAJ6tJDVVHUcN+DB1Vv3ty54OtbJ3Psuch1zUnOysoiLS0NSZJo06ZNnY51MVqHtOa9Ue/RKUK9aAzxCeHmLje77HwCgcAzmM1Wbr55KTfc8C1ms9Vpxw0G7gXGAd8CvYFizGxiU7kQ38EMZiELmctct4rUs6iFjv4DJADLari/AZiBmos6xKmWCQQCgUBQfWrlUY2MjOTMmTNVrt+5cyctWjihGq1G0Ol0dOjQQTuV1urSmubrr+GXX1QP6pw50Lx5nc2xh6Lk5uYSFBRU6+McP34cgObNm+Pj4+NYPnXqVCwWC9HR0XUztASj3shNXW7ixs438sepP0gvSMfH6HPpHTWM5saoQFAGT4zPwkILEyZ8x+rVapTGtGk/8t13N9XqWEXAR8DVQLeSZdMACbBi5TuWEU83AomjWYlAdbcHFUq9qB+U2OyDGro7sWR9OrAR1bs6qsx+9oq+4YD9FvS1wBgajxdVzKECrSPGqEDraKbq74QJE/jkk0+44447HMLEXozit99+48svv+Txxx93npUawMvLy9MmlFJQy9DfbdtK+3Y++ij06+dUs+qaO1G2kFJZmjVrVqfjVoUkSVzV8iqXHPuipKTAZ5/BZZdB374QGuqUw2pqjAoEF+DO8Zmba2bcuG/4/fcEAHx8DNx9d+3zUT8BlgB/oXpQ9agiFcCAgQIKAIUYWrCAhW4XqKB6UV9GzUEF6AO8AJStnZ4IzAFiKBWqZfuiRgDfoYYIS5S+xsaCmEMFWkeMUUFjo1bS96WXXiI6OpqePXty2223IUkSb7zxBgMHDmT06NF0796dp59+2tm2egxZltm/fz+yLHvaFLDkQHGm+tg3pvr7nTkDTz0FsgzjxsEttzjdNN86hhBfccUVTJo0icsvv7zc8h9++IEFCxaQkpJSp+Nrhu3b4cMP4Y47oGtX2L27zofU1BgVCC7AneMzK6uIkSMXOURqQIAXa9ZMZeTI2qcTTAc6A7OARE7zEi+RRppj/WhG04pWjGCER7yo3wK3oIpUH+AJ4GPKi9QLUYAVwM2oItULtb1O3Wbx+ouYQwVaR4xRgdZxxdislVANCgpi69atPP744yQmJuLt7c0ff/xBVlYWL7zwAn/++WedRYugCvJLQq5NEWCoxnuclgYffAAPPAA5OdCtm9oKxYm9juy5qXW90xcbG8vYsWPp3LlzueVxcXEcPHiQgoKCOh1fM+zYUfrYaIRO7ve+CATOIjMzk3/++YfMzEyPHzs1NZ+hQxewdetZAEJCvFm37jYGDapZ9Mke4C1UMQcQCCxADYt9lVf5mZ9ZwALH9k1ogjfl0wdc+b7YKZuLWoTqRf0WuImL/7jnA4+gemDzUHNRFwO3UVohWCAQCAQCT1Or0F8AHx8fnn32WZ599lln2iO4FDXNT01JgVdfhYgIiImB//wHnBw6Yg/5LSwsrFOOaqMhM1PNEZZl9caBt7enLRIIaoWiKCQkJJCeno7BYCA4ONhpDb9reuxz53K5+uqFHDqUCkBEhC/r1t1G9+5Nqn1Oey7q16gitTvQkdOEEEIAalXzu7mbr/ma0Yx27NcTeBiwz8rVtd1W8lfTGbmyXNQHgRup3t3nDEq9qPcBtyIEqkAgEAi0h8jIrm8U1LDi73ffQV6e6rl76y0ID3eZaWaz2WXHriu/HPuFe1bcw85zOz1tCrz3Hhw9qn42//63p60RCGpNVlYWGRkZGAwGMjIyyMrKQpZlbDZbhT9FUSosq+mxqyIxMYfBg7/g8OFUDAaJFi0C+eOP2+nSJbxSW+z2lGUPMAU1F1UBhpDFBl7jRm5kCUsc2/WjH+/wDp0pjfzoiFpc6cqSY6enpztEalW2nwDuBN6/6LtQORsp70X9BjWE91I/6GXvTAsvqkAgEAi0Tq08qtOnT7/kNpIk8fnnn9fm8JpDp9PRrVs3bVRas/dQ9b2IRzUtTf3btw8WLVKXXXst6PVw5IgqVl0oWLWGoih89M9H7Enaw6/Hf+WyZpfx5JVPMiBmgOeM8vODgQOddjhNjVFBo8DuNbTZbHh7e1NUVERCQgIhISGVVoVXFIVt27Y5nhsMBgYMqPw7WNWxq/JMhob6EBMTBBQxa1Zr2rcPIy3tKGlpFY9tp3fv3vj5+VXwogZQSCsW8yefOvqgnuVstd6Tv/76C0VRKCwsxGq1IkkSBoOhUtuTgUOo4bv3ADWJRRkKDAIGUH0vKkAH1FzWZqjCVgjUUsQcKtA6YowKtI5mqv5u2LChwsWCzWbj/Pnz2Gw2IiIi8PPzc4qBWqG4uBhvLYRoVqeH6rJl8OmncPo05OdDcDCsXq3+AcyYof41ErYlbmNP0h7H838S/yGt4CJXsPUUzYxRQaPA7vE0Go1IkoTRaCQjIwOTyVTp9oqiVDssuKpjZ2VlERISUmF7Hx8jK1bcwjPPrKRDhxC8vKonwfag5mmeRu2DGsoWcniZfeQBMIj/Z++8w6Mo3gf+uZZLT0iBUEJC70WKdEQBEZCOINItIIqAgB2Vrw392cAuCiJNRAlFBBSVDlKkhZaEQBJKQkivl2v7+2NzRy65JJfkLrmE/eS55+5mZ2dndid7+87b+vAUT1loT62RDMQDt9Rq/LOzzUKqabymvrvWqmX2ZO2BGPToPkoXUm8A3wIvIwY7kgOfUPaovApgYRn3uZuQ7qESzo40RyXuNsolqMbExFgt1+l0fPvttyxdupTdu3dXpF9OhdFoJCIignbt2pkDB1UJgtG21DSjR0OLFvDss6DRiNq7RYugZUtx+12kTQX4+sTXFt8b+jRkcLPBxdSunjjNHJW4Kyio8VSpVIAYVE2n06HRaOjRo4eFUGowGDh37hxt27YtdX6W1HZBzWRhwdfLS82yZSNtijqoAb6Ry9kA5JFHLrEo+YAsziDDdgHVxG7E4EsDunRh3OnT6HQ65HI5Op0OAK3BwOcZGRzw9WWtTIbJa9aWrK5G4HlEU2EfYEF++d2WOsbRSPdQCWdHmqMSzo4jov6WO5iSNVQqFbNnz+bChQvMnj2b33//3Z7NS+QlgUEDMgW4lZB4ICBANPt1dYWOHUVz35Yt7wiqdxFRyVHsjt6NTgcml7T7vWfw72G7Tv0qx2CAK1c8SE8XLbydkbNnq7oHEvaisMYTMGs+U1NTyczMLKL5lMvlKBSKUh+wSmrbpJm8eDGLefN2sW3bBIKCPM37ymSyUts/jahFvUweSSQhYzu1WYeCnDILqCY8EFPBuGk0pKammvuu0+m4oVazoXlzYjw8cDcY2KpUYrJn2QG8AXQHviimbTmiFnQlotmuhISEhITE3YJDntY7dOjAmjVrHNH03Y3J7NetPshLuHRaLWzbJn5+8EFRUK0EnDER9dfHv+V2Euh1+QUaH95/czzv60rczSHU5zqPs5ITdOE/OpOI7dFIS0cBNLNjexIS1jFpPPV6PWq12iIokkwmQ6/Xl+hPWtG2//33HI88coDsbB0DB65h796p+Pvblg5tB/AmcItEMogiiBV4crbcAqqJYcDDgsDpiAgS8/uuFwR216vHzoYNMSoUqPV6psXG8mTjxiWmBzMCvwKuwPD8snuBrkhaVAkJCQmJuwuHCKq7d++ucXlUncLMwuyfGlpyvb17xRQotWvDoEGi+a8DzX0VCgUGg8HprnlidiI/nf3ljpAKcGYq6KrGf7oHR3iab8zf+/M3l7h7c6hKbjbVE1OwIKVSaTVyr1KpJDc3t4hpri330NLa1ukEzp+/QV6eHoC6dT1xdS36M5ZEEmGEMZrRBHDn3tcd0Xy2CUlE8Ar96MxTrC63gFpc36+pVKxp3Jir7u4gCLRNS2NiTAz1lEpo1KhYQfUGorb3P0Rf1O5A7fxtkpDqeJzid15CogSkOSpxt1EuQfWtt96yWp6Wlsb+/fs5efIkL7/8coU65kwoFAratWtX1d24E/G3tNQ0v/4qvo8cCXXqODxwkumBUqvVlnlfrVZLWFgYTZs2pVOnTlYjhoWGhuLn54ebm5uVFopn1elV6AwFpFSDCk5NL3Mf7UUXTpg/5+JG1F2sAb3nHujQoap7IVEe5HI5nTt3NvtfWkOlUln8L9t6Dy2p7T//vMyCBX+SmqpDrxcYPrwFP/88tlhBdTnL6cZ9bEMgli/oRjeGMIRfAE+acZlvaWXHhSK5XE7Hzp1ZIwisUKnQA765uYy+epXxnp7U7tSpyHkxYQQ2IuZFzUXUps4G7q5oAlWL0/zOS0gUgzRHJZwdRyyklEtQXbx4sdXyWrVq0aRJE7755hueeuqpivTLqRAEgczMTLy8vOyWzL5c2BLx98oVOHkS5HJRUK1EcnJyyrzP1atX+f333/Hx8aFz585W64wfP77sfdHlsOr0KsvCi2P48v/q0L59mZuzC40/zcHvoBKZQU9mu07sfdt+Bg2CIJCTk4O7u3vVzlEbcHUVhdT8ODkS1RC1Wl1sdF9rlOUeaq3tNWvOMG3aNoxG0dF8/Pg2rFkzCpWq+B9FAQUvU5dLaFFxm7N8x0M8RC3kgMquQuqfwP+AvAL97gO84OJC3datS/3xPpb/AuiEaJ5c3269k7AFp/mdl5AoBmmOSjg7hfOT24NyPSk7IqqTM2M0Grly5UrVR1qzJeJvWJj43revaPpbiZQnf1JUVBQAzZo1K/bGe+vWLQwGA/7+/jY/HP987mfSNGmWhSdm0n6RXdOXlo3en4PmQzhzBrUg0Lu7/Zo2GIyEh0dV/RyVkLBCRe6h3357glmzfjcHQ5s2rSPffz8MhcLyfpOU/5dHHv/xHzIMhHCFTBoTSEMWMhG5zRlHy0YKkJf/2RMx+NFQQFYGv31XYA5ly4sqYT+c5ndeQqIYpDkq4ew4RdTf3NxcXnvtNe6//36GDRtm9w5JFINRBzk3xM/uxQiqGg1s3y5+HjOmcvrFHR9Vb2/vMu97+fJlAJo2bVpsnVWrVpGcnMwzzzxDaGhoqW0ajAaWn1xuWXilPyS3KHP/7I6rK3TrVtW9kJCoFnz66RHmz//T/P3ZZ7vy2WeDkcstF7WysrJYzWo+d99PsjwWFXGEEspxZmJEjYYczuNHL3o5pJ8tABfEoEevcsevtDSaAWqgPbAIx2hRTVGYvb298fX1dcARJCQkJCQkHEOZBVU3Nze+/fZbWreuePAJiTKQcwMwgsId1P7W6/z5J2RlQf36lSoMmXxUy7qSIgiChUbVXuy6vIvYtFjLwhNP2619CQkJxyMIApcvp5i/v/hiT95/f4BVy4t/Th5mebAn8Q3fwpU4QngLHRre4nVaIqblCnCgx+c9wD6grNbszYC95divLKSmpnLjxg3q168vCaoSEhISEtWKcpn+du7cmXPnztm7L06Na1WHKM0pEEipON+ETZvE9zFjRB/VSiYjI6NI7sSSuH37NhkZGSgUCho1amSXPgiCwNcnvrYsvNUOrvW0S/vOTJXPUQmJEijr/JTJZHz++RCys3U0aVKLRYv6WhVSNxLBM91cyZR3R46cTnjyNt8wlxm0zP+rDMorbDraVdsU5M4Z04c5G9I9VMLZkeaoxN1GuQTVpUuXMmTIENq2bcu0adNQKh2S5cZpUCgUtGxZOQ87xZIVI767FxNI6dIlOH8elEqoJibZJm1qaGgoKjtF1jl+8zgn409aFp6YRU1P7uAUc1RCohjKOz/lchk//DDCqoCahoap/Mtf1AZ1AD5k8z/UzKIfl6ic3NHVAUlQtQ3pHirh7EhzVMLZcYTvtM1qt/3793P79m0Apk6dilwuZ+bMmXh7e9OsWTPat29v8epQg3JPGI1GkpOTqzaIlEmj6hlqfbtJm9q/P5RBq1mV2OKfWlZCfEJ4puszeKm9AAhU14fIh+3WfrmIjobXX4dt2+DmTYccwinmqIREMdgyPw0GI3Pm7OS//yz/R4rTorbmtCikAt1I5DQtmYUY0juAAGYww6HmvtUFk6BalijNdyPSPVTC2ZHmqISzU6XBlO6//37Wrl3LhAkT8Pf3JyAggBYtnCA4TSUgCALXrl2rWv8eUw5VaxrVrCzYtUv8XIlBlCqKI/xT63jWYVHfRcztNpf14eu5FVuLN41VrPE/dAhWrBBfAAcPQuPGdj2EU8xRCYliKG1+6nQGJk/ezM8/n2f9+nD27p1G27ZFQxJZaFHxxqOAFrUgJkFVQtKo2op0D5VwdqQ5KuHsVGl6GkEQzB3Yu3ev3TsiUQol5VDduRNyc6FRI7jnnsrtVznRaDTExYnCtz0FVRNeai9mdpnJQY2Yk7BKOXHizmdfX/E6SUhIAKDR6Bk37hd++y0SgIyMPKKjU4oIqhuJYB6ppBfQoq6lM/XwqvQ+VxcMBoM52J0kqEpISEhIVDdqtnNpTUGfDdpk8XPhHKqCYBlEqZokgc7JyaFjx46kpqbi5+dX1d1xLDodqFTie9eu1eYaSUiYyM7OxtXV1e7+Jzk5OkaO3MDu3VcAUKsVhIWNZ8iQO4tXAvAZ8DG+pGMoVosqUZS8PDG7q0KhkPIuSkhISEhUO8okqFrzFbpb8PKqwlV7kzbVxR+UHpbbwsPh8mVQq2Ho0MrvWwHKco78/PyYP3++A3vjRHz9NWi14rVy4P9Qlc5RiRqLIAhcuHABvV5PVocOXHV3Zxpi3tCyUHh+ZmTk8fDD6zlwQLSs8PBQsW3bBB544I7FgR49SpTIgEACaU8sy2klaVFtRPJPLRvSPVTC2ZHmqMTdRpkE1UmTJjFp0iSb6spkMvR6fbk65WwoFAqaNGlSdR0w+acW1qYC/Pqr+D5oEFTRDUyhUGAwGKQV+5JwcYHOnR3WfJXPUYkaS2pqKkk6HT83bMg5NzcA2gJlSfhUeH6mpOTy0ENrOX5cDJzk7a1m586J9OwZDIi+qJ+xihiO8R3fMRMF3ZFzL/faa1h3BZJ/qu1I91AJZ0eaoxLOjiPkgDIJqgMGDKB58+Z274SzYzQaSUxMpHbt2sirID+pRQ7VgqSnw19/iZ/Hjq3cPhXA5AOVmZmJj4+P3dufNGkSOp2OunXrWt2+6cImrqReYfo90wlwvzujfFb5HJWosfyWkcEXrVqhcXNDLZNRG6hfxjYKzs/bt3MYOHAN4eGJAPj7u/Hnn5Pp1En8/44Engf+I4T6rOBf/qUXvSQRtRwIgoBarZY0qjYg3UMlnB1pjko4O1Ua9RfEtDSPPfaY3TtRmC+//JIPP/yQhIQEOnTowOeff86995b+mLJhwwYmTJjAiBEj2LJli936ExUVxY8//sjUqVOrJtKxKYdqYY3qb7+JJqUtW0KrVsXuHhUVxebNmxk1apRDAhdlZGSQkpLCkSNHeOSRR+zefv36xT8WG4wGPvn3E66mXuXL418yrs04nu36LCG+VrTPNRhBEEhISCAwMNAh7cfFxfH333/Tv39/GjYsJpevRLXF2vVNB5bo9Wzx9wegjVLJ20DrcrRfcH7+889Vs5AaFOTJ7t2Tadu2ttnM1wfIwpWGdOVlvqIXXe0yxruROnXqUKdOnaruRrXA0fdQCYmKIs1RCWfHEVF/nW5J5ueff2b+/Pm8+eabnDx5kg4dOjBo0CASExNL3C8mJoaFCxfSp08fu/ZHEAR27txJbGwsO3fudMhFKJUcK6a/RiOEhYmfSwiiJAgCO3bs4Ny5c+zYscPu/TcajaSlpWE0Gvnrr78cspqyadMmfvzxR6tzYPeV3VxNvQqA1qBl7dm1XMu4Zvc+3M0IgsCBAweIjo7mwIEDVfM/IOEwrF3ffcAjwA6DARkwKiODn5TKcgmphZkwoR2ffjqI4GBv9u8X09D8wQXGMY5DHKIO8CnwNwGMloRUCQkJCQmJuxanE1Q/+eQTnnrqKaZPn07r1q355ptvcHd3Z+XKlcXuYzAYmDhxIv/73/9obOf8lJGRkVy6dAkXFxcuXbpEZGSkXdsvFUGwnkP1xAmIiwMPD9E/tRgiIyO5ePEirq6uXLx40e79//vvv82+yKmpqfz99992bR/g8uXLnD9/npycnCLbvj7xtcX3NrXb0Cu4l937UC4uXoRXXhGjMsfFideyGhIbG0tMTAxqtZqYmBhiY2OruksSdqTg9b0UH89z6eksAFIEgcCsLF6JjGS+iwsuQBwQAWRW8Jjz5nXn3LlnCGzmwQj2MhqBC/iynOUICHQCKVyShISEhITEXY5TpafRarX8999/vPLKK+YyuVzOgAEDOHLkSLH7vfXWW9SuXZsnnniCAwcOlHiMvLw8c8h+EM1WwTLfnEwmQy6XYzAY2LlzJzqdDm9vb7Kysti1axdNmjSxiIBcsH5B5HI5MpnMajkUteW2Wp6XhMKQg4AcozoITH385RfkgHHIEAS12lwOojOz0WjEaDSyY8cOUlNTkcvluLu7s3PnTov+F9d3W8ZkNBoJM2l1ETUzYWFh9OvXzzyW4saqUCgQBMFqudFotNDamT4LgmDRn5MJJzl+47jF/jM7zUQQBHMfxeqWzt0OuU5WxiTbuxfZjz8i+/FHBMB49CjUq1fiWE3nvbjy4q6T0WjE19fXfGx7jUkul3Po0CHz/0BGRgaHDh0iJCTEIreyI8ZUWdeppL7X9DEZjUbz9b3dujW7OndGMBjwFwRGZ2Vx74ULuCuV+Pv7YzAYWCyTcVYm432jkQdsHNPZs7eIjEyme/dawJ3/vx0eV5gvpJvzojYSxrBM6IURY4XGVBOvkzQmx4+p4D20poypcB+lMVXvMQmCYPE7XxPGVBOv0908JkdY3NksqDrCpLMwSUlJGAyGIj41derU4dKlS1b3OXjwICtWrOD06dM2HWPJkiX873//K1J+/vx5PD09ATF1SsOGDTl06BCnTp0iLy8PnU6Hp6cnFy9eZP/+/Ra5P4ODg/H39ycqKgqNRmMub9y4Md7e3ly4cMFiArVo0QIXFxfCw8Mt+tCuXTu0Wi0RERHmMg9tBM0AvaoO58+L50CRmkrTP//ETaUi/YEHiC3QjpeXF02aNCExMZETJ05w5syZO215eHD+/Hl27dpFgwYNAAgKCiIoKIiYmBgyM+/oSWwZ0/Hjx0lKSgLupC5KSkpizZo1dOrUqdgxKRQK2rVrR2ZmJleuXDGXu7q60rJlS1JTU7l27Y75rkmTmpKSQnp6urn8k4ufAJiF5kDXQELyQkhMTDSP6coVI2Dpl+uI62RtTMF//om3Xo9KqcQQGMi55GRITi5ynRISEsztmObe9evXSUlJMZeXdp2io6PRaDSkpaXZdUxeXl5cuXIFmUyGRqNBJpMRFRVFbGwsXl5eFtfJ3mOqrOsExc+9mj6m8PBwIiMjuRYSwp99+4Ig4JOQwIyICJrm5aHVavGoWxe5XE50dDR6Ly9cXV25npBAqr9/qWM6cyaJZ589Qk6OnvXrR9KgQQMOnf+PN0LiOerZBIyeeMqzeV0np++F+sQQI10naUxVOqbMzMwaN6aaeJ3uxjGlp6eTlpZm/p2vCWOqidfpbh6TSqXC3sgEJ3I4u3nzJvXr1+fw4cP06NHDXP7iiy+yb98+jh49alE/MzOT9u3b89VXXzF48GAApk2bRlpaWrHBlKxpVIODg0lJScHb2xsQhS6ZTMayZcs4deoUWq0WlUpFnTp1SElJoV27dsyePbtUraRdVjmub0Zx8X2EgJ4Y7/lUPN7Klci+/RZZhw4Yv/vO6iqHwWDg888/58yZM2YBo379+kX6X96VG51Ox5w5c0hKSjJrME3vAQEBfPbZZ8jlcrus3Hz44YekpKQwa9Ysc6CXmLQY+v7Y16Leoj6LmNl5pkXfDx6Efv3uaFQPHIAePSpJozprFrK//kKm0SAMG4bxq69KHWt5V9h0Oh03btygfv365vNe0TEJgsDPP/9MVFQU3t7e5muckZFBs2bNePTRRynM3bJqWBPGZDAY2LBhA1FRUXh6e7Np4EDq3r5Nm0OHaNqwIaGhocjlcjp37oy7u3uZx7RnzxVGjPiZzEwxRUq3bnWYe7AvCxQZpCPea7sKt1gn60w9vKTrJI2pyjWqpnuoSqWqEWMq3EdpTNV7THq9nuvXr5t/52vCmGridbqbx5Seno6/vz/p6elmmaqiOJXpb0BAAAqFglu3blmU37p1i6CgoCL1o6OjiYmJYdiwYeYy0wlWKpVEREQUyTlVXKh+hUJhkf8nIiKCS5cuWQhmMpkMDw8PLl26RHR0dJEIwMXlD6pQee51AGQeoWK50QgmIXzMGPOEK8zly5e5dOkSbm5uZsG8pP6XtY979+4lOTnZ/E9mal8ul5OcnMzevXsZOHBgkXYSEhJwc3PDx8cHmUxmtf3CYyq4IGCqv/L0Sot/Di+1F5M7TLZoT7ymto/JHuUWY1q+HHQ6uHABmVJp01hLKy+uL3K5nLS0NIKDg4ucg7K0U7A8JiaGS5cuIZPJMBqNKJVKZDIZbm5uxMTEcO3aNUJDQx02pkq7Tjb0saaNKQP4OC0N2bVruLm5oZTLeeSff5ALAnkuLsTFxRESEoKfnx/u7u5l7suff0YzcuQGcnNF//VeQ4KRrfbjSQUg88adbN5CzSzZ/XYbU3nKnf06ladcGlP5y033UKg5YyqINKbqPSaZTGb1d746j6kmXqe7eUym53V74lTBlFxcXOjcubNFQB6j0cjff/9toWE10bJlS8LDwzl9+rT5NXz4cO6//35Onz5t/sEpK4IgsGvXLrOAZ/LF02q1yOVy8vLy2LVrV+VEP83OD1xjyqF66BDcugU+PjBgQKn9l8vldu+/0Sj6phqNRrOWTRAEs1BfcHth1qxZw+zZs9m/f3+5jg2QmpvKT+d+siib1G4SXmonDL+iUkGHDtCmTVX3pEwIghgJNi/f/FOr1aLT6dDpdOY5dOjQocr5H5CwKwLwrCCwBjjcsaNZI2/Iv8YFNfStSkh7VRzbtkUwbNhPZiG1wzv1if6tFSd8mwJwL4mcoSWzaG/HUVUPNBoNV65csTCtkpCQkJCQkLCOU2lUAebPn8/UqVPp0qUL9957L0uXLiU7O5vp06cDMGXKFOrXr8+SJUtwdXWlbdu2Fvv7+voCFCkvC3q9nuTkZNRqNdnZ2WZBzGQTrlarSU5ORq/XO8Qe2wKzoJqfmmbTJvF9+HBwcSm1/3l5eWZhwl79z8vLIzs720KbCne02XK5nOzsbPLy8nBzc7PYHhUVBVChXJw/nvkRjf6Ofb5SruSJTk+Uuz2JohgMBjIzM/H09ESn02E0GtFqtebtarWa9PR0DAYDSqXT3UYkSkAGPGkwcD41lZaxsRbX1YTp+hqNRosV0/eAy8BsoJOVtjdsOMekSWEYDAKoFbT+bhDRkzwxIuBqTOcdoxvPKvo5ZmDVAIPBwI0bN1AoFDRq1Mghq88SEhISEhI1Bad7whw/fjy3b9/mjTfeICEhgY4dO7Jr1y5zgKW4uLhiVdD2QqVSMX/+fLKzs9m+fTsXL15Er9fz0ksvmY/t6enpeCHVqIfcG+JnjxCIjxc1qgCjRtnUf61Wy7///gtA3759zXUq0n83NzfefvttkvMDA61YscJcnpubS+PGjRk3bpyFkAqiD3Jubi4uLi7l1nbn6fNYeWqlRdmIFiOo51WvmD3uDmQyGUFBQXZ78FUqlUyePJnc3Nxi67i5uUlCajVhH6AFTMb49ymV/FOnDtoS7iPWrm80cBZIt1J/5cpTPPnkNkxK9lFPdiJxUhc0sjSCjadZmtKQ9gGNKj6Yaoy7u7vZxyg3N9dsVi1R9dj7HiohYW+kOSrh7DhibjrlU+bs2bOZPXu21W179+4tcd9Vq1bZpQ++vr74+vri5eWFi4tLhYSrcpN7EwQDKFxBHQCbvxFzcd57L5SikTT1H6BRI/s/HIaEhBASImp5N2zYAMDo0aMJCwvj9u3b5gjKBbl8+TIgRigrzma+NDZd3ERSTpJF2ayus8rVVk1CLpdb9eOuCF5eXnh5OaE5tYTNpAMfArsAd4OBlno9wfk++r5eXmCn65uQkMVzz+1EcNXDzOOMCBzILy89xB8yGT7Uopf8fvKz0NzVyGQyPD09ycjIIDMzUxJUnQhH3EMlJOyJNEclnB1HKBKdUlB1RvLy8jAYDOUWsMqFyezXvSHoDXeCKI0da3MTqampLFmyBID/+7//s3MHLWnQoAFNmzbl8uXLHDp0iIcffthiu8nst1mzZtZ2L5bQ0FB8a/lyNu0s7xx6B61Bi0quQiaT0TekL60DW9ttDNy4AQXChBfBzw/q1y95vwsXYMcOaNtW9E1t0AD8/a3vZycMBgMxMTGEhoZW7hyVcFr2Ae8CKYDMaKTRyZP8c/kyY4cPx8PDo0xtCYIghs/397e6PSjIk492T+YZt2vUqnseY519GGTPMQTRPUGan3fw8vIiIyODrKysIqnYJKoOaY5KODvSHJVwdgpHHrYHkqBqI5WRR7YI2XHiu0cI7NsnCkL+/lDAhLc0CuZacjQGg4E+ffpw+fJljh07xoABA3B1dTVvL6+g6tXJi/f/fp+I3yPMvqkKuQIvFy9mdbGjNvXGDVFbnZVVfB1PTzh2zFLoLLxfXh4U9Pvz9BQ1V4X3szMF821J3L0U1KICNALmZmRw4dQpNIKATqcrU3tGo5E9e/Zw4cIFsh9/HIoRclN7BtMqtzZ613m8KJPjgqUPvSPnZ2pqKtnZ2Xh5eeHj4+Ow49gDk7VJVkn3GYkqQbqHSjg70hyVuNtwqqi/EoXIKRDx1xREaeRIcFK/QIPBQIsWLahduzYajYZjx46Zt2VlZREfHw9A06ZNbW5zR9QOJm6aSERyBGqFGrVCNFvUG/VkajPJ0trxYS8lRRQ25XIxUFXhl1wubi+scS28X0EUCvFlbT8JCTuzD3gEUUiVA1OBdUBvX1/Gjx/PqFGjzC4BZSE7OxuDwUBcXBx6vR5BENi5M4qTnOFt3saIkbnAU25qjsv60JvedhxV6Zw6dYpff/2VixcvVupxy0NBQVWKmi0hISEhIVE8zinxOBG1a1ehY5VJo5rlBsePi4JQCcFPqhqFQoFcLqdPnz5s2rSJgwcP0qtXLxQKhdk/NSgoyGa/R4PRwGt/v0Z2XjYqowoXDxeUSjdc5XpyDdkYBQPP//Y6nvc+hEJm3Qzm7NlyDESpFNPKgJgHtWDQKZOmNClJNPEFiI4WfYdN+ykU4rUSBFCrxXIrkVUlJOxFBqIWdWf+90bAm0DB2Ofl1TTK5XIGDx4sppwyGEhLTeX/fr3AH3Uv4pHtRQuPrbSjHSMZyfOAGFdYojjc3NxQKBQYDAZycnLKbIYtISEhISFxtyAJqqVg8iFq3Lhx5UdaM/moHsjXEvTuDU7sSG/SDnTq1Ik//viDtLQ0wsPD6dixo1lQLYs29eiNo0SnRqNChQwZqami3ChOWx+Q64jSXKbfxKNwrae9ByMKo4IA1hYrTpyAxx8XP+t0UFAzYhII9Po7bTkYmUxGcHCwFA3wLqSgL6ocmAzMBKwnryofKpWK4cOH8+W162Tqc/hnoicoepANNEqZQn+//iXuL83PO5gCKqWnp5OZmSkJqk6CNEclnB1pjko4O46Ym5LpbynI5XLUajVeXl4OT4tjgT4H8m6DUYDfj4hlY8ZU3vHLQUZGBiA+1Pbo0QOA/fv3IwgC165dA8rmn5qYnYhBMCBHjiDckfvMGBUgM4BHos1tFnCZLZncXDD5JZdX0FQqK81MWy6X4+/vX7lztBQks0bH8wmwAFFIbQSsBJ7DvkKqCZ3CyHnZDdJrpWFUK5Fp0ph+/ia/+s3Bi5KtJJxxflYlkp+q8yHNUQlnR5qjEs6OI+amNNtLwWg0kpeXx+3btx0SzapYckTBjiwjJOdA3bqQL/xVB7p3745SqeTmzZskJiYyb948PvzwQ+69916b26jtURuFTIGRYgJZyQ0gKCDbNvPse+6BDh1sPLjpWgsCpFvLGulcGAwGLl26VLlztATS0tLYsGEDSUlJpVeWKDcdsfRFbVti7fJzVHOC5if6keJxCoMRFP+d5Ptj3qy8d7BN+zvb/KxqfH198ff3l9I/ORHSHJVwdqQ5KuHsSFF/q4CU/AA4N2/erNwDm8x+Y7MBb9E3tRqtonl5eTFu3DgaNmyIn58fQJnzf3Wr340mtZpw/tZ5lKgstnl7C2hlOQS7t+C7dd1QlGJt4OoqCqkqVcn1zBRU31qL+NyjB+zM9wiMirpjBlyFVGaE59I4cOAAt27dYsuWLYwfP156ILcTGcBVwLTe8gCwCXBUhmcNGpbmfc57MV+T6aKBs8tRrbjJtidnMmiw7Wb84Fzzs6rx8/Mz3xclnAdpjko4O9IclbjbkATVUrhx40bVHDg7FjQaiM0CRS0YMaJq+lEBOnbsCNwJp15WYUUhVzCk2RAu3LqATqZFkOvAIAO5Aa0sBzcXNUuHv8N9zeycT0yvF1NwuLreEVJ1Okvh1cfnjnpWLgeZzIptcoH27jIGDhxIamoqKSkpbN68mXHjxlmkKpIoO1eApwED8AtgEnMcJaSe4QyLtG+w5+oZsrO08FsLPL7ry+8/Tee++0IddNTy8cADD/DAAw9UdTckJCQkJCQk7Ej1UdHdbWTHQWoaZKjhgQfE/KnVkMOHD7NkyRL++uuvMu+rNWj5Pep3FOQLoi45oM4EZR7B7i1YO3otQ5oNsV9n/fzEnKdGoyiYFtSkarXid09PsV5x+2m1RV/F7VeDcXV1ZeTIkXh6epKSksK2bdvQ34UCuz0JBgKAWkCag4+1m908yZMciAsn+6oK5j2E72cP8ffmp5xOSJUoSkpKCgkJCeTm5lZ1VyQkJCQkJMqNpFG1ERcXl8p1YM+IFn0jM+tVKIiSS+G8npVM7dq10ev1nDhxggcffLBMES43X9xMYnYiSpQgyNBr1bDvDbh5L9+t62Z/TWr9+nDsWMn5Tv38xHr22M+OyOVyGjdu7FRBFry9vRk5ciQbN27k5s2bHDlyhD59+lR1t6oV/wKdAVX+6xNETaqj/6u7051AAunh+wAbnvOCDDW7906mQ4fyRR13xvlZk4mPjyclJYWmTZvi5uZW1d2pFkhzVMLZkeaohLPjiLkpCao2olAoKi8kuCDA9TP5mrhQ6Ny53E35+fnx/vvv269vJWDNtLdJkybUq1ePmzdv8u+//9K/f8lpLEwYBSNfn/ja/F2GDOX17uiPzgMo1Se13NSvXz6Bsrz72QmZTIa3t3eVHb84AgICGD58OCdOnChTIK27nXTEvKi7gBn5LwBHJafSoGEHOxjFKGTI8MKLDWzAO8CbGVsSUShktGoVWO72nXV+1lS0+Xmb1Wp1Ffek+iDNUQlnR5qjEs6OlJ6mCsnNza28SGt5KZCaIH4ePFH0fywnOp2OY8eOcezYMTt1rnisraTIZDL69u0LiGbAOjERaqnsubqHyORIizKXMxMr3skaisFgIDw83CmjATZo0ICRI0dKD802sg94BFFIlSP6pDoSAwamM533eI+Vtzeg04lH9EZ8IGrbtnaFhFRw7vlZEzEJqlVtUVOdkOaohLMjzVEJZ8cRc1MSVJ2R8H/EQEq5anh4ZIWaSklJISwsjLCwMPv0rQQUCuumuO3bt8fHx4fMzEzOnDljU1vf/PeN+bO7uzuNvNuiuVRJeWTPnYOrV6tdECTpx6t6kw4somhe1FkOPq4CBYMZjFumDy8+fpjJkzdjMBSTEqoCSPOzchAEQRJUy4k0RyWcHWmOStxtSIKqM7IvX6j0bQoVNPMQBMEOHbKN4m6gSqWSXr16AbB///5S+3T21lkOxR0yf5fL5TzS6HmMhkqKGrtwIfTqBY0bi58lJBxMYS2qo/OiHtMc41TeKfP3Nqf6c6FdP1K2B/Dzz+d5++39DjqyhKMxCakAKpvzcUlISEhISDgfkqDqbGRkwOV/xc+teldtX8pISSt99957Ly4uLiQkJBAVFVViO9+c+Mbiu1qvRnYpC3f3RLv0s0QEAaKjxc96Pbi7O/6YVYS0Mlv1ZACvc0eLGoqoRX0OxwRMyiOPT/mUx4XHeT7zeWLjYzly5Br9719DSqyoRe3SpR7PPSf5E1dXCmpTKy2ugoSEhISEhAOQBNVSaNeuHQDNmzevnEhrv/8O7jliDs8m3Rx/PDtS2PT36tWrLFy4kDVr1uDu7k7Xrl0BUataHNczrvNb5G8WZe307Ui4EYFSmWP/ThcmMRGys+98b9zY8ce0A3K5nBYtWtg8RzUaDStWrGDv3r1S2pgqYh8wFtiJeCOeAqynZC1qVlZWuY93lrNMYAJrjWvR6/U0zm7M+YhUBg5cQ3p6HgC9egXz11+T8fe37wJNWeenRPmRzH7LhzRHJZwdaY5KODtS1N8qpFJuDIIAmzZBWy3U8gOPUMcf044UztkXFRXFrVu3SEgQA0P16dOHI0eOEBkZSUJCAkFBRWOYfn/yewzGO5o+d5U7nXSdSKQShFQQ08j89htcuSJqVrtVn8WCsjyYnjt3jpycHK5fv16sb7GEY8gG3kcUUEHUoi6mdDPf5ORkNm7cSNu2benVq5fN96Q88viKr1jPegQEvPK8eOLKE4Rcb8eYsb+j0YgLFQMGNGbLlvF4eDhGwJEEp8ohL09cdJCCl5UdaY5KODvSHJW425AE1VKQy+W4uLiQmZmJ0Wh07EP9yZMQexV6GcDHBzwaOu5YDqCgbxRgNvFt1qwZIKbK6dSpE2q12mpuv4y8DNaFr7Mom9B2Am7H3aCyBFWVSkwHVIGUQFWB0WgkPDycdu3alTpHDQYDp0+fBuCee+6RzAMrGRUQgahFnQQ8TelmvpmZmWzevJm8vDxu3ryJ0Wi0SVA9y1kWs5g44gB4WHiYvuf6khVv4K13/zMLqQ8/3JxffnkEV1fH/CSUZX5KVAxJo1o+pDkq4exIc1TC2TEa7R+IURJUS8FoNKLVaivHPHLTJvDQga8XKF3BtY7jj+lALl++DEDTpk3NZePGjSu2/tqza8nW3jG7lcvkPNX5KX46/pPjOnkXEhkZSVZWFu7u7rRo0aKqu3NXkAF4AApEofQtQI9twZI0Gg2bN28mKysLPz8/RowYgVJZ8q27sBY1kEAWsYiWKS05cusk4eG3OXkyA4Bx49qwdu0oVCrpwacmYDQakclkkqAqISEhIVHtkQTVUrh06RIAt27dcuyBUlLgn38gUAu1aoF7Q5BVXz+E1NRUkpKSkMlkFoIqQFJSEvv27cPX15f+/fsDoDPo+O7kdxb1Hm7+MA19qpdW2dkRBIGTJ08C0LFjx1IFHomKcxBRMH0MmJZf1tLGffV6Pb/99hspKSl4eHgwcuRIXF1Ljn5dWIs6jGE8z/N44825+HN4erpw44aAXi8wbVpHvv9+GApF5d1r8vLyJLNUB9K4cWMaNWpUqRHfJSQkJCQkHIH0lFoKOp2ucg60bZsYZbZtHXDNrnZmv4UxaVODg4OLPFjHx8dz9OhR3N3d6dOnDy4uLmy5tIVbWZaLAU93ebrS+nu3kJWVRXZ2Nkql0hwoTMKxpCNG9P0TmIyoVbWVmzdvcvPmTVxcXBg1ahTepaSr+oIv+JEfzVrU13iN3ojRwzUaDampqSgUcl57bTANGpzn5Zd7I5dXnun3xYsXOXDgAGPGjKF27dqVdty7DZlMJpn0S0hISEhUeyRB1Ubc3NwcF1DJaISw/NypXUKB8+AR4phjVRIm/9TC2lSANm3a4OfnR0pKCv/99x89evTAVelKiG8IsWmxAHRv0J2OQR0rs8viQsG1axAcDNVM0yiXy2nXrl2pc9TLy4vHH3+cxMREq37CEvYhFaiV/3kIYAAeomxCKkDDhg0ZPnw4KpWKgICAUuu74oqAYKFFFQSB3Fw9iYliUDNfX1/q1/fj1Vf7lLE35Ucul9OmTRu2bt1KXl4eFy5ckARVCafC1nuohERVIc1RCWfHEXNTmu024lAzqn//hZs3wcsL6udrH+0kqPr5+XH//fdz//3326W9kijoE1U4kFJB5HI5ffqID8kHDhzAaDQyrMUwDk4/yHfDvqNzvc7M6jLL4f0twtWr0KsXNGkCffvCkSOV34cKUDiYVXEolUrq1avn4N7cnZjyok7I/wwgA4ZT/ryojRo1okGDBla35ZHHTW6av09jGl/zNW/ypllIffXVv7nvvh+4fl2sV7du3XL2pGLo9Xoefvhh+vbtS9++faukDxISJWHrPVRCoqqQ5qjE3YYkqNqIRqNxSDQrkpLgjTdEbd6wYaC5Lpa728f0V61WM3jwYAYPHmyX9krCpKHT6XTExMQA1jWqAF26dMHNzY2kpCQuXrwIgEKuYGjzofw24TcGNB7g8P4W4coV8V2ng8uXwd2+uSQdidFoJCIiwjFzVMImCuZFTQGOOfh40UQzgQksYAE6RBcFJUq6IuYrNhoF5s7dxfvvHyI8PIGff47F3d0DPz8/B/esKKb5qVKp6NSpk6QRkHA6pHuohLMjzVEJZ8cRc1N6WqhqLl0SNXd6PYwcCpp8P007aVRTU1N58cUXefHFF+3SXkmYwqXHxMSg1+vx8vKiTh3rkYvVajXdu3cHRK1qYQr6V4WGhhIU1AyDwcGmqtHRlt8bN3bs8SRqBCYt6gJEATUUWAk4eqmlFrXIIIM00rjOdYttBoORGTN+4/PPRXE5L0+gVaumdOp0jyQkSkhISEhISFQLqpcjXk3k77/F9zZtIEAGkYDKG1x87NK8RqOxSzu2YDAYAKhTpw5PPfUUeXl5JQb06NmzJ/v37+fKlStcu3aN4OBgq/XGjx/PwYPw5psO6fYdhgyBgABRYE1MFE2xJaoUnU7Hnj176NatGz4+9vmfsCf7gHcRBdSy5EUtLzHEEEooAH748SmfEkII3twJsqTTGZg6dQs//XQOALlcxsqVw5k6taODeiVhL3Q6HSqVqqq7ISEhISEh4RRIgmpVkJQkvgB+/118b9UKLh0EjQZcm1dd3yqASVD19va2yQfNx8eHDh06cPLkSfbv38/EiROt1rt16xapqQbkcn+MRgemtQgNFV/VlJqYAHzfvn1cuHCBxMREHnvsMafRBmYAHwE78r+HAouxLS9qeSiYF3UJSxiQr69th2Xk5rw8PePH/8rWrREAKJVy1q0bzbhxbRzUM9upifPTXhgMBk6ePIlGo6F79+6SsFpFSHNUwtmR5qjE3YYkqJaCKTKlm5ub/W4QYWGwfDkYDBAZKZZt2wZXU6DDbfBrZJ/jVDJlPT/bIrZRt11dOAnh4eGkpqZSq1atIvVWrVpFbGwynp7PkJERaqfe1iwUCkWNTDfTo0cPEhMTue+++5xGSN2PqEVNpnK0qIXzop7lrFlQLUhOjo5Ro37mzz9FE3a1WsGvv47j4YerfuHL0fMzKyuLvLw83NzccK9GvuUmCt47s7KyrN4HJRxLTb2HStQcpDkq4ew4YiFFElRLweRjWb9+fQRBsE9uutGjxaiyp07B669DcrIYUMn7T8jYD42qIJCQHShLZOSMvAwW/rmQLG0WwQHBtMhswbVr16QHtHIiCAKZmZl4eXnVqPyJHh4eTJgwwSnGZE2L+ibgqMeGglpUAYEAAniN1+hD0bQy2dlahgxZz/79Ynond3cVW7c+yoABzuFn7ej5eezYMc6ePUv37t3Nvu/VDU9PTzQajSSoVhE19R4qUXOQ5qiEs+OIDCnOoaJwYuRyOS4uLuTm5tovmlVAALRsCTk54OoKbm7id7f873UcZUDoWDIyMkqvlM/68PVkabMAiJXF8pfvX9QKkR7OyovRaOTKlSs1Mhqgs/wg70UUUuXAFGA99hNSk0hiOctJQnQJOMtZJjCBdaxDQGAoQ9nIRqtCKoCrq5K6dT0B8PZW88cfk5xGSAXHz0+VSoWbmxvKapb/uCBe+T7xWVlZVdyTu5OafA+VqBlIc1TC2XHE3Ky+v+qVhNFoRKvVotfr7d/4hQviu6srCAJki9oQe0X8dVZ0Bh3fnfzO/F0ulzO02VBcta5ERUVZzb3qcHJyQK0Gyf9DogACYh5UgGHAeeBh7K9FNQmq3enOGtbYpEUtiEIhZ82aUbi6Kpk9+166dLm78uT26dPHnJu5uuLpKS40ZGZmVnFPJCoT0zOGhERpGAwGBEFAo9FIvqoSVYJKpar0uScJqqWQkpICiAF97M7Fi6BUwsSJ4OsCsfkPKO7Wo9/WFH6L/I34zHiLsocCH+Kjjz7Cx8eHl156qfJvwp99Bl9/LQZTatkSvvkGnESTJ1E17EdMM/Ml4IEosL7iwOPlkssCFpBKKgBDGcoCFlhE9C1IYVcElUrBqlUjHdhDCUdiElTz8vLQarW4uDjK61nCWdBqtVy9elXSkEnYhCAIyOVyYmNjncbSSOLuw9fXl6CgoEqbg5KgWgo3btwAHGB+mJQkpkBxcYFXXwVNflAl1yBQODCybRUjCAJfn/jaoqxr/a4M7zKc87vPk5aWRnh4OB07dqzcjl25AjodREWB0VgthVRXV9eq7kKJxAE/Av2BnmXYLwFRYOyWv6+tJAMrgFaI2tCyoAU+Bm4AaxCDJdmbpPy/LLJYxjJiiCGPPIII4mme5iEeKlZIvXo1lYkTw1i1aiTNm/s7oHf2x9nnZ1WjVCpxc3MjNzeXrKws/Pz8qrpLdx2VOUcFQSA+Ph6FQkFwcLDTBIuTcF4EQSAvLw+1Wi0JqhKVjiAI5OTkkJiYCEDdunUr5biSoGojrq6u9tXyXbwovjdqJPqoJpvMfhva7xhOyKFrhzifeN6ibFaXWahUKnr06MHu3bvZv38/HTp0qNwbcXT0nc9NmlTece2EQqGgZcuWVd0NqxiBDYiayTzgGrYJqgKwBfgUyAHOYJugKgB/Aq/lfw/EdkHVZOrrArwBHAIet3HfshJGGMtZjoBAFFEA5JBDNtksZSk55DCDGUX2i4xMpn//1Vy/nkH//qs5cGA6oaG+DuqlfXDm+elMeHp6SoJqFVHZc1Sv15OTk0O9evWqZaRqiarBzc2tqrsgcRdjmn+JiYnUrl27iFwkRf2tQvR6PUaj0X6rnib/1NatxfdsMfUEHqH2aT8fZzMfK6xNbVSrEQMbDwTEVCR79uzh+vXrXL16lcaNKzEYzOzZ4uLBlStw772Vd1w7YTQazel9nG1lfgXwbYHvtnh7JwDvAP+Wcb9kYAli4CMTXjbsZ4ro2xYYl1/WOf9lTwwY+Ju/2c9+5jCHvoj5htexjl/5lSUsoSXiw3IAAUX2Dw+/xcCBa7h1KxsAT08XVCrnut7WcOb56Ux4eXlx+/ZtKaBSFVDZc9SUd9zZfqMlnBdBEDAYDCgUCkmjKlFlmBbWdDpdEcFUCqZUheh0OvuGXTYJqq1aie85+YKqu301qn5+frz//vt2bbM4zp07R1JSEp07d8bHx6fI9ktJl9hzdY9F2czOM1HIxYnu6elJ586dOXr0KPv3769cQXXUKPFVTREEgWvXruHr61vVXSnCI4jRclsCu0upW1iL6oKoRd1pw35/Ah8gCp0K4ElgOqXf5ArmRd0HDAE8S9mnrJgeLnLJ5R3eIYcchjCEnvm65YlMZCc7aZn/Z40TJ24yaNBaUlJyAejQoQ5//jmZ2rU97Nxb++PM89OZkAIqVR1VNUclgUOiLGi1WkmrKlGllHTPckR6GklQrQoE4Y7pr1mjGiO+29n0V6fTcfLkSQCH5xfcu3cvubm5BAcHWxVUvz3xrcV3Pzc/Hmn9iEVZnz59OHr0KBcvXuT27dsEBgY6tM8S9icO2AU8hWhG6wv8ChykZEFVC8znjha1PWKe0mRKFlQLa1GbA4vz30vCWl7UxdhXSL3JTbblbEO1QcV9fe+jadOmTGUqBgzFCqTWOHgwjiFD1pGZKUYH7datPjt3TqRWrbvrgUWv11frFDSl4enpSYsWLfDy8rJf3m4JCQkJCYlqSs39xXdmEhMhJUVMhdKsGQhGyLkubrOz6W9KSgphYWGAYwVVvV5PdnY2Li4uhIaGFtl+K+sWYZfCLMqmd5yOm8ryQbt27dq0atWKixcvcvDgQUaNGsWkSZM4dkzH8uWV47gtUX4ygclANhACDMovt8VrwQXwy39/FpiAmLM0uZj69tKiyoFJiAGT7GGEJyAQTjhrWcte9pKuT+dh9cO4HXOjcePGPCF/osg+AQQwgxlWzX3/+usKI0ZsICdHB0DfviFs3z4BL6+aG3TNGunp6WzevJkuXbrQtm31zDVdGgqFgtq1a1d1NyQkJCQkJJwCyVnIRuzqs2Iy+23SRMzdmZsARi3IVOAWZL/j4Bg1vDU0Gg0AoaGhqFSqIttXnFqBzqAzf1cr1UztONVqW337in57J06cIDs7m/r16xMYGIrBcHc9mJcVLy9bvDEd3AdgInAvoka0NBKwFEQXAj/lt1HwP07GnXym5O/zAmLApAxE7ekaRC2uSUiNRcx5OqnAfhmIQZLm57cRihhReA4VF1INGPiTP5nGNB7ncf7hH4wYGeg1kE6tOjFixIhi7yPFCaq//RbBww+vNwupgwY1YefOidVSSK3o/IyIiCAtLY2///6bK1eu2KlXEhJ3cIZ7qETNYdq0aYwcObLEOv369WPevHk2t1mWZ9HXX3+dGTOKBuSTKB8XLlygQYMGZGdnV3VX7iokQdVG1Gq1/aJZFTH7NUX8DQZZ9bwkeXl5ADSxEjE3S5vF6jOrLcrGtR5HgHtR7RFA48aNqVevHjqdjn///ZdNmzaxf/+PuLsn2r/jIJpiV3MUCgVNmjQhMTGx0hYnQIzoux6ILFD2BGKE39L03/sQAxe9i6gdBfBG1MQWpBNwHNF82MQlRFNfBTATWE1RU189oiBsmjX7ueMvKwem5Pe9orq5TDJZwxpGMIJXeZXznMcFF0Yyko1s5AvZF0zsNBEPj7L7kl64cJu8PDHoysiRLdm69VHc3YsuBDk7pvlZkXto165dad26NYIgsGPHDuLj40vf6S4iJSWF1NRUdDpd6ZUlimCPOVplJCXB8uXiu4OZNm0aMpmMp58umrTr2WefRSaTMW3aNLses1+/fshkMmQyGa6urjRv3pwlS5ZY/a378ccf6dq1K+7u7nh5eXHfffexffv2IvUEQWD58uV069YNT09PfH196dKlC0uXLiUnJweAxYsXm49b8PXXX3/ZdXwlER8fz2OPPUbz5s1RKBS8/PLLNrkEJCQksGzZMl577bUi244cOYJCoWDo0KFFtu3duxeZTEZaWlqRbaGhoSxdutSibM+ePQwZMgR/f3/c3d1p3bo1CxYsMKd2dAQajYZnn30Wf39/PD09GTNmDLdu3Spxn6ysLGbPnk2DBg1wc3OjdevWfPPNNxZ1Cs4z06vgPG/dujXdu3fnk08+cci4agKOuH9WT6moEmnXrh0AderUsV80q8IRfx0USKkyMWlUmzcv6hn4U/hPZORlmL/LZDJmdplZbFsymYyBAwcyaNAgunfvzuXLl7l+/TxKZY79Ow7Qpw/06wdPPAHbtjnmGA7GaDQSHh7Ohg0b2LBhQ6UkkI8DZgCfIPqSmh6PFVhqP4ujAaJfahpi4KTSKDiiXsAsimpRC7e/GniPolrUFVRci3qDG3zMxwxlKMtYRgIJ1KIWM5jBdraziEU0pmIBwV56qTevvtqbCRPasnHjWNTq6umtYTQaSUhIqNC8lMlk9O/fn9DQUPR6PVu2bCElJcWOvazeXLlyhXPnzpkfsiXKhj3maJVRiYIqQHBwMBs2bCA3N9dcptFoWL9+PQ0bOuY55qmnniI+Pp6IiAheeeUV3njjjSKCxsKFC5k5cybjx4/n7NmzHDt2jN69ezNixAi++OILi7qTJ09m3rx5jBgxgj179nD69Glef/11tm7dyp9//mmu16ZNG+Lj4y1eJquvyiAvL4/AwEAWLVpEhw4dMBqNNi1Gf//99/Ts2ZOQkMJLv7BixQqee+459u/fz82bN8vdt2+//ZYBAwYQFBTEpk2buHDhAt988w3p6el8/PHH5W63NJ5//nl+++03fvnlF/bt28fNmzcZPXp0ifvMnz+fXbt2sXbtWi5evMi8efOYPXs22wo985nmmen1f//3fxbbp0+fztdff41eb0segrsPKepvFZKbm2sfTZUgFI34a9aoFr2hVAcEQUCn06FUKmnatGmR7dm6bFyVrmj0ojA7qMkgGtcq+QG+TZs2tGnTBsCxNwSNRkxJAxAZCfkLE9UNQRCIiYlBpVLh7+/v0PQKhfOiuiNqKst6M2kCfA+0pvQVs/2IAvFywOTBV9TT0xI10AqYBpzHfr6occTxBV+wl70Y88XnxjRmIhMZzGBc7OLpeod33nkAQQC5vPoG1hEEgYSEhAoHRzNpAX799Vdu3brFgQMHGDFihJ16WX0RBAGtVgy0JaU7KR/2mqMV6ID4e1QeNBowGsX3AsKjzbi6QhkCd3Xq1Ino6GjCwsKYOHEiAGFhYTRs2JBGjRpZ1N21axfvvPMO586dQ6FQ0KNHD5YtW2a2vlq9ejXPPPMMp06dolmzZgA888wz/PPPP5w8edKcCsPd3Z2gINE1avr06XzxxRfs3r2bWbNmAfDvv//y8ccf89lnn/Hcc8+Zj//uu++i0WiYP38+I0aMIDg4mI0bN7Ju3Tq2bNlicf8IDQ1l+PDhZGTcWVhXKpXm4xYmPDycuXPncuTIEdzd3RkzZgyffPKJOXp3YbKzs5k1axZhYWF4eXmxcOHCUs91aGgoy5YtA2DlypXmtEalsWHDBvO5KUhWVhY///wzJ06cICEhgVWrVvHqq6/a1GZBrl+/zpw5c5gzZw6ffvqpRX/79u1rVSNrD9LT01mxYgXr16/ngQceAOCHH36gVatW/Pvvv8XGYjl8+DBTp06lX79+AMyYMYNvv/2WY8eOMXz4cHO9gvPMGgMHDiQlJYV9+/bRv78tmd3vLhxh0SdpVEtBLpdjMBiIiYnh8uXLFW8wPh4yMkClEn1UATIiQHMbXGpVvP0qIC8vD0EQ8PLyspqkfl73eZyYcYIXer6Av7s/s7oUvXlaIzU1lR9//JFbt26RlpaAt3ecvbsOV6+iNxjIyclBbzDcuSbVjGvXrhEdHc3AgQPp1auXw45TUIuah+iL+jMwmtK1qCaDkII697aUfhOKQ/RbvQ6sLWN/L5w/z8PXr1MnJ4dhW7YwMi6uwmKkHDl72IMRIz3owRd8wYdxH5LyQwoJcQkVavvjjw/zxx+W9xmZTFaqkKrVaklLS6tUs++qQqVSMWLECFq3bs2gQWK4rri4OH744Qfi4hxwj6gGGAwGKS9ndUejEa17bH316AFdu4qvsWMhIkJ8N5X16GF7W+UQkB9//HF++OEH8/eVK1cyffr0IvWys7OZP38+J06c4O+//0YulzNq1Ciz5mXKlCkMGTKEiRMnotfr+f333/n+++9Zt26dWUgtiCAIHDhwgEuXLlnM9Z9++glPT09mzixqrbVgwQJ0Oh2bNm0CYN26dbRo0cLqIpdMJrOatcDauAYNGkStWrU4fvw4v/zyC3/99RezZ88udp8XXniBffv2mbW2e/fuNWdlsCcpKSlcuHCBLl26FNm2ceNGWrZsSYsWLZg0aRIrV64s1+/GL7/8glar5cUXX7S6vaQ0T4MHD8bT07PYl0lJYY3//vsPnU7HgAEDzGUtW7akYcOGHDlypNj9evbsybZt27hx4waCILBnzx4iIyN58MEHLeqtW7eOgIAA2rZtyyuvvFLEQsXFxYWOHTty4MCBYo8lYV8kjWopGAwGkpOTycnJYefOnTRv3rxiKQNM2tRmzcB0k828AnlJoHT+fIjWMJn9NmjQoNg6fm5+PN/jeZ7p+gwuCtsepFxdXYmKiiIjIwOdTkPDhge4fr0PthmW2obg4cG5++7DEBFBA62W2k2b2rH1ykEQBA4dOsStW7c4ceKEVfPrimJNizoPGIXtV6MTYnCjTmU8dl2gHqKp7mNl2O/q1avs3r0bhULBCHd34q5e5UB2No899pjN/8PZZLOZzSSQwELE1e8GNOAFXqAznWlCEwRBYP2B9URHR+Pi4lKm9k0IgsD//reP//1vH25uSnbtmkTfvrZbWCQkJBAbG0tgYCAtW9qe9qa64u7ubn7AMD24VuT8V3dM2lSFQlE9fSwlyk5qalFT34J+2wEB4EDt8KRJk3jllVeIjRUtwg4dOsSGDRvYu3evRb0xY8ZYfF+5ciWBgYFcuHDBHL3722+/pX379syZM4ewsDAWL15M586dLfb76quv+P7779Fqteh0OlxdXZkzZ455e2RkJE2aNLG6UFOvXj28vb2JjBSjKURFRdGiRQubxhkeHm6hIW3dujXHjh1j/fr1aDQaVq9ebY4/8MUXXzBs2DA++OAD6tSpY9FOVlYWK1asYO3atWZN3I8//ljic1N5iYuLQxAE6tWrV2TbihUrmDRJDDH40EMPkZ6ezr59+8yaRluJiorC29ubunXLno3h+++/tzAbL4y1gJwmEhIScHFxKSII16lTh4SE4heKP//8c2bMmEGDBg1QKpXI5XK+++47CzPuxx57jJCQEOrVq8fZs2d56aWXiIiIMGfOMFGvXj3zvJdwPJKgWgqHDh0yC2KXLl0iMjLS5hucVQr7pxryQHtb/OxaPdOvmAIpWbspFkattD1aqZubG40aNeLy5cvIZDJ8fWPw84skJaUC578QsUYj29u3x9CmDUqlkoluboTarfXKITY2lqtXr6JWq7l69SqxsbFWUwSVlzjgLeB0/vd7gdcpPVhSYdwR85SWFVX+8cpKSEgIjRo14tKlS9y4cQM3NzdiYmLKdH4SSGApS5Ej5zEeox7iHB/HOHOd2NhYYmJiUKvVZW4fREHrxRd389FH4mpwbq6eY8du2CyomkwWAWrVck6rDJlMhp+fn0MEyIqe/5qASVBVq6tfNGhnwZFz1CZcXaEsWpqkJEjOj5keEQEffAAvvQSm5xN/f1FYtfXYZSQwMJChQ4eyatUqBEFg6NChBFg5XlRUFG+88QZHjx4lKSnJrEmNi4szC6q1atVixYoVDBo0iJ49e/Lyyy8XaWfixIm89tprpKam8uabb9KzZ0969uxpUcdWzWBZNIgtWrSw8GM0/Y9dvHiRDh06WATJ69WrF0ajkYiIiCKCanR0NFqtlm7dupnL/Pz8yvw8aYtbj0kIdC10XSMiIjh27BibN28GRLPm8ePHs2LFijILqhXJ81y/fv1y7VcRPv/8c/7991+2bdtGSEgI+/fv59lnn6VevXpm7WzBCMnt2rWjbt269O/fn+joaItAoW5ublIsgGJwxP1TElRLQBAEIiMjEQQBhUKBTqdj165dFdOqmv1TG0D6Jci6DAaNGO1XEy+WAagDwNXGH5kqRqVS4eLiYveVQUEQSElJyb8hypHLdTRpsouUlObYQ6tq0kRmZ2cjCAIuLi4cOnSIkJCQaqORMY1Br9fj4+NDenq63cZgDy1qVSKXyxk8eDDnz5/HaDSi0+nM58va+THlP73EJbMg2oQmjGMcLWhhNcepqT2dTleu8280CsyevYOvvz5hLvv000HMm2d7zuPU1FTy8vJQKpVWHxSdAblc7pAgKxU9/zUF02KhZPZbfhw1R21GJgM3t9LrmQgOFl8gCppyOXToAJVoUfH444+bTV2//PJLq3WGDRtGSEgI3333HfXq1cNoNNK2bVvz4oqJ/fv3o1AoiI+PJzs7u0iqIB8fH3MMjI0bN9K0aVO6d+9uFjKaN2/OwYMH0Wq1Rf4Pbt68SUZGhtnaqHnz5ly6dMmmMbq4uFiNvVFVKBSKUu9tpt+B1NRUC5/rFStWoNfrLZQKgiCgVqv54osv8PHxwdvbGxB9QQtrLdPS0sxm0c2bNyc9PZ34+Pgya1UHDx5coulsSEgI58+ft7otKCjI7OpSsH+3bt0q1rc0NzeXV199lc2bN5sjHbdv357Tp0/z0UcfWZgRF8S0qHD58mULQTUlJcVqhgsJO6fyNLVp9xZrEJGRkdy+fdt8U/Dw8ODixYtm85EyYzTeSU0TcBWOTIJjMyE3XjT7Pf+uWHZkElwPK7ktJ8LPz4/69euXywSkJCIjI4mOjsbLywsXF3fy8rzx97+In185z38hTJoYo9GITCZDoVCYNTLVBdMYXF1dzeZQ9hiDETHoUHl8UZ2JmzdvYjAYUCgU5vfC58eAgd3sZjrTeZzH+ZiPuc1t8/YXeZERjLAaJMl0/t3c3JDJZBZa29LQ641Mn77VLKTKZLB8+cNlElIBc5qWOnXqOK3Zp9FoJC4uzu4RASty/msSppQ0kqBafhw1R2syDz30kNkU1+QvXpDk5GQiIiJYtGgR/fv3p1WrVqSmphapd/jwYT744AN+++03PD09S/TzBPD09GTu3LksXLjQrB199NFHycrK4ttvvy1S/6OPPkKlUpnNkB977DEiIyPZunVrkbqCIJCenl7q2Fu1asWZM2cscmoeOnQIuVxuVUvapEkTVCoVR48eNZelpqaW+XnSYDCUqhFu0qQJ3t7eXDApRhCDUq5evZqPP/6Y06dPm19nzpyhXr16/PTTTwA0a9YMuVzOf//9Z9HmlStXSE9PNwv7Y8eOxcXFpUhUXBMlBVP6/vvvLfpQ+LVjx45i9+3cuTMqlYq///7bXBYREUFcXBw9evSwuo9Op0On0xURohQKRYn/76dPnwYo8mx77tw57rnnnmL3u5uRov5WIoIgsGvXLovJrVaryc7OLr9W9fp1yM4WfVPvmQHZA+H406BwA7ka2iwC7/zVULV9NCN+fn7cf//9dmmrNAo+JO25uofO9TrjrfYuV1sFz7+fnx/Z2TJu3hRwccmmSZNdCELFtKoFNTG1a9fGaDSiUCjIyMioNhqZgmNwc3NDo9Hg6upqlzHIgfaI+UrnUX20qAUxnR+DwYC7uzu5ubnI5XJ0Oh2HDh3CP8SfrbKtbGADCYimsy64MJjBCJRuGlbw/JuCfri4uJCbm1vq+ddqDUyaFMYvv4gPEgqFjB9/HMnEie3LNEaNRmNO0VJSpMKqxmQdYU+Tr4qc/5qGpFGtOI6Yo5VGQADMmGG7qa+dUCgUXMxffLe2SFarVi38/f1Zvnw5devWJS4urohZb2ZmJpMnT2bOnDkMHjyYBg0a0LVrV4YNG8bYsWOLPfbMmTN5++232bRpE2PHjqVHjx7MnTuXF154Aa1Wy8iRI9HpdKxdu5Zly5axdOlSgvM10OPGjWPz5s1MmDCBRYsW8eCDDxIYGEh4eDiffvopzz33HCNHjixx7BMnTuTNN99k6tSpLF68mNu3b/Pcc88xefLkIma/IArXTzzxBC+88AL+/v7Url2b1157zSYNlElgysrKIjExkdOnT6NWq2ltciErhFwuZ8CAARw8eNA8ju3bt5OamsoTTzxRJFjUmDFjWLFiBU8//TReXl48+eSTLFiwAKVSSbt27bh27RovvfQS3bt3N5tbBwcH8+mnnzJ79mwyMjKYMmUKoaGhXL9+ndWrV+Pp6VlsipqK/I/5+PjwxBNPMH/+fPz8/PD29ua5556jR48eFhF/W7ZsyZIlSxg1ahTe3t7cd999vPDCC7i5uRESEsK+fftYvXq1OSdqdHQ069evN+eEPXv2LM8//zx9+/alffs7v8sxMTHcuHGjWC3s3Y4jAjpKgmoxREZGcvHiRXPSX5M9fkGtapl9VU2rWy1bgnsduLEZ5ErwbQc510Qh1ce+ZjtqtZrBgwfbtc3icMs3W0rMTmTa1mmoFWomtpvIk52epL532W5MpvPv4eFR4GFThlbrgb//ReLjI4Hy+6oW1MQUDEBSHj/GqqKwNgkoolUqyxhM8VJNxm8zgDGU3RfVWYiNjeX8rfNc6n6Jjlc6EqAJQC6Xc1t1m3W117EsbxlGV3H1rxa1eIRHGMtY/Cgaubq49stz/jUaPWPHbuT336MAUKnk/PzzWEaNalXmMZp8U319fa1GyKzJ2Hv+V2ek1DR3OSZBtQowmYpaQy6Xs2HDBubMmUPbtm1p0aIFn332mYU/5Ny5c/Hw8OC9994DRN/A9957j5kzZ9KjR49ihRo/Pz+mTJnC4sWLGT16NHK5nKVLl9K+fXu++uorFi1ahEKhoFOnTmzZsoVhw4aZ95XJZKxfv57ly5ezcuVK3n33XZRKJc2aNWPKlClWtcOFcXd3548//mDu3Ll07drVIj1NcXz44YdkZWUxbNgwvLy8WLBggU3a24Lau//++4+NGzcSEhJCTExMsfs8+eSTPPXUU/zf//0fcrmcFStWMGDAAKsRjceMGcP//d//cfbsWdq3b8+yZct4//33eemll4iNjSUoKIiBAwfy7rvvWiz+PfPMMzRv3pyPPvqIUaNGkZubS2hoKA8//DDz588vdVzl5dNPP0UulzNmzBjy8vIYNGgQX331lUWdiIgIi3O7YcMGXnnlFSZOnEhKSgohISG8++67PP3004B47/zrr79YunQp2dnZBAcHM2bMGBYtWmTR7k8//cSDDz5oNT+thGOQCXdDPoMSyMjIMPs2mW64giDw2Wefcfr0aXMgJUEQzBqL9PR0OnbsyJw5c8q2Yv/JJ7B+PYwfD/Ofg71DQZcOzZ6BqK+gx1q7C6qpqaksWbIEoFgTjYpiCk8+Z84cGjRowAcHP2DZ0WXm7R4uHpyeeRoPF9uiGhc8/6abak4OXL0qbnd1TadDh468/XYZz3+B9tevX0/G3r0M/ecf0vz9SfP3J7xrV3K8vMjMzKRFixZOHT3UNIaIiAi8vLwQBIG8vDzUajUymazMY9gPvIKY2/QH7qSSqa6Yzs+h5ENsG7eNCb9PwKAwcKrVKaIbRqM36nF1daWLXxcmysqe/7Tw+S9MSef/wIFYHnhgNXq9EVdXJZs3j+ehh8ruA2U0Gjl+/DharZaWLVtWXf5HGzAYDISHh9OuXTu7mCdX5PzXRM6cOUNGRgatWrVyWj9lZ8fec7Q0NBoNV69epVGjRkWC3khIWEMQBHJzcy0W50qq261bN55//nkmTJhQST2s2Wi1Wpo1a8b69esdmgbQ2Snp3pWamoqfn5+FTFVRJI2qFfR6PcnJyajVanP0NEEQzEKrWq0mOTkZvV5fYhjtIpj8U1u3hps7RCHVrR7UGwqC3m7mvgXRlDeBeDkwGAzk6HJYdWaVRfnIFiNtFlLB8vyb+q/TgVJp2q4mM7Mc579AP9PT06mXnk6t27epdVv0Rzzdti1atRq1Wk16ejoGgwGl0jn/RUxjUKvVaLVaBEHAaDSi1WqRyWRlHkMrwAXwBLKA0rPIOTem8+Pi4oJWoeX3Xr+T7nNndbVhfEN6x/ZmyYglqJTln0Om81+Yks5/nz4hrF07ihkztrN166P06xda5uODGNBBq9WiUqnw9/cvVxuVhUwmIygoyG5CY0XOf02kffv26HQ6p/VRrg7Ye45KSDgCW595ZDIZy5cvJzw83ME9unuIi4vj1VdfvauF1NKQov5WEiqVivnz55Odnc2HH35oLn/hhRfMnz09PcsmJBmNYIoy17IFXM1PkhwyAdzqQNOqMduxJwaDgZ/P/Uy6xtKUZWaXogm4S6Lg+Tfx33+Qn/oLgOefL+P5L4BSqWTy5MnI4uJwM5lLurjw6IIFYuRERBNgZ37ANY2hpFxkJY3BCBwFTKEHAoEfgWCqny9qYZJIIkmZRLep3XDXufO75+/k1sqlllCLHtoePJL3CB0bdsSthVu5hFSo+PkfP74tAwc2wc+vDFE+C2EKohQUFOSQSHv2RC6X29WHtqLnv6Yhk8kks98KYu85KiFhb2QyWZmeezp27EjHjh0d16G7jKZNmzpVBGhnxBHPInfHr3g58PX1xdfX1/zjn5eXR1BQUPlXrGNiIDdXDD/vcR1y4kDpCQ1G2K/TVYwgE/j2P8uIewObDKSpX9n/sU3n38TVq5CZeWd7Rd3xvLy8oHNnuHULoqPB3Z3a1ewhxcvLy2z2aDAYiImJITQ0tNQ5WjAv6lKgd355FSZmsCsb2MDXfI2bhygE+uBDCikoUXJKdYquHl15kAcrfJyC578kEhKy+PPPaKZM6WBRXhEhFTAHAasOD9dlmZ+2Yuv5l5CwBUfMUQkJe1LYxUdCwtkwGAx2b1MSVEtBqVSi1+srHnLZZPbbsiXErhc/B48GZc0JgLLn+h7i0uMsymZ1mVVFvbGBRx4RXwA1wFU7s6AkbwVreVFL3qP6cY1r/MZveOLJMpaRQALv8A6f8AktEf2/reVDdVh/rqXTv/9qoqJS0Gj0zJjR2W5t16lTx2p0SWeltPkpIVHVSHNUwtmR0idJ3G1Igmop2E2NbYr429YPUv8GmQJCHrVP206AgMCP53+0KOsY1JFu9btVUY/KSA1fnSyoRQUxL+oioF5xO1RT6lIXf/xRo6YWtfDFF4CW+X+VSXR0Cv37ryY2VjSFX7LkIJMmtcfdvXzmxhISEhISEhISdxOSoFoKdlu9MmlUg/M1jkEPgmtt+7RdBWRmZooBieqJok68PJ6I9AgLn7CnuzwtmadUMda0qPOonnlRi+M0p2lDG1SoUKLkIz7CF1/ccecSl6qkTxcu3GbAgNXEx2cB0LSpH3//PUUSUiUkJCQkJCQkbEQSVEtBr9cDYoCfcgtdBoMYSMldB8qLgBIaTSp1N2fmxIkTrFy50uyof1p52mJ7sE8wQ5oNqfyO3aXIZDKCg4Mt5mhN16LmkMMylrGJTTzFU8xEDNpVr8AIAwhgBjMcau6r1+vZtm0bAMOHD+fcuSQGDlxDUlIOAG3aBLJ792Tq1r17/SmtzU8J28jKyiIzMxM/Pz/UanVVd6fGIs1RieqAFDRNwplxxP3TuUNFOgG1a4taT6VSWX4z4Oho0GqhfTaolODXBbxb2LGXlU9UVBQADRs2JFWWSowixmL7jE4zUMqldZDKQi6X4+/vj1wuxwisByYgCqnuwKuIWtXqKqQmkcRylpNEEgDHOc54xrOJTQBkkYVAUT/jyhBUjUYjcXFxxMXFcfTode6//0ezkNqpU1327p12VwupYDk/JcpGdHQ0ly9fJi0traq7UqOR5qiEsyOTyVAqldJiioTT4oj7p3RHLgWTaatGoyl/NKuLF0FpgDY5or1l6ET7dbCKMAmqzZo144zyjMU2b7U3j7Z1cv/bpUvhqadgyRLYvr2qe1NhDAYDly5d4prBwAzgE0RT33sRTX9HU71NfU2C6jWusYQlzGIW8cRTj3p8zdcsYAGyKh5hZmYeQ4asJy1NzP3bs2cw//wzhYCAmhMwrbyY5qcjIgLWdEyRjbOysqq4JzUbaY5KODuCIJCbm4tQA4I/StRMHHH/lATVUjh9+jRAxW4MFy5A03RwV4JHCARWXrJgR5iJZGVlkZCQAIBvPV8ilBEW26d2mIqHi4fdj2tXDhyA33+Hzz+H776r6t7YBY1GgwyIpGZoUQuTTTbzmW/Woj7CI2xgA13pWsU9A6NR4OrVNLKztQA88EAj/vhjEj4+rlXcM+dBo9FUdReqJZ6enoAkqFYG0hyVsCfTpk1j5MiRJdbp168f8+bNs7nNsjyLTp48mffee8/m+hIls2vXLjp27ChFXq5kJEG1Mrh4HlqkgJurqE2VVd5p9/Pz4/333+f999+3W5uXL18GICgoiF8v/4oBcQVFJpOhUqh4/J7H7XYshxEdfedzkyZV1w8bSU1N5fbt21a3pRX4XBd4j5qjRb3EJf7hH97mbeKI4xrX8MKLl3iJJ3gCd5xDWymXy2ja1A8vLzVDhzZj+/YJeHpKvkQSFaegRlXSpEhYIyknieX/LScpJ8nhx5o2bRoymYynn366yLZnn30WmUzGtGnT7HrMfv36IZPJkMlkuLq60rx5c5YsWWL1/+HHH3+ka9euuLu74+XlxX333cd2K1ZTgiCwfPlyunXrhqenJ76+vnTp0oWlS5eSkyO6bixevNh83IKvv/76y67jK4mwsDAGDhxIYGAgPj4+3H///fzxxx+l7nfmzBl27NjBnDlzimz76aefUCgUPPvss0W2rVq1yiKHfUFkMhlbtmyxKNu0aRP9+vXDx8cHT09P2rdvz1tvvUVKSopN4ysPKSkpTJw4EW9vb3x9fXniiSdKXciLjo5m1KhRBAYG4u3tzbhx47h165ZFndDQ0CLXuuCz80MPPYRKpWLdunUOGZeEdSRB1dFotZD7H3jowCsI6g2t1MPrdDqOHTvGsWPH7NZmQbPfiCRLberolqOp4+nkuR31eujYURRQlUpo2rSqe1QqR48eZd26dUWu48/Aw0DB0t5Ufy2qDh3v8A596cswhrGFLQBo0JBOOh/wAWGEVW0nC+HurmLfvqmEhY3HzU2K7ithH1xdXVEoFBiNRrKzs6u6OxJOSGUKqgDBwcFs2LCB3Nxcc5lGo2H9+vU0bNjQIcd86qmniI+PJyIigldeeYU33niDb775xqLOwoULmTlzJuPHj+fs2bMcO3aM3r17M2LECL744guLupMnT2bevHmMGDGCPXv2cPr0aV5//XW2bt3Kn3/+aa7Xpk0b4uPjLV59+/Z1yBitsX//fgYOHMiOHTs4ceIEffv2Zfjw4Zw6darE/T7//HMeeeQRs0VGQVasWMGLL77ITz/9VCErgtdee43x48fTtWtXdu7cyblz5/j44485c+YMa9asKXe7pTFx4kTOnz/P7t272b59O/v372fGjBnF1s/OzubBBx9EJpPxzz//cOjQIbRaLcOGDSuiHX3rrbcsrvVzzz1nsX3atGl89tlnDhmXhHWkaDc24uLiUj4n4cuXoXkiKBTQdCIoKjdqY0pKCmFh4gN99+7d7dJmQUF1xv0z0B7WckZ5hkR5Ik93KbrK6nQolbBqlfhZrwedrkq7UxqZmZlERIgLAiEhIRbbYgEN8IdczvzGjWtMIJATnGAve6mNGMwslFCucIX3eM+cD9WRAZJsYe/eGPr2DUEul9O1q2h+3KZNHRQKRZX2yxmRy+U0rkHzszKRyWR4enqSnp5OVlaW1QdPiYpT1XNUEAQ0+vIJDRqdBqNgRKPTkKvLLX2HQrgqXcsUoKdTp05ER0cTFhbGxIlizI2wsDAaNmxIo0aNLOru2rWLd955h3PnzqFQKOjRowfLli2jSb4l0+rVq3nmmWc4deoUzZo1A+CZZ57hn3/+4eTJk7i7i1Yz7u7uBAUFATB9+nS++OILdu/ezaxZswD4999/+fjjj/nss88shIt3330XjUbD/PnzGTFiBMHBwWzcuJF169axZcsWRowYYa4bGhrK8OHDycjIMJcplUrzcQsTHh7O3LlzOXLkCO7u7owZM4ZPPvmk2P/R7OxsZs2aRVhYGF5eXixcuLDUc7106VLzZ0EQeP/999mxYwe//fYb99xzj9V9DAYDv/76q1XN39WrVzl8+DCbNm1iz549hIWF8dhjj5Xaj8IcO3aM9957j6VLlzJ37lxzeWhoKAMHDnRY8LeLFy+ya9cujh8/TpcuXQBRKB8yZAgfffSROa5MQQ4dOkRMTAynTp3C29sbEDXvtWrV4p9//mHAgAHmul5eXsVeb4Bhw4Yxe/ZsoqOjzXNY4g6OuH9KgqqNKBSK8kVau7AT/DWg9oaGj9i/Y6Vgb1Mxg8FAdL7ZrOlHpY6xDg9qH+TRYY/SIqCaRTNWKsWXE3P69GkEQaB+/foE1qlDBuCdv2020BoYKpMh8/YuvhEnRoeOPewhl1xGID40dKMbnelMV7oyghGkk84kJtEy/6+qWbbsX+bN+4NZs7rw5ZdD6NWr8vzOqyMymcz8gCBRdry8vMyCqoRjqOo5qtFr6PNDH5vr64169Ea9ed/4rHjG/jIWV6XoF6+UK22OvH9g+gHcVG5l6u/jjz/ODz/8YBZUV65cyfTp09m7d69FvezsbObPn0/79u3JysrijTfeYNSoUZw+fRq5XM6UKVPYvn07EydO5PDhw/zxxx98//33ZuGvMIIgcPDgQS5dumR+BgHRnNXT05OZM2cW2WfBggV88sknbNq0iXnz5rFu3TpatGhhIaSakMlk+Pj4lDr+7OxsBg0aRI8ePTh+/DiJiYk8+eSTzJ49m1WmhfBCvPDCC+zbt4+tW7dSu3ZtXn31VU6ePGlO81caJnNUU7qq4jh79izp6elmQa4gP/zwA0OHDsXHx4dJkyaxYsWKcgmq69atw9PTk2eeecbq9uLMh0HUUsfGxha7vU+fPuzcudPqtiNHjpjNtE0MGDAAuVzO0aNHGTVqVJF98vLykMlkFum9XF1dkcvlHDx40EJQff/993n77bdp2LAhjz32GM8//zzKAs+IDRs2pE6dOhw4cEASVK3giIjUzv2E7kTk5uZiMBjKri1J3Cy+q7qAuvgbS3Xh2rVraLVa3Nzciqxc+ap8q6ZTNRitVkt4eDgAQV27MgPRXv+b/Hd3RNNfg8HAhQsXaN26dbXT6B3hCK/yKrWoxRCGoEKFHDkrWGGuk056FfbQkvfeO8Brr/0DwNdfn2DIkGY8/HDzKu6Vc1Od56czYNLQZGZmVnFPai7VbY6malKLmPrGZ8WbPwe4BxDoHuiw40+aNIlXXnnFLHAcOnSIDRs2FBFUx4wZY/F95cqVBAYGcuHCBdq2bQvAt99+S/v27ZkzZw5hYWEsXryYzp07W+z31Vdf8f3336PVatHpdLi6ulr4X0ZGRtKkSROrASTr1auHt7c3kZGRgGgV1qKFbYvq4eHhFhrS1q1bc+zYMdavX49Go2H16tV4eIjBI7/44guGDRvGBx98QJ06li5QWVlZrFixgrVr19K/f39A1Oo1aNDApn6AKKS/9957ZGVlMW7cuGLrxcbGolAozOkVTRiNRlatWsXnn38OwKOPPsqCBQu4evVqEU14aURFRdG4cWNUqrK7uezYsQNdCZZsbm7FL5okJCQUGZdSqcTPz88c5LMw3bt3x8PDg5deeon33nsPQRB4+eWXMRgMxMff+Z+ZM2cOnTp1ws/Pj8OHD/PKK68QHx/PJ598YtFevXr1ShS072YcEfVXElRLQalUotfry7dz9jXgovi54QS79akquX79OgBNmjQpouK39cddp4MzZ6AsrhFnz9petyZx/vx5NFotkffcwy8hIeQhCqdXgcJredUhrYJJe6pHzxCGANCLXrSmNT3piQ4dKor+8FVGPtTSEASBRYv+4b33DprL3nijL0OHNithLwkT1WF+OiumB+Xs7GyMRqNkQu0gqnKOuipdOTD9gM31k3KSSM5JBiAiOYIPDn3AS71eooW/KID5u/sT4G7b/dKkhS0LgYGBDB06lFWrViEIAkOHDiUgoOjxoqKieOONNzh69ChJSUlmn8C4uDizoFqrVi1WrFjBoEGD6NmzJy+//HKRdiZOnMhrr71Gamoqb775Jj179qRnz54WdWy1ICuLpVmLFi3Ytm2b+btJK3fx4kU6dOhgFlIBevXqhdFoJCIiooigGh0djVarpVu3buYyPz8/mwVmgPXr17NkyRK2bNlSRFgrSG5uLmq1uoh2a/fu3WRnZzNkiPjbGxAQwMCBA1m5ciVvv/22zf2AilnrFXZhcjSBgYH88ssvzJo1i88++wy5XM6ECRPo1KmTxb10/vz55s/t27fHxcWFmTNnsmTJEgttrJubmzngloTjkQRVRxK9BvI0cNMTJvav6t7Yhd69e9OxY0erQT0KBlYoDp0OunWDUuIASOSjCQxk+/Dh3AoKwk0moyvwOtUvWFIccWxhC7/xG6mkUpvaDGIQivy/1awucX+ToOpokpOTMRqNBAZaaiIEQeD55/9g2bKj5rIPPhjAiy9KJr8SjsfV1RWVSoWLiwtarRZXVyntUU1DJpOVyfw22CeYYJ9gAFxVrshlcjoEdaBlQOW5Rjz++OPMnj0bgC+//NJqnWHDhhESEsJ3331HvXr1MBqNtG3bFq1Wa1Fv//79KBQK4uPjyc7ONke7NuHj40PT/MCHGzdupGnTpnTv3t1sttm8eXMOHjyIVqstolW9efMmGRkZNG/e3Fz30qVLNo3RxcXFfNyqZMOGDTz11FOsXbvWwlTVGgEBAeTk5BQ5FytWrCAlJcVCY2k0Gjl79iz/+9//kMvleHt7W10QM/mcmsyiTedbp9OVWataEdPfoKAgEhMTLcr0ej0pKSkl+pY++OCDREdHk5SUhFKpxNfXl6CgIBo3blzsPt26dUOv1xMTE2OxoJCSklLkGUHCcUjLsqUgl8vJcc/hXM9zJFGGiHq6DLjyKwjAjVAoYfWruhGdFU1g7aL/pIV/eKxx5ox9hNRyP6cZjRhnzSJp4ULSV6zAGBvLqVOn2LNnT5FQ5VWJEfgJWNigARmNG+Pv5sYrwFdUHyFVh44/+ZNZzGI0o1nNalJJJZBAhjMcLaXPl8okPDycNWvWsH//fotyg8HIjBm/WQipX3wxWBJSJSoNmUzGvffeS6dOnSQhVcJpeOihh8ymuIMGDSqyPTk5mYiICBYtWkT//v1p1aoVqampReodPnyYDz74gN9++w1PT0+z8Fscnp6ezJ07l4ULF5o1e48++ihZWVl8++23Rep/9NFHqFQqsxnyY489RmRkJFu3bi1SVxAE0tNLdzVp1aoVZ86csVi0P3ToEHK53KqWtEmTJqhUKo4evfM7kpqaajZHLomffvqJ6dOns379eh566KFS65t8Xi9cuGAuS05OZuvWrWzYsIHTp0+bX6dOnSI1NdUc6bhFixbo9XpOnz5t0ebJkycBzML+Y489RlZWFl999ZXVPpQUTGnHjh0WfSj8+v7774vdt0ePHqSlpfHff/+Zy/755x+MRqOFtro4AgIC8PX15Z9//iExMZHhw4cXW9fkR11Qe63RaIiOji42kJWE/ZE0qqVgNBrJ9cjlYu+LpMhTqIONqVfifoWcdEhTQ53u4AAH46rgaupVhqwfQjO/Zjzd5WlGtxpdpv3tkU/9nnugQ4dy7pyQAFu24JqTI+YY/ewzoj08uH79OvXq1StirlMVXAPeAkzyfFfgdZmsRAHV9OPoDCaBhbWnADJk9KQnoxlNb3qjwPl8wEJCQlAoFKjVavR6fb7Zv5GpU7ewfr3oJyyXy1ixYjjTpnWs2s5WM5xpflZXpHPnWKrzHA1wD2BG5xk2m/raC4VCwcWLF82fC1OrVi38/f1Zvnw5devWJS4urohZb2ZmJpMnT2bOnDkMHjyYBg0a0LVrV4YNG8bYsWOLPfbMmTN5++232bRpE2PHjqVHjx7MnTuXF154Aa1Wy8iRI9HpdKxdu5Zly5axdOlSgoNFDfS4cePYvHkzEyZMYNGiRTz44IMEBgYSHh7Op59+ynPPPcfIkSNLHPvEiRN58803mTp1KosXL+b27ds899xzTJ482epzhKenJ0888QQvvPAC/v7+1K5dm9dee63U+bZ+/XqmTp3KsmXL6NatG2lpaaSnp+Pu7l5s0KfAwEA6derEwYMHzULrmjVr8Pf3Z9y4cUVMgocMGcKKFSt46KGHaNOmDQ8++CCPP/44H3/8MY0bNyYiIoJ58+Yxfvx46tevD4jaxhdffJEFCxZw48YNRo0aRb169bh8+TLffPMNvXv3togGXJCKmP62atWKhx56iKeeeopvvvkGnU7H7NmzefTRR81xU27cuEH//v1ZvXo19957LyAGkWrVqhWBgYEcOXKEuXPn8vzzz5sXFY4cOcLRo0e5//778fLy4siRIzz//PNMmjSJWrVqmY//77//olar6dGjR7nHUJORov5WAWb/1LLImQYtxP5MilGNV2QdVA+0KdMx0xD9EIuGBKh6lv+3HEEQiEyOZP4f8/ny+Jd0pzuyMp2gO3z5JbRvb3t9V1dRSC2H/77IlStmHxmZXI68aVMo4ExflRgR86J+AWZf1LnAaGybftaCSFQWJt/TzWzmOMfN5YEEMiL/ry51q6x/tuDt7c0TTzxhEWnylVf+MgupSqWctWtHMX5826rqYrWmKuenhIQtVNc5ahJUq4KSIiXL5XI2bNjAnDlzaNu2LS1atOCzzz6jX79+5jpz587Fw8OD9957D4B27drx3nvvMXPmTHr06GEWjArj5+fHlClTWLx4MaNHj0Yul7N06VLat2/PV199xaJFi1AoFHTq1IktW7YwbNgw874ymYz169ezfPlyVq5cybvvvotSqaRZs2ZMmTLFqna4MO7u7vzxxx/MnTuXrl27WqSnKY4PP/yQrKwshg0bhpeXFwsWLChVe7t8+XL0ej3PPvsszz77rLl86tSpxUYXBnjyySdZvXq1WTu9cuVKRo0aZTUq65gxY5g8eTJJSUkEBATw888/8+abbzJz5kxu3rxJgwYNGDVqFK+//rrFfh988AGdO3fmyy+/5JtvvsFoNNKkSRPGjh3L1KlTSxxXRVi3bh2zZ8+mf//+yOVyxowZY5HbVKfTERERYeFHasq/m5KSQmhoKK+99hrPP/+8ebtarWbDhg0sXryYvLw8GjVqxPPPP2/htwqidnvixIlWI1JLOAaZYO/8JdWMjIwMfHx8SE9Pt7jhJuX/bdi1jX+M4Zzr+B/f13mH1orWgOgzV1xgF8313/k6M4L1tR7g/t1n+L/QxtCn9LDzOmBl/usexMiuFSU+Pp5PP/0UgP/7v/+rUFvJOcl0+a4Lefo8c9kzXZ8hY7uYc2zSpEm0L0XqPHjQ8lQcOAC9e1eoW2Xj77/JWrgQY0wMLnI5rhcu8OuBA1y/fp3BgweXKbCBPbGqRcV2M1+DwUB4eDjt2rWrkoiVj/M4ZxEjXlUH7amtJCRk0bfvD8TGpvPrr48wbFg1S7/kJFT1/KyppKWloVKpcHNzq5aaQGeisueoRqMxR1uVzLklbEEQBHJzc3Fzcys1DUhubi4tWrTg559/lrR/diIpKYkWLVpw4sSJMkdJrkmUdO9KTU3Fz8+viExVESSNajGEEcZylhPdO4Q091dRGM7ynvw98/YZ+X+FOSMI/M81kDivRpBiICaoHrRqVerxIoDFQFT+d2cMfP3jmR/J1eehQdT2yeVKRnd6kh9+/xRZdVnv6N+fox99xPnTp+nVsCGdncR3eB2ikFpWLWpVoEPHPvbRl7645Ov97+M+4omvNtpTWwkK8uTvv6cQFZXCAw9U3g+TIAgkJiYSEBAgCXYSVhEEwZy6qlu3btVWGyghIWF/3NzcWL16NUlJZYitIlEiMTExfPXVV3e1kFoVSIJqMYxmNH3py5TjqzjZW1ypftX4qoVGtTDJwCx9FlqVJy56HVqNSrRVtRKy3URBLaoB0dzXniFm/Pz8uP/++yvcjkavYeXpH0hDNFGVAS4tRzHFM4jubm645+RUmwel1NRUjAoF7m3bVqnvsMAdYXQ2kAM8jfMHS3qcx7nIRd7lXQYhmkg9yqNMYlK11p4CpKbmolTK8fK6E4o+ONiH4ODSE8Dbk7S0NCIjI4mNjaVr164OSaItUb0pGLyuPLkMJSQkajYFTawlKk6XLl3o0qVLVXfjrkMSVIvBZNrrnuWOSZxoQQtaUnzod39g2o3txOsy6BN3mxfrjoZCIdYLUliL2h8YhSi02Au1Ws3gwYMr3M6vF34lJScZd0CDKFDX7vK0RZ2SkjQ7G3K5HF9fXwAaNGiAq6trkXD4jsLki3oS+D/E2eWJaPrrbOjQcYAD9KUvyvzbRR/6kEQSOu4k7FajLq6JasPt29k8+OBafH1d2bHjMdzcqu7h35SE3N/fXxJSJaxiElRdXFykOSIhISEhUSORBFWbkRXxAdIg+pEOAZoDZEbz1KWPkSHnRNw4qAsUY6O9FXgPUYvqC7wMDABKD1ReNlJTU1myZAlQdh9VAfgdqCMY+eaE6DHrmv+6L/Q+ogNbYSxQv7qYKI4dO9YcUAmge/fulXr8eOBzRM35AaCvHdqUy+W0a9fObn5q17jGZjabI/d+yIfcj6iZn8xknuCJaq89LcjNm5kMGLCaixdFM6mnn/6dH38cWSV9ycvLIzk5GaDEvHDVCXvPT4k7gmrBRPQS5UeaoxLVgeqkEJC4+5Ci/lYRMgFcNGoCvCxNeD9H1IwdA9YC8ph1ou61Tj/4JxF6U6yg2hpRk9YfeAnwyy93BdojCq/2QFPOfDC3gXeBg4Dyyl/Epl6x8Jmc1WUWCwvtYzAYytfJKqCyH0YKmvnWB+YhJjG2ZxwprVZboaAcOnTsZS+b2cwxjpnLAwkkl1zzdzdq1g9lTEwa/fuv5soVMZVO/fpevPpqZUb4siQhIQEQE6t7eHhUWT/sTUXnp4QleXliULvq4nJRHZDmqISzIwiCZEEhcVchCao2IiCIWrgCSqQnEM03nwHkeclwc6e4oc4YiP9C/JwvqOoQg+Xcm79vM2ADEFroOA0R/VWrCpMW9WMgE1ABeSe+JiszE5lMhru7O23qtKFPw6JRjJ1eUD15EnbvhiZNoHFjaNeuAnlubOca8DaiSbcpJvI4Ox/DaDQSERFRroiVhbWnULMi95ZEZGQyAwas5to1MXJ1o0a+/P33FBo1qlXKno5BEASzoFpTtKlQsfkpYZ2Cpr8SFUeaoxLVAY1GI2lVJZyWgtaK9kISVG1AkIHWLY9DZHKTOmYfUj9gPfmasriNIOjAtz3E559WlQpcXMgCngKigdVg9nINrcQx2EJBLSqIWt+x8SeZce2IWQiVyWXM6jLL6oqe0/+4HzoEy5bd+R4V5VBBtXBe1A8Rr78zrIWWpD2taZF7i+PcuUQGDFjNrVvZALRsGcBff02mfn37hFQvDykpKWi1WlQqFQElBGGTkJAEVQkJCQmJmo4kqJaCyUTUKG/OG/L6uAJtgX7522UABg3E/SoWhE6E3RfEz/mrXp6IQultyNdXORfWtKgzgCnAMye+xaAXhVSFQkFdz7qMaDHCejvOnqLmypU7n4OCwIFmldeA/wGn87+b8qI6g5B6ilO8yIt3nfa0IP/9d5MHH1xLSopo0ty+fR12755M7dpVa2prCqJUp04dyVdOokQkH1UJCQkJiZqO9CRUDEkkcYlL5HrlIopyruSSSw9u40kkSRTITXVjO+jSwa0e1LkfLlxAJggo1WrzSsBLwEagtLTLV4DBwCT7D8kqt4HnEaMPZyJqUdcB04HrabH8HvU7er0eAKVSyZOdnkSlsK6FzMjIKPV4OXJgAaItbBmltsvAi1QgfU9uLijzr0iTJuVtpUSMwE/Ao4hCqjvwCvAVjk87U5xGW4eOBBLM3xvRiCyyCCSQJ3mSbWxjGcu4j/vuCiE1PPwWDzyw2iykdu1ajz17pla5kJqbm0tqqrh4UJPMfk04vcVFNUPSqNofaY5K2JNp06YxcuTIEuv069ePefPmOeT4kydP5r333nNI23cju3btomPHjg4xb5UoHklQLYYwwpjEJC613odBYQTZbW7LJrKH+3maxwgjTKwoGCFmvfg55DGQyeHCBTpfusS/V6+SvwVf7gRMKgk9ovCYXMb+pgALEYUiWxCA7Yi+kgcRtajPAj8AjfPrfHfyO4yCEb1BFFQ9XTyZ1L78IvRh4M2WiFLcQ4gOuWXgIGLE3Evl7cA330B0NBw4AG/ZPxnMNWAmomY6D1GLugEYg+M1qQqFwqpv1XGOM4QhvMZr5jJffFnBCraznad5usab+BameXN/undvAECfPg35668p+PmV7vMTFxfH7t27iYuLs3ufUlNTOXbsGHq9nlq1atU4H6Ti5qdE+cnOziY7O7vcAfMkLKnOczQnKYf/lv9HTlKOw481bdo0ZDIZTz/9dJFtzz77LDKZjGnTptn1mP369UMmkyGTyXB1daV58+YsWbLEqhXXjz/+SNeuXXF3d8fLy4v77ruP7du3F6knCALLly+nW7dueHp64uvrS5cuXVi6dCk5OeJ5XLx4sfm4BV9//fWXXcdXEgcPHqRXr174+/vj7u5Op06dWLp0aan7nTlzhh07djBnzpwi23766ScUCgXPPvtskW2rVq0yp+4rjEwmY8uWLRZlmzZtol+/fvj4+ODp6Un79u156623SElJsWV45SIlJYWJEyfi7e2Nr68vTzzxBFlZWSXuk5CQwOTJkwkKCsLDw4NOnTqxadMm8/a9e/davdYymYzjx48D8NBDD6FSqVi3bp3DxlbdccT9UxJUi2E0o1nLWkb/0IJOB95FqX2U2kIUMmTMYQ6jGS1WvH0QcuJA6QUNhkNGBty4IW5rWXzO1eIIQfR7/dKGujcRFZN5+a+9wD4b9hMQBdrFFNWimqaYwWhgb8xe8XO+6e+ENhPwVpfdfy8LMUfoHCBVBSQCs4BSnvdzETWRJj3tNuAiUKG1LJVK1Ka2alWRViwoqEU9BbhReVpUE4IgkJGRgVbQkkiiuTyUUNJJ5yY3yeCOxrs1re8K7ak11GolmzeP5+WXe7Fr1yS8vUs3nTQajURFRZGXl0dUVJRdV1QFQSAmJoacnBzkcnmN1Kaa5qfTuwdUEwRBwM3NDUEQiI+Pl86rHajOc7QyBVWA4OBgNmzYQG7unWjwGo2G9evX07BhGVegbeSpp54iPj6eiIgIXnnlFd544w2++eYbizoLFy5k5syZjB8/nrNnz3Ls2DF69+7NiBEj+OKLLyzqTp48mXnz5jFixAj27NnD6dOnef3119m6dSt//vmnuV6bNm2Ij4+3ePXta4+kcrbh4eHB7Nmz2b9/PxcuXODVV19l0aJFLF++vMT9Pv/8cx555BE8PT2LbFuxYgUvvvgiP/30U4UWul577TXGjx9P165d2blzJ+fOnePjjz/mzJkzrFmzptztlsbEiRM5f/48u3fvZvv27ezfv58ZM2aUuM+UKVOIiIhg27ZthIeHM3r0aMaNG8epU6cA6NmzZ5Hr/OSTT9KoUSO6dOlibmfatGl89tlnDhtbdccR909JUC2GAAJoSUtayVrR8VgtfJJU9BX6EkwwoxhFAPmBTq6uFd+DR4PSHS5ezP8eXGxqmpJQI+ZkbVxKPSNiFNmtwHdlPIYMMfqsNS2qCYVcwZ6pe3i548v4aH1QyBTM7TO3jEcStajjEIVMGTDgNjAaOIEoMRfDSWACYgTkj/PL3gaWA03L3AvHYU2L+jOVo0UtSKwxlncy3mEIQ1jMYnN5IIGsZCXb2Y43VRckqKrJy9NbfHd3V7FkyQDc3W0LpnXt2jXzD7pGo+HatWt261taWhopKSmoVCr0en211OiUhtFo5MqVK5LJlJ1IS0sjPT0dlUpFamoqaWlpVd2lak9Vz1FBENDl6sr30ugQjAI6Tfn2L+vDZadOnQgODiYsLMxcFhYWRsOGDbnnnnss6u7atYvevXvj6+uLv78/Dz/8MNHR0ebtq1evxtPTk6ioKHPZM888Q8uWLc2aTQB3d3eCgoIICQlh+vTptG/fnt27d5u3//vvv3z88cd8+OGHLFy4kKZNm9KqVSveffdd5s2bx/z588337Y0bN7Ju3Tp++uknXn31Vbp27UpoaCgjRozgn3/+4f777ze3q1QqCQoKsniZzO3Dw8N54IEHcHNzw9/fnxkzZpSo2cvOzmbKlCl4enpSt25dPv7442LrmrjnnnuYMGECbdq0ITQ0lLFjxzJo0CAOHDhQ7D4Gg4Fff/2VYcOGFdl29epVDh8+zMsvv0zz5s0trmFZOHbsGO+99575nPfs2ZPQ0FAGDhzIpk2bmDp1arnaLY2LFy+ya9cuvv/+e7p160bv3r35/PPP2bBhAzdv3ix2v8OHD/Pcc89x77330rhxYxYtWoSvry///fcfILpQFLzG/v7+bN26lenTp1sEDx02bBgnTpywmMMSd5Ci/lYB9erV4+KNi8iMMhYIC6hLXbzwAkBIP88O5R4elPuiChkv7nAhP5CSHTV21pAj5uJcC4y0oX6OhwdRiGlxQNT+9aZk61uVQkXTvKb0v9mfBh0aUN+7vs39ywI+QRRQAYKBN4Dsm/BlXvH75SJqkzfkf6+D6LML0Mbmo1cO2YgBpzIRtahzEWXwylr90aFjH/sII4xj8mPk+OXgjjsxxJCD+BmgLW0rqUfOyerVZ3j77f3s2TOVBg3KLqybtKmmhzlBELhw4QJJSUnI5XKaNGnCoUOHAHjwwQfLJGiatKkGgwFXV1c0Gg0xMTH4+vpKufIkrCLNmZqJXqPnhz4/2FzfqDdi1BvN+2bFZ/HL2F9QuoqPdXKlHLnStl+j6Qemo3IrWwT8xx9/nB9++IGJEycCsHLlSqZPn87evXst6mVnZzN//nzat29PVlYWb7zxBqNGjeL06dPI5XKmTJnC9u3bmThxIocPH+aPP/7g+++/58iRI7i7uxc5riAIHDx4kEuXLtGsWTNz+U8//YSnpyczZ84sss+CBQv45JNP2LRpE/PmzWPdunW0aNGCESOKBoaUyWT4+PiUOv7s7GwGDRpEjx49OH78OImJiTz55JPMnj2bVatWWd3nhRdeYN++fWzdupXatWvz6quvcvLkSTp27Fjq8UycPn2aw4cP88477xRb5+zZs6Snp1toAk388MMPDB06FB8fHyZNmsSKFSt47LHHbD6+iXXr1uHp6ckzzzxjdXtx5sMgaqljY2OL3d6nTx927txpdduRI0fMZtomBgwYgFwu5+jRo4waNcrqfj179uTnn39m6NCh+Pr6snHjRjQaDf369bNaf9u2bSQnJzN9+nSL8oYNG1KnTh0OHDhAEwfFOpGwRBJUS+H06dO4ubvR5lAbAsYF4FfA03R78mL+1/QmGxu584NrgCigmDSqrVuX63jJiFpSN0SNogkjsAnwAR7ML+sL9EHU3MUX056Liwu36tZlz/Dh3EQ0K3ZFFKZsMdCJihLNnXu37G3zGA4D7yBa+Mryx/FM/nEPArwAeFDEvvkkoonw9fzvIxGF8aKGK86BB2LQqxOIEX0ry8y3uLynHbI78JT6Ke5T3B1BkWzh66+P88wzOwAYMGA1R448Qa1aZfP/LKhNNWEwGEhNTUWtVqPX64mIiABg4MCBZWq7oDZVJpOhUqlISUkhLS2NWrWqJperhHMjzRkJAE2qpoipb1b8HW2ee4A77oFFBT17MWnSJF555RWzwHHo0CE2bNhQRFAdM2aMxfeVK1cSGBjIhQsXaNtWXET99ttvad++PXPmzCEsLIzFixfTuXNni/2++uorvv/+e7RaLTqdDldXVwv/y8jISJo0aWI1uFi9evXw9vYmMjISEJ9rWrRoYdM4w8PDLcxnW7duzbFjx1i/fj0ajYbVq1fjkZ9B4IsvvmDYsGF88MEH1KlTx6KdrKwsVqxYwdq1a+nfvz8g+tM2aNDApn40aNCA27dvo9frefPNN3nyySeLrRsbG4tCoaB27doW5UajkVWrVvH5558D8Oijj7JgwQKuXr1Ko0aNbOqHiaioKBo3boyqHCn+duzYgU6nK3Z7STEaEhISioxLqVTi5+dnzkFujY0bNzJ+/Hj8/f1RKpW4u7uzefNmmja1bqO3YsUKBg0aZPX61KtXr0RBW8K+SIKqDbjnuNPuSDsCxhXIa5ibgGdSOL7eCh7weAy5SY9m0qhWQFD9CgjkjqB6E1GAOwF4A124E5iptPVzPz8/Pps7l0dlMjyBdESB0RYEQeDy5csAVv+ZTd4h6/JEFanc25u3KKpFvafwjg8iRpf6UfxqTYv6OtC90G47EDW19yOenzKxZQsYDNC4seijWg6zbFNe1I6ASV8+Pf9VHi1qZGQkWVlZtG3bttTInRba0wJ5TwMIYAQjGGYcRlZyFs38mklCaj4ff3yYhQvvmIYNHNgYHx9bZ7+ISZtqQi6XIwgCgiBgNBpp1aoVHh4eZp+lsqSUKagZM/3YKxQKdDpdjdSQubqW7dxLFOVumzOVTVXOUaWrkukHppdeMZ+cpBxykkVBNTkimUMfHKLXS73wb+EPgLu/O+4BtgmqJi1sWQgMDGTo0KGsWrUKQRAYOnSo1dzPUVFRvPHGGxw9epSkpCSzaWBcXJxZUK1Vq5ZZMOjZsycvv/xykXYmTpzIa6+9RmpqKm+++SY9e/akZ8+eFnVsNWEui6lzixYt2LZtm/m7KR3UxYsX6dChg1lIBejVqxdGo5GIiIgigmp0dDRarZZu3bqZy/z8/GwWmA8cOEBmZiYHDhzgjTfeoFmzZkyYMMFq3dzcXNRqdZF7we7du8nOzmbIkCEABAQEMHDgQFauXMnbb79tUz9MVMQXMSQkpNz7lpfXX3+dtLQ0/vrrLwICAtiyZQvjxo3jwIEDtGvXzqLu9evX+eOPP9i4caPVttzc3CzM0iUciySo2oirq6ulSV/sBu5P8aSjYizenReKZSkpXFLHQCi0bNGCJJIII4zRjL7j02oDenRcI55E3NlHAJ8hCnOuiP6QvqXsLwD/Igp6Op2OiydPMtPFhWGdOpVJhBEEgSeffJKoqCirq22d8t83GMRgS9/WqsURimpRi7Ac0Rk3BSI84CNs06J+jxh/qTnlEFSXLoX81VQeeghWrixrC3yP2PUmwBrAhfKb+RqNRg4fPkxaWhoymYx77rmn2PmynOX8wi8W2tMe9GA0o+lDH1EwVQBlj91VIxEEgbfe2sfixXdCi730Ui+WLOlf5of4gtrUglEAjUYjOp2O7OxsAgIC6NSpUyktFaWwZsx0jJqoIVMoFLQsR3A5CUvupjlT2VT1HJXJZGUyv/UJ9sEnWDRRVbmqkMllBHUIIqCl7c8aFeXxxx9n9uzZAHz5pfUQkMOGDSMkJITvvvuOevXqYTQaadu2rTm9kon9+/ejUCiIj48nOzsbLy8vi+0+Pj7mBfONGzfStGlTunfvzoABAwBo3rw5Bw8eRKvVFln4vXnzJhkZGTRv3txc99Il2/IHuLi4FKt1q0xMz2Dt27cnNTWVxYsXFyuoBgQEkJOTU+RcrFixgpSUFAuNpdFo5OzZs/zvf/9DLpfj7e1NdnY2RqPRYuHV5AdvMos2nW+dTldmrWpFTH+DgoJITEy0KNPr9aSkpBQbiDA6OpovvviCc+fO0aaN6ETWoUMHDhw4wJdfflkkKNcPP/yAv78/w4cPt9peSkoKgYFlfgq9K5Ci/lYher3+jpOwPhuubwagVsMnzBosbUQ4r8+6wZQlN/nGYw3xxLOc5ZY5V205Fnpiuc0cFHyAKKR2QtQ4jqfki2ZAzIv6HGL6mZSUFMLCwji8YYNNQmpC1h3TCblcTocOHRg7dqxNSeWnZmfTHFGYm08JmttfgF+BJ+HDpqKQWgf4AliEA0x9DQa4evXO93L6FTwC1M9/t8cKT+fOnQkKCjLfOJNIYjnLLXKeAtzgBqmkEkAAT/AEW9nKZ3xGP/qZ557RaCQ5OfmuD1YjCAIvvfSXhZD69tv3l0tINRqNZosCU9tGoxGNRkNSUpLZ4qA859ykGdPr9chkMgwGg/klk8nQ6/XExMRUywik1pDmZ8W52+ZMZSPN0bLz0EMPmU1xBw0aVGR7cnIyERERLFq0iP79+9OqVStzruiCHD58mA8++IDffvsNT09Ps/BbHJ6ensydO5eFCxea5/ujjz5KVlYW3377bZH6H330ESqVymyG/NhjjxEZGcnWrVuL1BUEgfT09FLH3qpVK86cOUN2dra57NChQ8jlcqta0iZNmqBSqTh69Ki5LDU11WyObAuC8P/snXd8Tef/wN935WYnMkhCCEJi71klsVVVtX5qxSiqLW2p0VJtdaiqUr5oo8TeLdXWXgkxaseqESGoHdnJzd2/P657uO7NlEjCeed1XtznPOc5z3Puc885n+ezjOh0OvR6PWp19sE+zD6v/5ot/DB9F3/++Sdr1qwhJiZG2E6ePElSUpIQ6TgoKAidTkdMTIxFmydOnAAQhP2+ffuSnp7Ozz//bLMPOQV427Jli0UfntwWLlyY7bEtWrQgOTlZCIIEsGfPHgwGg4W2+nHM2s8nLZ5kMpnV791oNLJ48WIGDBhgUwDPysoiLi7OKmiYiAkxmFIxotU+FhnvxkaTsOoUAF4thDrqS2eogpKrNaQsZCF/8ReJJLKJTZzghM127bF/lOoGOMRBUqiCDnvOYo87JqHz/8jbqoKeR3lRM8ifecapO6fouqorXQK78G7jd2nk1yjH+tOAHUBduRyFToe3VstK8hDt1gtTqOKHpv+vU7S+qNfv3+dK06a0PngQqdEIeVwdvQFsA4ZiGlMZTH7ChfGjkUql1KlTx8rkJIEEhjOcJSyh2sPQV2GEEUroI+2pDYxGIzdu3MgxgMHzjsFg5IMPtvDzz8eEspkzOzJ6dIscjsqpPYOVH41Wq+XMmTMYjUacnJyQy+VWK895wWg0olKpkMvl6B9aJDyOXC5HpVJhNBqfC1NOcX4+PS/anHnWlOY56ujlSKN3GuXZ1LewkMlknH8Yl8OWJqVMmTJ4enry66+/4uvry/Xr163MetPS0ggLC+PDDz+kS5cuVKhQgSZNmtCtWzd69uyZ7bmHDx/ON998w/r16+nZsyctWrTgo48+Yty4cWg0Gl5//XW0Wi0rVqxg9uzZzJo1C39/fwB69erFH3/8QZ8+fZg0aRIdO3bE29ubM2fO8NNPP/HBBx/w+uuv5zj2fv368eWXXzJw4EAmT57M/fv3+eCDDwgLC7My+wWTcD1kyBDGjRuHp6cnZcuW5bPPPsv12TFv3jwqVqxIcHAwRqOR3bt3M2PGDJv5Uc14e3vTsGFD9u/fLwity5cvx9PTk169elndH1555RUiIiLo3LkztWrVomPHjrz99tvMmDGDKlWqcPHiRUaNGsVbb71F+fKmoJrNmjVj/PjxjBkzhps3b9KjRw/8/Py4fPky4eHhtGrVio8+sp0p4mlMf2vUqEHnzp0ZNmwY4eHhaLVaRo4cSe/evfHzM0UKuXnzJu3atWPZsmU0bdqU4OBgAgMDGT58OD/++COenp5s3LhRSG/zOHv27OHq1avZ+gD/888/KJVKWrQo2HvF805RLJSKgmp+Mejh2mrT/wP6geTRTcbl1BXGnfKhenAzFpY/zSUucZe7hBOO/UP9ovzhn5kylOEN3iDh4d86VnKfdzFixI+7TESHDzoS8crRfPhxQ5eamHKkViH7IEu2CD8WjsFoYHPsZjbHbubV6q/ya7fsc3X9DWQBdytUoMJDv6k8vR4lYHK8VcBoBQyvl49O5gM9JjfYBT4+aNeuZeHNm9Q/fx5q5xwF1+yLOhdTyplKPApgVZg/GCNGTnJSiM57gQuoUXOXuyxhCWGE4YUXgQ//RLLHYDAyZMhfLFkSA4BEAuHhr/LOOzkvtuSEXC6nZcuWVi//5vQ0crmcl156Cbk8/7NCKpXSqFGjHANKKBSKfAvAIs8v4pwRyQ6zoFocuOYQ70EqlbJmzRo+/PBDateuTVBQEP/73/8sIq1+9NFHODk58d133wFQp04dvvvuO4YPH06LFi0EwehJPDw8GDBgAJMnT+aNN95AKpUya9Ys6taty88//8ykSZOQyWQ0bNiQjRs3WqRqkUgkrFq1il9//ZVFixYxZcoU5HI51apVY8CAATa1w0/i6OjI9u3b+eijj2jSpAmOjo68+eabzJw5M9tjpk+fTnp6Ot26dcPFxYUxY8bkqr01GAxMmDCBq1evIpfLqVy5Mt9//z3vvvtujscNHTqUZcuWCdrpRYsW0aNHD5uLWG+++SZhYWEkJCTg5eXF2rVr+fLLLxk+fDi3bt2iQoUK9OjRg88//9ziuGnTptGoUSPBfNZgMFC1alV69uxZZOlpwBRxeOTIkbRr1w6pVMqbb75pkdtUq9Vy8eJFQZOqUCjYsmULn376Kd26dSM9PZ3AwECWLl0q+OuaiYiIoGXLltm6AaxevZp+/frZjEgtUjRIjC+4nVBqaipubm6kpKTYvOFOnDgRnU6HSqVi1qxZyO7thlMTwa4MtNkMssdExM6d+fXl8/w62hmdo4IHPECHZf7GBg//zDjhxEQm8uvDv/skkEJPPDHgTjQSjBgxMpzhvEPOCY2XY3L9fBMEvdvt27f56aefAPjhhx+yPfZGyg1aLmqJ3vBopX7iyxMZ2TR7E5zNwF3g7MSJyHU6RowYketK2f798PLLgCeQBdHboFUeAwq/gclHdSGmgEY5EYdJWD//WNlMTJGSc+IG8BUQ8/BzEwo3oq8RI3HEsevh31GO4oorSkym1SpU6NAJKZDeefiXG3q9njNnzlCnTp3nMg9nbhiNRsaM2cFPP/2DVCph6dLX6d+/bpGc6+bNm/z222/IZDKGDBkiPrDywIs+P0VKPs96jmZlZQnRVsVAYyJ5wWxV4eDgkKvVhEqlIigoiLVr14rav0IiISGBoKAgjh07lu8oyc8TOd27kpKS8PDwyFamKgiiRjWPSKVSMBrh6gpTQcX/sxRS79+HhATeiPKg9ZjFgD0XuMC3fMskJhH8MNKNVzaa0Td4g9YPxajHj6tKVSYykXvcQ41aEGhsEfYU41twYoGFkOqocCSsbs4tdn3473idSRhXqVR5P+GD/PYwbwhaVEALuGAS2pNzOe5JLaojj/KiPq0R3ZPCaTzxwj5vvBnOcF7m5WznS155MvjEi4REImHGjI5otXpCQgJ4882CRd3OC35+fpQrV467d+9y+vRpmjd/Mj61iC1e5PkpUjoQ56hISSevFhMODg4sW7aMhIT8xUgRyZ74+Hh+/vnnF1pILQ5EQTUXzDcFpVKJLPUMpP4LUjuToPo4D/00vDyq42Vf32JX8MO/nLAlwAYTzB3ucJvbbGQjJzjBZCZTl8LVFKVkpbDqzCqLsr51+uJmn3vS68d5Morfs+ZJLWprYCIwlpwF1RuY0v+cfPi5MLSoRoxc4Qo72WklnCpQ0IIWtKc9rWmN8xPeuXmZL08ik8le+OTTEomEOXNeyb1iIZynYcOGbN26lVOnTtG4ceMCmf++SIjzU6SkI85RkZKORCLJl/b9cRNrkaencePGNG7cuLi7UaIpCmsU8e0qF8wRrLRaLcary03aNb+uJtPfxzFHV3sYwbWwCCGEWcxiClO4znWGMIR+9OM93stRu5pX9AY9X+39ikRVIlKJFIVUgVwmZ1jDYYXQ+0Lg5k1ITDT9v2JFUChMZQ9ThuDhAeXLowHeAxIxaVHHJSTQ5fZt0/dVoQIolXD7NphzX3l4YChf3kKL6oApqNPTaFGvcpXtbM+3cPq0GAwG7t27R9myZV8IH7XUVDW9e//OF1+0oXnzvCVML0yqVavGmTNnxBfbPPKizU+R0oc4R0VKOuaov3K5XAyYJlIiEaP+FgO6h2ateq0K7h03STAB/awrmgXVGjWEIi+8eId38mW+aeu4VrRiLWv5iZ/4m79ZwQr2se+ptatbYrcwYfcEzt47K0TqkklldKjSAX83/wK3W2jcvAlNm0J6uunzihUmofODD+DMGVOZszMcOYJd+fJ8COwGJt6+jXfjxqbjjEbqvPceLpUq4bZkCcaLF5EAN6pX5+uoKE46OZGlUlE9NZWvFQpqe3jkq4tGTNdN8lC0/YM/WIVJO51f4bSg8wVMD7A7d+68ELm9HjzIpHPnlRw7dotDh/4jMnIg9evbzp9WVEil0hwjUopY8iLNT5HSiThHRUoDWq1WtOARKbGIUX+Lgfr16xMTE4NMnwIYwbsVOAdYVjIaBdNfaj7yjTMLHvnF1nGuuPIlX9KOdhba1b705X3eR4mSBBLYwAbe4I1chZ0tsVvov6E/GdoMjEZTwCYJEnQGHQduHGBL7BZeqVb0ZpQ5kphoEjalUpDL0Sgl3PcykuQpQ29vz9L/+z+qX7pEq3/+gerV6WpvT9dq1ZAkJDw6TiLh4+nThR+PxNWV/S1b8smXX6KWSHAA2sTEUP7wYRRdupg0tA/J7XquZCV/8AfjGU9TmgLQkY78x38F0pwWdL68SNy5k06HDss5e9aU8Fsmk2AwvNDx4EREREREREREnktE+5ZcSEtLIyszjQfJKmLvyiCgv3Wlu3chKQlkMqhWLd/nOHDgABMnTuTAgQO51jVrV7vRDSNGVrKSPvThNKdJIIFf+ZUELJ3ns7KyuHPnDrGxsYDJ3Pez3Z+h1qsfpd2QmPwflDIlGp2GkRtGEns5Nse+aLVw7Bjcv5/K1avxbNoUw/795LidPp3vywNyOSgUqO0lJHgaSfaUsbZnT34eOpRvx4wh4+OPoVMnJO+/b2mya151lMmQSCSmcSqVBF+5glKjoYlKxVqgyokT3PrvPw4ePGhx2sevp9nn1MAjs4b4h3+72CWU1aY2M5nJK7wiCKlHjhzhq6++4siRIwUYvIiZGzdSaNNmiSCk+vg4s3fvIBo29C3mnsH169dZvHgx169fL+6uiIiIiIiIiIg8F4ga1Vy4fPkyqcmJZOmkbL3kS7UyDa39F81mv4GBYGf35N4cMRgMbN++naysLLZv306LFi1y9Y/JTrvajnYWgpQGDXZl7FA4K9De17JlyxZGfjiSqJtRxCXFIZfKydJlWbTtZOeERq3hRuYNFkUvYlLgJADssUf2MOmNFi0ZWg0hL8s4ddiO119PRiLVs/fYbj6b0oNc1z8ey+ahQ4l5GurQoUaNFCkOOAh1MpV6jHZS9FKToW2WnYHOe/5ma5uX6PHbaoxkkOEgB3s9kAGSTOzkRhQASiU6iYF9DWrR4p9DOMrleCUmsuS99/BYuxi9m5xLNy+BHcTExpCiSxHMajLJRI2atazlDGeIJ55FLBLMrXvSkwY0EKI128JgMLBz507S0tLYuXMnjRs3LhL/J4lEgoeHx3PrtxIXl0i7dsu4ds2Uc65iRTd27x5AYGD+TLWLAqPRSHR0NHFxcdjZ2dG3b9/n9nsoKM/7/BQp/YhzVKQ0IKb3EinJFMX9UxRUc0GtziJLo0OCkfP3nLkUG0tQUJBlJRtmv3ll//79pKWlAZCens7+/ftp3Tq3bJ8mWtGKn/mZGcwgiih+53cccOACFwBY8/DPtaErNW/VJCYmhiV/LuGLSl+QbkjH+ITJpEFiIEWagl6pBwlE1Irgj7Q/ABh3eRxBGaZxH6p4iMWea7nweigNNJWRSnUYJXBk2t/YTT6ERpP33EnnXX4ghLYA7GEPE5lIYxoTTjgACWX0dF2bSoYTXK/0ALVSyfhJSUwZdRujcQBnquv5DjlIJOBwDGgDVVRMeEXHm9uUIJHQbe18oltUJTBuHac6m9ILVfzvP970n8jJlCtkvpcp9Gdj6kZcPEwpCrLI4ja3Wc5y7LFHjpx9KfvQX3mUxscXX2KJxdHR0XpeAIcPHyYlJQWJREJKSgqHDx8ukpxmUqmUihUrFnq7JYHz5+/Tvv1ybt0y/U4CAz3YtSuMSpXci7djD7l27Rrx8fEolUri4+O5du0aAQEBxd2tEsXzPD9Fng/EOSpS0pFIJCiVTx9EU0SkqCgKRYwoqGZHVgLGrPukJt3HaDQilRjR6o1s+2sd1Yf1RGLvDfYP/RZtBFLKCwaDgW3btmE0GjEYDMjlcnbt2kWrVq3y/GXvZCf/8A86dCSQQEUq8i3fAnDfeJ/UrFSUaiVqtVrQ7qkGq0xBgAxYhre1A73koRAmBZ1UR3JKMgD79u3j4s2LACR2SMTQHOQyNZUq7bPoj719cp4FVQdHyDFoanIyGzpncr4G6GUAg7BXQ6KHhEQPACNe9yV4O5U3RfWV2l5prHf+CAebeeGWecmi3Gg0kpmZaVGWRBIJxgThukiQkEYaEiTYYced5Dvs3bvX6hze3t5WgqrBYGDPnj3Cd6vT6dizZw/NmjUr9B+zwWDgv//+o0KFCs9VxMqYmDt07Lic+/dN31PNmt7s2hWGr2/JyHdoNBo5cOAAWq0WNzc3UlJSOHDgAJUqVRI1M4/xvM5PkecHcY6KlHSMRiMajQY7Ozvx+SJSIhGj/j5L/tvApQNL0WnLInnov+kkTeP8uWNc+nMbQS8NhMB3sg2klBf279+PSqWyiJKVX63qG7whmJ6e5zxTmMIkJhFMMJfjL7Nyzkr0d/RIJBIcHBzQpeqYeGgik42TySADjKaxGSVGnHHGkGkgS5+Fq96VGZdmII01PbClXlKkXqb/e+FFv9M/MyxiE9JaqzEaJUilMtp93hYj0KbN29Sp0z7HftvbQ91q4PBY9Lr2tCeEECGCLsnJvLEBmj90Hb1YS873E+DTOc4ExclBp8PrjhavDRugdu1HjV85i/zvEEy2vzD1+1V8/eMqpEZ4XDIf+GtbKj2wjJqsclRRqVElur7alYtcZBrThOsJkEEGd6vftRqPq6u1cG7Wpsoe+sjKZLIi06oajUYSExMpX758obZb3Jw9e08QUhs08GH79v54ezsVc68eYdamOjg4CL8xUatqzfM6P0WeH8Q5KlLYDBo0iOTkZDZu3JhtnZCQEOrXr8+sWbPy1KZer8+90kPCwsKoUaMGEydOzPMxItmzbds2Pv30U06cOCEuZmVDUUT9Fa90NhjL92Dbvc4YpfZIpApAgtK5HFppGVN5+R6mijdvQmqqKb9nPnIqGgwGdu3aZRISzRFpH66Q7dq1K8+rEl54EfzwrwYmjW4wwQQZg4j7Kw5lohKZ0aRp9Pb2RmmnRK/X42zv/EggNIJELkEukaM36pEb5PTx6MPgvoMZ2GcgA/sMJKxfGP369aNfv3506tAJmV5Otco7H45FjlwmQ4ECqU7C+VN/06aFnNCX7LLdWjSyw0lhh/SxKShFih12KMwSZkAAXion6v4ro+55OXWuOiExQJ2zUPeMafO7K8POqMDu8T+jAqlRAjodaLVINTqUmToUKtNndDqMRiOX/41FppdZbM5pziTuTaSmriZ1qCNcT/Nfo0qNeOWVV6y2Vq1aWX2/Zm2q+XuVSCQW5SK5079/XebNe4UWLSqwZ8/AEiWkPq5NtXvom25nZ4dWq+XAgQNFcsMWERERsSIrAS7/avq3iBk0aBASiYR3333Xat+IESOQSCQMGjSoUM8ZEhIiBES0t7enevXqTJ061eY9dunSpTRp0gRHR0dcXFxo06YNmzZtsqpnNBr59ddfadasGc7Ozri7u9O4cWNmzZolWFpNnjxZOO/j265du6zaexYcOHAAV1dXGjRokGvdU6dOsWXLFj788EOrfatXr0YmkzFixAirfUuWLMHd3d1mmxKJxEroXr9+PSEhIbi5ueHs7EzdunX5+uuvSUxMzNOYCkJiYiL9+vXD1dUVd3d3hgwZQro5jWE23Llzh7CwMHx8fHBycqJhw4asX78+X+127twZhULBypUri2RcIrYRBdVsuHQ9kfNxN1HYKZE8NCmVyB1wcnblfNxNLl1/+CM0m/1Wq2YSVvPI/v37SU9PF6LuPi6wmrWqT9X/S5c4f/489vb2FoKSk5MTVy9c5bf2v9G/bn+Usof+DgbQ6rV44slLd1/i9Vqv59j+2bO7cXB4gMEgxayllEgkSKVSHjx4wO7du5+q/4ApVYyrKzg6mlSwOh0YDSZhU6MBg8GUR/XJ3KceHqZyg8FU78nNYEAll5ORja+H0Whk7dq1T9X1J7WpgJVWVSRvvP9+E/btG4y7u31xd8WCJ7WpgJVWVURERKTIUT8UVNVFL6gC+Pv7s2bNGlQqlVCWlZXFqlWriszPd9iwYdy+fZuLFy8yYcIEvvjiC8LDwy3qjB07luHDh/PWW29x+vRpjhw5QqtWrejevTtz5861qBsWFsaoUaPo3r07kZGRxMTE8Pnnn/Pnn3+yY8cOoV6tWrW4ffu2xZZXi7fCJDk5mYEDBxISEpKn+nPmzOH//u//cHa2TpEXERHB+PHjWb16NVlZWTaOzhufffYZb731Fk2aNGHr1q2cPXuWGTNmcOrUKZYvX17gdnOjX79+nDt3jp07d7Jp0yb27dvHO+/knNpvwIABXLx4kb/++oszZ87wxhtv0KtXL06ePJmvdgcNGsT//ve/IhmXiG1EQdUGRqORbdu2oVarBQHSaASNVotUKkWtVgu+pQUx+zVrU8HkeCyXy5HJZBbat/xoVc2Y83B6Gj2F/svl8of9N/k2mPt/Yu8Jlr6+lDtj7jDtlWm8++a7rO27lq63ulI+szyBgYE59v/o0Q1IJAaMRglgwGAwCP03GAxs2LDh6bWG5cvDkSNw4AAcPIjXsi284z0Jr2VbHuW7OXLEVM/WcdnkyNFFRTGjVy9SXbL3c4yJicFd5847vJNrTtoneVJrar42j38ubK2qRCLBx8en1Put/PnnBZYvP2VVLpeXrFuVWZuqVquRSqVotVphM//GRK3qI56X+Sny/FLsc9RoBJ2qYJs+y7SIq88q2PH5vE81bNgQf39/NmzYIJRt2LCBihUrWmn7tm3bRqtWrXB3d8fT05NXX32VuLg4Yf+yZctwdnYW0ucBvP/++wQHB1vEkHB0dMTHx4dKlSoxePBg6taty86dO4X9//zzDzNmzGD69OmMHTuWwMBAatSowZQpUxg1ahQff/wxN27cAGDdunWsXLmS1atXM3HiRJo0aUJAQADdu3dnz549hIaGCu3K5XJ8fHwsNrMFzZkzZ2jbti0ODg54enryzjvv5KjZy8jIYMCAATg7O+Pr68uMGTPyfM3fffdd+vTpkye3Ib1ez++//063bt2s9l29epWDBw/y6aefUr16dYvvMD8cOXKE7777TrjmLVu2JCAggA4dOrB+/XoGDhxYoHZz4/z582zbto2FCxfSrFkzWrVqxZw5c1izZg23bt3K9riDBw/ywQcf0LRpU6pUqcKkSZNwd3fn+PHj+Wq3W7duHDt2zGIOizxCjPr7jNDpdDx48AClUklWVpYpOq5ERpZaD5IslEolDx48QKfToTBrVPMhqKrVamEVS8hjChZa1aysLNRqNQ4ODtm28yRmQVWr0wr9V6vVFm0CFv13d3BnXJNxANy8eZM1GWtQKBQ5roqq1WrU6gyMRilSqeHhOEwKTDAJ3xkZGfnuPxs3Qtu2Ji2qmfLlBUHUC3gnh1QwFjx23JOkJyeTYmOV8XGMRiP26fa8457zKp0tNBoNKpUKqVRqU1CRSqWoVCo0Gg329oWjJZRKpfj4+BRKW8XF6tVnCAv7A6MRHBwU9OyZ/yjazwq9Xk9KSgpKpRKNRmO1X6lUkpKSgl6vF9Idvcg8D/NT5Pmm2OeoPgt2vZz3+gYdGHWPjlXdhv09QfbwmSKRgzSP95720SDPx7MaePvtt1m8eDH9+vUDYNGiRQwePJioqCiLehkZGXz88cfUrVuX9PR0vvjiC3r06EFMTAxSqZQBAwawadMm+vXrx8GDB9m+fTsLFy7k0KFDODo6Wp3XaDSyf/9+Lly4QLXH8tavXr0aZ2dnhg8fbnXMmDFjmDlzJuvXr2fUqFGsXLmSoKAgunfvblVXIpHg5uaW6/gzMjLo1KkTLVq04OjRo9y7d4+hQ4cycuRIlixZYvOYcePGsXfvXv7880/Kli3LxIkTOXHiBPXr18/xXIsXL+bKlSusWLGCb7/9Nte+nT59mpSUFBo3bmyzra5du+Lm5kb//v2JiIigb9++ubb5JCtXrsTZ2Zn333/f5v7szIfBpKXOyeLo5ZdfZuvWrTb3HTp0SDDTNtO+fXukUimHDx+mR48eNo9r2bIla9eupWvXrri7u7Nu3TqysrIEDXVe261YsSLlypUjOjqaqvlw93tREKP+PiMUCgUff/wxGRkZXLp0iT///BN3d3fefvttIYeVs7MzCpmsQBpVBwcHPvzwQ5KSkoiPjxdu7N7e3sIKmKenZ/6EvGz6f/XqVX7//XfAdJM04+zsjOIJU+XLly8DULVq1Rxfrh0cHPi///uG9957IJRNnw61aj2q4+3tnb/+r10Lo0ebruPKlVCuXN6PzSfu7u4MGDCA27dvZ1vHz88vxxttTtjb2zNixAiSk5OzrVOmTJlCE1LBJDjFx8cTEBBQKvOsLVp0kqFD/xIW9rdujS3RgqpcLicsLMzC9O1JHBwcRCH1IaV9foo8/5S6OapJsjb1VT32TFN6gb13kZ2+f//+TJgwQRA4Dhw4wJo1a6wE1TfffNPi86JFi/D29ubff/+l9sMgiPPnz6du3bp8+OGHbNiwgcmTJ9OoUSOL437++WcWLlyIRqNBq9Vib29v4X956dIlqlatKmg7H8fPzw9XV1cuXTJF/o+1lWYwG86cOWNhPluzZk2OHDnCqlWryMrKYtmyZTg5mWInzJ07l27dujFt2jTKPfEOk56eTkREBCtWrKBdu3aAyZ+2QoUKOZ4/NjaWTz/9lOjoaGQyGTqdLtc+X7t2DZlMRtmyZS3KDQYDS5YsYc6cOQD07t2bMWPGcPXqVSpXrpz7xXiiX1WqVLF6j8wLW7ZsQavVZrs/p3fHO3fuWI1LLpfj4eHBnTt3sj1u3bp1vPXWW3h6eiKXy3F0dOSPP/4QrAfz066fn5/o2pMN+Qn2lVfEt6hscHd3x93dncuXL2NnZ8eDBw8oX7685QPs2jXIzDSlRsnnj7xChQpUqFABvV4v+Ki2adOGOnXqFGr/MzIyhBt3btEMnZycqFmzJjXykGbH27sSd+9WEj5XrgwNGxaws7t2wdixpv//+y907w47d0IOprlPS926dalbt27uFQuIr68vvr6+Rda+Lcz5eEsbc+Yc5sMPtwmfhw9vxM8/dy3GHuUNFxcXXIpwjj5vlNb5KfLiUKxzVGZv0mzmFXUCqB8uFqddhH+nQc1PwOWhAKb0NAmreT13PvH29qZr164sWbIEo9FI165d8fKyPl9sbCxffPEFhw8fJiEhQXB5uX79uiColilThoiICDp16kTLli359NNPrdrp168fn332GUlJSXz55Ze0bNmSli1bWtTJq6tFflwygoKC+Ouvv4TP5jym58+fp169eoKQCvDSSy9hMBi4ePGilaAaFxeHRqOhWbNmQpmHh0eOArNer6dv37589dVXVK9eXXDjyg2VSoVSqbQyw9y5cycZGRm88sorAHh5edGhQwcWLVrEN998k2u7j/M0bi2VKlXKvVIh8/nnn5OcnMyuXbvw8vJi48aN9OrVi+jo6Hy/dzs4OFilNhQpOkRBNRdy9CM0m/0GBcFTrsAqFApeeumlp2ojJ4wYUWlVOCiyX6lq3LixTVORIsffH7y9wbxq1bNnkQqpIiWH77/fz4QJjwJvjR7dnBkzOj4zPzG9Xo9arbZpYiYiIiLyzJBI8md+K/cHJ3/T/2X2IJGCez1wCy6a/tng7bffZuTIkQDMmzfPZp1u3bpRqVIlFixYgJ+fHwaDgdq1a1u5TOzbtw+ZTMbt27fJyMiwWgR0c3MTtF/r1q0jMDCQ5s2b0769KRVe9erV2b9/v5Bn9HFu3bpFamoq1atXF+peuHAhT2O0s7PLMWZHUZKWlsaxY8c4efKkcJ0NBgNGoxG5XM6OHTto27at1XFeXl5kZmZaXYuIiAgSExMtNJYGg4HTp0/z1VdfIZVKcXV1JSMjA4PBYGHGabYQM5tFm6+3VqvNt1b1aUx/fXx8uHfvnkWZTqcjMTExW9P9uLg45s6dy9mzZ6n10PSvXr16REdHM2/ePMLDw/PVbmJiIt7eRWetIGJJyYpQUgIxrxA+uToGFDh/qi20Wi0HDx4sspDe/0n/o/GCxvx48EcSMp9NZMA8ExQEf/8NgYEQFgZjxhR3j0SKGKPRyKRJeyyE1M8/b/1MhVSAhIQEjh8/zsWLF5/ZOUVERESeBzp37iyY4nbq1Mlq/4MHD7h48SKTJk2iXbt21KhRg6SkJKt6Bw8eZNq0afz99984OzsLQll2ODs789FHHzF27FhBs9e7d2/S09OZP3++Vf0ff/wRhUIhmCH37dtXcOt6EqPRSEpKSq5jr1GjBqdOnSIjI0MoO3DgAFKp1KaWtGrVqigUCouI/0lJSYI5si1cXV05c+YMMTExxMTEcPLkSYYOHUpQUBAxMTEW2tnHMfu8/mtWpmD6Lv7880/WrFkjtGduMykpSYh0HBQUhE6nIyYmxqLNEydOAAjCft++fUlPT+fnn3+22YecXJ+2bNli0Ycnt4ULF2Z7bIsWLUhOThaCIAFCcMrsrodZ+/mk/6Q5iGl+2s3KyiIuLi5PKYJECgdRo5oL5hWWxMRE6xfoAgRSepIyZcoI7W7cuJHevXvj8WS6lafAvJoWo4ghRZXCzEMzmXtkLqOaj2JU81GFdp6npnx52LQJnJxMK8si+UIikeDv718qoqoajUY+/ng7s2Y9emB//307PvmkVQ5HFQ1mP2VRo1q0lKb5KfJiUqrnqNILAt/Ju6lvISGTyTj/cMHell9vmTJl8PT05Ndff8XX15fr169bmfWmpaURFhbGhx9+SJcuXahQoQJNmjShW7du9OzZM9tzDx8+nG+++Yb169fTs2dPWrRowUcffcS4cePQaDS8/vrraLVaVqxYwezZs5k1axb+/iYNdK9evfjjjz/o06cPkyZNomPHjnh7e3PmzBl++uknPvjgA15//fUcx96vXz++/PJLBg4cyOTJk7l//z4ffPABYWFhNhUbzs7ODBkyhHHjxuHp6UnZsmX57LPPcgw+I5VKBfNoMD07fXx8sLe3tyh/Em9vbxo2bMj+/fsFoXX58uV4enrSq1cvqzn+yiuvEBERQefOnalVqxYdO3bk7bffZsaMGVSpUoWLFy8yatQo3nrrLcGFrFmzZowfP54xY8Zw8+ZNevTogZ+fH5cvXyY8PJxWrVrx0Ucf2ezf05j+1qhRg86dOzNs2DDCw8PRarWMHDmS3r174+fnB5gCg7Zr145ly5bRtGlTgoODCQwMZPjw4fz44494enqyceNGIQ1NXtsFU3RppVKZp+jLLyJFcf8UNaq5cPfuXcDkVG1xQzEYwGw6kgefzuyQSqWCjyrA/fv3C9yWLTw9PXkgecAN6Q2hTKPX4OngWajnKRRcXZ/ahLog7Nq1i5UrVxIfH//Mz11YSKVSPD09iyTiWmETF5fEggUnhM//+1/nYhFS09PTSUtLQyKR2LaYECk0StP8FHkxKdVz1P6hoGr/bAVVMGn9XB+P1P8YUqmUNWvWcPz4cWrXrs3o0aOZPn26RZ2PPvoIJycnvvvuOwDq1KnDd999x/Dhw7l582a25/Xw8GDAgAFMnjxZ0IrNmjWLn3/+mdWrV1O7dm0aN27Mvn372LhxIx988IFwrEQiYdWqVcycOZONGzfSpk0b6taty+TJk+nevbtN7fCTODo6sn37dhITE2nSpAk9e/akXbt2VvlaH2f69Om8/PLLdOvWjfbt29OqVSuroFE5Yc5VnxeGDh3KypUrhc+LFi2iR48eNgWJN998k7/++ouEBJO13dq1a2nTpg3Dhw+nVq1afPjhh3Tv3t1K0zlt2jRWrVrF4cOH6dSpE7Vq1RIiPBdVehowRRwODg6mXbt2vPLKK7Rq1Ypff/1V2K/Varl48aKgSVUoFGzZskUIWFq3bl2WLVvG0qVLBX/dvLQLpujS/fr1Exe3s6Eo7p8S4wue6C81NRU3NzdSUlJs3mxXrVpFTEwMWVlZ/PTTT49WDa9cgV69wNERoqKggF/OjRs3hAhsYAry079//wK1ZYvExEQ6zOzARdlFYXweDh4ce+cY9vKCR53dvx9efiySfnQ0tMqLrKFSwZQpMG4c5CEE/LPg999/57///qNLly55jgRY0tDr9cTGxlKtWrVSEbEyKiqeV19dxf/+14W33y4eE5rY2Fju3LmDt7c3wcHPzq/rRaS0zU+RF49nPUezsrKEaKuFGQFe5PnFaDSSlZWFvb19rporlUpFUFAQa9euFbV/hURCQgJBQUEcO3Ys31GSnydyunclJSXh4eGRrUxVEETT3zxiJc+fO2f6Nzi4wEKqLcwrWoXF+f/OEyuLtSgbXH/wUwmpBUarheHDTVF+DxyA1auhBORWbNWqFWq12mbEwtKEOU9uaSAkJIArVz6ibFmn3CsXAXq9XrBeEPN7PhtK0/wUeTER56hISSevuiUHBweWLVtW6O+ULzLx8fH8/PPPL7SQWhyIgmpBMQdSegqzX1uYw7cXlvr8t8u/YeBR5GKlXMngBoMLpe188803JiEV4OJFGDQItm4tdp9UUVApWlQqLatXn2Xw4PoWq8DFJaSCyfdcr9fj4OCQp+TuIiIiIiIipYmQkJDi7sJzRbFlxnjBKYXOGCWEQgik9Diurq5IpVI0Gg2pqamF0maaOo0/4y2j2vWu1RsPh8IL1pQvBgwwBU0Ck8n0998Xu5AqUrSkpal55ZVVDBnyF5MnRxV3dwDTirQ5iJKvr2/pDJ4iIiIiIiIiIvKcIwqqecTOzu6RllOnA3NI8afUqLq6utKhQwdCQkKEaL+FFVBp1ZlVZOoeJSWWSCS80+idQmm7QAQGmtLQ1KkDERHwMBqdyNMjlUqpUqVKiQoEkpycRceOK4iKigdg5sx/uHEj97D/RU1aWhoZGRlIpVLKli1b3N15ISiJ81NE5HHEOSpSGlAqlcXdBRGRbCmK+6do+ptHZDLZI83LlSug0YCzM1So8FTturm50aFDBwAuX75MQkICCQkJVKtW7ana1eq1LDixwEJb1LlqZyqXsbatv3TpEhUrVnw2AR18fEzmvuLLQKEikUgKzXG9MLh/P4OOHVcQE3MHAHd3e7Zv74+/f/Gb2Zq1qV5eXvlOVC5SMEra/BQReRJxjoqUdCQSiRiMTqREI6anKQZq1aoFgIuLC3q93lRoNvutUeOpBa7r168zZ84cFi5cKATzKQyN6qZLm7iVdsvC8f69Ju9Z1UtLS+Obb77hnXfeQaVSPfV588QzEFJPnDjBihUrOHnyZK51T548SWRkpJCKqDSi1+s5c+bMozlajNy6lUZIyFJBSPX2diQqaiBNm5Yv5p6ZcHZ2xt7eHl9f3+LuygtDSZqfIiK2EOeoSEnHaDSSmZmZ54BKIiLPmqK4f4qCai6Y1dgWqwSF6J+amJjItWvXiIuLEzQ8T/tFG41Gfjn2i+n/mG5oPgYfGvtZO4FfvnzZtN/HBwcHh6c6rwUnT8Jbb0FSUuG1mQ/MmmmNRpNr3bi4OE6dOkVycnLRd6wIKQkvWNeuJdO69WL+/de02FK+vAv79g2mXr2SE7CqfPnyNG7cGBcXl+LuygtFSZifIiI5Ic5RERERkZKFaPqbC5ce+qJaaNvMEX8LQVB1dnZGIpHg4OBA48aNadq06VPbeB+4cYCz985alNXX1bdZNzbWlLrmaU2NLYiLg7AwSEyE7t1NaWjKP1ttWtJDAdnd3f2ZnvdFJjb2Ae3aLePGDVMwsMqV3dm9ewCVK5cp5p5ZIwZQEhEREREREREp2Yga1VyoXr06AOXKlTMVaDTwULgrjNQ0SqUSiUSCQqFALpcjkUhIT0/HYDDkfnA2hB8Lt/jsZnQjQB9gs26hC6pGI3zwgUlIBbh8GWbOLJy284FZO1qmTMkTkp5HjEYjAwduFITUoCBP9u0bXCKFVBERERERERERkZKPKKjmETc3N5Om8/JlU9RfNzcoAh+3qVOn8vXXXxc4SfOVpCvsubrHoqyerh5SG1+1Xq/nypUrAAQGBhbofFZIJDBvHlSsaPrcqBF8+23htJ1HsrKyBH/bF0WjKpVKCQoKKraIlRKJhBUr3qB8eRfq1i3H3r2DqFBBDEwiYqK456eISG6Ic7T4iIqKQiKR5Mv9ZvLkydR/hpkDQkJCGDVq1FO3o9FoCAwM5ODBgwU6/pkEvSxlfPrpp3zwwQfF3Q0Riibqr3hHzoWzZ00mtDdu3DAVPG72WwTmg05OTgAFFlQru1fm916/075KewA87D0I0gXZrHvjxg00Gg0ODg74+fkVrMM2O1EZ/vrLZPa7fDkUpu9rHjCb/To7O2NnZ/dMz12cFPdYq1QpQ2TkQCIjB1KunHOx9kWk5FHc81NEJDdK6xxNSkri6NGjwrOvqAgPD8fFxQWdTieUpaeno1AoCAkJsahrFj7j4uJybbdly5bcvn0bN7fCjQpfWMKlLTZs2EDHjh3x9PREIpEQExOTp+PCw8OpXLkyLVu2tNo3fPhwZDIZv/32m9W+QYMG0aNHDyu3FVtCvkaj4YcffqBevXo4Ojri5eXFSy+9xOLFi9FqtfkaZ344ffo0L7/8Mvb29vj7+/PDDz/keoxEIrHa1qxZI+y/ffs2ffv2pXr16kilUpvf59ixY1m6dKmgeBF5vhAF1VwwR1dTqVQmc9zHI/4WAebIv/fu3SvQ8RKJhJb+LVnWYxlRg6KYGjoVBQqbPnlms9/AwMDCXwUpWxZ++QWKQaPp5uZG586dadGiRZ7qV6hQgcDAwFIdXMdgMHDmzJmnMhnPL8eP30Kt1lmUVavmiYfHs12YECn5FMf8FBHJD6V1jhqNRuLj43nw4AHx8fFFGhE2NDSU9PR0jh07JpRFR0fj4+PD4cOHycrKEsojIyOpWLEiVatWzbVdOzs7fHx8SlXsgIyMDFq1asW0adPyfIzRaGTu3LkMGTLEal9mZiZr1qxh/PjxLFq0KNs2csvOoNFo6NSpE99//z3vvPMOBw8e5MiRI4wYMYI5c+Zw7ty5PPc3P6SmptKxY0cqVarE8ePHmT59OpMnT+bXX3/N9djFixdz+/ZtYXv99deFfWq1Gm9vbyZNmkS9evVsHu/l5UWnTp345ZdfCms4IgWkKO6foqCaXwox4q8typYtC+Rfo6o36Dl44yAbL2zk4I2D6A16qntWp7G7KdKvrYdXgfxTb96EM2dwjDtDbc5Qm9M05BiOcWfgzBnT/mLG0dGR4OBgIbVQbjRv3pxXX321cLXKzzlbtsTSqtVievdej1YrRsoUERERKQz0en2eNvMLYXJyMomJicjlchITE4s0en1QUBC+vr5ERUUJZVFRUXTv3p3KlSvzzz//WJSHhoYCppfXqVOnUrlyZRwcHKhXrx6///67Rd0ntYILFizA398fR0dHevTowcyZM2268ixfvpyAgADc3Nzo3bs3aWlpgEkDuXfvXmbPni1o6uLj4wGTpVyXLl1wdnamXLlyhIWFWbxzZWRkMGDAAJydnfH19WXGjBlW5w0LC+OLL76gffv2eb5+x48fJy4ujq5du1rt++2336hZsyaffvop+/bte2TFl09mzZrFvn372L17NyNGjKB+/fpUqVKFvn37cvjw4cINnPkYK1euRKPRsGjRImrVqkXv3r358MMPmZmHGCXu7u74+PgI2+PmzQEBAcyePZsBAwbkqHHv1q2bhSZW5PlBFFTzQ1aWKaItFJmgatao5kdQ3RK7hca/Nqbzis7029CPzis60/jXxmyJ3ZLj6lu+BdWbN6FpU2jVirrvt2I/rThKU/6hBXXfewlatTLtLwHCqkjRsX79v7z++hqysnRs3HiBefOOFneXRERERJ4LDh48mKftzp07gjZVr9djZ2eHXq9/JlrVyMhI4XNkZCQhISG0adNGKFepVBw+fFgQVKdOncqyZcsIDw/n3LlzjB49mv79+7N3716b5zhw4ADvvvsuH330ETExMXTo0IEpU6ZY1YuLi2Pjxo1s2rSJTZs2sXfvXr7//nsAZs+eTYsWLRg2bJigqfP39yc5OZm2bdvSoEEDjh07xrZt27h79y69evUS2h03bhx79+7lzz//ZMeOHURFRXHixImnvnbR0dFUr17dpvVWREQE/fv3x83NjS5durBkyZICnWPlypW0b9+eBg0aWO1TKBSCe9mTXL9+HWdn5xy37777LtvzHjp0iNatW1uYz3fq1ImLFy/mapI+YsQIvLy8aNq0KYsWLSrQ/G3atCn//fefsBgh8vwgpqfJD5cugcEAHh7g7V0kp8iv6e+W2C3039AftV6No9wRmVSG3qDnUuIl+m/oz7Rmts1SMjIySE1NRSKR5Mk0BzBF8k1PB6kUg0IOqFFgylMq0ahA4Wjan5j4zNPRiDwbli8/xaBBf2IwmB4kb71VixEjmhRzr0RERERePMzaVIVCIWQPMGtViyrifWhoKKNGjUKn06FSqTh58iRt2rRBq9USHm7KOHDo0CHUajWhoaGo1Wq+++47du3aJbjjVKlShf379zN//nzatGljdY45c+bQpUsXxo4dC5iyLxw8eJBNmzZZ1DMYDCxZskQQ/MLCwti9ezdTpkzBzc0NOzs7HB0d8fF5lMd77ty5NGjQwELoWrRoEf7+/ly6dAk/Pz8iIiJYsWIF7dq1A2Dp0qVUqFDhqa/dtWvXbFpuxcbG8s8//7BhwwYA+vfvz8cff8ykSZPybQ4dGxtr5S+cF/z8/HL1s/Xw8Mh23507d6hcubJFmTlbxp07d7Kdj19//TVt27bF0dGRHTt28P7775Oens6HH36Y7/6D6RoHBATk61iRko0oqOYRBwcHpBcvmj4UciAls1kKgPdDATg9PR2VSoVDDoGI9AY9n+3+DLVejaudq9CGVCbFVepKqiaVWadn0YUuVlF/nZyc+PXXX7l9+3aO57CJXA4GOY4kA6ZzSkqZX8/zhlQqpU6dOkUasTI8/BjvvbdZ+DxoUH0WLuyGTCYaZojkzLOYnyIiT0NJmaO2guxkx+nTp9Hr9SgUCgBkMhlarZb4+Hjc3d2LxOczJCSEjIwMIXhT9erV8fb2pk2bNgwePJisrCyioqKoUqUKFStW5Ny5c2RmZtKhQweLdjQajU2tH8DFixfp0aOHRVnTpk2tBNWAgAAL7aSvr2+ui/ynTp0iMjISZ2frgH9xcXGoVCo0Gg3NmjUTyj08PAgKsh2UMj+oVCqbUXsXLVpEp06dBEXFK6+8wpAhQ9izZ48gLJvJ7X2toNp0uVxeeNkf8sHnn38u/L9BgwZkZGQwffr0fAuq5uuSmZlZqP0TyR9Fcf8UBdU8YjQai8Q/1d/f38IZ397eHhcXF9LS0khISMDf3z/bYw/fPExcUhxKmZJMXSYKqQKFTIEEk+DrKHfkevp1bktvU95greGUyWQFXiU0SiQk4UEZkpChR+/ogtxOYcozK1IsaDSaIgtdP3PmIcaM2SF8HjGiCf/7Xxek0tIT/EKkeCnK+SkiUhiUhDkqk8nyVC8pKclCmwo8E61qYGAgFSpUIDIykqSkJEEj6ufnh7+/PwcPHiQyMpK2bdsCpkV3gM2bN1P+CUsrpVL5VH0xC+hmJBJJrsFc0tPT6datm80gSL6+vly+fPmp+pQTXl5enDlzxqJMr9ezdOlS7ty5g1wutyhftGiRIKi6urpy7do1jEajxQJEcnIyMplMMOmtXr06Fy5cyHffrl+/Ts1c3m0nTpzIxIkTbe7z8fHh7t27FmXmz49rtHOjWbNmfPPNN6jV6nzNj8TEROCRskfk+UEUVHPBPOmzsrIwxseb9IeFGPFXr9ejVqsBUxAg8znT0tK4f/9+joLqvYx76I16JAYJ6dp0odxeZo+bvZvJDNioJ1NSNCtMemQk4oE9WTjYOyKn6MKei+SMwWDg4sWL1KlTJ88vOnnBaDTyzTf7+PLLKKFs/PiWfP99+1IVoVGkeCmq+SkiUliUpjlq9k3V6XQolUr0+kcB7SQSCTqdrki1qqGhoURFRZGUlMS4ceOE8tatW7N161aOHDnCe++9B0DNmjVRKpVcv37dppmvLYKCgjh61DL2wZOf84LZb/dxGjZsyPr16wkICLAQDM1UrVoVhULB4cOHqfgwH3xSUhKXLl3Kc/+zo0GDBvzyyy8WwuaWLVtIS0vj5MmTFvPu7NmzDB48mOTkZNzd3QkKCmLNmjWkpKRYBJU6ceIElStXFoT2vn37MnHiRE6ePGmlsdZqtWg0Gpt+qk9r+tuiRQs+++wztFqt0JedO3cSFBSUrwWTmJgYypQpk+9FjLNnz6JQKPIcRFOkaBCj/hYD5pWgcp6eSMxO2oWoUT137hyTJ0+28JfIa0Clsk5lkUlkaAyWWkzzDVBv0CNFiqPRsdD6+yQGpGRSdO2LFC8LF56wEFK//jpEFFJFREREihGj0YhKpUIul9uMCCyXy1GpVEUWVCk0NJT9+/cTExNjIby1adOG+fPno9FohEBKLi4ujB07ltGjR7N06VLi4uI4ceIEc+bMYenSpTbb/+CDD9iyZQszZ84kNjaW+fPns3Xr1nw/dwICAjh8+DDx8fEkJCRgMBgYMWIEiYmJ9OnTh6NHjxIXF8f27dsZPHgwer0eZ2dnhgwZwrhx49izZw9nz55l0KBBViaNiYmJxMTE8O9DS7uLFy8SExPDnTt3crxu6enpFiliIiIi6Nq1K/Xq1aN27drC1qtXL9zd3Vm5ciUA/fr1QyKRMGzYMI4fP87ly5dZtGgRs2bNYsyYMUJ7o0aN4qWXXqJdu3bMmzePU6dOceXKFdatW0fz5s2FIJpPYjb9zWnLSVDt27cvdnZ2DBkyhHPnzrF27Vpmz57Nxx9/LNT5448/CA4OFj7//fffLFy4kLNnz3L58mV++eUXvvvuOz744AOLtmNiYoiJiSE9PZ379+9bXHcz0dHRvPzyy/l3ZRMp8Yga1VwwmxPcu3kTjEZTflBPz0Jr32AwYDAYLFb9mjVrRs2aNXNNl9KsfDOqlqnKqbungEcCqkKmwGg0kqnLpJJLJXxTfJ++owkJkJHx9O2IlCp6965NRMRJDh++yYwZHfn447zlphURERERKRqkUimNGjVCq83eikmhUBSZv21oaCgqlYrg4GAhYA6YBNW0tDQhjY2Zb775Bm9vb6ZOncqVK1dwd3enYcOG2ZqRvvTSS4SHh/PVV18xadIkOnXqxOjRo5k7d26++jl27FgGDhxIzZo1UalUXL16lYCAAA4cOMAnn3xCx44dUavVVKpUic6dOwvXa/r06YKJsIuLC2PGjCElJcWi7b/++ovBgwcLn3v37g3Al19+yeTJk232x9PTkx49erBy5UqmTp3K3bt32bx5M6tWrbKqK5VK6dGjBxEREYwYMQJ3d3f27dvH+PHj6d69OykpKQQGBjJz5kyLvKxKpZKdO3fy008/MX/+fMaOHYujoyM1atTgww8/pHbt2vm6hnnFzc2NHTt2MGLECBo1aoSXlxdffPEF77zzjlAnJSWFi+ZYL5jm6Lx58xg9ejRGo1EYz7BhwyzaflwzfPz4cVatWkWlSpUsIvyuWbMm2+suUrqRGIsyjnkpIDU1FTc3N1JSUnB1dbXaHxkZydatW9HcucNP//6LJCQEfvyx0M5/48YN5syZQ5kyZZgwYUK+j19+ajkDNw7EiBHJw8BGbvZuaPQalDIl05pNI3aLaQXthx9+KFgn09LgzTdN/549C3I5GqOc5Mfu2+5uYCfRmaIi798PdeoU7FwiBUKv1/Pvv/9Ss2bNQjdbS0pSsWNHHG+9VTQPOJHnn6KcnyIihcGznqNZWVlcvXqVypUrF7tfbGlg2LBhXLhwgejo6OLuylNx+vRpOnToQFxcnM2ATjlh1qQ7ODiIVk2PsXXrVsaMGcPp06dtmnOLFC453buSkpLw8PDIVqYqCKLpby6YzXCDFAqTGFhE+VMfR6VSsX79eiIiInI13XGzd8Pd3h259DEnfIOeIM8gVryxgjZ+T+dTgUYDQ4aYBNQrV0CtBp0OqVaDkkebVKsxCanOzqb0PaWIzZs3s2DBAuLMOXJLITKZrFB8q7RaPQkJlj7NZco4iEKqyFNRWPNTRKSoEOdoyeLHH3/k1KlTXL58WTATHjhwYHF366mpW7cu06ZN4+rVq/k+ViKR4OjoKAqpT5CRkcHixYtFIbUEUBT3T/FbzQVzqPMrSUkYAUkhBlLKDoVCwdGjRzEYDILGNzuO3TqGvdwepUyJ1qCllnctprafSrPyzZBJZU+f/PjWLTD7NMhkUK0azJrF6Vs+DBz0qNrSn6FhQ0xCainLoapSqcjIyECn0xV3VwqM0WgkLS0NFxeXAj/EsrJ09Or1G1evJhMVNRBPT9H3WKRwKIz5KSJSlIhztGRx5MgRfvjhB9LS0qhSpQr/+9//GDp0aHF3q1AYNGhQgY4zGo0YDAakUqk4Rx+jZ8+exd0FkYcUhZGuKKjmwt27d0Gvx/gwMm9hRvzNDrlcjoeHBwkJCdy/fz9XQRVMK212Mjt61OhBS/9Hedg8n9afNiAA/voL+vQx+an+/jvUqEHmfjj7WLXMqoBo7VtsGAwGrly5UmCNQEaGhh491rJz5xUAevRYy969g8SHoUih8LTzU0SkqBHnaMli3bp1xd2FEolarRYDBomUWMSov8VFVpbpX19feCwseFGSl8i/Kq2Ks/fOWpQ19mts8fnJYAsFWu3w94c//4TVq5+JoC7ybElNVdO580pBSHVyUjB5ckipFVJTUlKKLNqliIiIiIiIiIjIs0EUVPPCQ0HV+FhY7aLGnL/1/v372dY5dfcUOsMjc1WpREoDH8u8WQ8ePLD4vGbNGiZNmsQ///yTvw55esITOblESj+JiSratVvG/v3XAXB1VbJjRxht21Yu5p4VjIyMDE6fPs2xY8eKZGVPRERERERERETk2SAKqnkhIwOJXm/SLD4j8qJRNZv9mqnpXRMnO8tEzk9qli5cuMC1a9eskmALHDwIqakF6LFIcZPfyJF376YTErKEY8duAeDp6UBk5EBatiy6eZ6UlMTRo0dJSkoqsrZ1Oh1OTk5FlppBpGCIkU1FSjriHBUp6ZRWSycRkYIi+qjmhawsJHo9Uh+fZ3bKvGhUj946avG5iV+THNvUarVcu3YNgOrVq1tX+Ocf6NsXqlaFVavgsfxozzNNmzalVq1aFnnfShsymcwikXZu/PdfKu3bL+PiRZPG3cfHmZ07w6hdu2xRdRGj0Uh8fDwPHjxALpfj7u5eaA9do9HI1atXycjIQC6X4/MMf6siuZPf+Ski8qwR56hISUcikYj+qSIlmqLw7xdVDrmhVsNDP09DQEChN6/VavHy8rLKp2UWVBMTE21qP41Go5VG9Un/VMDipnb16lX0ej2urq6Cxlbg/HkYNMiUjub8eejWDe7eLeCoShcVK1akRo0ahZbzqTgwGAw8ePAgT+aud++m07r1YkFI9fd3Zd++QUUqpAIkJyeTmJiIXC4nMTGR5OTkp2ovIyND2G7fvi0s6mRrLSBSbORnfoqIFAfiHBUp6RiNRnQ6nRiDQaTEUhT3T1Gjmh0JCaYtLg4e3hSM//0HZmHGy8u0PSWOjo7Y2dnh6GiZCsTFxQU7Ozs0Gg2JiYmC4GrmavJVklSW5pO2BNXH80pdunQJgGrVqllrsmQycHJ6ZPZbv36hjE/k2WA0Grlx4wbueQj25e3txMsvV+Lq1WSqVi3D7t0DqFQp9+Oetn/x8fHo9Xrs7e3JysoiPj7+qbSqMTExGAwGIQm6TqdDIpEglUq5du0aZcqUEc2kSgj5mZ8iIsWBOEdFSgMajUbUqoqUWIpiEUXUqGbHhg3Qvz+1tm4FnY7yKhXS776D/v1N24YNhXKazMxM0tPTSUlJsSiXSqWC1tOW+e/Rm5Zmv+Wcy1HBtYJVPZVKJfw/Li4OgMDAQOuOVK8Of/9typPaqhXMnWsSXkWeO6RSCRERrzFuXEv27Rtc5EIqPNKmKhQKJBIJCoXiqbWqdnZ2Qnt6vR6pVIpMJsPOzq5QNLYiIiIiIkVPVFQUEokkX/fsyZMnU79+/SLr05OEhIQwatSop27nwYMHlC1b9ulz3IsIfPrpp3zwwQfF3Q2RIkIUVLPjjTdgxQqkX38NAQFIlUoMEyfCihWm7Y03CuU0qampJCcn2xRGg4ODadCggZW2FcDfzZ83arxBRbeKADT2bWxTe6TTPYoKHBsbC5g0qjbx8zOloYmIADu7ggyn2FCr1VapeEQeodVamsPK5VJ++KEDfn4uRX7ux7WpZv8FmUyGXq8nPj6+wCtwTZo0oVmzZjg7OyOXy3FycsLR0RG5XP7UbYuIiIiUFs6ePcvYsWM5e/Zs7pWfgvDwcFxcXCzeK9LT01EoFISEhFjUNQuf5gXynGjZsiW3b9/OMWd8QSgs4fJJtFotn3zyCXXq1MHJyQk/Pz8GDBjArVu3cj12ypQpdO/enQAbrmSdOnVCJpNx9OhRq33ZjWXJkiVWVgCpqal89tlnBAcHY29vj4+PD+3bt2fDhg1F+kyMioqiYcOthX57AADfWklEQVSGKJVKAgMDWbJkSa7HbN++nebNm+Pi4oK3tzdvvvmmhRB/+/Zt+vbtS/Xq1ZFKpTavwdixY1m6dClXrlwpvMGIlBhEQTU7vLwgOJhLWVlgb891BwcICoLgYNP2DMxiO3fuTJ8+fWze0Fr6t2TuK3P5Z+g/xLwbw2etP8uxLZ1OR0pKCjKZjMqVc0g94u4OLkUvvBQ2J06cYN68eURHR+f72H///Zd//vknxwjLpQGXbL63ffuuERQ0l3Pn7j3jHpkwa1NlMhlqtRqNRlNoWtUnNbVAobUtUrhkNz9FREoKpXGOGo1G1qxZw7Fjx1izZk2RCiKhoaGkp6dz7Nij+BjR0dH4+Phw+PBhssw554HIyEgqVqxI1apVc23Xzs4OHx+fUuOqkZmZyYkTJ/j88885ceIEGzZs4OLFi7z22mu5HhcREcGQIUOs9l2/fp2DBw8ycuRIFi1alG0buUWzT05OpmXLlixbtowJEyZw4sQJ9u3bx1tvvcX48eOtrPcKi6tXr9K1a1dCQ0OJiYlh1KhRDB06lO3bt+d4TPfu3Wnbti0xMTFs376dhIQE3nhMEaRWq/H29mbSpEnUq1fPZjteXl506tSJX375pdDHJVL8iIJqLpij41bRaoskmlVOqNVq4uLi+Pfff3OsV9apLAHuAbm2BVCpUiXs5HL43//gOXqJT0tLA7Cpfc4Ns6D6ZM7Z0oRMJqNq1apWc3THjjg6d17B1avJtG+/nKtXCz8tTE6YtanmFXitVotWq0Wv1yORSNDpdAXWfD7ettn817w9bdsihUt281NEpKRQUuZoVlZWnjbzPfXMmTPExMTg6OhITEwMZ86csWjPYDDk2E5+CAoKwtfXl6ioKKEsKiqK7t27U7lyZYv87FFRUYSGhgp9mDp1KpUrV8bBwYF69erx+++/W9R90vR3wYIF+Pv74+joSI8ePZg5c6ZN/+Hly5cTEBCAm5sbvXv3Ft4FBg0axN69e5k9ezYSiQSJRCJo6s6ePUuXLl1wdnamXLlyhIWFWSxUZ2RkMGDAAJydnfH19WXGjBkW53Rzc2Pnzp306tWLoKAgmjdvzty5czl+/DjXr1/P9vpt2bIFpVJJ8+bNrfYtXryYV199lffee4/Vq1dbuG2ZkUgk2Nvb5yjQT5w4kfj4eA4fPszAgQOpWbMm1atXZ9iwYcTExFgF7iwswsPDqVy5MjNmzKBGjRqMHDmSnj178tNPP2V7zPHjx9Hr9Xz77bdUrVqVhg0bMnbsWGJiYgQLuYCAAGbPns2AAQNy1Lh369aNNWvWFPq4RPJHUdw/xWBKeUEuR1OlCgYPj0KX7J2cnISbz5PcvXuX+fPn4+rqSs2aNZ/qPOYHUrXAQPj6a/j1V5Of7erVUIrTspjp2LEjL7/8coFWZKtUqUKZMmUK3ezoWWIwGLh37x5ly5YVVlz//PMCvXr9jkZjMvutX9+HcuWK5iGVHeZAR2ZzXKPRiNFoFCLzyuVyVCoVRqMx39/dk20/ydO0LVK42JqfIiIliZIyR4cNG5anegMGDKB9+/asW7cOjUZD2bJluXfvHuvWraNOnTrCPe/WrVtMmDAh23aWL1+er/6FhoYSGRnJp59+Cpg0p+PHj0ev1xMZGUlISAgqlYrDhw/z9ttvAzB16lRWrFhBeHg41apVY9++ffTv3x9vb2/atGljdY4DBw7w7rvvMm3aNF577TV27drF559/blUvLi6OjRs3smnTJpKSkujVqxfff/89U6ZMYfbs2Vy6dInatWvz9ddfA6ZsCsnJybRt25ahQ4fy008/oVKp+OSTT+jVqxd79uwBYNy4cezdu5c///yTsmXLMnHiRE6cOJGjT2xKSgoSiSTHYFzR0dE0atTIqtxoNLJ48WLmzZtHcHAwgYGB/P7774SFhVnV02q1yOVym880g8HAmjVr6NevH35+flb7cxJSo6Oj6dKlS7b7AebPn0+/fv1s7jt06BDt27e3KOvUqVOOpteNGjVCKpWyePFiBg0aRHp6OsuXL6d9+/YoFIoc+/IkTZs25b///iM+Pt6mFaLIs0GM+lsMnD17FuRyLsvlGD09C7198+qYnQ2fUHMwpdTUVNRqNUqlMt/te3h4AODq6kqXLl2od/y4SUgFuHQJ3nwT9u6FfN4USiIFjYTXsGHDQu7Js8doNHLnzh0hOvSaNWfp338Der1Jm9ijRzCrV7+JUvlsf/JSqZRGjRqh1WpJTk7m8uXLODs7W+QrVCgUBXoxfLzt7Cho2yKFy5PzU0SkpFEa56hZm+ri4oJEIsHFxUXQqtatW7dIzhkaGsqoUaPQ6XSoVCpOnjxJmzZt0Gq1hIeHAyahRa1WExoailqt5rvvvmPXrl20aNECMC0O79+/n/nz59sUVOfMmUOXLl0YO3YsYLJsO3jwIJs2bbKoZzAYWLJkiWCyHRYWxu7du5kyZQpubm5CRoXH82rPnTuXBg0a8N133wllixYtwt/fn0uXLuHn50dERAQrVqygXbt2ACxdupQKFayDVZrJysrik08+oU+fPjmmubt27ZpNAXLXrl1kZmbSqVMnAPr3709ERISVoAoIgqotEhISSEpKKlA+4MaNGxMTE5NjnXLlymW7786dO1b7y5UrR2pqKiqVyub7WeXKldmxYwe9evVi+PDh6PV6WrRowZYtW/Ldf/N1vXbtmiioFiNFYcEmCqq5UJxmg46Ojjg7O5Oenk5CQgLly5fPdxtm4dbOzo6OHTtC1aomTerNmyCRwIQJz4WQKvKIRYtOMnToX+asSvTrV4clS15HLi8egU2pVKJUKlGr1chkMhQKRaGZH5nbFhEREXkeWLBgQZ7qyWQyvvrqKzQajaDFc3BwIC0tzUKr6ufnl+c280JISAgZGRkcPXqUpKQkqlevLmhGBw8eTFZWFlFRUVSpUoWKFSty7tw5MjMz6dChg0U7Go2GBg0a2DzHxYsX6dGjh0VZ06ZNrQTVgIAAC79iX19f7t3LORbDqVOniIyMtPkMiouLQ6VSodFoaNasmVDu4eFBUFCQzfa0Wi29evXCaDTm6iOpUqlsWs8tWrSIt956SxBA+/Tpw7hx44iLi8uTj6+Zp3lfdXBwsJ0Rogi5c+cOw4YNY+DAgfTp04e0tDS++OILevbsyc6dO/NlCWUWhDMzM4uquyLFRIlUNcybN4+AgADs7e1p1qwZR44cybbuggULePnllylTpgxlypShffv2OdYviSQlJREfH2/Tlv/JFDUGowGtPu/RbZOTk0lJSeHSpUv89ddfJkH1779NAaGmTIFu3QpnEKWY69evs3jx4hx9S0oL8+YdZciQR0LqsGENWbq0+IRUEREREZG8Y29vn6ft/PnzFtpUwEqrCibLk5zayS+BgYFUqFCByMhIIiMjBY2on58f/v7+HDx4kMjISNq2bQuYogIDbN68mZiYGGH7999/LfxUC8KT5qESiSRX08P09HS6detm0ZeYmBhiY2Np3bp1vs5vFlKvXbvGzp07c9Smgul9LinJMk5EYmIif/zxBz///DNyuRy5XE758uXR6XQWQZVcXV1JNee5f4zk5GTBbcnb2xt3d3cuXLiQr3GAyfTX2dk5x23lypXZHu/j48Pdu3ctyu7evYurq2u21m7z5s3Dzc2NH374gQYNGtC6dWtWrFjB7t27OXz4cL76n5iYCFCqLCJE8kaJ06iuXbuWjz/+mPDwcJo1a8asWbPo1KkTFy9epGzZslb1o6Ki6NOnDy1btsTe3p5p06bRsWNHzp07VyANZHbIZLIi8XMzB34xB0LQ6XQWZh3e3t7Ex8cLgur5++fptrobDXwb0Ni3MY39GtO+Svts+5aZmcn9+/fR6XQsWLCAV199FamPD2zdCqImCqPRSHR0NHFxcdjZ2dG3b99S6c8okUhYu/Y/vv/+uFA2alQzZs7sVCrHI/J8IZFI8PDwEOeiSImlNM1Ro9HIunXrUKlUODo6CsESwfSuolKprHxVC5PQ0FCioqJISkpi3LhxQnnr1q3ZunUrR44c4b333gOgZs2aKJVKrl+/btPM1xZBQUFWKVpspWzJDTs7O6v4BQ0bNmT9+vUEBATYNKGtWrUqCoWCw4cPU7GiKf1fUlISly5dsui/WUiNjY0lMjISzzy4hjVo0IAVK1ZYlK1cuZIKFSqwceNGi/IdO3YwY8YMvv76a2QyGUFBQezYscMqWM2JEyeEoJ9SqZTevXuzfPlyvvzySysz4/T0dOzt7W2O+2lNf22Z7O7cuVMw97ZFZmamlWuOeXz59XU8e/YsCoWCWrVq5es4kcKlKO43JU7NMnPmTIYNG8bgwYOpWbMm4eHhODo6Zhuue+XKlbz//vvUr1+f4OBgFi5ciMFgYPfu3YXaLzs7uyLxdVu2bBkODg4YDAZcXFystKrm1SFzRLojN4+Qpcvi0I1DzDkyhy+jvsxxYuzdu1fw4bt79+6jm6EopAImf4b4+HiUSiXx8fFcu3atuLtUIKRSKd7ejx6Ukya9LAqpIiUGqVRKxYoVRX9hkRJLaZqjOp2Ou3fv4uDgQGZmptXm4ODAvXv3LPKdFiahoaHs37+fmJgYC+GtTZs2zJ8/H41GI0T8dXFxYezYsYwePZqlS5cSFxfHiRMnmDNnDkuXLrXZ/gcffMCWLVuYOXMmsbGxzJ8/n61bt+b7eRYQEMDhw4eJj48nISEBg8HAiBEjSExMpE+fPhw9epS4uDi2b9/O4MGD0ev1ODs7M2TIEMaNG8eePXs4e/YsgwYNspgXWq2Wnj17cuzYMVauXIler+fOnTvcuXMHjUaTbX86derEuXPnLLSqERER9OzZk9q1a1tsQ4YMISEhgW3btgHw3nvvcenSJcaNG8eZM2e4ePEiM2fOZPXq1YwZM0Zob8qUKfj7+9OsWTOWLVvGv//+S2xsLIsWLaJBgwaChvtJzKa/OW05pW969913uXLlCuPHj+fChQv8/PPPrFu3jtGjRwt15s6dK/j9AnTt2pWjR4/y9ddfExsby4kTJxg8eDCVKlWyMAs3a73T09O5f/++oJF/nOjoaF5++eUCxyoRKRyK4v5ZojSqGo2G48ePW0Sok0qltG/fnkOHDuWpjczMTLRarRBE6EnUarXF6qPZlMKc1gJMKwJSqRSDwSBEKdVqtYK288kVOnP9J8ulUqmQNuPJcvN4r169ipubm5Bi4+zZs6jVamHFq0yZMoDJ9Fev13P0puWqYmO/xlbty2QyDAYD+hs3qDRlCs7BwWQolUJkuW7dugl9yK7veRvTo5U903WyXgUzn+fJcplMhtFotFluvu65lT/5PeXW9yfLjUYjBw4cQKvV4ubmRkpKCgcOHKBChQpCXVt9L4lj0mq1vPGGL6mprbGzkzFhwss5zr3iHJPBYBAsCQo+90rWmHLr+4s+JoPBwK1bt2wGJCmtY8qp7+KYSt+YDAYDN2/epHz58igUimcyJvP7hXm/RCKx6Wf4ZLlcLmfq1KlCKhZbuLq6IpfL8912XsrNkX2Dg4MpW7asEFm9devWpKWlERQUhI+Pj1D+9ddf4+XlxdSpU7ly5Qru7u40bNiQCRMmWIzf/P+WLVvyyy+/8PXXXzNp0iQheuy8efMs6j7+r63/jxkzhkGDBlGzZk1UKhVXr16lUqVK7N+/n08//ZSOHTuiVqupVKkSnTp1Esb8ww8/CCbCLi4ufPzxx6SkpAj9u3nzpsmVCqwiAe/Zs4eQkBCb30nt2rVp2LAha9euZfjw4Rw/fpxTp07x66+/Wn1Prq6utGvXjoiICF555RWqVKnC3r17mThxIu3bt0ej0RAcHMxvv/1Gp06dhOPLlCnDoUOHmDZtGt9++y3Xrl2jTJky1KlThx9++AE3N7dCmQdPEhAQwKZNm/j444+ZPXs2FSpUYMGCBabYKA+/j/v37xMXFye017ZtW1auXMn06dP54YcfcHR0pEWLFmzbtg17e3uh3uNC6/Hjx1m1ahWVKlXi6tWrQh/XrFnDl19+afVbe5ox5URh/p6Ksjw/5LXtx3+zT97fimJxTGIsQUkGb926Rfny5Tl48KCFucD48ePZu3dvnmzW33//fbZv3865c+ds+l9MnjyZr776yqrcbJ8PJsf5ihUrcv36ddavX09cXBx6vZ7PP/8cPz8/4uLiLB4Q/v7+eHp6cuHCBYu8ZFWqVMHV1ZUzZ85YPDiDgoKws7Pj888/Jy0tDblcjsFgQKPRIJPJcHV1pVevXoBJkN60aRNyuZz/+7//Y8C+AdxR3UEikSCXy/m8+ee0cHp0rVxcXKhatSp3L15E27UryuvXuefiwo8NG3I+LQ2DwUC/fv2EVVAfHx98fHzyPaalS+MYNOiRk/+uXWpCQuRWOdzq1KmDRqPh4sWLQplMJqNOnTqkpqZy5coVodze3p7g4GAePHjAjRs3rMZkXrE08/j3ZPZPyM+Y7t+/z/79+7G3t8fBwYHU1FT0ej2tWrXC29tb+J5Kw5j+/fdfbt++LZiu5Tb3imNM//77Lzdu3EChUODh4VHguVeSxlTQufeijcmckqhu3bpWK+GldUzw/H1PL/KYjEYjiYmJeHl5Ua9evSIf06VLl1CpVFSsWBGlUomdnZ1FSi0zSqUSmUxmFSTGnDHgyXybDg4OGI1Gqxypjo6O6PV6i4V6iUSCg4MDOp3OQhNo9ms15702I5PJhMB4j19fhUKBQqEgKyvLQrgvzDENGzaM8+fPs3PnzlI9pm3btvHZZ59x9uxZJBJJvr4nrVYrpGQrSWMyU1xzb9euXXz88cccPnxYuDalfUwl+XtSq9XcuHGD6tWrk5ycbHHfk8vl1KlTh5SUlFx9tvPKcyWofv/99/zwww9ERUVlG5rdlkbV39+fxMRE4aI+vhK6c+dOdu3ahUql4qeffkKhUBTK6q5Op2Po0KG4ubkJq7FZWVnY2dmRmprKr7/+ilwuR6fT8cUXX2A0Ghk2ahhtVlv6eOzsv5NgL8tQ5DKZDMOgQSStWYPBYEAqlXLWy4uJlSqh0Wjw8fFhw4YNQv8KsmK9d6+ekJBHGtV9+4y0alV6VuGNRiNr164lNjYWNzc3IQhDamoq1apV46233srWV6IkjEmnM/Dee5vp3j2Y7t2D0Wg0nDt3jlq1aiGTyUqktiQpKYkrV67g5OREtWrVRA3QCzQmvV7PuXPnbPrMldYx5dR3cUylb0zmOVqrVi3s7OyKfEwZGRlcu3aNypUrC4vqJUFbUtzlZn788Uc6dOiAk5MTW7duZezYscybN4+hQ4cWex+f9nuaNWsWb775Jv7+/vlq2/yeaBZAStKYcqKo+7J+/XoqVKhgEam5IO3kh5I2x57lmLKysrh69SpVqlQR7pVmkpOT8fLyKlRBtUSZ/np5eSGTyWxGDns8D5YtfvzxR77//nt27dqVY/6w7NJZyGQyKyd1qVSKn58fEokEHx8f4aH2ZL3H28hr+axZs3BxcUGv1yOXy4W2dTodLi4u/O9//2PcuHHIZDLKlClDamoqB+IPWLThonQhWOOC7AkNBcDBChUoK5XiZTCQ4OBARK1aSDIyhOv7999/88YbbxSo77bKJRIJEkn+2pFIJDbLs7Nxz295Tn2Jj48nPj4eBwcH4YYvlUpxcHAgPj6e//77T8jFVdLGpNHo6dfvD9avP8+qVWfZtKkvoaGVhHM/fv7CmKuFNaYyZcrYTHZeGH3Mb3lxzr2iKi/pYzLdI2z3Mbt2SvqYClIujqnkjunxcTyLMZl/E48v3jy5kJNbeX7Ib9vFVQ6m4EnTp08nLS2NKlWq8L///Y9hw4aVmD4+zff0uN9mQdp+2jlTFGPKjaLsS8+ePQvtvPmhpM2xZzWmx+ffk/e37O53T0OJElTt7Oxo1KgRu3fv5vXXXwcQAiONHDky2+N++OEHpkyZwvbt22ncuHGh9slssvPgwYNCmQRgEkbPnj2Lu7u7hRnA44Lw2bNnBZ/YESNG4OTkxDf7vrFop6FbDWTNW8ATzvFGoEFmJhgMyICFtWqRrFSiT00VVuUWL17M66+/XiSTqqRjNJp8U9VqNXZ2dlbfgVqt5sCBA1SqVKnQvvPCQqXS0rPnb2zZEguA0Qjp6RphMaWk9VdEBBDnp0iJR5yjJYt169YVdxdKJE+m5BERKUkUxf2zRAmqAB9//DEDBw6kcePGNG3alFmzZpGRkcHgwYMBGDBgAOXLl2fq1KkATJs2jS+++IJVq1YREBAg2Eqb8z49LWbTH4VCUWhCXWpqqmBCbGv1Vq/Xo1AoSE1NxcPDQ4i0dvTWE4GUnAIhPRKkUngs3LjRaEQDSB9OmMyHpllmcyipVEpaWhpZWVk4OjoWyphKE3q9npSUFJRKpc0IfUqlkpSUFEHbXVJIT9fw2muriYyMB8DeXs7GjW/RqZMpSXduVgciIsWFVCoV56dIiUacoyIlHYlEIgqqIiWa516jCvDWW29x//59vvjiC+7cuUP9+vXZtm2bkL/p+vXrFhfil19+QaPRWKn+v/zySyZPnvzU/fHy8gLA09PTQrB8Gjw8PBg0aBDx8fE8ePCAK1euoNVqkcvlNG3aFDAFhTBHLj5z5gy/bfiNf2T/YOdgJ7TTpMzDfFFyOSgUGAGt1qRRtXfxwKhVI9NqkMkUyOV2DB36k3BsmTI+nDhRcCH19OkCH1rsyOVywsLCrBzMH8fBwaFECanJyVm88spKDh36DwBnZzs2bepDmzYBgEn4jo+PJyAgoFDmqIhIYSLOT5GSjjhHRUo6RqMRtVqNUqkUNf8iJZIn4wMUBiXnTfwxRo4cma2pb1RUlMXn+Pj4Iu3LvXv3APjvv/8Ktd1WrVrRqlUrrl27xvTp03nw4AHVq1enf//+VnWVSiVXVVfROmqxwySoSiQSGrjVEOoYgYT7oBUiQ8uQo8AeHVqtFI1GxvDhLxXqGEozLi4uOeYEK0kkJGTSseNyTp40WQu4u9uzbVs/mjWzTPWRU6oCEZHiRpyfIiUdcY6KlHSeDPAlIvK8UyIF1ZLEk4GdChupVCrkbEtLS8NoNFqtlHl7e3NHesfiBhXsFYyLwkn4bEhKxeVh/iI1dmRgNnt+NqtuNjIBiRQCt2+n0b79cv799z4A3t6O7NwZRr16oomaiIiIiIiIiIjI88uLF0mnBKJQKKhTpw6zZs2yac7h5ubGPfk9i3D5TfyaWNSR6LUoMG1yCl/1nhMNGkC9es/0lC8M588nEBv7AAA/Pxf27h0kCqkiIiIiIiIiIiLPPaJGNY8oFIoi9QnIyR9SIpFwX3EfdAh5URv7NSYnedTNDRQAGiNKpUnjGR1d6N3G3t4kpIr+/UVD27aVWbfu/xg3bifbt/enSpUyNutJJBL8/f1FvxWREok4P0VKOuIcFSkN2NnZ5V5JRKSYeCGi/pZUHs91+qy5lnINlcQU+MesUW3s1xhuPPSn0ekwSmVoHpr56pFijxY7iQ691BQUWCaDVq2KpfsiT8nrrwfzyivVsLPLPsCHVCrF09PzGfZKRCTviPNTpKQjztHiIyoqitDQUJKSknB3d8/TMZMnT2bjxo3ExMQUad/MhISEUL9+fWbNmvVU7Tx48IAaNWpw5MgRIVd7XpFIJCUqyGNJoXfv3jRp0oQxY8YUd1deeIpCThJNf/NIVlZWkUSzygsnbp8QvnyDwYCXoxeV3CqBhwc4O4PBgFGmQIMSDUqMSJFqNWAwoHd0JF1cgSs1nDhxm9mz/7Eqz0lIBVOktQsXLhTbHBURyQlxfoqUdErzHI2NjeWHH34gNja2SM8THh6Oi4sLOp0QtZH09HQUCgUhISEWdaOiopBIJMTFxeXabsuWLbl9+zZubm6F2t+QkBBGjRpVqG2amTx5MsHBwTg5OVGmTBnat2/P4cOHcz1uypQpdO/e3aaQ2qlTJ2QyGUePHrXaFxISwkcffYRKpcJoNArlS5YssRLuU1NT+eyzzwgODsbe3h4fHx/at2/Phg0bLI4tbKKiomjYsCFKpZLAwECWLFmS6zFGo5Eff/yR6tWro1QqKV++PFOmTBH2Dxo0CIlEYrXVqlVLqDNp0iSmTJlCSkpKUQxLJB+8MFF/SyJF9eN2dXWlQ4cOODg4ZFunR3APpK2kLNq6CLWLmubBzU3q9fLl4cgRSEzk9AkYOOjRMUt/hoYN4dqDByTOm/eMQiqJPA2HDt2gS5eVpKSokUgkfPhhs3wdn5WVVUQ9ExF5esT5KVLSKY1z1Gg0smXLFs6ePYtSqeTDDz8sMvPl0NBQ0tPTOXbsGM2bNwcgOjoaHx8fDh8+TFZWFvYPIytGRkZSsWJFqlatmmu7dnZ2pS6HbfXq1Zk7dy5VqlRBpVLx008/0bFjRy5fvoy3t7fNYzIzM4mIiGD79u1W+65fv87BgwcZOXIkixYtokmTJjZayP1dNDk5mVatWpGSksK3335LkyZNkMvl7N27l/Hjx9O2bds8a63zw9WrV+natSvvvvsuK1euZPfu3QwdOhRfX186deqU7XEfffQRO3bs4Mcff6ROnTokJiaSmJgo7J89ezbff/+98Fmn01GvXj3+7//+TyirXbs2VatWZcWKFYwYMaLQxyZSvIga1WLGYDCQmZmZY05PiURC/Yr1CdYH01rdmq9Cv3q0s3x5qFOHzKp1OMujLbNqHahTB6Of3zMYRfFw7tw5Ll++jFqtLu6uPDWRkVfp0GE5KSmmsfz++7/odGIYehEREZEXCbVanafNrLk4f/4858+fx97envPnz3Pu3Llsj7F1jvwQFBSEr6+vRZrAqKgounfvTuXKlfnnn38sykNDQwHTe87UqVOpXLkyDg4O1KtXj99//92irkQiITk5WShbsGAB/v7+ODo60qNHD2bOnGlTwFq+fDkBAQG4ubnRu3dvIcXQoEGD2Lt3L7Nnzxa0cOZ0hmfPnqVLly44OztTrlw5wsLCSEhIENrMyMhgwIABODs74+vry4wZM6zO27dvX9q3b0+VKlWoVasWM2fOJDU1ldM5JJnfsmULSqVSEPIfZ/Hixbz66qu89957rF69Osd3wpyYOHEi8fHxHD58mIEDB1KzZk2qV6/OsGHDiImJwdnZOfdGCkB4eDiVK1dmxowZ1KhRg5EjR9KzZ09++umnbI85f/48v/zyC3/++SevvfYalStXplGjRnTo0EGo4+bmho+Pj7AdO3aMpKQkBg8ebNFWt27dWLNmTZGMTaR4ETWquVCrVi1iYmLw8vIqkvYvXLjAtm3bUCgUFj/OJzGv0KWkpAgJn/NCaVwhzgtGo5GoqCi0Wi1hYWF5vh4lka1bY3njjXVkZZnMqdq3r8LGjW8hl4vrSCIiIiIvEp9//nme6r3++uu0aNGCZcuWkZSUhFQqxWAw8OOPP+Lt7W2lVXVycuLLL7+0OscPP/yQr/6FhoYSGRnJp59+Cpg0p+PHj0ev1xMZGUlISAgqlYrDhw/z9ttvAzB16lRWrFhBeHg41apVY9++ffTv3x9vb2/atGljdY4DBw7w7rvvMm3aNF577TV27dpl87rExcWxceNGNm3aRFJSEr169eL7779nypQpzJ49m0uXLlG7dm2+/vprwPQelZycTNu2bRk6dCg//fQTKpWKTz75hF69erFnzx4Axo0bx969e/nzzz8pW7YsEydO5MSJE9SvX9/mNdFoNPz666+4ublRL4cUCNHR0TRq1Miq3Gg0snjxYubNm0dwcDCBgYH8/vvvhIWF5fxlPIHBYGDNmjX069cPPxtKipyE1OjoaLp06ZJj+/Pnz6dfv3429x06dIj27dtblHXq1ClH0+u///6bKlWqsGnTJjp37ozRaKR9+/b88MMPeHh42DwmIiKC9u3bU6lSJYvypk2bMmXKlHy9H4uUDkRBNRfMvqHu7u5F4iScmpqKXC7PNYmzk5MTr776KmXKlMlXP5RKpdUP+nkgMzMTrVaLRCIpdL+WZ8mGDefp3ft3tFrT99+tW3XWrfs/7O3z99OUSqVUqVKl2AJ+iYjkhDg/RUo6pXGOXrp0iTt37lj47mVlZaFWqwUT3MImNDSUUaNGodPpUKlUnDx5kjZt2qDVagkPDwdMQotarSY0NBS1Ws13333Hrl27aNGiBQBVqlRh//79zJ8/36agOmfOHLp06cLYsWMBk5ntwYMH2bRpk0U9g8HAkiVLcHFxASAsLIzdu3czZcoU3NzcsLOzw9HR0cKseO7cuTRo0IDvvvtOKFu0aBH+/v5cunQJPz8/IiIiWLFiBe3atQNg6dKlVKhQwaqfmzZtonfv3mRmZuLr68vOnTtzVGpcu3bNpgC5a9cuMjMzBRPZ/v37ExERYVNQzUkIS0hIICkpieDg4GzrZEfjxo1zDUxVrly5bPfduXPHan+5cuVITU1FpVLZdG+7cuUK165d47fffmPZsmXo9XpGjx5Nz549hUWDx7l16xZbt25l1apVVvv8/PzQaDTcuXPnuXznLS0Uxf1TFFRz4dKlSwDcvHmzSPw+HBwcSEpKQpGH/C6tW7fOdp/v73P4ljsAnKYu8BZgMhsuTQ/evJKUlASYfHxLaxS8FStOM2jQRvR6k89Jr161WLGiBwpFzoGTbCGRSHB1dS3sLoqIFAri/BQp6ZSUOfrNN9/kqZ5MJmPevHnIZDIqVKiARCLBaDSSlJREQEAA77//frbvLHk9hy1CQkLIyMjg6NGjJCUlUb16dUEzOnjwYLKysoiKiqJKlSpUrFiRc+fOkZmZaWUxptFoaNCggc1zXLx4kR49eliUNW3a1EpQDQgIEIRUAF9fX+7du5dj/0+dOkVkZKRN7WJcXBwqlQqNRkOzZo9iRHh4eBAUFGRVPzQ0lJiYGBISEliwYAG9evXi8OHDlC1b1ua5VSqVzQWERYsW8dZbbwnvMn369GHcuHHExcVZ+PhKJBJksuzfD54mloqDgwOBgYEFPr4gGAwG1Go1y5Yto3r16oBJY9qoUSMuXrxodc2XLl2Ku7s7r7/+ulVbZkE4MzOzyPstkj1ieppioHr16hw+fBg3Nzf0en2ON4mCoFAo8uwnEhsby7///kulSpWsTFA8ozcymPMA/MVrmAXV5xWzL0uZMrbzipZ0fv31OO++uwnzc2XQoPosXNgNmaxgiwp6vZ5///2XmjVrFvocFRF5WsT5KVLSKSlzNK9mixcvXuT8+fM4OTkJi9ESiQQnJycuXbrEtWvXbApX+TmHLQIDA6lQoQKRkZEkJSUJGlE/Pz/8/f05ePAgkZGRtG3bFjBFBQbYvHkz5cuXL7R+AFYL/BKJJFfrtPT0dLp168a0adOs9vn6+nL58uU8n9/JyYnAwEACAwNp3rw51apVIyIiggkTJtis7+XlJSyym0lMTOSPP/5Aq9Xyyy+/COV6vZ5FixYJEXBdXV1JSUkhMzMTBwcHQSBITk4WrMq8vb1xd3fnwoULeR6Dmac1/fXx8eHu3bsWZXfv3sXV1TXbYKG+vr7I5XJBSAWoUaMGYAou9fj8NRqNLFq0iLCwMJu5ZM0BmLILZCXybBCj/hYjRRnS28zp06fZtWsXlStXpkePHugNeuYfn09D34bUK1eP69evc+DAAVQqVba+Ei8K5pt9UUSvK2pSUrL48ssoQUh9//3GzJnzClLp061Elca0CiIvDuL8FCnplJY5ajQa2bZtm2Diq9FohH1SqRS1Ws22bduoXr16kWg4QkNDiYqKIikpiXHjxgnlrVu3ZuvWrRw5coT33nsPgJo1a6JUKrl+/bpNM19bBAUFWaVosZWyJTfs7OysvtOGDRuyfv16AgICbFpjVa1aFYVCweHDh6lYsSJget+4dOlSrv03awizo0GDBqxYscKibOXKlVSoUIGNGzdalO/YsYMZM2bw9ddfI5PJCAoKYseOHVZtnjhxQhD0pFIpvXv3Zvny5Xz55ZdWZsbp6enY29vbHPfTmv62aNGCLVu2WJTt3LlTMPe2xUsvvYROp7PQHJutGJ803927dy+XL19myJAhNts6e/YsFSpUKLJ4MiLFhyio5sLZs2cBrFaKioLU1FROnjxJVlYWPXr04NKDS3y771sA5FI5lRwq0ZjGFtHpzGjdPLmL6SaSjLtQnp1DemlHoVDg5uZWKhO0u7nZs2NHf9q0WcLQoQ2ZNq19kaUTEBERERF5vtDpdDx48AClUmkzYKJSqeTBgwfodLo8uRXll9DQUEaMGIFWq7UQ3tq0acPIkSPRaDRCxF8XFxfGjh3L6NGjMRgMQuqUAwcO4OrqysCBA63a/+CDD2jdujUzZ86kW7du7Nmzh61bt+b7ORkQEMDhw4eJj4/H2dkZDw8PRowYwYIFC+jTpw/jx4/Hw8ODy5cvs2bNGhYuXIizszNDhgxh3LhxeHp6UrZsWT777DMLF6qMjAymTJnCa6+9hq+vLwkJCcybN4+bN29apE15kk6dOjFhwgSSkpIEa7CIiAh69uxJ7dq1Ler6+/szYcIEtm3bRteuXXnvvfeYO3cuY8eOZfjw4djb27N582ZWr17N33//LRw3ZcoUoqKiaNasGVOmTKFx48YoFAqio6OZOnUqR48etbnA/7Smv++++y5z585l/PjxvP322+zZs4d169axefNmoc7cuXP5448/2L17NwDt27enYcOGvP3228yaNQuDwcCIESPo0KGDhZbVfJ2aNWtmdZ3MREdH07FjxwL3X6TkIgqqufAsNKlmzA7/d+6YfE2P3Tom7NMZdGQaM1Gg4P79+xiNRoub9sWv1/Hyy4/ain74r9m05nkThJo3b24zxHtpoU6dcpw58x5+fi7P3XcjIiIiIlJ0KBQKPv74YzIyMrKt4+zsXCRCKpgEVZVKRXBwsIWWrU2bNqSlpQlpbMx88803eHt7M3XqVK5cuYK7uzsNGzZk4sSJNtt/6aWXCA8P56uvvmLSpEl06tSJ0aNHM3fu3Hz1c+zYsUKKFpVKxdWrVwkICODAgQN88skndOzYEbVaTaVKlejcubMgjE6fPl0wEXZxcWHMmDGkpKQI7cpkMi5cuMDSpUtJSEjA09OTJk2aEB0dTa1atbLtT506dWjYsCHr1q1j+PDhHD9+nFOnTrFgwQKrum5ubrRr146IiAi6du1KlSpV2Lt3LxMmTKBDhw5oNBqCg4P57bff6Ny5s3Cch4cH//zzD99//z3ffvst165do0yZMtSpU4fp06cXWfDJypUrs3nzZkaPHs3s2bOpUKECCxcutMihmpCQQFxcnPBZKpXy999/CwsTTk5OdOnSxSodUEpKCuvXr2f27Nk2z52VlcXGjRvZtm1bkYxNpHiRGJ+lJFYCSU1Nxc3NjZSUFJuBFFatWkVMTAxGo5Effvih0IWKQ4cOMXPmTJRKJeHh4YK5zIIFC/gk8hN++/c3oe7rQa/jvM8UAOCLL76wCAawfz+Wgmo0tGoFN27cYM6cOUD+w9CLFA4Gg5EVK07Tr1+dAvug5obRaBSSrYuCr0hJQ5yfIiWdZz1Hs7KyuHr1KpUrVy6yCL3PE8OGDePChQtER0fnXrkEs3nzZsaNG8fZs2fzHejSaDQKSgrxPvqIX375hT/++MOmabRI4ZPTvSslJQV3d/dsZaqC8PyFgy0insVNwdnZWYhgd+fOHY7esvTJaFqhqWCyYcv81xYFTRotUjjo9QaGDfuLgQM38s47f2MwFN26kK0AAyUJtVrN3bt3haAHIi8WJX1+ioiIc7Tk8OOPP3Lq1CkuX77MnDlzWLp0qU0z4dJG165deeedd7h582aBjhcFVGsUCoWgkBF5/hAF1TyiUqlyjSZXEMzpY8xRBs3mvxeuXyA+Od6ibhO/JkJEs/v37xd6X0QKF61WT//+f7BoUQwAS5ac4ujRgj2ccsNgMHDmzJkimaOFRXp6OpcuXeLGjRvF3RWRZ0xpmJ8iLzbiHC1ZHDlyhA4dOlCnTh3Cw8P53//+x9ChQ4u7W4XCqFGj8Pf3L9CxovLBmqFDh2Yb4Vrk2VIU90/RR7WYKV++PAEBAYJjva+vL7GxsRyKP2RRz8nOiSCvIGK9Y4mNjc2zRrW05hgt7ajVOt5663f+/PMiAHK5lNWr36RZM+uk4SIiIiIiIiKPWLduXXF3QUREpAQgSjHFjL+/v0U+L3MAgpP3TsJjFh4NfBogl8qF0NtPalTdj+zg/0gG4Ab+gCkkuFQqtcrbJVK0ZGZq6dFjLTt2mIIGKJUy1q/vRdeu1XM5UkREREREREREREQEREE1V4o6ebBerxfybjk6OgqmvxfTL4LLo3pN/JpY9OdJQbXCymnM4jwAf/EaZkFVpVKRnJws+jU8I1JT1bz66iqio68D4Oio4K+/etOuXZVi7pmIiIiIiIiIiIhI6UEUVHPBLDgGBwfnO0JbXjh37hwrVqzAzs6Ob7/9Fh8fHwwYuKW/hctjkmpjv8YAlC1bFj8/P6FfuWErx5pI0ZCYqKJLl5UcOWLyQ3V1VbJlS19eeqlikZ9bKpVSp06dIpmjIiJPizg/RUo64hwVKQ04ODgUdxdERLKlKO6foqCaC+YIpfHx8UXSvsFgwGAwoNfrAShXrhzl6pZDrrb8ahr5NQKgTJkyjBo1qkj6IvJ0jBq1TRBSPTwc2LGjP40a+T2z82s0GjHNgUiJRZyfIiUdcY6KlHTM6WlERF4UxKXDXDBHsMrKyiqSaFaenp5IpVIh35BCoSC4bbDFqlmQVxCuykf5iDIyMoiPj7dIQH3hq7U04CQNOMmnfF/o/RTJnZkzO1Gzpjflyjmxd++gZyqkGgwGLl68KEasFCmRiPNTpKQjzlGR0oBoJSdSkhGj/hYD5uBF5cqVe2bnPHbrmMXnxr6NLT7//vvvnDt3ju7du/PSSy8BoHP34t4z66GILby8HNm1K4z0dA3VqnkWd3dERERERERERERESi2iRjUX7t0ziX937959JuczGo0cvXXUoszsn2omu8i/tlAoFIXXORELYmMfkJJiubrp6+siCqkiIiIiIiJ5JCoqColEQnJycp6PmTx5MvXr1y+yPj1JSEhIobhdPXjwgLJlyxaZO9mLSPPmzVm/fn1xd0OkiBAF1Vx4VgKqmZtpN7mbbnnOJuWbWHwuW7YskDdB1ZyfVaRwOX36Lq1aLeaVV1aRnq4p7u4AIJPJirsLIiLZIs5PkZJOaZ2j169fZ/HixVy/fr1IzxMeHo6Liws6nU4oS09PR6FQEBISYlHXLHzGxcXl2m7Lli25ffs2bm5uhdrfwhIuc+Pdd99FIpEwa9asXOtOmTKF7t27ExAQYLWvU6dOyGQyjh49arUvu7EsWbIEd3d3i7LU1FQ+++wzgoODsbe3x8fHh/bt27NhwwaMRmMeR5V/oqKiaNiwIUqlksDAQJYsWZJj/fj4eCQSidX2zz//CHWWLFlitf9JP/JJkybx6aefimb7zymioJpHHBwcnslDTKVV0bR8UxwUDmj0Gtzt3ansXtmijlmjmpCQkGt7Wq22SPr5InP06E1CQpZw714GBw/e4NNPdxV3l5DJZNSpU6dEv2iVKVOGFi1aULt27eLuisgzpjTMT5EXm9I6R41GI9HR0cTFxREdHV2kgkhoaCjp6ekcO/bIPSk6OhofHx8OHz5s4T8ZGRlJxYoVqVq1aq7t2tnZ4ePjUyqDBP3xxx/8888/+PnlHpMiMzOTiIgIhgwZYrXv+vXrHDx4kJEjR7Jo0SKbx0skEhwdHXO8TsnJybRs2ZJly5YxYcIETpw4wb59+3jrrbcYP368RWyTwuTq1at07dqV0NBQYmJiGDVqFEOHDmX79u25Hrtr1y5u374tbI0aNbLY7+rqarH/2rVrFvu7dOlCWloaW7duLdQxieSforh/ioJqHtHr9UXyAHjw4AH379/nv//+Y0vsFnr91os9V/dwJ/0Oar0anUHH1suWPz6zoJqcnCwIosrb8QRxgSAu4MstoW5SUlKh9/lFZv/+67Rrt4ykJNMDuVmz8nzzTWgx98r0spKamlqkLylPi1QqRS6Xl7oXQZGnpzTMT5EXm5IyR7Varc0tO23RtWvXiI+PR6lUEh8fb/USn9u58kNQUBC+vr5ERUUJZVFRUXTv3p3KlStbaMKioqIIDTU9Gw0GA1OnTqVy5co4ODhQr149fv/9d4u6T5r+LliwAH9/fxwdHenRowczZ8600hwCLF++nICAANzc3OjduzdpaWkADBo0iL179zJ79mxBE2c2tz179ixdunTB2dmZcuXKERYWZrHwn5GRwYABA3B2dsbX15cZM2bYvB43b97kgw8+YOXKlXlys9qyZQtKpZLmzZtb7Vu8eDGvvvoq7733HqtXr0alUlnVMRqNub6LTpw4kfj4eA4fPszAgQOpWbMm1atXZ9iwYcTExODs7JxrPwtCeHg4lStXZsaMGdSoUYORI0fSs2dPfvrpp1yP9fT0xMfHR9ievJYSicRi/5MxY2QyGa+88gpr1qwp1DGJ5J+iuH+Kgmoe0Wg0RWJWoFKpSE9P51/tv/Tf0J9LiZdQypS42LlgL7PnZtpN+m/oz5bYLcIxzs7OODg4YDQaefDgAQDVvh/CHtqyh7Z8wdcW5xCFg8Jh5844OnZcTlqaydS3TZtK7NwZRpkyxZ/XzGAwcOXKFdH0RaREIs5PkZJOSZmj8+bNs7ndunXLqq7RaOTAgQNotVqcnZ3RarUcOHAgzy+L2WnuciI0NJTIyEjhc2RkJCEhIbRp00YoV6lUHD58WBBUp06dyrJlywgPD+fcuXOMHj2a/v37s3fvXpvnOHDgAO+++y4fffQRMTExdOjQgSlTpljVi4uLY+PGjWzatIlNmzaxd+9evv/elPVg9uzZtGjRgmHDhgmaOH9/f5KTk2nbti0NGjTg2LFjbNu2jbt379KrVy+h3XHjxrF3717+/PNPduzYQVRUFCdOnLA4t8FgICwsjHHjxlGrVq08Xbvo6GgrbSGYvsfFixfTv39/goODCQwMtBDkH0etVmfbvsFgYM2aNfTr18+mhtfZ2Rm53HYM1ejoaJydnXPcVq5cme25Dx06RPv27S3KOnXqxKFDh7I9xsxrr71G2bJladWqFX/99ZfV/vT0dCpVqoS/vz/du3fn3LlzVnWaNm1KdHR0rucSKVrEqL/PKUaMnPI8hVqvxtXOFYlEglYLRqMUe1zJ1KQy6q9JODXthEwiAyQYDF6kp98gMvI+/v4+BKTbXnVwcnLC39//WQ/puePvvy/Ss+dvaDSmfLedOlVlw4a3cHQUg1WJiIiIiDx7zNpUBwcHJBIJDg4OglbVlg9kYRAaGsqoUaPQ6XSoVCpOnjxJmzZt0Gq1hIeHAyahRa1WExoailqt5rvvvmPXrl20aNECgCpVqrB//37mz59PmzZtrM4xZ84cunTpwtixYwGoXr06Bw8eZNOmTRb1DAYDS5YswcXFBYCwsDB2797NlClTcHNzw87ODkdHR3x8fIRj5s6dS4MGDfjuu++EskWLFuHv78+lS5fw8/MjIiKCFStW0K5dOwCWLl1KhQoVLM49bdo05HI5H374YZ6v3bVr12wKkLt27SIzM5NOnToB0L9/fyIiIggLC8tz22ByB0tKSiI4ODhfxwE0btyYmJiYHOvklP3izp07VvvLlStHamoqKpXKIuWiGWdnZ2bMmMFLL72EVCpl/fr1vP7662zcuJHXXnsNMGnxFy1aRN26dUlJSeHHH3+kZcuWnDt3zuI78fPz48aNGxgMBqRSUQf3PCEKqsWMXq/H4Gcg3S4dN7kbEomE+wmgEyxyJCB1JFZ1mZB+h+FGSwBq1vTGx+cGW7bc59o12AXUsNG+rZuDSP5Yu/Ys/fv/gU5nWil6/fVg1qx5E6VS/PmIiIiIiBQeI0aMsFn+pFXU49pUR0dHwOTrqVKpOHDgAJUqVcrV5/Ptt9/Od/9CQkLIyMjg6NGjJCUlUb16dby9vWnTpg2DBw8mKyuLqKgoqlSpQsWKFTl37hyZmZl06NDBoh2NRkODBg1snuPixYv06NHDoqxp06ZWgmpAQIAgpAL4+voKmRqy49SpU0RGRto0gY2Li0OlUqHRaGjWrJlQ7uHhQVBQkPD5+PHjzJ49mxMnTuTLr1alUlkFAgKToPzWW28J2s4+ffowbtw44uLi8uTja+ZpzC4dHBwIDAws8PEFwcvLi48//lj43KRJE27dusX06dMFQbVFixbCAgeYAm/VqFGD+fPn88033wjlDg4OGAwG1Gq1+N77nCG+aeeRonLyt7e3R6PQYJQYkUllaLWPC6lG02aQgUQPTo9uwJmZ3gA4Opr8KibxLc6kA3CXcix6eC/MzsxDJG/s2XOVvn03YDCYHgB9+9ZhyZLuKBQlz5Ta1gNQRKSkIM5PkZJOSZijeU0p96Q2Fci3VrUg6esCAwOpUKECkZGRJCUlCRpRPz8//P39OXjwIJGRkbRt2xYwmW0CbN68mfLly1u0pVQq833+nPovkUhyNT1MT0+nW7duTJs2zWqfr68vly9fzvW80dHR3Lt3j4oVKwpler2eMWPGMGvWrGxTz3h5eVnFDUlMTOSPP/5Aq9Xyyy+/WLS3aNEiweTZ1dWV1NRUq3fR5ORkIVqyt7c37u7uXLhwIdcx2BpTly5dcqwzf/58+vXrZ3Ofj4+PVZaMu3fv4urqmi/BsVmzZuzcuTPb/QqFggYNGlh9T4mJiTg5OYlC6nOIKMXkQq1atYiJiaFatWpF4ueZlZWFg8H0w8rSZSE3mlZGJRINMlkmBoMcA0owyiCjrHBcZqYpoJKDg0lQ/YdHK04NGkC9eqb/23LIF8k7rVpV5JVXqrFp0yWGDm1AePiryGQlz6xEJpMVyNxHRORZIM5PkZJOaZqjZm2qWq3Gzs7OIiiSVCpFrVbnWataEEJDQ4mKiiIpKYlx48YJ5a1bt2br1q0cOXKE9957D4CaNWuiVCq5fv26TTNfWwQFBVmlaLGVsiU37Ozs0Ov1FmUNGzZk/fr1BAQE2FzIr1q1KgqFgsOHDwuCaFJSEpcuXRL6HxYWZtMfMywsjMGDB2fbnwYNGrBixQqLspUrV1KhQgU2btxoUb5jxw5mzJjB119/jUwmIygoiB07dlgJYidOnKB69eqA6bvv3bs3y5cv58svv7QyM05PT8fe3t7muJ/W9LdFixZs2bLFomznzp0W2tC8EBMTg6+vb7b79Xo9Z86c4ZVXXrEoP3v2bLYaepFnR1HISaKgmgtmW3edTlcktu8ZGRmU1ZZFapSSok5BTibInTHqTV+Nwk6PRJmJv2MQC1Y2Q/bwmaPRVCcraxxOTmV4fF7Y25uEVPNC4+P5zko7UVFRKBQK6tWrV2SR657Ezk7Gb7/9H4sXn+TddxuX2PD5BoOBpKQkypQpI/pniJQ4xPkpUtIpTXNUr9eTkpKCUqlEo7HO461UKklJSUGv1xeJVVVoaCgjRoxAq9VaCJ9t2rRh5MiRaDQaIZCSi4sLY8eOZfTo0RgMBlq1akVKSgoHDhzA1dWVgQMHWrX/wQcf0Lp1a2bOnEm3bt3Ys2cPW7duzffzNyAggMOHDxMfH4+zszMeHh6MGDGCBQsW0KdPH8aPH4+HhweXL19mzZo1LFy4EGdnZ4YMGcK4cePw9PSkbNmyfPbZZxZzwtPTE09PT4tzKRQKfHx8LEyEn6RTp05MmDBBmGcAERER9OzZ0yptm7+/PxMmTGDbtm107dqV9957j7lz5zJy5EiGDRuGvb09mzdvZvXq1fz999/CcVOmTCEqKopmzZoxZcoUGjdujEKhIDo6mqlTp3L06FGb0ZOf1vT33XffZe7cuYwfP563336bPXv2sG7dOjZv3izUmTt3Ln/88Qe7d+8GTL6/dnZ2goC5YcMGFi1axMKFC4Vjvv76a5o3b05gYCDJyclMnz6da9euMXToUIvzR0dH07FjxwL3X6RwEIMpFQOXLl0C4MqVK0UWtj5RlojEKEGCBB1aUCaD2hWjxIBOqsPVzpVZr31Lm2qPr1Q4PNxeDAwGA2fOnEGv1xdpHk5TJGUVXl6OQpm9vZz33mtSZOcsDIxGIzdu3LD5ABIRKW7E+SlS0ilNc1QulxMWFpajxZSDg0ORuf6EhoaiUqkIDg620LK1adOGtLQ0IY2NmW+++QZvb2+mTp3KlStXcHd3p2HDhkycONFm+y+99BLh4eF89dVXTJo0iU6dOjF69Gjmzp2br36OHTtWSNGiUqm4evUqAQEBHDhwgE8++YSOHTuiVqupVKkSnTt3FoTR6dOnCybCLi4ujBkzplDyj9apU4eGDRuybt06hg8fzvHjxzl16hQLFiywquvm5ka7du2IiIiga9euVKlShb179zJhwgQ6dOiARqMhODiY3377jc6dOwvHeXh48M8///D999/z7bffcu3aNcqUKUOdOnWYPn26YCZc2FSuXJnNmzczevRoZs+eTYUKFVi4cKEQIApMwZ7i4uIsjvvmm2+4du0acrmc4OBg1q5dS8+ePYX9SUlJDBs2jDt37lCmTBkaNWrEwYMHqVmzplDn5s2bHDx40EpbLfLsKQo5SWIs7qRhxUxqaipubm6kpKTg6upqtf/MmTMsX74cV1dXJkyYUOhq7e3btzM2cixX7K6gdFKSqk5DrzeAzh6JRIur1plZr81k0EuDrI5duXIlV69eJSwsjEqVKtls//jx4yxevBilUpltLrCSSlJSEpcvXyYwMBAXFxfOnDlDcnIybdq0KZIVb6PRyLhxO1m37hzR0YOpVMm9QO083m/zqmlRYzaHKY0J60Wef8T5KVLSedZzNCsri6tXr1K5cuUS4Rtb0hk2bBgXLlwo9SlINm/ezLhx4zh79my+32OMRqMQQbekWncVB5988glJSUn8+uuvxd2VF4Kc7l1JSUl4eHhkK1MVBFGjWsykksoVhUlbay+3R6JXknigN1xvQ1mXv2lePosaclvxfE1mw6mpqdy/f9+moGo0Grl9+7Zg/ms0GkvNzc1oNBIfH8+DBw+Qy+XUr1+/SP0PDAYjI0ZsJjz8OADt2y/n9Ol3cXDIX7CJJ/vt7u5eaq65iIiIiIhISeDHH3+kQ4cOODk5sXXrVpYuXcrPP/9c3N16arp27UpsbCw3b94UUwcWEmXLlv1/9u47PoqqXeD4b3bTO0mABAiEkgaEEKp01CBdQJSgQbqAgIgUkVcFRUVEQbDBRbpKEakiohTpSJNQQyCBEAiEQHpPdnfuH3FHlmwqKQuc7/vZe92pZ3YPk33mnPMcg+zBwuNFBKpFOH/+PAB3794tl+Pvit+FzH8BpLWZLRyYCVlOqNzvINU8RGxsrNF9XV1diYiI4N69e9C9O4SF5a3o3Ru+/pqkpCS0Wi2SJKHVaklKSqqwFr6HlZSUREJCAmZmZiQkJJRr2TUaHSNGbGP16jMASBJMm9auxEEqVGy5H3R/mn5BMDWifgqmTtRR03H8+HHmzp1Lamoq9erV46uvvso3LvFRNXHixFLva+rjpyvD5MmTK7sIQjkSgWoR9D2jLS0ty7w7UEp2Crvu7EKlUik3n6DqL7MqywmAnJy8SaoLClSrVs2boubu3buQk5P3AsjNVVr2JElCkiTl/aPQwifLMlevXiU3Nxdra2uys7PLrew5OVoGDdrEhg0XAVCrJVat6ktISJNSlTsqKgqtVouVlRVZWVkV9pmr1eoSzbcmCBVJ1E/B1Ik6alp+/vnnyi6CyZEkSXQTF0xaeQybEI9miik3N7fMs1n9ePZHMnIzlPcqSUXvmq8p73Ny8pIR3L592+j++kD13r17+dbpW/bS0tJISkoiOTlZaeEzdUlJSdy9exedTkdOTg7m5ublUvasLA0vvLBeCVLNzVVs2PBSqYJU+O8zNzc3R5Kkciu3MTqdjtjY2HLJuCYID0vUT8HUiToqmDpZlsn9tyFCEEyRyPpbiTQaTZneHHK1uXz/j2Gmt17evahu9d8E0voW1du3bxsdX+rqmjeX6r1799CFhKBKSABA9vZWWvZyc3PJzs4G8pJFmHqrqr41Vd9lWaPRKPPElWXZ09Jy6NNnHXv3XgPyMvtu3hxMt26lS89+f2uqfhJytVpd5uUu7PyxsbHKwwtBMCWifgqmTtRR4VGQm5tbbtmcBeFhlcdDFFHbK8nmS5u5k3bHYNmYFmNIi/jvfU5OXtr3jIwM0tLS8o2fqVKlihIMJffrp4yFTEpMJOGff5SWPb37W/hMdaxqUlIS9+7dU7osQ95DgrIse3a2hq5df+TIkRsA2NlZsH37y3Tq5PlQ5b6/NRXI16pqqp+5IAiCIAiCIJga0fW3EsiyzOKTiw2WeUgeNHVr+sB2lrzwwhQ+++wzbG1t8x1HrVYrk07ru//qW/Y0Gk2+Fjx9C2VUVJRJdh2RZZlr164pWYpVKhVmZmaoVKoyLbulpRmdOuVlSXZysmLXrlcfKkh98DPXarXKS1/ua9eucePGDbRa7UOVXRAEQRAEQRCeBCJQLYaUlBTu3LnDvn37yuR4+6/v59K9SwBKy6F3srfRyYrr1g2gRo0aBWZ603f/1Wcl1s+zZWZmhlarRa1W4+joSJ06dThx4gRmZmZkZmaWaaAaHR3NihUriI6OfqjjyLJMWlqa8l6tVqNSqbh+/Tp//vknt2/fLrOyf/LJM7z3Xgf++msITz1V66GO9eBn/uDLzMyM5ORkrl27xpkzZ8jKynro8j9IkiScnZ1Ntku38GQT9VMwdaKOCo8CMQ+1YMrK4/4puv4WwcXFhaSkJHQ6Hb/88gtPP/30Q6cHX3RykcF7B40DnrIn+/fvx8PjZaD4N6IHEyqpVCqaN29Obm4uABcvXuTUqVNIksSdO3do3rw51tbWZZbiXJZlDh48SGRkJBYWFrzyyiulrqgqlYoqVaqg0+moWrUqderUQafTceTIEXJycrhx4wbBwcGlKrtWq0Ot/m8/SZL46KNnSlVOY+W+/zM3Jisri4iICNLT0wkNDcXPzw9HR8cyOb++DLVr1y56Q0GoBKJ+CqZO1FHB1EmShKWlZWUXQxAKVB7TJ4kW1SLcvn1b6YoaHx/Pnj17Hup4F+IucPD6QeV9ZmYmTXKbIJF3Azp0KH+ramF8fHzo0qULjRs3VpZZWlpiZ2eHnZ0dR44cUQJHWZbZtm1bmd7orl+/TlRUFJaWlkRFRXH9+vVSH0uj0ZCSkoJaraZOnTrY2dlx4cIFUlNTkSSJ1NRUQkNDS3zciIgE/P0XcfBg6ctWlPs/c2MvV1dXAgMDsbOzIzc3l3PnzhWYzbk0dDod0dHRImOlYJJE/RRM3SNXR2Ni4Ny5gl8xMZVdQqGMybJMdnZ2pQzdev/99xk1alSFn/dxdfHiRWrVqkV6enplF6VMlcf9UwSqhdDpdOzatUt5L8symzZteqgv4lbqLarZVlPem+WY4a3xVrL6Xr16CCj+OMYGDRrQpUsX6i1cCM8/n/f6/HMgL/C7efOmwfZnz55VAu8HabVali1bxrJly5RMwYWRZZnDhw+Tm5urBGCHDx82ehPNyMhQjv3gKzk5Oe+zMDOjRYsWNGjQADs7O3Q6HXv37kWn06FWqw3eF9fFi3fp2HEFYWH36NlzDadO3Sr2vmXN0tKSJk2aULVqVWRZJiIigoiIiDL5hy3LMgkJCSY59lhPq9WSnZ1Njn6+X+GJ8SjUT+HJ9kjV0ZgYaNUK2rcv+NWqVbkEq0OHDkWSJObMmWOwfMuWLU9Mt2n9ZyBJEhYWFjRo0IBZs2YV+NuqLFVGnovY2FgWLlzIu+++m2/d0aNHUavV9OzZM9+6ffv2IUmS0Sn6PD09WbBggcGyv/76ix49euDi4oKNjQ0NGzZk8uTJxJTjQ5esrCzGjRuHi4sLdnZ29O/fnzt37hS5X1hYGM8//zyOjo7Y2trSsmVLZfhbVFSUUj8efG3YsAGAhg0b8tRTTzF//vxyu7bKUB73TxGoFmLPnj1KwCZJEiqV6qFbVbvU78Lx147zZdcvcch1oGF2Q9RyXldfjUaDtbUlvr7Fb1XVarX8/fff3N23D/nkSTh5EiIjAVizZk2+SiPLMmvWrCnweKmpqaSmpharsulbU62trZEkCWtr6wJbVXU6nXLsB1/3B2qWlpa4u+fNH3vs2DGSk5NRq9VIkoRarSY5OZljx44V67MJDY2lU6eV3L6dN+61Th0natZ0KNa+5UWtVuPj44OnpyeQ12J//vz5QrsNPy6SkpI4fvw4YWFhlV0UQRCER1dCAqSlgUoFFhb5XypV3vp/p6wra1ZWVnz22WckJiaWy/EfBd26deP27dtcuXKFyZMn88EHH/D5v40E5aEyH/AuXbqUtm3bUqdOnXzrli1bxhtvvMGBAwe4dav0DQH/93//R1BQEG5ubmzcuJGLFy+yePFikpOTmTdv3sMUv1BvvfUWv/76Kxs2bGD//v3cunWLF154odB9IiMjad++Pb6+vuzbt4+zZ8/y/vvvY2VlBYCHhwe3b982eH344YfY2dnRvXt35TjDhg1j0aJFFfKA41EmAtUC6HQ6Nm3aZLBMpVKVSauqhdqCF/1epH1kewJyApTl+lbVpk2L36qqUqnYsWOHMo5WT6PRcPbsWYNt9U87C2pVlSSJgQMHMnDgQCwsLAo97/2tqfpt9fOdGmtVtba2Vo794OvBaXcAg9bT+6d7KW6r6t9/3+Tpp1dx714GAM2bu7Nv3xDc3OwK3a8iSJKEh4cHDRs2VILv0NDQx64LiCAIglBCMTFw/Hje68QJ49ukpMD9LWtmZmBu/t/LzAxkOa8LsP5Y/yZczCcursRF1AcUn376aaHbHTp0iA4dOmBtbY2HhwcTJkxQ/s7NmjXLYMiSXtOmTXn//feBvJbLvn37Mnv2bKpXr46Tk5PScjl16lScnZ2pVasWK1asMDjGtGnT8Pb2xsbGhnr16vH+++8bPAz+4IMPaNq0KT/88AOenp44OjoycOBAUlNTi/0ZWFpa4ubmRp06dXj99dcJCgpi27ZtAGRnZzNlyhRq1qyJra0trVu3NkjGGR8fz8svv0zNmjWxsbHB39+ftWvXGhy/c+fOjB8/nokTJ+Lq6krXrl2RZZlPPvmEOnXqYGlpSY0aNZgwYYKyT2JiIoMHD6ZKlSrY2NjQvXt3rly5oqxfuXIlTk5O/PHHH/j5+WFnZ6cE3IVZt24dvXv3zrc8LS2N9evX8/rrr9OzZ09WrlxZ7M/vfjdv3mTChAlMmDCB5cuX07lzZzw9PenYsSNLly5lxowZpTpuUZKTk1m2bBnz58/nmWeeoXnz5qxYsYIjR47w999/F7jfu+++S48ePZg7dy6BgYHUr1+f559/nmrV8npLqtVq3NzcDF6bN29mwIAB2Nn99xu0S5cuJCQksH///nK5vseFCFQLsGfPHuLj4wEMAqWyaFUF+PHHH7GytEKXqyMnJ0d5WqbRaHB2Ln6rqiRJuLq6El2jBimtWkFQEPj7G21N1SuoVVWlUin/qIoaEP1ga6q+LAW1qhr7h6t/GZu8+sHWVP3xi9Oqum9fFF26/EBSUl523bZtPdizZzAuLjaFXlNFc3FxISAgACsrKzQazUNl85MkCTc3tyem65XwaBH1UzB1JlNH162Dvn3zXi++aHybCxcgIwOSk/NaTY11B5VlePPN/461d6/xY903vKm41Go1s2fP5uuvv843vEgvMjKSbt260b9/f86ePcv69es5dOgQ48ePB2D48OGEhYVx4r5g/PTp05w9e5Zhw4Ypy/bu3cutW7c4cOAA8+fPZ+bMmfTq1YsqVapw7NgxxowZw+jRow3KYW9vz8qVK7l48SILFy7k+++/58svv8xXvi1btrB9+3a2b9/O/v3783VnLglra2vld9z48eM5evQo69at4+zZs7z00kt069ZNCRqzsrJo3rw5v/32G+fPn2fUqFG8+uqrHD9+3OCYq1atwsLCgsOHD7N48WI2btzIN998w+LFi7ly5QpbtmzB399f2X7o0KGcPHmSbdu2cfToUWRZpkePHgZBekZGBl988QU//PADBw4cIDo6milTphR4XQkJCVy8eJEWLVrkW/fzzz/j6+uLj48PgwYNYvny5aXq+rlhwwZycnJ4++23ja53cnIqcN/u3bsXmh+kUaNGBe576tQpcnNzCQoKUpb5+vpSu3Ztjh49anQfnU7Hb7/9hre3N127dqVatWq0bt2aLVu2FHqe0NBQRowYYbDcwsKCpk2bcvDgwQL2fPSUx/1TBKpG6FtT9a12siwjy7LSuvfg+pLSarUcOnQItVqttKLqX5D3R6AkrapVq1bljw4dODNpEqxejWb06Hytqfrr0CtsrGpR9K2p2dnZqFQqcnNzlZdKpSI7O7vAsarF8WCrqU6nU17G1t9v584Iunf/ibS0vD8Yzz5blz//HISjo1WpylLebG1tadq0KY0bN1a6jZSG/iFDeWRcE4SHJeqnYOpEHS2Zfv360bRpU2bOnGl0/aeffkpISAgTJ07Ey8uLtm3b8tVXX7F69WqysrKoVasWXbt2NWgNXbFiBZ06daJevXrKMmdnZ7766it8fHwYPnw4Pj4+ZGRk8L///Q8vLy+mT5+OhYUFhw4dUvZ57733aNu2LZ6envTu3ZspU6bw888/G5RPp9OxcuVKGjduTIcOHXj11VdL1QAhyzK7d+/mjz/+4JlnnlGm69uwYQMdOnSgfv36TJkyhfbt2yvXWrNmTaZMmULTpk2pV68eb7zxBt26dctXRi8vL+bOnYuPjw8+Pj7cuHEDNzc3unTpQu3atWnVqhWvvfYaAFeuXGHbtm0sXbqUDh06EBAQwE8//URMTIxBEJWbm8vixYtp0aIFzZo1Y/z48YVed3R0NLIsU6NGjXzrli1bxqBBg4C8rtDJycmlah28cuUKDg4OyrCvkli6dCmhoaEFvnbs2FHgvrGxsVhYWOQLhKtXr05sbKzRfeLi4khLS2POnDl069aNP//8k379+vHCCy8UeO3Lli3Dz8+Ptm3b5ltXo0aNh0pCamrK4/4ppqcxIjs7m/T0dFQqlUEwpP9vlUpFeno62dnZWFtbl/j46enpSJKkzHOq71Ksp9VqsbGRMDNLB4oeU/ngXKr3z0VamLS0tEKfVBVEq9WSnJyMpaWl0XETlpaWJCcnK3OIllROTg6ZmZn5Phc9lUpFZmYmOTk5BsHd5s1hBAf/Qm5u3vfUs6cXv/wyACsr067m5ubmmJubP9QxtFotUVFReHp6innWBJMj6qdg6kQdLbnPPvuMZ555xmiL3JkzZzh79iw//fSTskz/wP/atWv4+fnx2muvMXz4cObPn49KpWLNmjX5Wj4bNWpk8OO3evXqBl2G1Wo1Li4uxN3XhXn9+vV89dVXREZGkpaWhkajwcHB8LeUp6enwbAjd3d3g2MUZfv27UoSSZ1OxyuvvMIHH3zAvn370Gq1eHt7G2yfnZ2Ni4sLkFfXZs+ezc8//0xMTAw5OTlkZ2djY2PY66t58+YG71988UW+/PJL6tWrR7du3ejRowe9e/fGzMyMsLAwzMzMaN26tbK9i4sLPj4+BnkhbGxsqF+/frGvOzMzEyDfg/Tw8HCOHz/O5s2bgbxkmMHBwSxbtozOnTsX9fEZ0DfYlEbNmjVLtV9p6eOAPn368NZbbwF53dWPHDnC4sWL6dSpk8H2mZmZrFmzRunO/iBra2syMjLKt9AVqDySfZn2L/hKYm1tzUcffUR8fDzLli0D8oKnUaNGKX/AqlatWuwgNTQ2FGdrZ2o75s3R5uDgwPjx47l16xb//PMPZ8+eRZZlhg0bRkQEfPYZpKfXRKPJu7Heu3ePv//+G7VabTAQW+/BuVSdnJwICQnh1KlTZGZmcuXKFaytrQ1uHrVq1SpVkAp5N6RXX31VuYEZY21tXaogFfJuiOPGjTOaKU6vSpUq+W6c2dlaNJq8m8hLLzXkxx9fwMLiyfnBUZLxNYJQ0UT9FEydSdTRgQOhQ4e8/y7ox3ujRmBj8994VGOBtSTBwoXQoEHe+7p1jR+rS5dSF7Vjx4507dqV6dOnM3ToUIN1aWlpjB492mAMpZ5+vtrevXtjaWnJ5s2blRwXLz7Q3fnBh7iSJBldpg8gjh49SkhICB9++CFdu3bF0dGRdevW5UvIU9gxiuPpp59m0aJFWFhYUKNGDeX3TlpaGmq1mlOnTuV74KEfn/j555+zcOFCFixYgL+/P7a2tkycODHfg39bW1uD9x4eHoSGhnL48GF2797N2LFj+fzzz0vUimnsugvr/aZvCElMTFR+a0JeK6FGozFoaZVlGUtLS7755hscHR2VhwPJycn5fm8mJSUpc8l7e3uTnJzM7du3S9yq2r1790K7ztapU4cLFy4YXefm5kZOTg5JSUkG5btz5w5ubm5G93F1dcXMzIyGDRsaLPfz8zNo1df75ZdfyMjIYPDgwUaPl5CQYPDgQMhPBKoFqFOnDnXq1GHdunVAXites2bNSvykVZZlpvw5hUv3LtHLuxejm48m0D2QJk2a0KRJExwdHTl9+jSWlpZ069aNQ4fgwSEf9+7dY/369bi6uhYaqN69L1lCYGAgGo2GpKQk5WmasX1Ly97e3mgSpLLi7u5e4hvWwIGNycjI5eDBaL7/vjdmZqILlyAIgvAIqVkz71UYBwfD4PTBYTwaTV6g6u+f9ypMtWqFry/CnDlzaNq0KT4+PgbLmzVrxsWLF2mgD5SNMDMzY8iQIaxYsQILCwsGDhxYql5q9zty5Ah16tQxmEqlPLpW2traGr22wMBAtFotcXFxdNA/cHjA4cOH6dOnj9JtVqfTcfny5XzBjzHW1tb07t2b559/nnHjxuHr68u5c+fw8/NDo9Fw7NgxpYtpfHw84eHhxTpuQerXr4+DgwMXL15UWok1Gg2rV69m3rx5PPfccwbb9+3bl7Vr1zJmzBi8vLxQqVScOnXKIGPw1atXSU5OVo734osv8s477zB37tx8LepAvkDyfkuXLi200aSw3mrNmzfH3NycPXv20L9/fyCvpTg6Opo2bdoY3cfCwoKWLVsSHh5usPzy5csFZkV+/vnnDYL8+50/fz7fwxnBkAhUy9mh6ENcvHsRgG3h29gWvo0NL22gXe12xT6GPmCLj48nNzc33z88/ROvtLQ0MjMzH/pG/ygbPjyQYcOaVmpCjMTERLKysrC3tzfI8CYIgiAID83ZGezs8qagKWjaEju7vO3Kmb+/PyEhIXz11VcGy6dNm8ZTTz3F+PHjGTlyJLa2tly8eJFdu3bxzTffKNuNHDkSPz8/IC+Ae1heXl5ER0ezbt06WrZsyW+//aZ0T60I3t7ehISEMHjwYObNm0dgYCB3795lz549NGnShJ49e+Ll5cUvv/zCkSNHqFKlCvPnz+fOnTtFBpQrV64kKyuL9u3bY2try48//oi1tTV16tTBxcWFPn368Nprr/F///d/2Nvb884771CzZk369OlT6utRqVQEBQVx6NAh+vbtC+R1e05MTGTEiBFKq6he//79WbZsGWPGjMHe3p6RI0cyefJkzMzM8Pf358aNG0rd0AfUHh4efPnll4wfP56UlBQGDx6Mp6cnN2/eZPXq1djZ2RU4Rc3DdP11dHRkxIgRTJo0CWdnZxwcHHjjjTdo06YNTz31lLKdr68vn376Kf369QNg6tSpBAcH07FjR55++ml27tzJr7/+apDZGSAiIoIDBw4UOE42KiqKmJgYg2ROQn6iyamYzM3NSxX8LDq5yOB93Sp1earWf/8AbG1tMTMzyzc24X4ODg5YW1sjy7LRiYgtLS15OiyM5w4dIuedd2D9+hKX81E0e/ZBvv/+VL7llZ218c6dO0RERJCcnFxh59RPeVPZ1y4Ixoj6KZi6R6qO1qyZN+XMoUMFv44fL7pltozMmjUrX7fZJk2asH//fi5fvkyHDh0IDAxkxowZ+ZLy6BMt+fr6GoyvLK3nn3+et956i/HjxytjBwsaH1heVqxYweDBg5k8eTI+Pj707duXEydOKF2e33vvPZo1a0bXrl3p3Lkzbm5uShBYGCcnJ1auXEn79u1p0qQJu3fv5tdff1XGvq5YsYLmzZvTq1cv2rRpgyzL7Nix46FzYIwcOZJ169Yp3/GyZcsICgrKF6RCXqB68uRJJaHnwoULGTJkCNOmTaNRo0YMHTqUJk2a8Ouvvxr8Wxs7dix//vknMTEx9OvXD19fX0aOHImDg0OhWYkf1pdffkmvXr3o378/HTt2xM3NLd/UlOHh4Qa/5/r168fixYuZO3cu/v7+LF26lI0bN9K+fXuD/ZYvX06tWrXytTrrrV27lueee85oS+yjqjzun5Jc2tSsj4mUlBQcHR1JTk7ON9geMEiXPXfu3BIdO+xuGM+uftZg2ZygOQwO+K+v+tWrV5k5cyaurq58+eWXHDr03/AUgIMHoX17mDFjBteuXWPChAm0bNky37lu+vricPMmllZWmPXrh/r77zlx4gRJSUls3rwZe3t7PvvssxKV31TJssy77+7l008PIUnwww/9CAlpUtnFUly6dIm7d+9Sr169Ch/ob8ri4+O5ePEiDg4OBAQEFL2DIAjCYyorK4tr165Rt27dh8r4/qiTZRkvLy/Gjh3LpEmTKrs4ghGyLNO6dWveeustXn755couzmMhJycHLy8v1qxZQ7t2xe9haQoKu3cVFVOVhmhRLaasrKwSZbPS6rTM3DeTLE0WOdocZFnG2dqZAY0GGGx3584d7Ozsijy2vvtvvpTZMTFw7hw2Oh1mgDYri2unThG7axfWERFY/ptg6XEhyzITJ+7k008P/fsebt8uXpbjx5lWq+XSpUvlknGtrNjb29OwYUM8PT0ruyhCBXsU6qfwZBN1tOLdvXuXb775htjYWIO5UwXjZFkmMzOz1FP/lZYkSSxZsqTUUxoK+UVHR/O///3vkQtSiyKy/laiktwYdlzZwdu73lbGpgKoVWp6evXEyszw6UNubi6WlpZFNpcbDVRjYqBVK0hLwzkjA1mnQ5Zl6p49i6p/f1x1OjRWVvzeowe6ckx8VFG0Wh1jxmxn6dLTyrJvvunOuHGtKrFUpiMrK6uyi1AoCwsLpYuS8OQx9fopCKKOVqxq1arh6urKkiVLqFKlSmUXB8gLIAobK3rx4kWlC29lqKxOkE2bNqVp06aVcu7HUYMGDQpNNCb8RwSqZWzHlR0M2jSItBzDVj6tTsvGSxt5oeEL9PDqoSyvUaMG5ubmRWbQ1afKvnXr1n8LExLykimoVODkRHZODjqNBjNzc9Tm5pCTg1lmJrbZ2ZhA0v2HkpurZejQraxZcw4AlUpi2bLnGTq0aeUWTBAEQRCEEjPFkWc1atQgNDS00PWCIFQcEaiWIa1Oy7t73iVLk4VWpzVoJbVWW5OjzeG9ve/RtX5X1Kq81PL61tSiBrvrW1SNJVPCzAyNJJGt1YIkYWVtDZKEpNOh02hITk4m5saNsrvQCpadrWHgwI1s2XIJADMzFT/+2I/g4MZF7CkIgiAIglA8ZmZmoqVLEEyIGKNaTBYWFqhUhX9cx2KOEZkYibnaHB7oyWtraYuNmQ0RCREcizlW4vNXr14dyJuQPC0t/5hMjZF+4WqVCrVKhU6ne2THFmRk5NKnzzolSLWwULNp0wCCgxsjyzKJiYncvn27kktZ+VQqFfXq1SuyjgpCZRD1UzB1oo4KjwJLS8vKLoIgFKg87p+iRbWY1Gp1keNI49Lj0MpaVLLhF2WhtkAtqZFUElqNlrj0uP/2iYvj9u3bJCUlFXpsKysrPDw8sLS0JCMjI9/8nCqVCkmSkGUZnU6H6r7JwNVqNer7Jwd/hFy9msjRozcBsLExZ+vWgQQF1QPysoudP38etVpNtWrVHtlrLCtxcXn1qqwyrQlCWZEkSdRLwaSJOiqYOkmSnvjfOYJpK4/pacSjw2LKzMwsMptVNdtqqCU12dpsg+UWKgsgr2uwWlJTzbaasu727dtkZWUVGagCzJ49m5kzZ1KtWjXDFTodFubmecE0GMxnpr+xPew8WpWlceNq7NjxCu7udvzxxyAlSIX/5pfVarVKkPakunXrFlevXuXMmTP5M0MLQiXTarWcO3dOZFQVTJaoo4Kpk2WZjIwMkxzbKwhQPll/RaBahlrXbE3dKnXR6DQGNxJztXneDUaTQQPnBrSu+d+k1hkZGQ93Uo0GEhMhKQnzfwPU+wNV+d/3j2rXX4B27WoTGTmB9u0NM+1JkqSM3b19+/YTffOuWrUqFhYWyLLMlStXiIyMfKI/D8H0iABAMHWijgqCIJgWEaiWIbVKzfCmwwGQkZFlWQkWUnJSsFRb8vEzHyuJlMpETk7e/8/KwjwtDbVWi5yTA7m5eUGsLKPRaEwiUE1NTSUsLKzQ1uNbt1KZPftgviDL2tp4i3C1atVQqVSkp6eTmmoauY19fHxo3759hWYHVKvVODo6Kmnzb926xfnz58nNza2wMgiCIAiCIAhCWRGBahmzUFvgZOWEmSpv+K8kSeRoc/Bx8eHHF340mJrmoaWlwX2tp5iZodbpUOXm5gWwOh2yrS1pFhZld86HcPv2be7du2c8czEQFZVEhw4rePfdvUybtrtYLYLm5ua4uroCmEyXV0mSlFdFn9fDwwM/Pz9UKhVJSUmEhoaSnp5eoeUQBEEQKoZWp+XIjSNsubSFIzeOoNWJVmFjoqKikCSp0Kln9u3bhyRJxRqKVZmGDRtG3759K+x8xfnsylJxv4c9e/bg5+cnekKUoaeeeoqNGzdWdjEMiEC1mKysrIqVzcq/uj/DA4fTtlZbnK2d6ePdh52DdnLitRNlG6QCtGkDf/4JwcFQrRoZK1fy3Suv8F1ICJp9++DQIeK2byfB2rpsz1sKubm53L17F/hvqp37Xb4cT8eOK7h6NRGAX365SFJS8SZf1x/v7t27T2wLokqlwsfHB5VKhaurKwEBAVhaWpKVlcWZM2eIj4+v7CIKT7D766cgmKJHsY7uuLKDFkta0O3HboRsCqHbj91osaQFO67sKLdzDh061OBhrP7VrVu3cjvnk8pYgGhlZcWCBQtYuXJlpZXLVLz99tu89957+RJMZWZm4uzsjKurK9nZ2fn2kySJLVu25Fs+dOjQfA8AIiIiGDZsGLVq1cLS0pK6devy8ssvc/LkybK8lHy+/fZbPD09sbKyonXr1hw/frzIfZKSkhg3bhzu7u5YWlri7e3Njh3/3Qu0Wi3vv/8+devWxdramvr16/PRRx8ZNAq99957vPPOOwZDCEtCZP2tRMVtHWtfuz3ta7cHIDkrmSxNFtXtqhe4vbOzM1ZWVqUv2LPP5r3i47Fxdibh4kVycnJIrFWLqlWroouJKf2xy1BcXBw6nQ5bW1vs7e0N1p0/H0dQ0Gru3Mlr+fP1dWX37lepUqV4Aba9vT22trakp6cTFxdHzZo1y7z8jwKL+1rO7ezsaNq0KZcuXSI5OZmLFy/i5eWFm5tbJZZQeJJZmEjPDkEoiCnU0fiM4j1U3HV1F6//9jrZmmyszayxVlmjlbVcTrjMoE2D8vXgSshMMNpLycXGpcRl7NatGytWrDBYJqZNqRiSJOHo6FjhPbbKQ05OTqn/zR06dIjIyEj69++fb93GjRtp1KgRsiyzZcsWgoODS3WOkydP8uyzz9K4cWP+7//+D19fX1JTU9m6dSuTJ09m//79pTpuUdavX8+kSZNYvHgxrVu3ZsGCBXTt2pXw8PD8yVT/lZOTQ5cuXahWrRq//PILNWvW5Pr16zg5OSnbfPbZZyxatIhVq1bRqFEjTp48ybBhw3B0dGTChAkAdO/enZEjR/L777/Ts2fPcrm+knp0Hh1WsszMzBI/YXC0ciw0SAW4evVq2XQTdXFBkiSlG6y+9dIUyLKszHXq5uZmcK2nTt2iU6eVSpDapEl19u8fSs2axZ8mQJIkJQB7UpMq6XQ6zp07Z1BHLSwsaNy4MW5ubpiZmeHo6FiJJRSeZMbqpyCYElOpo/6L/It8Nf6uMa9ufpWkrCSyNFkkZSdxL/MeGp0GBwsHsrXZvLf3PYNuwB1XdDR6rNKwtLTEzc3N4FWlShVlvSRJLF26lH79+mFjY4OXlxfbtm1T1icmJhISEkLVqlWxtrbGy8vLIPC9ceMGAwYMwMnJCWdnZ/r06UNUVJSyXt/yNXv2bKpXr46TkxOzZs1Co9EwdepUnJ2dqVWrVr5gGuDSpUu0bdsWKysrGjduXGSwcejQITp06IC1tTUeHh5MmDChWMNp/ve//9G6det8ywMCApg1axaQV+dmzZqltNY1bdqUnTt3KtvWrVsXgMDAQCRJ4umnnyYzMzNf19/OnTszYcIE3n77bZydnXFzc+ODDz7Id93t27fHysqKhg0bsnv37gJbFgty9epVnn76aWxsbAgICODo0aPKuvj4eF5++WVq1qyJjY0N/v7+rF271mD/zp07M378eCZOnIirqytdu3YFYMeOHXh7e2Ntbc3TTz9t8F0XZN26dXTp0sVoQ8+yZcsYNGgQgwYNYtmyZcW+vvvJsszQoUPx8vLi4MGD9OzZk/r169O0aVNmzpzJ1q1bS3Xc4pg/fz6vvfYaw4YNo2HDhixevBgbGxuWL19e4D7Lly8nISGBLVu20K5dOzw9PenUqRMBAQHKNkeOHKFPnz707NkTT09PXnzxRZ577jmD1lq1Wk2PHj1Yt25dqcpeHvdPEahWsqSkJBwdHfO1MpZW//79mTRpEl5eXkDeHxRHR8dKnR8uJSWFzMxMZb5TvcOHo3nmmdUkJGQC0KpVTf76awjVqtmW+BzVq1dHrVaTmZlJSkpKmZX9UadSqfDy8qJZs2ZYm0AXcEEQBOHh5Opy0eq0SOR/yC1JEjZmNkQkRHAs5lgllRA+/PBDBgwYwNmzZ+nRowchISEkJCQA8P7773Px4kV+//13wsLCWLRokfKQPTc3l65du2Jvb8/Bgwc5fPgwdnZ2dOvWjRx98khg79693Lp1iwMHDjB//nxmzpxJr169qFKlCseOHWPMmDGMHj2amzdvGpRr6tSpTJ48mdOnT9OmTRt69+5d4NCYyMhIunXrRv/+/Tl79izr16/n0KFDjB8/vsjrDwkJ4fjx40RGRirLLly4wNmzZ3nllVcAWLhwIfPmzeOLL77g7NmzdO3aleeff54rV64AKAHE7t27uX37dqFjB1etWoWtrS3Hjh1j7ty5zJo1i127dgF5XT779u2LjY0Nx44dY8mSJbz77rtFXsOD3n33XaZMmUJoaCje3t68/PLLSqLOrKwsmjdvzm+//cb58+cZNWoUr776ar4uq6tWrcLCwoLDhw+zePFibty4wQsvvEDv3r0JDQ1l5MiRvPPOO0WW5eDBg7Ro0SLf8sjISI4ePcqAAQMYMGAABw8e5Pr16yW+1tDQUC5cuMDkyZONdme9v6XyQbNnz8bOzq7QV3R0tNF9c3JyOHXqFEFBQcoylUpFUFCQwYOBB23bto02bdowbtw4qlevTuPGjZk9e7bB+N22bduyZ88eLl++DMCZM2c4dOgQ3bt3NzhWq1atOHjwYIHnqmgiUK1kZd365+HhgZubmzJvqqWlJc7Ozri4lLx7T1nRt6ZWrVoVM7O83uZ79lzlued+JCUlb/xAx4512LXrVZydSxdMqdVq6tSpg6+vb5kF/Y8T0S1LEATh8aCTC2+1UKvUaGUtcenlM7/49u3b8/3wnj17tsE2Q4cO5eWXX6ZBgwbMnj2btLQ0JWiJjo4mMDCQFi1a4OnpSVBQEL179wbyuj3qdDqWLl2Kv78/fn5+rFixgujoaPbt26cc39nZma+++gofHx+GDx+Oj48PGRkZ/O9//8PLy4vp06djYWHBoUOHDMo1fvx4+vfvj5+fH4sWLcLR0bHAVrdPP/2UkJAQJk6ciJeXF23btuWrr75i9erVZGUVnkOjUaNGBAQEsGbNGmXZTz/9ROvWrWnQoAEAX3zxBdOmTWPgwIH4+Pjw2Wef0bRpUxYsWADk/WYCcHFxwc3NDWdn5wLP16RJE2bOnImXlxeDBw+mRYsW7NmzB4Bdu3YRGRnJ6tWrCQgIoH379nzyySeFlt+YKVOm0LNnT7y9vfnwww+5fv06ERERANSsWZMpU6bQtGlT6tWrxxtvvEG3bt34+eefDY7h5eXF3Llz8fHxwcfHh0WLFlG/fn3mzZuHj48PISEhDB06tMiyXL9+3ejMCsuXL6d79+5UqVIFZ2dnunbtarRlvSj6hwW+vr4l3nfMmDGEhoYW+ipoVoh79+6h1WqpXt2wN2b16tULTRh69epVfvnlF7RaLTt27OD9999n3rx5fPzxx8o277zzDgMHDsTX1xdzc3MCAwOZOHEiISEhBseqUaMGN27cqPTeJXpijGolc3BwIDY2tvhdf3Nz4aWXkHv1Qnr5ZbA1bH28ceMGf/zxBzY2NspTu8qUk5PDvXv3gP+SHmm1Ot566w8yMvISHz33XH02bw7Gxsb4FDTF9aSOTRUEQRCeHCqp8DYGrU6LWlJTzdb4eLaH9fTTT7No0SKDZQ8GUU2aNFH+29bWFgcHB+Li8gLn119/nf79+/PPP//w3HPP0bdvX9q2bQvktfJERETke+CclZVl0DrZqFEjg5YufSuSnlqtxsXFRTmnXps2bZT/NjMzo0WLFoSFhRm9zjNnznD27Fl++uknZZksy+h0Oq5du4afn5/R/fRCQkJYvnw577//PrIss3btWiZNmgTk9TS7desW7dq1M9inXbt2nDlzptDjGnP/5w15v7f01x4eHq40Yui1atXqoc6h/z0XFxeHr68vWq2W2bNn8/PPPxMTE0NOTg7Z2dnY2NgYHKN58+YG78PCwvJ1kb7/OypIZmZmvm6/Wq2WVatWsXDhQmXZoEGDmDJlCjNmzChRop+HaURydnYu9KFCedDpdFSrVo0lS5agVqtp3rw5MTExfP7558ycOROAn3/+mZ9++ok1a9bQqFEjQkNDmThxIjVq1GDIkCHKsaytrdHpdGRnZ5tETzwRqBaTtbV1oZU8V5uLRqfB2rxkX2pJ59qMX74cs927kXbvxuGLL2DVKnjgH/nly5crtavv/e7cuYMsy9jb22NnZweAWq1i+/ZX6NBhBYGBbqxf/yKWlqIqPgyVSoW/v/8jlbFSeHKI+imYOlOpo+deP1fkNlqdlmd/eJbIhEjsLeyVB92SJCHLMhmaDHxcfGhd87/fBgeGHSizHly2trZKq2BB9L269CRJUlpounfvzvXr19mxYwe7du3i2WefZdy4cXzxxRekpaXRvHlzg+BQT9/CWNDxCztnaaSlpTF69Ggl0cz99HOWF+bll19m2rRp/PPPP2RmZnLjxo1SJ/bRKyhwKOtrL+oc+jqnP8fnn3/OwoULWbBgAf7+/tja2jJx4kSD7tqQV3fKgqurK4mJiQbL/vjjD2JiYvJ9xlqtlj179tClSxcgLwFncnJyvmPqh+IBeHt7A3ljewMDA0tUttmzZ+frYfCgixcvGq1Drq6uqNXqfNM43rlzp9BkmO7u7pibmxtkQPbz8yM2NlZJWjV16lSlVRXA39+f69ev8+mnnxoEqgkJCdja2pYqSBVZfytRUTf4v2/+TcimEPyr+9PCvQVP1XqK7l7dC90HUMZsFivzmSzjsGYNaf/eGGRZRmrY0GAT/TiPlJQUo2m5K9qDral6tWs7cvjwcKpXt8XcXG1sV6GEcnJyHi6DtCCUI1E/BVNnCnW0uFl4Pwv6jEGbBpGWm4aNmQ1qlRqNVkOGJgNLtSUfP/MxatV/f1udrSu2hacoVatWZciQIQwZMoQOHTowdepUvvjiC5o1a8b69eupVq1auTxw//vvv+nYsSMAGo2GU6dOFTjmtFmzZly8eLHIoLwgtWrVolOnTvz0009kZmYqWVkhrzddjRo1OHz4MJ06dVL2OXz4sNLaqf9deP84w9I8bPDx8eHGjRvcuXNH6VJ64sSJUl1TQQ4fPkyfPn0YNGgQkBfAXr58mYYP/EZ9kJ+fn0GiLcj7jooSGBjIxYsXDZYtW7aMgQMH5ht/+8knn7Bs2TIlUPXx8eHUqVMGwZlWq+XMmTOMHDkSgKZNm9KwYUPmzZtHcHBwvgAsKSmpwHGqY8aMYcCAAYWWv6BGKgsLC5o3b86ePXuUhFk6nY49e/YUOja6Xbt2rFmzBp1Op5T18uXLuLu7K/UoIyMj33Wo1ep8DzTOnz9f4uC8PInH28WUlZVV6NOpE7dOoNFpOH37NN//8z3fnfyu7Auh0WDWrx9p/z7lSH3+eXige4y1tbXScnn37l3s7OyYM2cOn376admXpxgCAgLw8fHh6NF4MjMN5zitVctBBKllRKfTER4ebjJjCgThfqJ+CqbuUaujPbx68OMLP+Lt7E22NpvUnFSytdn4uPjkm5qmrGVnZxMbG2vw0j+ULo4ZM2awdetWIiIiuHDhAtu3b1e60YaEhODq6kqfPn04ePAg165dY9++fUyYMCFfYqTS+Pbbb9m8eTOXLl1i3LhxJCYmMnz4cKPbTps2jSNHjjB+/HhCQ0O5cuUKW7duLVYyJb2QkBDWrVvHhg0b8o0FnDp1Kp999hnr168nPDycd955h9DQUN58800gryHD2tqanTt3cufOHZKTk4scG2tMly5dqF+/PkOGDOHs2bMcPnyY9957Dyj+1ItF8fLyYteuXRw5coSwsDBGjx6dr1XQmDFjxnDlyhWmTp1KeHg4a9asKdYcsV27djUYf3z37l1+/fVXhgwZQuPGjQ1egwcPZsuWLUoyr0mTJrF06VK+++47rly5QmhoKKNGjSIxMVEJVCVJYsWKFVy+fJkOHTqwY8cOrl69ytmzZ/nkk0/o06dPgWVzdnamQYMGhb70+VqMmTRpEt9//z2rVq0iLCyM119/nfT0dIYNG6ZsM3jwYKZPn668f/3110lISODNN9/k8uXL/Pbbb8yePZtx48Yp2/Tu3ZtPPvmE3377jaioKDZv3sz8+fPp16+fwfkPHjzIc889V+R3YIzI+mvCTt4ynPy3hXv+bGQPzdwcaeJElo0axZpmzYh69lmjm+m7x9y7dw9JklCpVJXWnUmlUvHDDxH067eBF1/cQE6OtuidBEEQBEEoVA+vHpwcdZKdg3by0ws/sXPQTk68dqJcg1SAnTt34u7ubvBq3759sfe3sLBg+vTpNGnShI4dO6JWq5XpMGxsbDhw4AC1a9fmhRdewM/PjxEjRpCVlVUmLaxz5sxhzpw5BAQEcOjQIbZt26b0RHtQkyZN2L9/vxKsBAYGMmPGjBIN2XrxxReJj48nIyPDYEoZgAkTJjBp0iQmT56Mv78/O3fuZNu2bcqsDWZmZnz11Vf83//9HzVq1Mi3f3Gp1Wq2bNlCWloaLVu2ZOTIkUqrY1n1IHjvvfdo1qwZXbt2pXPnzri5uRWrvLVr12bjxo1s2bKFgIAAFi9eXGS3Wch7AHDhwgXCw8MBWL16Nba2tjxr5Hfxs88+i7W1NT/++COQ1yV76dKlLF++nObNm9OtWzdiY2M5cOCAQRKjVq1acfLkSRo0aMBrr72Gn58fzz//PBcuXFASXpWH4OBgvvjiC2bMmEHTpk0JDQ1l586dBmWLjo5WEpVCXiLVP/74gxMnTtCkSRMmTJjAm2++aZBB+euvv+bFF19k7Nix+Pn5MWXKFEaPHs1HH32kbBMTE8ORI0cMguLKJslP4qST90lJScHR0ZHk5GSjN8G3334byBu4vWDBAoP+33o6WYfft36kZqcqy77v/T09vYueLPfo0aPMnz8fS0tLfvzxRw4dgg4d/lt/8CA8eP//9ttv+fvvvwkODqZXr175jrlhwwZOnDhBly5daNOmDfPnzwfynmJWJFmWmTVrPx988N88ZT/99AKvvFK6uduEgmm1Ws6dO4e/v7/ROioIlUnUT8HUVXQdzcrK4tq1a9StW7fSuxsLjwZZlsnMzMTa2vqhW0IPHz5M+/btiYiIoH79+mVUwoo1depUUlJS+L//+7/KLspjY9q0aSQmJrJkyZICtyns3pWYmIizs3OBMVVpiDGqZeBy/GWDIBWgRY1yaFH9l368Z0Gpqu9vUZVlmbS0tDLr3lFcsiwzbdpuPv/8iLLs44+fFkFqOTL1ACAxMZGIiAjs7OyKzJYoPH5MvX4KgqijwuNq8+bN2NnZ4eXlRUREBG+++Sbt2rV7ZINUyJvX9bvvvjMYlyk8nGrVqimZqU2F+GaLydrausA/Yg92+/Vw9KC6XXWj2z7I0dFRmSOruPTbFhWo3r17t9jHLEs6ncz48TsMgtQvv+zKu+92rJTyPAnUarXJt1bpdDqysrLyZQEUHn+PQv0UnmyijgolcfDgwXxzyd7/Kg+SJGFjY1OqhofU1FTGjRuHr68vQ4cOpWXLlmzduhXIy1Jb0HV07150UtDK4uTkxP/+9z8RpJahyZMn55vDtSTK4/4pWlSLSavV5mXZNXKDeJjxqVZWVtSrV6/wNNC//w4+PlCvHvBfi+r9/dPvpx9vERsbS0xMDBqNptCB22VJo9ExcuQ2Vq3KmwdMkmDx4l6MGtW8iD2FhyHLMqmpqdjb21d467kgFEXUT8HUiToqlESLFi0IDQ2t0HPq53BVqVQlrqODBw9m8ODBRtcVlqXWFObRFB4d5TGaVASqxZSTk4NOpzP6tODELcM03yXp9mtjY4OFhUW+SZH11BmpZL/+OmZZWai6d0eaOBG3f+d3SklJISMjI9++zs7OqFQqMjIyOHPmDNevX6+Qp8S5uVoGDdrMzz9fyCu7WmLlyr4MGtSkiD2Fh6XT6bh69Wq5tggU9KBGEIpSEfVTEB6GqKNCSVhbW5d62pqHkZ2dXebBo7OzM87OpjV9kfBoEll/TVB8RjzXEq8ZLGtZs2Wx98/IyCAtLc3o5MMAVX5fSW5iIpmZmeh+/x0uX8ba2lqZlNhY919zc3OeeuqpIuevKmtz5hxSglRzcxU///ySCFIfE7IsEx4ezrVr18rliZkgCIIgCIIg3E+0qD6kU7dPGby3MbfB19W32PtHREQQFxdX4I9/+4hQzM3N87p7uLvDv+m++/bti5mZGS4uxicH79u3LydOnCAmJqbYZXlYkya14c8/r3Ly5C02bhxAjx5eFXZuoXylpKQoY57T09Px9fWtsO7kgiAIgiAIwpNH/NIspoK6PJ6IMez228y9GWaq4n+sd+/eVcYdGHNt2jJqWr2OvGgRUosWYG4OQFBQUKHHjY+P58qVK6Smpha6XVmytbXgt99e4cKFONq08aiw8wp5ynOKA0dHR3x9fbl8+TKJiYmEhobSqFEjMX5FKDYxBYdg6kQdFUydGH4jPGlEoFpMVlZWRsetnLz9QCKlEk5Lo9Foit6oRQukZcugBF0uw8PDOXLkCPb29iUqT0nEx2eQna2lRo3/zuHgYPnEB6nZ2dlotVrMzc0x//fBQnlTq9X4+ha/Jb80qlatirW1NRcvXiQzM5PTp0/j5+dHlSpVyvW8wqOvIuqnIDwMUUcFUydJkng4LJi08hjfL8aoFpNGo8nX6pmrzSU0NtRgWUkDVXNzc5KTk4vX8lmCJ2n6KWrS0tJKVJ7iio1No3PnVTz77Gri4tLL5RyPqmvXrnHq1Cni4uIq7Jw6nY74+PhyGch+Pzs7O5o2bYqDgwNarZbz589z8+ZNMW5VKFRF1U9BKC1RRwVTJ8syGo1G/L0VTJZIplSJcnNz890czsedJ1uTbbCsuXvJpmExMzMjMzOTrKyshy7j/fSBam5uboHZikvrxo1kOnVayfnzcVy6dI8hQ7aU2bGF0pFlmRs3blTIHzALCwv8/f2VubauXbvG5cuXxQ88oUAVWT8FoTREHS17nTt3ZuLEiQ99nKioKCRJqpDpYFauXImTk5PBsiVLluDh4YFKpWLBggV88MEHNG3atNzL4unpyYIFCwyWVfY85CX5LiryexNMQ3ncP0Wg+hDOxZ0zeO/t4o2jlWOpjlUlMxPOncMm8hyt+Bt/ztCYvPecOwclTIrk4OCgBKeyLJdZF9TIyAQ6dFjB5cvxANSu7cjXX5vuhNBC+VCpVHh5eVG/fn0A4uLiuHLlSqH7WFlZUbNmTWWeX0EQBOHh3eMeS1jCPe6V+7mGDh2KJEmMGTMm37px48YhSRJDhw4FYNOmTXz00UflXqayFBwczOXLl5X3KSkpjB8/nmnTphETE8OoUaOYMmUKe/bsKbNzGguOAU6cOMGoUaPK7DxF0X+3kiRhbm5O3bp1efvttw0aUjw8PLh9+zaNGzeusHIJTzYxRvUhDA4YTFC9IE7eOsmJmBNUs61W4mNoNBqcMzP5dM8eOHCAJho4QBZqtORggcVYMzCTwM4Ojh+HmjWLdVyVSoWDgwN3795FpVIVbyxsEcLC7hIU9AO3buV1U27QwJndu1+lTh2nhz628OiRJIkaNWpgbW1NREQEtWvXLnR7W1tb6tWrV0GlEwRBeDLoA9WOdMSV8n8Q6OHhwbp16/jyyy+VMZNZWVmsWbPG4O/Aozg3p7W1tcE40OjoaHJzc+nZsyfu7u7Kcjs7u3Ivi75nXEXq1q0bK1asIDc3l1OnTjFkyBAkSeKzzz4D8sYgurm5VXi5hCeXaFEtJpXK+EdVw74Gz/s8z0fPfMQbrd8o8XFjY2Oxy8nBSqMBlQrZzAw1GiR0WJKFlJsLKhWkpUFCQomO7eDgAOQFFA8bqIaGxtKp00olSG3YsCoHDgx9pIJUjUbDrVu3uHev/J86V4byTJxVmCpVqtCiRQuR5EEoVGXVT0EoLlOoo5ml+J8WrbK/Fi2ZZJJNdrGOWxrNmjXDw8ODTZs2Kcs2bdpE7dq1CQwMVJY92PX3u+++w8vLCysrK6pXr86LL76orNPpdMydO5cGDRpgaWlJ7dq1+eSTT4yeX6vVMmLECOrWrYu1tTU+Pj4sXLjQYJt9+/bRqlUrbG1tcXJyol27dly/fh2AM2fO8PTTT2Nvb4+DgwPNmzfn5Mm8xJj3t26uXLkSf39/AOrVq4ckSURFRRnt+rt8+XIaNWqEpaUl7u7ujB8/Xlk3f/58/P39sbW1xcPDg7Fjxyr5Q/bt28ewYcNITk5WWjM/+OADIH/X3+joaIKDg5VyDxgwgDt37ijr9eX64Ycf8PT0xNHRkYEDB5Zo9gdLS0vc3Nzw8PCgb9++BAUFsWvXLmX9g915ExMTCQkJUZItenl5sWLFCqPH1mq1DB8+HF9fX6Kjo4tdJuHJJlpUi8nS0rLE4zxzc+HMGShs+Ont26Af2pcjm0F29r9PD/ISJ+msbVGbSVCKcQkeHh5cvXoVJycnLCwsSry/3rFjN+nW7SeSkvIuJDDQjT//fBVXV5tSH7MyxMbGcu3aNezs7B677qdqtVrphlsZRMp8oTCVXT8FoSimUkc70KFY22n+/R/AS7wEwCUucYQjzGc+zWjGalYr2/emN0kk5TvOSU7mW1Ycw4cPZ8WKFYSEhAB5gdqwYcPYt2+f0e1PnjzJhAkT+OGHH2jbti0JCQkcPHhQWT99+nS+//57vvzyS9q3b8/t27e5dOmS0WPpdDpq1arFhg0bcHFx4ciRI4waNQp3d3cGDBiARqOhb9++vPbaa6xdu5acnByOHz+u/J0KCQkhMDCQRYsWoVarCQ0NNTo8Kjg4GA8PD4KCgjh+/DgeHh5GWzkXLVrEpEmTmDNnDt27dyc5OZnDhw8r61UqFV999RV169bl6tWrjB07lrfffpvvvvuOtm3bsmDBAmbMmEF4eDhgvLVWp9PRt29f7Ozs2L9/PxqNhnHjxhEcHGzwmUdGRrJlyxa2b99OYmIiAwYMYM6cOQUG/YU5f/48R44coU6dOgVu8/7773Px4kV+//13XF1diYiIIDMz/wOQ7OxsXn75ZaKiojh48GCltBYL5a88sv6KQLWY9EmJCmpZzb89tG4Np08Xvl2DBjI9PNTIQFIy6LDBGrAhEx0qMLcAcktV5rp163Ly5Ens7OyKXe4HXbkST1DQD6Sl5QXKbdrUYseOEJycHr355qpVq0ZUVBRpaWmkpqaaxNPzsqLT6YiLi6NatWql/q4FobyI+imYuketjiaSqIxJ/YmfcMCBj/mYFFKIIYYqlO+0YYMGDWL69OlKK+Xhw4dZt25dgYFqdHQ0tra29OrVC3t7e+rUqaO0vqamprJw4UK++eYbhgwZAkD9+vVp37690WOZm5vz4YcfKu/r1q3L0aNH+fnnnxkwYAApKSkkJyfTq1cv5eGDn5+fQVmmTp2qTEfk5eVl9DzW1ta4uLgAed1wC+ry+vHHHzN58mTefPNNZVnLli2V/76/VdnT05OPP/6YMWPG8N1332FhYYGjoyOSJBXapXbPnj2cO3eOy5cvU7duXSRJYvXq1TRq1IgTJ04o59PpdKxcuVL5ffPqq6+yZ8+eYgeq27dvx87ODo1GQ3Z2NiqVim+++abA7aOjowkMDKRFixbK9T0oLS2Nnj17kp2dzV9//YWjY+lyuQimrzySaopAtRgyMjLIzc0lNDSUZs2aFWufM2eKDlIBnJ3tcHV1Up706VCRjh0Z2KJCR5VCGqpyc3OJiooiISGB1q1b51vv5uaGRqMhKyur1N0yGzRwZuDARixdepqnn/Zk27aXsbPLa529dOkSV69epV69eo/E/HMWFha4urpy9+5dTp8+TUpKCrVq1Srz7H1ZWVmkp6eTnl5x0/bIskxsbKzJP6VMTEwkIiKCBg0aiPlXnyCPSv0UnlymUkcPcrDojcgblxpPXlLDq1xlNrN5j/fwwotccqmK4XX8yq9lWs6qVavSs2dPVq5ciSzL9OzZs9CeSl26dKFOnTrUq1ePbt260a1bN/r164eNjQ1hYWFkZ2fz7LPPFvv83377LcuXLyc6OprMzExycnKUv+XOzs4MHTqUrl270qVLF4KCghgwYIAyxnTSpEmMHDmSH374gaCgIF566aVSt6bHxcVx69atQsu+e/duPv30Uy5dukRKSoryuywjIwMbm+L1TAsLC8PDw8MgmG3YsCFOTk6EhYUpgaqnp6fBQ3h3d/cSTZX39NNPs2jRItLT0/nyyy8xMzOjf//+BW7/+uuv079/f/755x+ee+45+vbtS9u2bQ22efnll6lVqxZ79+4VQ4QecyLrbyXJzc1r0bx161axx3oWb7YZGX9/NSqVvqPv/WskVGZqCkvWm5qayqxZs/j222+NlsvFxYXs7GxkWSYrK6tUTzokSWLx4l7Mm/ccv/32ihKkarVarl69ik6nIzIyskySNVUEd3d3NBoNKSkpANy8ebNMyy7LMklJSWg0Gu7evSumOriPLMtERUURHx9PVFSU+GwEQRAeYF3M/3ngQdN//9eQhgD44ksjGtGUptSkZrGO+zCGDx/OypUrWbVqFcOHDy90W3t7e/755x/Wrl2Lu7s7M2bMICAggKSkpBIHL+vWrWPKlCmMGDGCP//8k9DQUIYNG2YwdcuKFSs4evQobdu2Zf369Xh7e/P3338DeWM5L1y4QM+ePdm7dy8NGzZk8+bNJf8AoMiyR0VF0atXL5o0acLGjRs5deoU3377LVA+U8082IVZkqQS/faztbWlQYMGBAQEsHz5co4dO8ayZcsK3L579+5cv36dt956SwnYp0yZYrBNjx49OHv2LEePHi3ZxQgCIlAtUkZGhsH7s2fPkp6TzhdHvmBf1D5SslOKdZxvv4WDBw1ff/2VhJ9fBvrhfU6OMi4u4OICrq7gWjV/AHs//dhTWZaNJgiKj49XnvLp54grDv1YVD21WsWkSW2wtv7vBhgeHq7c/GRZNkjnbsocHBzyBaZnz54ts+MnJSWRlZWFJEmkpaWRlJRUZsd+1CUlJZGQkICZmRkJCQnisxEEQXiEdevWjZycHHJzc+natWuR25uZmREUFMTcuXM5e/YsUVFR7N27Fy8vL6ytrYs95cvhw4dp27YtY8eOJTAwkAYNGhAZGZlvu8DAQKZPn86RI0do3Lgxa9asUdZ5e3vz1ltv8eeff/LCCy8UmACoKPb29nh6ehZY9lOnTqHT6Zg3bx5PPfUU3t7e3Lp1y2AbCwsLtFqt0f31/Pz8uHHjBjdv3lSWXbx4kaSkJBo2bFiqshdFpVLxv//9j/fee8/ouFO9qlWrMmTIEH788UcWLFjAkiVLDNa//vrrzJkzh+eff579+/eXS1mFx5fo+lsIjUajtKbq3bp1i9Qqqcw/Oh/Ie1rVqGojfg/5HbWq4EHETZrA/cMtZFkmNDQKleq/m5NKl4Nakv4LTvWnLqDFT6VS4ebmRnR0NLdu3TLoEqLT6YiIiFC6FMuyTEREhDJpdUGWLz/N22/vYs+ewQQEuHHjxo18Aa5WqyU5Odlg2fXr1/Hx8SmXgdRlSavVkp1tmA3x1q1bVKtWjVq1aj3UsfUthiqVCgsLCzQaDVFRUTg5OZV7siFJknB2djbZpEb6z0ar1WJlZUVWVlaFfTZC5TP1+ikIj3IddcWVUYyqkKlp7qdWqwkLC1P+uzDbt2/n6tWrdOzYkSpVqrBjxw50Oh0+Pj5YWVkxbdo03n77bSwsLGjXrh13797lwoULjBgxIt+xvLy8WL16NX/88Qd169blhx9+4MSJE9StWxeAa9eusWTJEp5//nlq1KhBeHg4V65cYfDgwWRmZjJ16lRefPFF6taty82bNzlx4kSh3VuL8sEHHzBmzBiqVatG9+7dSU1N5fDhw7zxxhs0aNCA3Nxcvv76a3r37s3hw4dZvHixwf6enp6kpaWxZ88eAgICsLGxydclOCgoCH9/f4YPH87ChQvRarWMHTuWTp06KeNDy8NLL73E1KlT+fbbb/O1lALMmDGD5s2b06hRI7Kzs9m+fbvBeGC9N954A61WS69evfj9998LHH8sPNrK4/4pWlQLoW9p04/x3LhxIwC/nvpvrIcsy5ipzAoNUo25v3Upw8qKbAsLNNnZyNnZeRl+73/pdHnzqBqZk0w/5iI2NtZg+Y0bN8jKyuLChQtKV1djQef9vv76GCNGbCM+PpMuXX4gJiYFnU6HVqs1eBl7sqbVapWMdaasoNZTfUKIh6H/Ts3NzZVXRbUcqlQqateubbJJQO7/bPSTiYtW1SeHqddPQXiU62hlBaqQ10tJPxVeYZycnNi0aRPPPPMMfn5+LF68mLVr19KoUSMgL3vs5MmTmTFjBn5+fgQHBxc4tnL06NG88MILBAcH07p1a+Lj4xk7dqyy3sbGhkuXLtG/f3+8vb0ZNWoU48aNY/To0ajVauLj4xk8eDDe3t4MGDCA7t27GyRnKqkhQ4awYMECvvvuOxo1akSvXr24cuUKAAEBAcyfP5/PPvuMxo0b89NPP/Hpp58a7N+2bVvGjBlDcHAwVatWZe7cufnOIUkSW7duxcXFhU6dOhEUFES9evVYv359qctdHGZmZowfP565c+cazbthYWHB9OnTadKkCR07dkStVrNu3Tqjx5o4cSIffvghPXr04MiRI+VabqFylMf9U5Kf8IFiKSkpODo6kpycbHCz1Wg07Ny5k61bt2JjY4OZmRk5OTl06tSJb29/S3jOf0HZyGYjmfX0LIPjHjoEHe7LMn/w4H8tqnmtqaHcvXuXixcvEhUVRZWMDEhIoH///jRq1Cj/UwlnZ6hpOOYE4JdffmHr1q08/fTTyhgRnU7HX3/9RWZmJufPn+fs2bPk5uZSt25dnnvuOZ555pl8lWnOnENMn/5f15W33nqKefOeQ6PRGHSV1Wq1HDhwwOiYB7VazXPPPWeyrar677Qg3bp1w8ysdJ0M7v9OrayskCRJGRtctWpVmjZtWq5P6nU6HTdv3qRWrVom90Orsj8bofKZcv0UBKj4OpqVlcW1a9eoW7cuVlaPXhZ9oeLJskxOTg4WFhbib6ZQaQq7dyUlJVGlSpV8MdXDEL8YCqBvecvNzVWCFwsLC37d/isRGREGXYJb1ChZt4v7W5ciIyORZZlEGxsSatbkxzNnSPLwAH9/w5eRIBVQuvvevn1bWaZvTdUHBBqNBkmSuHPnTr5WVVmWee+9vQZB6vvvd2TevOeUli9ra2vlFR0dXeDAfFNvVS1qLOrDjFV9sMUQqNCWQ1mWSUhIMMkERZX92QiVz5TrpyCAqKPCo6GosayCUJlE1t8KotFouHXrFllZWTg5ORms0znqSNekGwRrLWu0pLj0Y/U0Go0yXlKSJCWYjI+PJyIiothf9oNdf/VjU2VZRpZl4uLikGVZyfwWFhbGlStX0Ol0yLLMpEl/8Mkn/6XDnzPnWWbNetro0zqtVltkF9nr16+b5I1U/50WpiRZne93/3cqSZJBN2n9d/ukZrkVn40gCIIgVK7o6Gjs7OwKfEVHR1d2EQXBKJFMyQh9sLJ9+3Zlsmf494mrVYIyZlVGpqZ9Tdzt3Yt9bFmWyczMxMzMjF27dmFtba0EvVqtFhsbG7Zs2ULz5s2L1bVD36KqzzZrZmZGbm4ukiQRFxdHfHy8chxzc3Pu3LlDTEwMGo2WN97YyZIl/yjH+vrr7owf36rAc+kDjMLotzG17r/FDUA1Gk2Ju//e/50a+3zMzMzIzMxUHhg8ScRnIwiCIAiVq0aNGoSGhha6XhBMkQhUjbCysqJhw4YGkybrxapjkSQJMzMzJKQSd/tVqVQ0b96czMxM1q9fj42NjcE0L5IkcffuXXQ6XbHGydja2uLg4EBKSgqxsbF4enrStm1bMjIyWLVqFYByfEdHR5KSkrh79y4jR27nhx/yurpKEixd+jzDhwcWei4LCwueeuop0tLSCtzG3t4eCwuLYn0WFcnKyopmzZqRmppa4DaOjo6lGiuk/04fzBB9P3Nz83Id9yRJEm5ubiYX7JnCZyNUPlOtn4KgJ+qo8Ch4cJ7U4jIzM6NBgwZlXBpBMFQe908RqBZg3rx5RoOWO+o7yhyZjo6OJer2q2dpacmiRYuU1tT7uz1qtVqsra355ptvmDhxYrGO5+bmRmpqKvHx8Xh6euLg4MDt27e5du0aNjY2ylyw5ubm2NvbExERga9vfQDUaokff3yBgQMbF+tcrq6uuLpWfGbBslCeTwwtLS2xtLQst+MXRT9VkSmq7M9GqHymXD8FAUQdFUyfPreDIJiq8mh0EIGqEVlZWcqHfX+XxCyySFIlKdvJyCVuUYW87qVhYWHY2NgoY/Xup1KpCAsLK3Y31DfeeAM7OztlW1mW2blzJ9nZ2VhbWyvb5eTkoFKpyM7OxtY2ms8/D8LLy4U+fXxLfA2CadFqtURFReHp6Wly3a4FQdRPwdSJOiqYOlmWyc7OxtLSUrT8CyapPHLUiEDViJiYGCXou/9mEGf235xekiRhjjkNqzYs8fFTU1NRq9XodDokScp3w9HpdKjValJTU6lSpUqRx3sw4ZNGoyE+Ph5LS0tycnKU5VlZWUBeC1d8fDzvv99KPJ17jBTWrVkQKpuon4KpE3VUMHUFzbogCI8rEagaUb9+ffz9/bl69SqRkZHK8vgG/yUmMjMzo6VHS8zVJQ/0HB0dkWWZW7duYW1tTWZmJgB9+vQhJiYGyJsAujhBqjHm5uZMmjSJ9PR0rl27xoIFC8jJ0dK69cs891zeGAU7OzsRpAqCIAiCIAiCYJJEoFqA4cOHA/Diiy8qU704N3bG/uZ/CZZauJe82y/A3r17uXfvHmZmZnTv3p1Dhw6h0+kYNGgQFy5cwNzcnNq1az9U+Z2cnHByciIuLpXsbC06nczo0QfYurUG3bt7PdSxBUEQBEEQBEEQypNItVlMsiQTeifUYFlpxqcCbN26FchrubW2tsbMzEzpatywYUO8vb1LlX32QbdupTJ48DZ0urxkTVWqWFOzpsNDH1cwPZIk4eHhIcatCCZJ1E/B1Ik6Wj46d+5c7MSQhYmKikKSpEKnWCkrK1euzDekasmSJXh4eKBSqViwYAEffPABTZs2LfeyeHp6smDBAuV9Zc6qUJLvsqy+95IYOnQoffv2LdE+D36+lVWOx0V53D9FoFpMqVapZOZmGixrXqN5iY9z/fp17t69iyRJvPLKK1hZWZGamkp6ejpQdl/y9etJdOy4gqioe8px9+8fSpMm1cvk+IJpUalUuLi4iGleBJMk6qdg6kQdLZ6hQ4ciSRJjxozJt27cuHFIksTQoUOVZZs2beKjjz6qwBI+vODgYC5fvqy8T0lJYfz48UybNo2YmBhGjRrFlClT2LNnT5md01hwDHDixAlGjRoF8N/UiGUcDOzbt0/JlyJJElWrVqVHjx6cO3fOYLuK+C5XrVpFy5YtsbGxwd7enk6dOrF9+/Zi7btw4UJWrlxZovPd//kKD6887p/ijlxMkk7ipYYvUbdKXQDqO9fH2dq5xMc5efIkHh4etGzZkoYNG5KdnU1ubm6h80yW1JUr8XTosILIyERUqrysxVZWZvj6PprTyghF02q1XLp0qVwyrgnCwxL1UzB1oo4Wn4eHB+vWrVPya0BessY1a9bkG7bk7OxsdE56U2ZtbU21atWU99HR0eTm5tKzZ0/c3d2xsbHBzs4OFxeXci9L1apVsbGxAfKy/mZmZhpMaViWwsPDuX37Nn/88QfZ2dn07NnTICFneX+XU6ZMYfTo0QQHB3P27FmOHz9O+/bt6dOnD998802B+2m1WnQ6HY6OjkaD/cLc//kKD6887p8iUC0m+2x75j83n8PDD3Pu9XMs7rm4xMdIS0vj1KlTSJJE//79gbwW1rJ04UIcHTuu5MaNFOzs1DRt6oKVlRkqlejO9LjTZ3UWBFMk6qdg6kyhjmaW4nX/T0Ptv8uyi3nc0mjWrBkeHh5s2rRJWbZp0yZq165NYGCgwbYPdgH97rvv8PLywsrKiurVq/Piiy8q63Q6HXPnzqVBgwZYWlpSu3ZtPvnkE6Nl0Gq1jBgxgrp162JtbY2Pjw8LFy402Gbfvn20atUKW1tbnJycaNeunfKb68yZMzz99NPY29vj4OBA8+bNOXnyJGDYurly5Ur8/f0BqFevHpIkERUVZbTr7/Lly2nUqBGWlpa4u7szfvx4Zd38+fPx9/fH1tYWDw8Pxo4dS1pamlLOYcOGkZycrLRqfvDBB0D+rqnR0dH07dsXOzs7HBwcGDBgAHfu3FHW68v1ww8/4OnpiaOjIwMHDixWRutq1arh5uZGs2bNmDhxIjdu3ODSpUvK+pJ8lw/67bffcHR05KeffjK6/u+//2bevHl8/vnnTJkyhQYNGuDn58cnn3zCxIkTmTRpEjdu3AD++362bdtGw4YNsbS0JDo6Ol+X29TUVEJCQrC1tcXd3Z0vv/wy3zU8+PlKksTSpUvp168fNjY2eHl5sW3bNmV9ceqdULZEoFoEY0+uXGxcaFStUYmPdfToUTQaDbVq1aJu3byW2Vu3bj10GfX++ec2nTqtJDY27+bXp08tJk70w9XVBXd39zI7jyAIgiAIZa9DKV5/3bf/X/8ue+OB4/YuYN/SGj58OCtWrFDeL1++nGHDhhW6z8mTJ5kwYQKzZs0iPDycnTt30rFjR2X99OnTmTNnDu+//z4XL15kzZo1VK9ufLiSTqejVq1abNiwgYsXLzJjxgz+97//8fPPPwN50/T17duXTp06cfbsWY4ePcqoUaOUbrMhISHUqlWLEydOcOrUKd555x2jMyEEBweze/duAI4fP87t27fx8PDIt92iRYsYN24co0aN4ty5c2zbto0GDRoo61UqFV999RUXLlxg1apV7N27l7fffhvIm+VhwYIFODg4cPv2bW7fvs2UKVOMXvOAAQNISEhg//797Nq1i6tXrxIcHGywXWRkJFu2bGH79u1s376d/fv3M2fOnEK/m/slJyezbt06oOAxsUV9l/dbs2YNL7/8Mj/99BMhISFGt1m7di12dnaMHj0637rJkyeTm5vLxo0blWUZGRl89tlnLF26lAsXLhi0gOtNmjSJw4cPs23bNnbt2sXBgwf5559/irz+Dz/8kAEDBnD27Fl69OhBSEgICQkJQNH1Tih7IutvBdFqczl69CgAHTt2LJcBx0lJWaSl5XXTaNmyBjNmdOL69QgsLCzKJDmTIAiCIAjCoEGDmD59utJCefjwYdatW8e+ffsK3Cc6OhpbW1t69eqFvb09derUUVpgU1NTWbhwId988w1DhgwB8hJOtm/f3uixzM3N+fDDD5X3devW5ejRo/z8888MGDCAlJQUkpOT6dWrF/Xr1wfAz8/PoCxTp07F19cXAC8v47MhWFtbK118q1atipubm9HtPv74YyZPnsybb76pLGvZsqXy3w+24n388ceMGTOG7777DgsLCxwdHZEkqcDjA+zZs4cLFy5w9epVpYv16tWradSoESdOnFDOp9PpWLlypdJN99VXX2XPnj0Ftk7r1apVC0DJmfL8888rn8+DCvsu7/ftt9/y7rvv8uuvv9KpU6cCz3358mXq169vNDCuUaMGDg4OBuOGc3Nz+e677wgICDB6vNTUVFatWsWaNWt49tlnAVixYgU1atQosAx6Q4cO5eWXXwZg9uzZfPXVVxw/fpxu3boVWe+EsicC1SJIkoQs543zfJhBwteunSYtLQ0nJyelGwnkdbXQz536MLKysrh6dQ9DhiRz5Uojtmx5mdTUe2g0GsB4y7Dw+FCpVNSrV08kAhFMkqifgqkzlTp6sBT73P/T/ul/j/HgVfxa6hIZV7VqVXr27MnKlSuRZZmePXvi6lp4HowuXbpQp04d6tWrR7du3ejWrZvSxTIsLIzs7GwlqCiOb7/9luXLlxMdHU1mZiY5OTlKd1xnZ2eGDh1K165d6dKlC0FBQQwYMEDpXTZp0iRGjhzJDz/8QFBQEC+99JIS0JZUXFwct27dKrTsu3fv5tNPP+XSpUukpKSg0WjIysoiIyOj2GMkw8LC8PDwMGjRbdiwIU5OToSFhSmBqqenp8FYUnd3d+Li4oo8/sGDB7GxseHvv/9m9uzZLF5c8BC3wr5LvV9++YW4uDgOHz5sELQXpCS/Uy0sLGjSpEmB669evUpubi6tWrVSljk6OuLj41Pkse8/rq2tLQ4ODgafX2H17kknkilVstK3guoIDz8AQPv27VGr1cqaOnXqlEHJ8v7RHjlyhIyMWNas6YaDgyVAmSZpEkyXJEk4ODiIqRUEkyTqp2DqTKWOWpfipb5vf/W/yyyLedyHMXz4cFauXMmqVauUuecLY29vzz///MPatWtxd3dnxowZBAQEkJSUhLV1yUqzbt06pkyZwogRI/jzzz8JDQ1l2LBhBsl/VqxYwdGjR2nbti3r16/H29ubv//+G8gby3nhwgV69uzJ3r17adiwIZs3by7ZB/CvosoeFRVFr169aNKkCRs3buTUqVN8++23AAblLYq+bhZVRx/swixJEjqdrsjj161bFx8fH4YMGcLIkSPzdSm+X2HfpV5gYCBVq1Zl+fLlRQah3t7eXL161ejncevWLVJSUvD29laWWVtbl9u/1cI+v+LUuyeZmJ6mkujQIcvyQ2Wzatw4iAYNGuR7qmRvb1/q1s5ffrnIRx/tB/KeYujHcsTGxpa6nMKjSavVcu7cOZGxUjBJon4Kpk7U0ZLr1q0bOTk55Obm0rVr12LtY2ZmRlBQEHPnzuXs2bNERUWxd+9evLy8sLa2LvaUL4cPH6Zt27aMHTuWwMBAGjRoQGRkZL7tAgMDmT59OkeOHKFx48asWbNGWeft7c1bb73Fn3/+yQsvvGAw5rYk7O3t8fT0LLDsp06dQqfTMW/ePJ566im8vb3z5SexsLAosu75+vpy48YNoqOjlWUXL14kKSmJhg0blqrsBRk3bhznz58vNHgv6LvUq1+/Pn/99Rdbt27ljTceHDVtaODAgaSlpfF///d/+dZ98cUXmJubK0lIi6NevXqYm5tz4sQJZVlycrJB9+HSKG69e1KVx/1TdP0tglbSsst7F/ZZ9nxy8BNa1WrF055PY2n24PPKwkjUqdOU9u2b5ltz9epV0tLSSpwee/XqMwwbthWdTsbKyoypU9vh7u7OzZs3RaD6hBI/sARTJuqnYOpEHS0ZtVpNWFiY8t9F2b59O1evXqVjx45UqVKFHTt2oNPp8PHxwcrKimnTpvH2229jYWFBu3btuHv3LhcuXGDEiBH5juXl5cXq1av5448/qFu3Lj/88AMnTpxQElVeu3aNJUuW8Pzzz1OjRg3Cw8O5cuUKgwcPJjMzk6lTp/Liiy9St25dbt68yYkTJ0oUCD3ogw8+YMyYMVSrVo3u3buTmprK4cOHeeONN2jQoAG5ubl8/fXX9O7dm8OHD+frVuvp6UlaWhp79uwhICAAGxubfL8Lg4KCaNSoEYMGDWLBggVoNBrGjh1Lp06daNGiRanLboyNjQ2vvfYaM2fOpG/fvvlaygr7Lu/n7e3NX3/9RefOnTEzMzPIsHu/Nm3a8OabbzJ16lRycnLo27cvubm5/PjjjyxcuJAFCxYYTWJVEHt7e4YMGcLUqVNxdnamWrVqzJw5E5VK9VCtfkXVO6HsiRbVIiRZJaFVaUmySWLxqcW89utraHSaYu9vZ3eLVq2+5Nq1U2VWpsWLTzJkyBZ0uryW2LCwe8iyrIy9EIGqIAiCIAjlzcHBAQcHh2Jt6+TkxKZNm3jmmWfw8/Nj8eLFrF27lkaN8mZReP/995k8eTIzZszAz8+P4ODgAsdWjh49mhdeeIHg4GBat25NfHw8Y8eOVdbb2Nhw6dIl+vfvj7e3N6NGjWLcuHGMHj0atVpNfHw8gwcPxtvbmwEDBtC9e3eDJDklNWTIEBYsWMB3331Ho0aN6NWrF1euXAEgICCA+fPn89lnn9G4cWN++uknPv30U4P927Zty5gxYwgODqZq1arMnTs33zkkSeLnn3+mSpUqdOzYkaCgIOrVq8f69etLXe7CjB8/nrCwMDZs2JBvXVHf5f18fHzYu3cva9euZfLkyQWeT//5rV27lsaNG9OiRQsOHDjAli1bimyRNWb+/Pm0adOGXr16ERQURLt27fDz83uo5KJF1Tuh7EnyE55lJyUlBUdHR5KTk43ebAPHBHKx6kVkZBzsHWjm3ow/X/2zyOMeOgQdOkCLFjOoUiWOzp3b8L//Dcm33eLFi9m5cyc2NjYGXVIKMn/+USZP/u/848a15KuvuqNSSRw8eJAlS5bQsGFDpk+fTkxMDKdPn+bPP//EysrK6I1PeDzou635+/sX68m2IFQkUT8FU1fRdTQrK4tr165Rt25dkZVfKBZZlsnMzCzX8ZmPs/T0dGrWrMm8efOMttILxVPYvSsxMRFnZ+cCY6rSEC2qhdhxZQcXq10kR51DrjqXhMwEztw5w44rO4q1v41NFFWrXsLcPJ7q1b2L3qEQsizz0Uf7DYLUt99uy9df5wWpgJLW/Pbt28o2lpZ5XZTFTe3xplKp8PHxqfSMlYJgjKifgqkTdVR4FIiHGsV3+vRp1q5dS2RkJP/8848yh2ufPn0quWSPL5H1twLtuLKDQZsGkaMyzOSVnJXMoE2DihWsenn9iCTJpKVVwcenjdFt9JMUG5toWk+WZaZP38OMGfuUZbNmdWbOnCCDAFTf9TcxMZGsrCxl3/v/v/D4KmhibkEwBaJ+CqZO1FHB1IlGh5L54osvCAgIICgoiPT0dA4ePFjkNEqCaRHJlIzQ6rS8u+ddMjWZD6yRsJYcyMxJZ+K297Bt1RW1ZLyL0IkTybi5nQfg2rXuBZ6rWrVqhISE0LhxY6PrdTqZN9/8nW+++S9z2bx5zzFpUv7A187ODjs7O9LS0rhz5w7u7u7UrFmT3NxckSTiMafT6UTXSsFkifopmDpRR4VHgb7rr1C0wMBATp0qu/wwQtGKMw1SSYlA1YhjMceITIzEXGVOFlnKclmrIinVDFQ2XMmMoHPIMbjR1ugxfHzWUL++luxsa27c6F3guSwsLHBzc6NWrVpG18fFpbNp0yXl/aJFPRkzpuDsbm5ubkRERHD79m3q1KmDubk59+7dE0/hBEEQBEEQBEF4ZIiuv0bEpcehlbX3tZb+G+TJ/77XqUHSgq3xbHSgwcPjCAA3brQDzChoWEF2djbR0dEFzsPk5mbH7t2v4uZmx6pVfQsNUgGaNm1Khw4dcHZ2LnQ7QRAEQRAEQRAEUyVaVI2oZlsNtaRGK+d1l5X+/b/KKE+VNi9oTa9mdP86dbZgYZGFVmtGZOQrBAZCQIDxc925c4cNGzbw22+/FZj118+vKleuvIGdXdHjZx4cJG5ra1vkPoIgCIIgCIIgCKZEBKpGtK7ZmvpV6hN2LyzfOgcHmRwpAw8bH77/qTVqIz1qFy/+g8xMqFPHn48/ticgAArKlXTv3j2D9xkZuSxY8Ddvv90OM7P/GrzvD1IzMjIA8k0GLTy5VCoV/v7+ImOlYJJE/RRMnaijwqNAjE8VTFl53D9FoGqEWqXmk2c/YcCGAWSTjay0perIkVKwtrBkwfMf08krf8KF8+fPk5ubglotMWnSYAoYeqqoWrUqVlZWqFQqUlKy6dVrDQcPRnPx4l1WreqLWm34pd+8eZNr165RtWpVfH19i7wWfVArPP5ycnJE6nrBZIn6KZg6UUcFUyfLssg5IjxRxKPDAvTw6sGE1hOQ5PtuCJIODxsffnzhR3p49TC63+XLlzE3N6ddu3YFJki6n6OjI+bm5mi1MkFBqzl4MBqAX3+9TGRkYr7tnZycgLyW2JycnHzrHySmpXky6HQ6wsPDyyXjmiA8LFE/BVMn6qjwKNBPPSgIpqg87p8iUC1EM/dmWGgtMNdYYKYxh9vN+b7ViQKD1MTERGrXrs0HH3zAwIEDi32e3Fwt4eHxnDhxCwAXF2v++msI3t4u+ba1s7PD3t4eWZa5c+dO6S5MEARBEATBBHl6erJgwYJS779y5Urlob5g6GE/25J49dVXmT17doWc60mwc+dOmjZt+sQ9TBOBahEkJFSyCrWshiynAudNBTh48CA///wzx48fp3r16sU6fkxMPBYWtjRo4AbkZfndt28ozZq5F7iPu3veutjYWNFiKgiCIAhChRg6dCh9+/Yt13OcOHGCUaNGFWtbY4FXcHAwly9fLvX5V65ciSRJSJKESqXC3d2d4OBgoqOjS31MU1GSz/ZhnDlzhh07djBhwoR869auXYtarWbcuHH51hX2kEGSJLZs2WKwbOPGjXTu3BlHR0fs7Oxo0qQJs2bNIiEhoSwuw6iEhARCQkJwcHDAycmJESNGkJaWVuR+R48e5ZlnnsHW1hYHBwc6duxIZmYmAPv27VPq3IOvEydOANCtWzfMzc356aefyu3aTJEIVMtIZmamUplatCh8Chm9a9cS+eabA1hYmGNpaY6HhwMHDgylcWPj2YT1XF1dMTMzIysri8TE/N2DhSeTmKReMGWifgqmTtRR01C1atWHShZpbW1NtWqF/44qioODA7dv3yYmJoaNGzcSHh7OSy+99FDHLI7c3NxyPf7DfrbF9fXXX/PSSy9hZ2eXb92yZct4++23Wbt27UN1ZX733XcJDg6mZcuW/P7775w/f5558+Zx5swZfvjhh4cpfqFCQkK4cOECu3btYvv27Rw4cKDI4P/o0aN069aN5557juPHj3PixAnGjx+vJB9q27Ytt2/fNniNHDmSunXrGsQUQ4cO5auvviq3azNFIlAtRG3H2tRIrolbYt6La88UuO2xY8fIzs7Gzc0NLy+vIo8dHn6PDh1WkJGRF2iqVBIHDw7Dyyt/d98HqdVqpcX29u3bhW5rYVH0lDbCo0+tVuPv7y9+aAkmSdRPwdRVdh1NToZDhyrvlZxcNtexf/9+WrVqhaWlJe7u7rzzzjtoNBplfWpqKiEhIdja2uLu7s6XX35J586dmThxorLN/a2ksizzwQcfULt2bSwtLalRo4bSSte5c2euX7/OW2+9pbQ+gfFWuV9//ZWWLVtiZWWFq6sr/fr1K/Q6JEnCzc0Nd3d32rZty4gRIzh+/DgpKSnKNlu3bqVZs2ZYWVlRr149PvzwQ4NrvXTpEu3bt8fKyoqGDRuye/dug1bBqKgoJEli/fr1dOrUCSsrK6W1bOnSpfj5+WFlZYWvry/fffcdkiRhY2NDbm4u48ePx93dHSsrK+rUqcOnn35a5Of14GcLEB0dTZ8+fbCzs8PBwYEBAwYYDCv74IMPaNq0KT/88AOenp44OjoycOBAUlNTC/zstFotv/zyC71798637tq1axw5coR33nkHb29vNm3aVOj3UJDjx48ze/Zs5s2bx+eff07btm3x9PSkS5cubNy4kSFDhpTquEUJCwtj586dLF26lNatW9O+fXu+/vpr1q1bx61btwrc76233mLChAm88847NGrUCB8fHwYMGIClpSWQ91vdzc1Nebm4uLB161aGDRtmkDyrd+/enDx5ksjIyHK5vodVHvdPkfW3EK1qtqLprUA0Gh2yLHH98DSj22k0Gg4fPgxAx44di5WRberUXcTEpOLiko1KlTf9TJ06TsUum5ubGzExMSQkJJCdna1U9gc5OztTp04dkSXuMSfLMqmpqdjb24vvWjA5on4Kpq6y6+i5c9ChQ4WfVnHwILRv/3DHiImJoUePHgwdOpTVq1dz6dIlXnvtNaysrPjggw8AmDRpEocPH2bbtm1Ur16dGTNm8M8//9C0aVOjx9y4cSNffvkl69ato1GjRsTGxnLmzBkANm3aREBAAKNGjeK1114rsFy//fYb/fr1491332X16tXk5OSwY8eOYl9XXFwcmzdvRq1WKz/EDx48yODBg/nqq6/o0KEDkZGRSqvazJkz0Wq19O3bl9q1a3Ps2DFSU1OZPHmy0eO/8847zJs3j8DAQCVYnTFjBt988w2BgYGcPn2a1157DRsbG1599VUWLlzItm3b+Pnnn6lduzY3btzgxo0bRX5eD9LpdEqQun//fjQaDePGjSM4OJh9+/Yp20VGRrJlyxa2b99OYmIiAwYMYM6cOXzyySdGj3v27FmSk5ON9i5csWIFPXv2xNHRkUGDBrFs2TJeeeWVYn8Xej/99BN2dnaMHTvW6PrCxig3atSI69evF7i+Q4cO/P7770bXHT16FCcnJ4NrCwoKQqVScezYMaMPQOLi4jh27BghISG0bduWyMhIfH19+eSTT2hfwD+6bdu2ER8fz7BhwwyW165dm+rVq3Pw4EHq169f4DVUlvIYjigC1SLIskxey3zBH77+H6W9vT0BAQHFOu7KlX15+ulV2NtbYmVlVuIv18bGBkdHR5KTk4mNjaVOnToG67VaLXfv3kWlUol54Z4AOp2Oq1evilYrwSSJ+imYOlFHH953332Hh4cH33zzDZIk4evry61bt5g2bRozZswgPT2dVatWsWbNGp599lkgL3CpUaNGgceMjo7Gzc2NoKAgzM3NqV27Nq1atQLyHsSr1Wrs7e1xc3Mr8BiffPIJAwcO5MMPP1SWFfVbLTk5GTs7O2RZVqb5mzBhAra2tgB8+OGHvPPOO0rLXb169fjoo494++23mTlzJrt27SIyMpJ9+/YpZfvkk0/o0qVLvnNNnDiRF154QXk/c+ZM5s2bpyyrW7cuFy9eZMmSJQwYMIDo6Gi8vLxo3749kiQZ/P4r7PN60J49ezh37hzXrl3Dw8MDgNWrV9OoUSNOnDhBy5Ytgbx/GytXrsTe3h7IS5K0Z8+eAgPV69evo1ar83W/1h/n66+/BmDgwIFMnjyZa9euUbdu3QK/C2OuXLlCvXr1MDc3L9F+ADt27Ci0i3Vhc9XGxsbmuy4zMzOcnZ2JjY01us/Vq1eBvNbpL774gqZNm7J69WqeffZZzp8/b7QX5rJly+jatavR2UNq1KhRaKBdmUTWXxMkyzIHDhwA8vqYF/cfjbOzNbt2vUqvXoGlfnp7f1Kl+yuHTqdjzJgxTJ06lbt375bq2IIgCIIgCMUVFhZGmzZtDH7TtGvXjrS0NG7evMnVq1fJzc01CJwcHR3x8fEp8JgvvfQSmZmZ1KtXj9dee43NmzcbdK8tjtDQUCUwLi57e3tCQ0M5efIk8+bNo1mzZgaB2ZkzZ5g1axZ2dnbK67XXXuP27dtkZGQQHh6Oh4eHQQBdUMB4f+tceno6kZGRjBgxwuDYH3/8sdLdc+jQoYSGhuLj48OECRP4888/lf1L8nmFhYXh4eGhBKkADRs2xMnJibCwMGWZp6enEqRC3m/PuLi4Aj+7zMxMLC0t8/223bVrF+np6fTokTdzhqurK126dGH58uUFHqsgD9NyV6dOHRo0aFDgq2bNmqU+tjH63+ejR49m2LBhBAYG8uWXX+Lj42P02m/evMkff/zBiBEjjB7P2tpaeXjyJBAtqg8pMjKSW7duYW5uzlNPPVXgdgcOXKdx42o4O//3pKZaNVu8vesTERFeqn90Li4u2Nvb4+LiYrC/SqXCxcWFmJiYQm8mgiAIgiBUPn//vO63lXl+U+Th4UF4eDi7d+9m165djB07ls8//5z9+/cXu2GgsBaygqhUKho0aACAn58fkZGRvP7660qSnrS0ND788EODllA9KyurEp1L30qrPy7A999/T+vWrfOVCaBZs2Zcu3aN33//nd27dzNgwACCgoL45ZdfyuTzetCD+0mSVGjLmaurKxkZGeTk5BjkSVm2bBkJCQkG34dOp+Ps2bN8+OGHqFQqHBwcSE9PR6fTGfQGTEpKAvIebAB4e3tz6NAhcnNzS3xdD9P1183NLd/vao1GQ0JCQoGt+vpGpYYNGxos9/PzM5pJesWKFbi4uPD8888bPV5CQgJVq1YtsPyPGxGoPqSD//5ladGihcHN5n7btoXz0ksbCAiozu7dg3Fw+G88qYODA9nZ2aU6t0qlKnBch7u7uzKGVXgylPSPoyBUJFE/BVNXmXXU0fHhx4hWNj8/PzZu3Igsy0pr2uHDh7G3t6dWrVpUqVIFc3NzTpw4Qe3atYG8LraXL1+mY8eOBR7X2tqa3r1707t3b8aNG4evry/nzp2jWbNmWFhYoNVqCy1XkyZN2LNnT77xfiXxzjvvUL9+fd566y2aNWtGs2bNCA8PV4LZB/n4+HDjxg3u3LmjJL/UzwxRmOrVq1OjRg2uXr1KSEiIwTpZlpUsuQ4ODgQHBxMcHMyLL75It27dSEhIwNnZudDP635+fn7K+FZ9q+rFixdJSkrKF1SVhP536cWLF5X/jo+PZ+vWrcrYWT2tVkv79u35888/6datGz4+Pmg0GkJDQw3K+88//wB5ASrAK6+8wldffcV3333Hm2++ma8MSUlJBY5TfZiuv23atCEpKYlTp07RvHlzAPbu3YtOp8v3YEHP09OTGjVqEB4ebrD88uXLdO/e3WCZLMusWLGCwYMHGw3As7KyiIyMJDAwsMAyPm5EoFoESYKCGjsvXLjAgQMHcHJyokMBWRDWrTvPoEGb0GplTpy4xZdfHmXmzM7lV+B/6W+M+kBVJDB5vKnVanx9fSu7GIJglKifgqkTdbT4kpOTCQ0NNVjm4uLC2LFjWbBgAW+88Qbjx48nPDycmTNnMmnSJFQqFfb29gwZMoSpU6fi7OxMtWrVmDlzJiqVqsDfKCtXrkSr1dK6dWtsbGz48ccfsba2VsZlenp6cuDAAQYOHIilpSWurq75jjFz5kyeffZZ6tevz8CBA9FoNOzYsYNp04wnyDTGw8ODfv36MWPGDLZv386MGTPo1asXtWvX5sUXX0SlUnHmzBnOnz/Pxx9/TJcuXahfvz5Dhgxh7ty5pKam8t577wFF/x778MMPmTBhAo6OjnTr1o3s7GxOnjxJYmIikyZNYv78+bi7uxMYGIhKpWLDhg24ubnh5ORU5Od1v6CgIPz9/QkJCWHBggVoNBrGjh1Lp06dij3NojFVq1alWbNmHDp0SAlUf/jhB1xcXBgwYEC+6+/RowfLli2jW7duNGrUiOeee47hw4czb9486tWrR3h4OBMnTiQ4OFjpltu6dWvefvttJk+eTExMDP369aNGjRpERESwePFi2rdvbzSABYx+FsXl5+dHt27deO2111i8eLGSgXngwIHKWOuYmBieffZZVq9eTatWrZAkialTpzJz5kwCAgJo2rQpq1at4tKlS/zyyy8Gx9+7dy/Xrl1j5MiRRs//999/Y2lpSZs2bUp9DeWpPMb3izGqhTgcfZiTtU5wqvZJTtU+CZ1nGqw/evQoOTk5Bd4cly8/zSuvbESrzYt0Bw1qwrvvFvzUsCzdP34VyicTl2A6dDod8fHx5TKQXRAelqifgqkTdbT49u3bR2BgoMHrww8/pGbNmuzYsYPjx48TEBDAmDFjGDFihBKgAcyfP582bdrQq1cvgoKCaNeunTINizFOTk58//33tGvXjiZNmrB7925+/fVXXFzypvKbNWsWUVFR1K9fv8DukJ07d2bDhg1s27aNpk2b8swzz3D8+PESX/dbb73Fb7/9xvHjx+natSvbt2/nzz//pGXLljz11FN8+eWXShCkVqvZsmULaWlptGzZkpEjR/Luu+8CRbfcjxw5kqVLl7JixQr8/f3p1KkTK1euxNPTE41Gg52dHXPnzqVFixa0bNmSqKgoduzYgUqlKvLzup8kSWzdupUqVarQsWNHgoKCqFevHuvXry/xZ2PsGvRT7QAsX76cfv36GQ3S+/fvz7Zt27h37x6AMl3P6NGjadSoERMmTKBPnz4sXbrUYL/PPvuMNWvWcOzYMbp27UqjRo2YNGkSTZo0KbfpaSAv47Cvry/PPvssPXr0oH379ixZskRZn5ubS3h4uME40okTJzJ9+nTeeustAgIC2LNnD7t27cqXuXfZsmW0bdu2wIdma9euJSQkpELmwi2N8rh/SvITHsGkpKQo2XMdHBwM1v1y8RdeXfOq8j7rekcOjv6D9u3h3r17TJ06FY1Gw7vvvpuvUn3zzXHeeOO/Pu6jRjVj0aJeqFSG/0h3797NqlWrAMp0guLLly/z0Ucf4ejoqFTouXPnltnxBdOi1Wo5d+6cyFgpmCRRPwVTV9F1NCsrS8l2+iR3i09PT6dmzZrMmzevwOQxj4vDhw/Tvn17IiIiSjW1iCzLZGZmYm1tbfK95DIzM/Hx8WH9+vUm2/r3qLl37x4+Pj6cPHmyxFmSy1Jh967ExEScnZ2NxlSlJbr+FkGSJKOtkcuXLyc9PZ1atWrly1g3d+5hpk3brbyfOLE18+d3NXpjKa+pY/SDupOSkh6Jm5ogCIIgCI+306dPc+nSJVq1akVycjKzZs0CoE+fPpVcsrK3efNm7Ozs8PLyIiIigjfffJN27dqZ5PyXZc3a2prVq1crraTCw4uKiuK7776r1CC1MohAtRTS09M5ffo0siwr/c8h72nXzJn7+OijA8q2777bgY8+errAQNHT0xPAaNfhh2Fvb4+NjQ1paWnk5uYaZF4TBEEQBEGoDF988QXh4eFYWFjQvHlzDh48WOa/gUxBamoq06ZNIzo6GldXV4KCgpg3b15lF6vCdO7cubKL8Fhp0aLFQ40dflSJQLUU1q1bh0ajwdLSkv79+9+3/LxBkDp79jNMn248yZKep6cny5YtK/MWT0mScHR0JC4ujtjYWIM5sITHk/iOBVMm6qdg6kQdLX+BgYGcOnWqsotRIQYPHszgwYPL9Jjl1QtPEEyVqPFFeLDXr1ar4cCBvGC0devWBumjX3qpES+84AfAwoXdigxSIe+mY2FhUer5rQrj6uqKLMtoNBqSk5PL/PiC6VCr1dSvX1+M/xNMkqifgqkTdVQwdZIkYWVlJYZyCSZLZP2tFIaR6j//7CAjIwO1Ws2gQYMM1pmZqVi7tj87drzChAnG51N6UHJyMrt27VLmYy1LnTt3pmvXrmV+XMH06HQ6YmNjRcZKwSSJ+imYusqqo094PkuhBGRZJjc3V9QZoVIVVv/K4/4pAtUSOn16BwANGzbE1taBqKgkg/UWFmq6d/cq9vGuX7/O6tWrDVJbl5WaNWsq09QIjzdZlomNjRV/wASTJOqnYOoquo7qWx5ycnIq5HzC4yE3N7eyiyA84fTT7hjrCVoe908xRrVIEipVXuZfM7N0MlITUakkXnxxIP36ref06dscPDiM+vWdS3X0uLi4Mi6voezs7HI9viAIgiAIJWNmZoaNjQ13797F3NxcjD0UiiTLMtnZ2UiSJLr/ChVOlmUyMjKIi4vDycmpwoZJiEC1CPp7gSRJWFqlUtWqNnXrujF+/FH++isKgF691nLu3OuYmZX8D83NmzfJyMh4oudREwRBEIQniSRJuLu7c+3aNa5fv17ZxREeAfquv+bm5iJQFSqNk5OTMgVmRRCBapH+uxlYWKQgZ8P27WoOHowCwM7OgsWLe5YqSIW8iXO1Wq3oEic8FEmScHZ2Fn+8BJMk6qdg6iqjjlpYWODl5SW6/wrFoh9H7ebmJlrghUphbm5eaEtqedw/RaBaBBkZHTpkZGTgUngWBw/mZdB1crJi584QWreuVbmFLISlpWVlF0GoACqVitq1a1d2MQTBKFE/BVNXWXVUpVKJHlVCsdWrV6+yiyAIBSqPBygm+Ujm22+/xdPTEysrK1q3bs3x48cL3X7Dhg34+vpiZWWFv78/O3bsKJNy/HP7H7LVWWSrsslR5ZBV5Tr7fTaA12WqVrVh374hDx2kajQapRl97ty5ZVLu+2VmZqLVatFoNOzcubPMjy+YBp1OR3R0tMiqKpgkUT8FUyfqqGDqRB0VTN0TkfV3/fr1TJo0iZkzZ/LPP/8QEBBA165dC0w6dOTIEV5++WVGjBjB6dOn6du3L3379uX8+fMPVY4dV3bw1bGv/m1HzSMhgcs9pP6bmbWuFgEBD9dHW5ZlYmJilO5GsbGxZdoFSKPRkJ6ergy8X79+vbjBPaZkWSYhIUF0IRdMkqifgqkTdVQwdaKOCqauPOqmyQWq8+fP57XXXmPYsGE0bNiQxYsXY2Njw/Lly41uv3DhQrp168bUqVPx8/Pjo48+olmzZnzzzTelLoNWp+XdPe+i0WmAvABV/z8bM0usbGHJlc/R6rSlOv6lS5c4efIkFy9eJD4+XunTLUkS8+fPL3W5HxQbG8u1a9eQpLysxcnJyfz+++9ldnxBEARBEARBEITyYFJjVHNycjh16hTTp09XlqlUKoKCgjh69KjRfY4ePcqkSZMMlnXt2pUtW7YY3T47O9tgypbk5LzxpomJiWi1eYHn8dvHiUiIQM6VUUv/DRpWoUJCQper40r8FXZd2kVr99ZIkoRKpVL2v7/skiTlW753717i4+OpVasWdnZ2Buvi4uK4c+cOFhYWyjK1Wo0sy/laQ9VqNTqdLt8TDLVaTU5ODnfu3CEiIgLIC4J1Oh0///wzTz31lNKPvKCyl/Sa9Md7sIwFLS/NNT24XF/GgpY/SdeUk5NDamoqiYmJqNXqx+KaHsfv6Um9Jq1WS2pqKsnJyfmSLTyq11RY2cU1PXrXpK+jiYmJWFhYPBbX9GAZxTU92teUm5tr8Hf+cbimx/F7epKvSR9TlWXLqkkFqvfu3UOr1VK9enWD5dWrV+fSpUtG94mNjTW6fWxsrNHtP/30Uz788MN8yz09Pf974wu8ADZmNpjL+Se0VaEiLT2N7v27g/FiFcnHx4eePXvm+9EmSRKjR49m69atpTvwvyZOnEi1atWUrsT68yQnJxMQEEBMTMxDHV8QBEEQBEEQBOF+8fHxODo6lsmxTCpQrQjTp083aIHV6XQkJCTg4uJSYFrllJQUPDw8uHHjBg4ODv+tmFbepX1448aNq+wiCBWgwDoqCCZA1E/B1Ik6Kpg6UUcFU5ecnEzt2rVxdnYus2OaVKDq6uqKWq3mzp07Bsvv3LlT4OSybm5uJdre0tIy35QtTk5OxSqfg4ODuDkIJk3UUcGUifopmDpRRwVTJ+qoYOrKcpoak0qmZGFhQfPmzdmzZ4+yTKfTsWfPHtq0aWN0nzZt2hhsD7Br164CtxcEQRAEQRAEQRBMm0m1qAJMmjSJIUOG0KJFC1q1asWCBQtIT09n2LBhAAwePJiaNWvy6aefAvDmm2/SqVMn5s2bR8+ePVm3bh0nT55kyZIllXkZgiAIgiAIgiAIQimZXKAaHBzM3bt3mTFjBrGxsTRt2pSdO3cqCZOio6MNmpTbtm3LmjVreO+99/jf//6Hl5cXW7ZsoXHjxmVWJktLS2bOnJmvy7AgmApRRwVTJuqnYOpEHRVMnaijgqkrjzoqyWLmYEEQBEEQBEEQBMGEmNQYVUEQBEEQBEEQBEEQgaogCIIgCIIgCIJgUkSgKgiCIAiCIAiCIJgUEagKgiAIgiAIgiAIJkUEqv/69ttv8fT0xMrKitatW3P8+PFCt9+wYQO+vr5YWVnh7+/Pjh07KqikwpOoJPXz+++/p0OHDlSpUoUqVaoQFBRUZH0WhIdV0nuo3rp165Akib59+5ZvAYUnXknraFJSEuPGjcPd3R1LS0u8vb3F33qhXJW0ji5YsAAfHx+sra3x8PDgrbfeIisrq4JKKzxJDhw4QO/evalRowaSJLFly5Yi99m3bx/NmjXD0tKSBg0asHLlyhKfVwSqwPr165k0aRIzZ87kn3/+ISAggK5duxIXF2d0+yNHjvDyyy8zYsQITp8+Td++fenbty/nz5+v4JILT4KS1s99+/bx8ssv89dff3H06FE8PDx47rnniImJqeCSC0+KktZRvaioKKZMmUKHDh0qqKTCk6qkdTQnJ4cuXboQFRXFL7/8Qnh4ON9//z01a9as4JILT4qS1tE1a9bwzjvvMHPmTMLCwli2bBnr16/nf//7XwWXXHgSpKenExAQwLffflus7a9du0bPnj15+umnCQ0NZeLEiYwcOZI//vijZCeWBblVq1byuHHjlPdarVauUaOG/OmnnxrdfsCAAXLPnj0NlrVu3VoePXp0uZZTeDKVtH4+SKPRyPb29vKqVavKq4jCE640dVSj0cht27aVly5dKg8ZMkTu06dPBZRUeFKVtI4uWrRIrlevnpyTk1NRRRSecCWto+PGjZOfeeYZg2WTJk2S27VrV67lFARA3rx5c6HbvP3223KjRo0MlgUHB8tdu3Yt0bme+BbVnJwcTp06RVBQkLJMpVIRFBTE0aNHje5z9OhRg+0BunbtWuD2glBapamfD8rIyCA3NxdnZ+fyKqbwBCttHZ01axbVqlVjxIgRFVFM4QlWmjq6bds22rRpw7hx46hevTqNGzdm9uzZaLXaiiq28AQpTR1t27Ytp06dUroHX716lR07dtCjR48KKbMgFKasYiWzsizUo+jevXtotVqqV69usLx69epcunTJ6D6xsbFGt4+NjS23cgpPptLUzwdNmzaNGjVq5LthCEJZKE0dPXToEMuWLSM0NLQCSig86UpTR69evcrevXsJCQlhx44dREREMHbsWHJzc5k5c2ZFFFt4gpSmjr7yyivcu3eP9u3bI8syGo2GMWPGiK6/gkkoKFZKSUkhMzMTa2vrYh3niW9RFYTH2Zw5c1i3bh2bN2/GysqqsosjCKSmpvLqq6/y/fff4+rqWtnFEQSjdDod1apVY8mSJTRv3pzg4GDeffddFi9eXNlFEwQgLx/F7Nmz+e677/jnn3/YtGkTv/32Gx999FFlF00QyswT36Lq6uqKWq3mzp07Bsvv3LmDm5ub0X3c3NxKtL0glFZp6qfeF198wZw5c9i9ezdNmjQpz2IKT7CS1tHIyEiioqLo3bu3skyn0wFgZmZGeHg49evXL99CC0+U0txH3d3dMTc3R61WK8v8/PyIjY0lJycHCwuLci2z8GQpTR19//33efXVVxk5ciQA/v7+pKenM2rUKN59911UKtEWJVSegmIlBweHYremgmhRxcLCgubNm7Nnzx5lmU6nY8+ePbRp08boPm3atDHYHmDXrl0Fbi8IpVWa+gkwd+5cPvroI3bu3EmLFi0qoqjCE6qkddTX15dz584RGhqqvJ5//nklM6CHh0dFFl94ApTmPtquXTsiIiKUhygAly9fxt3dXQSpQpkrTR3NyMjIF4zqH6zk5bsRhMpTZrFSyfI8PZ7WrVsnW1payitXrpQvXrwojxo1SnZycpJjY2NlWZblV199VX7nnXeU7Q8fPiybmZnJX3zxhRwWFibPnDlTNjc3l8+dO1dZlyA8xkpaP+fMmSNbWFjIv/zyi3z79m3llZqaWlmXIDzmSlpHHySy/grlraR1NDo6Wra3t5fHjx8vh4eHy9u3b5erVasmf/zxx5V1CcJjrqR1dObMmbK9vb28du1a+erVq/Kff/4p169fXx4wYEBlXYLwGEtNTZVPnz4tnz59Wgbk+fPny6dPn5avX78uy7Isv/POO/Krr76qbH/16lXZxsZGnjp1qhwWFiZ/++23slqtlnfu3Fmi84pA9V9ff/21XLt2bdnCwkJu1aqV/PfffyvrOnXqJA8ZMsRg+59//ln29vaWLSws5EaNGsm//fZbBZdYeJKUpH7WqVNHBvK9Zs6cWfEFF54YJb2H3k8EqkJFKGkdPXLkiNy6dWvZ0tJSrlevnvzJJ5/IGo2mgkstPElKUkdzc3PlDz74QK5fv75sZWUle3h4yGPHjpUTExMrvuDCY++vv/76//buPCbK440D+HeBFSinAoIVBMQDD8QWoygqUgWqYmO9WBXL4UEFpdajHm0VKkitBaVUbZAIaLEiovGoJxUTKVoPsNHYKipQxQZXcbFRsBzz+8Ps1mWXS7GQX7+fhMSdd+ad5333TeBx5p3R+rel8pkMDAwUnp6eGm0GDhwoOnToILp37y5SUlJa3K9ECM4PICIiIiIiovbjP/+OKhEREREREbUvTFSJiIiIiIioXWGiSkRERERERO0KE1UiIiIiIiJqV5ioEhERERERUbvCRJWIiIiIiIjaFSaqRERERERE1K4wUSUiIiIiIqJ2hYkqERG9NqdPn4ZEIsHp06fbOpTXSiKRIDIysll1HRwcEBQU9Frj+X8RFhYGb2/vtg4DAFBdXQ07Ozts2bKlrUMhIvpPYKJKREQaUlNTIZFItP6sWLGircNrVP3YDQwM0KtXLyxYsABlZWX/Sgx5eXmIjIyEQqH4V/prDgcHB7X7YmRkhMGDB2PHjh0vfc4jR440O0FvqaKiIiQnJ2PVqlWqsuLi4gafS3d3d1W9oKAgtWOmpqZwdXVFXFwcnj17pqoXGRmpVk8qlcLBwQEREREa351UKsXixYsRExODqqqq13LNRET0D722DoCIiNqvL774Ao6Ojmpl/fv3b6NoWkYZe1VVFXJzc7F161YcOXIEV69exRtvvNGqfVVWVkJP759fqXl5eYiKikJQUBDMzc3V6l6/fh06Om3z/8QDBw7EkiVLAAB//vknkpOTERgYiGfPnmHu3LktPt+RI0ewefPm15KsJiQkwNHREV5eXhrHpk+fjnHjxqmVWVlZqX3W19dHcnIyAEChUCArKwtLly7FhQsXsHv3brW6W7duhbGxMZ48eYKffvoJiYmJyM/PR25urlq94OBgrFixArt27UJISEhrXCYRETWAiSoRETVo7NixGDRoUFuH8VJejH3OnDmwsLBAfHw8Dhw4gOnTp7dqXwYGBs2uq6+v36p9t0TXrl0REBCg+hwUFITu3btj48aNL5Wovi7V1dVIT0/Hhx9+qPX422+/rXYd2ujp6anVCQsLw5AhQ5CRkYH4+Hi8+eabqmNTpkyBpaUlACA0NBQymQwZGRk4f/48Bg8erKpnbm4OHx8fpKamMlElInrNOPWXiIharKSkBGFhYejduzcMDQ1hYWGBqVOnori4uMm2hYWFmDx5MmxsbGBgYABbW1vIZDJUVFSo1fv+++/h5uYGQ0NDdOrUCTKZDHfu3HnpmN955x0Az6eUAkBNTQ3Wrl0LJycn6Ovrw8HBAatWrVKbGgoAFy9ehK+vLywtLWFoaAhHR0eNJOXFd1QjIyOxbNkyAICjo6NqWqny3rz4jurFixchkUiQlpamEe/x48chkUhw+PBhVVlpaSlCQkJgbW0NfX199OvXD9u3b3/pe2JlZQVnZ2fcunVLrfzMmTOYOnUqunXrBn19fdjZ2eHjjz9GZWWlqk5QUBA2b96sun7lj1JdXR02bdqEfv36wcDAANbW1ggNDcWjR4+ajCs3NxcPHjzAmDFjXvra6tPR0cGoUaMAoMnndMSIEQCgcV8AwNvbG7m5uSgvL2+12IiISBNHVImIqEEVFRV48OCBWpmlpSUuXLiAvLw8yGQy2Nraori4GFu3bsWoUaNw7dq1BqfW/v333/D19cWzZ8+wcOFC2NjYoLS0FIcPH4ZCoYCZmRkAICYmBp9//jmmTZuGOXPmQC6XIzExESNHjkRBQYHGdNrmUCYdFhYWAJ6PsqalpWHKlClYsmQJfvnlF8TGxuK3337D/v37AQD379+Hj48PrKyssGLFCpibm6O4uBj79u1rsJ9Jkybhxo0b+OGHH7Bx40bVSF39qakAMGjQIHTv3h179uxBYGCg2rGMjAx07NgRvr6+AICysjK4u7tDIpFgwYIFsLKywtGjRzF79mw8fvwYixYtavE9qampwd27d9GxY0e18szMTDx9+hTz58+HhYUFzp8/j8TERNy9exeZmZkAno883rt3DydPnsTOnTs1zh0aGorU1FQEBwcjIiICRUVF+Pbbb1FQUICff/4ZUqm0wbjy8vIgkUjw1ltvaT3+9OlTjefSzMys0XMCms9AQ5SJbP37AgBubm4QQiAvLw9+fn6NnoeIiF6BICIiqiclJUUA0PojhBBPnz7VaHP27FkBQOzYsUNVlpOTIwCInJwcIYQQBQUFAoDIzMxssO/i4mKhq6srYmJi1MqvXLki9PT0NMobij07O1vI5XJx584dsXv3bmFhYSEMDQ3F3bt3xeXLlwUAMWfOHLW2S5cuFQDEqVOnhBBC7N+/XwAQFy5caLRPAGLNmjWqzxs2bBAARFFRkUZde3t7ERgYqPq8cuVKIZVKRXl5uars2bNnwtzcXISEhKjKZs+eLbp06SIePHigdj6ZTCbMzMy0fif1+/Xx8RFyuVzI5XJx5coVMWvWLAFAhIeHq9XVdq7Y2FghkUhESUmJqiw8PFxo+1PizJkzAoBIT09XKz927JjW8voCAgKEhYWFRnlRUVGDz6XyGRNCiMDAQGFkZKS61ps3b4p169YJiUQiBgwYoKq3Zs0aAUBcv35dyOVyUVxcLLZv3y4MDQ2FlZWVePLkiUYM9+7dEwDE+vXrG70GIiJ6NRxRJSKiBm3evBm9evXSKDc0NFT9u7q6Go8fP0aPHj1gbm6O/Px8zJo1S+v5lCOmx48fx7hx47SOvO7btw91dXWYNm2a2qiZjY0NevbsiZycHLWVYBtSf9qovb090tPT0bVrV9VKt4sXL1ars2TJEnz99df48ccf4eXlpRq5PXz4MFxdXZscsXsZ/v7+iI2Nxb59+zB79mwAwIkTJ6BQKODv7w8AEEIgKysL06ZNgxBC7b74+vpi9+7dyM/Ph4eHR6N9nThxQmNkNzg4GBs2bFAre/H7ffLkCSorKzFs2DAIIVBQUIBu3bo12k9mZibMzMzg7e2tFqubmxuMjY2Rk5ODGTNmNNj+4cOHWkczlebNm4epU6eqlbm6uqp9fvLkica1Dhs2TOvob+/evdU+u7i4ICUlRevzqYyr/oguERG1LiaqRETUoMGDB2tdTKmyshKxsbFISUlBaWkphBCqY/XfNX2Ro6MjFi9ejPj4eKSnp2PEiBF47733EBAQoEpiCwsLIYRAz549tZ6jucmiMsnW09ODtbU1evfurVptt6SkBDo6OujRo4daGxsbG5ibm6OkpAQA4OnpicmTJyMqKgobN27EqFGjMHHiRMyYMaPVFkVydXWFs7MzMjIyVIlqRkYGLC0tVe/VyuVyKBQKJCUlISkpSet57t+/32RfQ4YMQXR0NGpra3H16lVER0fj0aNH6NChg1q9P/74A6tXr8bBgwc13ilt7PtVKiwsREVFBTp37vzSsb74TNXXs2fPJt9fNTAwwKFDhwA8X8DK0dERtra2WutmZWXB1NQUcrkc33zzDYqKitSSdW1xvfg+LhERtT4mqkRE1GILFy5ESkoKFi1ahKFDh8LMzAwSiQQymQx1dXWNto2Li0NQUBAOHDiAEydOICIiArGxsTh37hxsbW1RV1cHiUSCo0ePQldXV6O9sbFxs2JsKMl+UVPJhkQiwd69e3Hu3DkcOnQIx48fR0hICOLi4nDu3Llmx9IUf39/xMTE4MGDBzAxMcHBgwcxffp01ZY3ynsaEBCg8S6r0oABA5rsx9LSUpXg+fr6wtnZGX5+fkhISFCNLtfW1sLb2xvl5eVYvnw5nJ2dYWRkhNLSUgQFBTX5/Srj7dy5M9LT07Ue1/a+7ossLCyatehSY3R1dZu9GNPIkSNV7xJPmDABLi4umDlzJi5duqSxlZAyLmV9IiJ6PZioEhFRi+3duxeBgYGIi4tTlVVVVUGhUDSrvYuLC1xcXPDZZ58hLy8PHh4e+O677xAdHQ0nJycIIeDo6Kh12nFrsLe3R11dHQoLC9GnTx9VeVlZGRQKBezt7dXqu7u7w93dHTExMdi1axdmzpyJ3bt3Y86cOVrP39LRNn9/f0RFRSErKwvW1tZ4/PgxZDKZ6riVlRVMTExQW1vbqivhjh8/Hp6enli3bh1CQ0NhZGSEK1eu4MaNG0hLS8MHH3ygqnvy5EmN9g1dp5OTE7Kzs+Hh4dHgyGRjnJ2dkZ6ejoqKCtVI+7/F2NgYa9asQXBwMPbs2aP2PQD/rBr94nNDREStj9vTEBFRi+nq6mpMzUxMTERtbW2j7R4/foyamhq1MhcXF+jo6Ki2hZk0aRJ0dXURFRWl0YcQAg8fPnzl+MeNGwcA2LRpk1p5fHw8gOcJHPB89Kx+DAMHDgQAjW1sXmRkZAQAzU7c+/TpAxcXF2RkZCAjIwNdunTByJEjVcd1dXUxefJkZGVl4erVqxrt5XJ5s/rRZvny5Xj48CG2bdum6gtQn3orhEBCQoJG24auc9q0aaitrcXatWs12tTU1DR5X4YOHQohBC5dutSSS2k1M2fOhK2tLdavX69x7NKlS5BIJBg6dGgbREZE9N/BEVUiImoxPz8/7Ny5E2ZmZujbty/Onj2L7OzsJrf9OHXqFBYsWICpU6eiV69eqKmpwc6dO1WJGPB8NC46OhorV65EcXExJk6cCBMTExQVFWH//v2YN28eli5d+krxu7q6IjAwEElJSVAoFPD09MT58+eRlpaGiRMnwsvLCwCQlpaGLVu24P3334eTkxP++usvbNu2DaampqpkVxs3NzcAwKeffgqZTAapVIoJEyaoEjtt/P39sXr1ahgYGGD27NkaU06//PJL5OTkYMiQIZg7dy769u2L8vJy5OfnIzs7+6X39Rw7diz69++P+Ph4hIeHw9nZGU5OTli6dClKS0thamqKrKwsrVNxldcZEREBX19f6OrqQiaTwdPTE6GhoYiNjcXly5fh4+MDqVSKwsJCZGZmIiEhAVOmTGkwpuHDh8PCwgLZ2dmq93T/TVKpFB999BGWLVuGY8eO4d1331UdO3nyJDw8PJp81omI6BW1wUrDRETUzim3eGloW5ZHjx6J4OBgYWlpKYyNjYWvr6/4/fffNbZeqb89ze3bt0VISIhwcnISBgYGolOnTsLLy0tkZ2dr9JGVlSWGDx8ujIyMhJGRkXB2dhbh4eHi+vXrrxS7UnV1tYiKihKOjo5CKpUKOzs7sXLlSlFVVaWqk5+fL6ZPny66desm9PX1RefOnYWfn5+4ePGi2rlQb3saIYRYu3at6Nq1q9DR0VHbqqb+PVIqLCxUbbWSm5urNeaysjIRHh4u7OzshFQqFTY2NmL06NEiKSmp0WtV9jt+/Hitx1JTUwUAkZKSIoQQ4tq1a2LMmDHC2NhYWFpairlz54pff/1VrY4QQtTU1IiFCxcKKysrIZFINLaqSUpKEm5ubsLQ0FCYmJgIFxcX8cknn4h79+41GW9ERITo0aOHWplye5oNGzY02la5PU1TlNvTyOVyjWMVFRXCzMxMeHp6qsoUCoXo0KGDSE5ObvLcRET0aiRCNLKsHhEREVEbuH37NpydnXH06FGMHj26rcMB8Hyq+FdffYVbt2691Lu3RETUfExUiYiIqF2aP38+bt68qXUhp39bdXU1nJycsGLFCoSFhbV1OERE//eYqBIREREREVG7wlV/iYiIiIiIqF1hokpERERERETtChNVIiIiIiIialeYqBIREREREVG7wkSViIiIiIiI2hUmqkRERERERNSuMFElIiIiIiKidoWJKhEREREREbUrTFSJiIiIiIioXWGiSkRERERERO3K/wCUgj82y+J6VgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1100x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot the best classifiers of all folds\n",
    "results_lists = []\n",
    "results_lists.extend(list_folds_best_models)\n",
    "results_lists.extend(list_folds_weighted_clfs)\n",
    "results_lists.append(constrained_points)\n",
    "results_lists.append(ensemble_results_hard)\n",
    "results_lists.append(misclassification_risk)\n",
    "\n",
    "\n",
    "# --- Plot the final comparison ---\n",
    "print(\"\\n--- Plotting all ROC curves for comparison ---\")\n",
    "plot_roc_comparison(\n",
    "    results_lists=results_lists,\n",
    "    names=[\"MaxROCFold 1\", \"MaxROCFold 2\", \"MaxROCFold 3\", \"MaxROCFold 4\", \"Weighted1\", \"Weighted2\", \"Weighted3\", \"Weighted4\", \"Neyman_Pearson\", \"Ensemble_voting_hard\", \"Misclassification_Risk\"],\n",
    "    results_original_roc=results_original_roc, plot_name=\"logistic_undersampling_BreastCancer\", prior_prob=prior_proba\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLlabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
